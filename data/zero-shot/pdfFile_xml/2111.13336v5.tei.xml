<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhong</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
						</author>
						<title level="a" type="main">MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In object detection, the detection backbone consumes more than half of the overall inference cost. Recent researches attempt to reduce this cost by optimizing the backbone architecture with the help of Neural Architecture Search (NAS). However, existing NAS methods for object detection require hundreds to thousands of GPU hours of searching, making them impractical in fast-paced research and development. In this work, we propose a novel zero-shot NAS method to address this issue. The proposed method, named MAE-DET, automatically designs efficient detection backbones via the Maximum Entropy Principle without training network parameters, reducing the architecture design cost to nearly zero yet delivering the state-of-the-art (SOTA) performance. Under the hood, MAE-DET maximizes the differential entropy of detection backbones, leading to a better feature extractor for object detection under the same computational budgets. After merely one GPU day of fully automatic design, MAE-DET innovates SOTA detection backbones on multiple detection benchmark datasets with little human intervention. Comparing to ResNet-50 backbone, MAE-DET is +2.0% better in mAP when using the same amount of FLOPs/parameters, and is 1.54 times faster on NVIDIA V100 at the same mAP. Code and pre-trained models are available at https://github.com/alibaba/lightweight-neuralarchitecture-search.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Seeking better and faster deep models for object detection is never an outdated task in computer vision. The performance of a deep object detection network heavily depends on the * Equal contribution 1 Alibaba Group. Correspondence to: Xiuyu Sun &lt;xiuyu.sxy@alibaba-inc.com&gt;.</p><p>Accepted by the Proceedings of the 39 th International Conference on Machine Learning (ICML), 2022. feature extraction backbone <ref type="bibr" target="#b28">(Li et al., 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2019b)</ref>. Currently, most state-of-the-art (SOTA) detection backbones <ref type="bibr" target="#b16">(He et al., 2016;</ref><ref type="bibr">Xie et al., 2017;</ref><ref type="bibr" target="#b51">Zhu et al., 2019;</ref><ref type="bibr" target="#b25">Li et al., 2021a)</ref> are designed manually by human experts, which can take years to develop. Since the detection backbone consumes more than half of the total inference cost in many detection frameworks, it is critical to optimize the backbone architecture for better speed-accuracy trade-off on different hardware platforms, ranging from server-side GPUs to mobile-side chipsets. To reduce time cost and human labor, Neural Architecture Search (NAS) has emerged to facilitate the architecture design. Various NAS methods have demonstrated their efficacy in designing SOTA image classification models <ref type="bibr" target="#b52">(Zoph et al., 2018;</ref><ref type="bibr" target="#b35">Liu et al., 2018;</ref><ref type="bibr" target="#b2">Cai et al., 2019;</ref><ref type="bibr">Tan &amp; Le, 2019;</ref><ref type="bibr" target="#b13">Fang et al., 2020)</ref>. These successful stories inspire recent researchers to use NAS to design detection backbones <ref type="bibr" target="#b8">(Chen et al., 2019b;</ref><ref type="bibr" target="#b41">Peng et al., 2019;</ref><ref type="bibr">Xiong et al., 2021;</ref><ref type="bibr" target="#b12">Du et al., 2020;</ref><ref type="bibr" target="#b22">Jiang et al., 2020)</ref> in an end-to-end way.</p><p>To date, existing NAS methods for detection task are all training-based, meaning they need to train network parameters to evaluate the performance of network candidates on the target dataset, a process that consumes enormous hardware resources. This makes the training-based NAS methods inefficient in modern fast-paced research and development. To reduce the searching cost, training-free methods are recently proposed, also known as zero-shot NAS in some literatures <ref type="bibr">(Tanaka et al., 2020;</ref><ref type="bibr" target="#b37">Mellor et al., 2021;</ref><ref type="bibr" target="#b7">Chen et al., 2021c;</ref><ref type="bibr" target="#b31">Lin et al., 2021)</ref>. The zero-shot NAS predicts network performance without training network parameters, and is therefore much faster than training-based NAS. As a relatively new technique, existing zero-shot NAS methods are mostly validated on classification tasks. Applying zero-shot NAS to detection task is still an intact challenge.</p><p>In this work, we present the first effort to introduce zero-shot NAS technique to design efficient object detection backbones. We show that directly transferring existing zero-shot NAS methods from image classification to detection backbone design will encounter fundamental difficulties. While image classification network only needs to predict the class probability, object detection network needs to additionally predict the bounding boxes of multiple objects, making arXiv:2111.13336v5 [cs.CV] 15 Jun 2022 the direct architecture transfer sub-optimal. To this end, a novel zero-shot NAS method, termed MAximum-Entropy DETection (MAE-DET), is proposed for searching object detection backbones. The key idea behind MAE-DET is inspired by the Maximum Entropy Principle <ref type="bibr" target="#b21">(Jaynes, 1957;</ref><ref type="bibr" target="#b44">Reza, 1994;</ref><ref type="bibr" target="#b24">Kullback, 1997;</ref><ref type="bibr" target="#b1">Brillouin, 2013)</ref>. Informally speaking, when a detection network is formulated as an information processing system, its capacity is maximized when its entropy achieves maximum under the given inference budgets, leading to a better feature extractor for object detection. Based on this observation, MAE-DET maximizes the differential entropy <ref type="bibr" target="#b48">(Shannon, 1948)</ref> of detection backbones by searching for the optimal configuration of network depth and width without training network parameters.</p><p>The Principle of Maximum Entropy is one of the fundamental first principles in Physics and Information Theory. As well as the widespread applications of deep learning, many theoretical studies attempt to understand the success of deep learning from the Maximum Entropy Principle <ref type="bibr" target="#b46">(Saxe et al., 2018;</ref><ref type="bibr" target="#b3">Chan et al., 2021;</ref><ref type="bibr" target="#b49">Yu et al., 2020)</ref>. Inspired by these pioneer works, MAE-DET establishes a connection from Maximum Entropy Principle to zero-shot object detection NAS. This leads to a conceptually simple design, yet endowed with strong empirical performance. Only using the standard single-branch convolutional blocks, the MAE-DET can outperform previous detection backbones built by much more involved engineering. This encouraging result again verifies an old-school doctrine: simple is better.</p><p>While the Maximum Entropy Principle has been applied in various scientific problems, its application in zero-shot NAS is new. Particularly, a direct application of the principle to object detection will raise several technical challenges. The first challenge is how to estimate the entropy of a deep network. The exact computation of entropy requires knowing the precise probability distribution of deep features in high dimensional space, which is difficult to estimate in practice. To address this issue, MAE-DET estimates the Gaussian upper bound of the differential entropy, which only requires to estimate the variance of the feature maps. The second challenge is how to efficiently extract deep features for objects of different scales. In real-world object detection datasets, such as MS COCO <ref type="bibr" target="#b32">(Lin et al., 2014)</ref>, the distribution of object size is data-dependent and nonuniform. To bring this prior knowledge in backbone design, we introduce the Multi-Scale Entropy Prior (MSEP) in the entropy estimation. We find that the MSEP significantly improves detection performance. The overall computation of MAE-DET takes one forward inference of the detection backbone, therefore its cost is nearly zero compared to previous training-based NAS methods.</p><p>The contributions of this work are summarized as follows:</p><p>? We revisit the Maximum Entropy Principle in zero-shot object detection NAS. The proposed MAE-DET is conceptually simple, yet delivers superior performance without bells and whistles.</p><p>? Using less than one GPU day and 2GB memory, MAE-DET achieves competitive performance over previous NAS methods on COCO with at least 50x times faster. ? MAE-DET is the first zero-shot NAS method for object detection with SOTA performance on multiple benchmark datasets under multiple detection frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Backbone Design for Object Detection Recently, the design of object detection models composing of backbone, neck and head has become increasingly popular due to their effectiveness and high performance <ref type="bibr" target="#b33">(Lin et al., 2017a;</ref><ref type="bibr">Tian et al., 2019;</ref><ref type="bibr" target="#b26">Li et al., 2020;</ref><ref type="bibr" target="#b12">Tan et al., 2020;</ref><ref type="bibr" target="#b23">Jiang et al., 2022)</ref>. Prevailing detectors directly use the backbones designed for image classification to extract multiscale features from input images, such as ResNet <ref type="bibr" target="#b16">(He et al., 2016)</ref>, <ref type="bibr">ResNeXt (Xie et al., 2017)</ref> and Deformable Convolutional Network (DCN) <ref type="bibr" target="#b51">(Zhu et al., 2019)</ref>. Nevertheless, the backbone migrated from image classification may be suboptimal in object detection <ref type="bibr" target="#b14">(Ghiasi et al., 2019)</ref>. To tackle the gap, several architectures optimized for object detection are proposed, including Stacked Hourglass <ref type="bibr" target="#b38">(Newell et al., 2016</ref><ref type="bibr">), FishNet (Sun et al., 2018</ref>, DetNet <ref type="bibr" target="#b28">(Li et al., 2018</ref><ref type="bibr">), HRNet (Wang et al., 2020a</ref> and so on. Albeit with good performance, these hand-crafted architectures heavily rely on expert knowledge and a tedious trial-and-error design.</p><p>Neural Architecture Search Neural Architecture Search (NAS) is initially developed to automatically design network architectures for image classification <ref type="bibr" target="#b52">(Zoph et al., 2018;</ref><ref type="bibr" target="#b35">Liu et al., 2018;</ref><ref type="bibr" target="#b43">Real et al., 2019;</ref><ref type="bibr" target="#b2">Cai et al., 2019;</ref><ref type="bibr" target="#b9">Chu et al., 2021;</ref><ref type="bibr" target="#b12">Lin et al., 2020;</ref><ref type="bibr">Tan &amp; Le, 2019;</ref><ref type="bibr" target="#b5">Chen et al., 2021a;</ref><ref type="bibr" target="#b31">Lin et al., 2021)</ref>. Using NAS to design object detection models has not been well explored. Currently, existing detection NAS methods are all training-based methods. Some methods focus on searching detection backbones, such as DetNAS <ref type="bibr" target="#b8">(Chen et al., 2019b)</ref>, SpineNet <ref type="bibr" target="#b12">(Du et al., 2020)</ref> and SP-NAS <ref type="bibr" target="#b22">(Jiang et al., 2020)</ref>, while others focus on searching FPN neck, such as NAS-FPN <ref type="bibr" target="#b14">(Ghiasi et al., 2019)</ref>, <ref type="bibr">NAS-FCOS (Wang et al., 2020b)</ref> and OPANet <ref type="bibr" target="#b29">(Liang et al., 2021)</ref>. These methods require training and evaluation of the target datasets, which is intensive in computation. MAE-DET distinguishes itself as the first zero-shot NAS method for the backbone design of object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary</head><p>In this section, we first formulate a deep network as a system endowed with continuous state space. Then we define the differential entropy of this system and show how to estimate this entropy via its Gaussian upper bound. Finally, we introduce the basic concept of vanilla network search space for designing our detection backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous State Space of Deep Networks</head><formula xml:id="formula_0">A deep net- work F (?) : R d ? ? R maps an input image x ? R d to its label y ? R.</formula><p>The topology of a network can be abstracted as a graph G = (V, E) where the vertex set V consists of neurons and the edge set E consists of spikes between neurons. For any v ? V and e ? E, h(v) ? R and h(e) ? R present the values endowed with each vertex v and each edge e respectively. The set S = {h(v), h(e) : ?v ? V, e ? E} defines the continuous state space of the network F .</p><p>According to the Principle of Maximum Entropy, we want to maximize the differential entropy of network F , under some given computational budgets. The entropy H(S) of set S measures the total information contained in the system (network) F , including the information contained in the latent features H(S v ) = {h(v) : v ? V} and in the network parameters H(S e ) = {h(e) : e ? E}. As for object detection backbone design, we only care about the entropy of latent features H(S v ) rather than the entropy of network parameters H(S e ). Informally speaking, H(S v ) measures the feature representation power of F while H(S e ) measures the model complexity of F . Therefore, in the remainder of this work, the differential entropy of F refers to the entropy H(S v ) by default.</p><p>Entropy of Gaussian Distribution The differential entropy of Gaussian distribution can be found in many textbooks, such as <ref type="bibr" target="#b39">(Norwich, 1993)</ref>. Suppose x is sampled from Gaussian distribution N (?, ? 2 ). Then the differential entropy of x is given by</p><formula xml:id="formula_1">H * (x) = 1 2 log(2?) + 1 2 + H(x) H(x) := log(?) . (1)</formula><p>From Eq. 1, the entropy of Gaussian distribution only depends on the variance. In the following, we use H(x) instead of H * (x) as constants do not matter in our discussion.</p><p>Gaussian Entropy Upper Bound Since the probability distribution P(S v ) is a high dimensional function, it is difficult to compute the precise value of its entropy directly. Instead, we propose to estimate the upper bound of the entropy, given by the following well-known theorem <ref type="bibr" target="#b10">(Cover &amp; Thomas, 2012)</ref>: Theorem 1. For any continuous distribution P(x) of mean ? and variance ? 2 , its differential entropy is maximized when P(x) is a Gaussian distribution N (?, ? 2 ).</p><p>Theorem 1 says the differential entropy of a distribution is upper bounded by a Gaussian distribution with the same mean and variance. Combining this with Eq. (1), we can easily estimate the network entropy H(S v ) by simply computing the feature map variance and then using Eq.</p><p>(1) to get the Gaussian entropy upper bound for the network.</p><p>Vanilla Network Search Space Following previous works, we design our backbones in the vanilla convolutional network space <ref type="bibr" target="#b28">(Li et al., 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2019b;</ref><ref type="bibr" target="#b12">Du et al., 2020;</ref><ref type="bibr" target="#b31">Lin et al., 2021)</ref>. This space is one of the most simple spaces proposed in the early age of deep learning, and is now widely adopted in detection backbones. It is also a popular prototype in many theoretical studies <ref type="bibr" target="#b42">(Poole et al., 2016;</ref><ref type="bibr" target="#b47">Serra et al., 2018;</ref><ref type="bibr" target="#b15">Hanin &amp; Rolnick, 2019)</ref>.</p><p>A vanilla network is stacked by multiple convolutional layers, followed by RELU activations. Consider a vanilla convolutional network with D layers of weights W 1 , ..., W D whose output feature maps are x 1 , ..., x D . The input image is x 0 . Let ?(?) denote the RELU activation function. Then the forward inference is given by</p><formula xml:id="formula_2">x l = ?(h l ), h l = W l * x l?1 l = 1, ..., D .<label>(2)</label></formula><p>For simplicity, we set the bias of the convolutional layer to zero.</p><p>Simple is Better The vanilla convolutional network is very simple to implement. Most deep learning frameworks <ref type="bibr" target="#b40">(Paszke et al., 2019;</ref><ref type="bibr" target="#b0">Abadi et al., 2015)</ref> provide well optimized convolutional operators on GPU. The training of convolutional networks is well studied, such as adding residual link <ref type="bibr" target="#b16">(He et al., 2016)</ref> and Batch Normalization (BN) <ref type="bibr" target="#b20">(Ioffe &amp; Szegedy, 2015)</ref> will greatly improve convergence speed and stability. While we stick to the simple vanilla design on purpose, the building blocks used in MAE-DET can be combined with other auxiliary components to "modernize" the backbone to boost performance, such as Squeeze-and-Excitation (SE) block <ref type="bibr" target="#b19">(Hu et al., 2018)</ref> or self-attention block <ref type="bibr" target="#b50">(Zhao et al., 2020)</ref>. Thanks to the simplicity of MAE-DET, these auxiliary components can be easily plugged into the backbone without special modification. Once again, we deliberately avoid using these auxiliary components to keep our design simple and universal. By default, we only use residual link and BN layer to accelerate the convergence. In this way, it is clear that the improvements of MAE-DET indeed come from a better backbone design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Maximum Entropy Zero-Shot NAS for Object Detection</head><p>In this section, we first describe how to compute the differential entropy for deep networks. Then we introduce the Multi-Scale Entropy Prior (MSEP) to better capture the prior distribution of object size in real-world images. Finally, we present the complete MAE-DET backbone designed by a customized Evolutionary Algorithm (EA). <ref type="figure">Figure 1</ref>. Single-scale entropy with rescaling for deep networks.</p><formula xml:id="formula_3">Conv !! "~( 0,1) W !~( 0,1) ? # + # = # # + #$% ! = ! Conv ! "~( 0,1) W !~( 0,1) Conv ? # #$% #&amp;% W !"#~( 0,1) Scale the feature map = 1 2 log Var !! + 8 #'% ! log( # )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Differential Entropy for Deep Networks</head><p>In this subsection, we present the computation of differential entropy for the final feature map generated by a deep network. First, all parameters are initialized by the standard Gaussian distribution N (0, 1). Then we randomly generate an image filled with the standard Gaussian noise and perform forward inference. Based on the discussion in Section 3, the (Gaussian upper bound) entropy H(F ) of the network F is given by</p><formula xml:id="formula_4">H(F ) = 1 2 log(Var(h D )) .<label>(3)</label></formula><p>Please note the variance is computed on the last preactivation feature map h D .</p><p>For deep vanilla networks, directly using Eq.</p><p>(3) might cause numerical overflow. This is because every layer amplifies the output norm by a large factor. The same issue is also reported in Zen-NAS <ref type="bibr" target="#b31">(Lin et al., 2021)</ref>. Inspired by the BN rescaling technique proposed in Zen-NAS, we propose an alternative solution without BN layers. We directly re-scale each feature map x l by some constants ? l during inference, that is x l = ?(h l )/? l , and then compensate the entropy of the network by</p><formula xml:id="formula_5">H(F ) = 1 2 log(Var(? D )) + D l=1 log(? l ) .<label>(4)</label></formula><p>The values of ? l can be arbitrarily given, as long as the forward inference does not overflow or underflow. In practice, we find that simply setting ? l to the Euclidean norm of the feature map works well. The process is illustrated in <ref type="figure">Figure  1</ref>. Finally, H(F ) is multiplied by the size of the feature map as the entropy estimation for this feature map.</p><p>Compare with Zen-NAS The principles behind MAE-DET and Zen-NAS are fundamentally different. Zen-NAS uses the gradient norm of the input image as ranking score, and proposes to use two feed-forward inferences to approximate the gradient norm for classification. In contrast, MAE-DET uses an entropy-based score, which only requires one  <ref type="figure">Figure 2</ref>. Multi-scale entropy for detection backbone with multiscale features.</p><formula xml:id="formula_6">Z(F) = ?3H(C3) + ?4H(C4) + ?5H(C5) H(C4) H(C5)</formula><p>feed-forward inference. Please see the Experiment section for more empirical comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-Scale Entropy Prior (MSEP) for Object Detection</head><p>In real-world images, the distribution of object size is not uniform. To bring in this prior knowledge, the detection backbone has 5 stages, where each stage downsamples the feature resolution to half. The MSEP collects the feature map from the last layer of each stage and weighted-sum the corresponding feature map entropies as a new measurement. We name this new measurement multi-scale entropy. The process is illustrated in <ref type="figure">Figure 2</ref>. In this figure, the backbone extracts multi-scale features C = (C1, C2, ..., C5) at different resolutions. Then the FPN neck fuses C as input features P = (P1, P2, ..., P7) for the detection head. The multi-scale entropy Z(F ) of backbone F is then defined by</p><formula xml:id="formula_7">Z(F ) := ? 1 H(C1) + ? 2 H(C2) + ? ? ? + ? 5 H(C5) (5)</formula><p>where H(Ci) is the entropy of Ci for i = 1, 2, ? ? ? , 5. The weights ? = (? 1 , ? 2 , ? ? ? , ? 5 ) store the multi-scale entropy prior to balance the expressivity of different scale features.</p><p>How to choose ? As a concrete example in <ref type="figure">Fig. 2</ref>, the parts of P3 and P4 are generated by up-sampling of P5, and P6 and P7 are directly generated by down-sampling of P5 (generated by C5). Meanwhile, based on the fact that C5 carries sufficient context for detecting objects on various scales <ref type="bibr" target="#b6">(Chen et al., 2021b)</ref>, C5 is important in the backbone search, so it is good to set a larger value for the weight ? 5 . Then, different combinations of ? and correlation analysis are explored in Appendix D, indicating that ? = (0, 0, 1, 1, 6) is good enough for the FPN structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evolutionary Algorithm for MAE-DET</head><p>Combining all above, we present our NAS algorithm for MAE-DET in Algorithm 1. The MAE-DET maximizes the multi-scale differential entropy of detection backbones using</p><formula xml:id="formula_8">Algorithm 1 MAE-DET with Coarse-to-Fine Evolution Require: Search space S, inference budget B, maximal depth L, total number of iterations T , evolutionary population size N , initial structure F 0 , fine-search flag Flag. Ensure: NAS-designed MAE-DET backbone F * . 1: Initialize population P = {F 0 }, Flag=False. 2: for t = 1, 2, ? ? ? , T do 3:</formula><p>if t equals to T /2 then 4:</p><p>Keep top 10 networks of highest multi-scale entropy in P and remove the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Set F lag = T rue. Randomly select F t ? P.</p><p>8:</p><formula xml:id="formula_9">MutateF t = MUTATE(F t , S, F lag) 9:</formula><p>ifF t exceeds inference budget or has more than L layers then 10:</p><p>Do nothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>else 12:</p><p>Get multi-scale entropy Z(F t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>AppendF t to P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>end if 15:</p><p>Remove networks of the smallest multi-scale entropy if the size of P exceeds B. 16: end for 17: Return F * , the network of the highest multi-scale entropy in P.</p><p>a customized Evolutionary Algorithm (EA). To improve evolution efficiency, a coarse-to-fine strategy is proposed to reduce the search space gradually. First, we randomly generate N seed architectures to fill the population P. As shown in <ref type="figure">Figure 3</ref>, a seed architecture F t consists of a sequence of building blocks, such as ResNet block <ref type="bibr" target="#b16">(He et al., 2016)</ref> or MobileNet block <ref type="bibr" target="#b45">(Sandler et al., 2018)</ref>. Then we randomly select one block and replace it with its mutated version. We use coarse-mutation in the early stages of EA, and switch to fine-mutation after T /2 EA iterations. In the coarse-mutation, the block type, kernel size, depth and width are randomly mutated. In the fine-mutation, only kernel size and width are mutated.</p><p>After the mutation, if the inference cost of the new structur? F t does not exceed the budget (e.g., FLOPs, parameters and latency) and its depth is smaller than budget L,F t is appended into the population P. The maximal depth L prevents the algorithm from generating over-deep structures, which will have high entropy with unreasonable structure, and the performance will be worse. During EA iterations, the population is maintained to a certain size by discarding the worst candidate of the smallest multi-scale entropy. At the end of evolution, the backbone with the highest multiscale entropy is returned. Uniformly alternate the kernel size, width within some range. 4: else 5:</p><p>Uniformly alternate the block type, kernel size, width and depth within some range. 6: end if 7: Return the mutated structureF t .</p><formula xml:id="formula_10">! "! "" "# "$ "% C2 C3 C4 C5 C1 Uniformly select a block h Structure F t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse mutation</head><p>Fine mutation type kernel width depth kernel width</p><p>Get the mutated block ! ?</p><formula xml:id="formula_11">! "! "" "# "$ "% C2 C3 C4 C5 C1</formula><p>Mutated structure ! ! <ref type="figure">Figure 3</ref>. Visualization of Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first describe detail settings for search and training. Then in Subsection 5.2, we apply MAE-DET to design better ResNet-like backbones on COCO dataset <ref type="bibr" target="#b32">(Lin et al., 2014)</ref>. We align the inference budget with ResNet-50/101. The performance of MAE-DET and ResNet are compared under multiple detection frameworks, including RetinaNet <ref type="bibr" target="#b34">(Lin et al., 2017b)</ref>, <ref type="bibr">FCOS (Tian et al., 2019)</ref>, and GFLV2 <ref type="bibr" target="#b27">(Li et al., 2021b)</ref>. For fairness, we use the same training setting in all experiments for all backbones. In Subsection 5.3, we compare the search cost of MAE-DET to SOTA NAS methods for object detection. Subsection 5.4 reports the ablation studies of different components in MAE-DET. Finally, Subsection 5.5 verifies the transferability of MAE-DET on several detection datasets and segmentation tasks. Due to space limitations, more experiments are postponed to Appendix. Appendix B reports the performance of MAE-DET on mobile devices. Appendix E compare MAE-DET against previous zero-shot NAS methods designed for image classification tasks, showing that these zero-shot NAS methods perform sub-optimally in object detection.  <ref type="bibr" target="#b11">(Deng et al., 2009</ref>) with a batch size of 256 for 120 epochs. Other setting details can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Design Better ResNet-like Backbones</head><p>We search efficient MAE-DET backbones for object detection and align with ResNet-50/101 in <ref type="table" target="#tab_2">Table 1</ref>. MAE-DET-S uses 60% less FLOPs than ResNet-50; MAE-DET-M is aligned with ResNet-50 with similar FLOPs and number of parameters as ResNet-50; MAE-DET-L is aligned with ResNet-101. The feature dimension in the FPN and heads is set to 256 for MAE-DET-M and MAE-DET-L but is set to 192 for MAE-DET-S. The fine-tuned results of models pre-trained on ImageNet-1k are reported in Appendix C.</p><p>In <ref type="table" target="#tab_2">Table 1</ref>, MAE-DET outperforms ResNet by a large margin. The improvements are consistent across three detection  frameworks. Particularly, when using the newest framework GFLV2, MAE-DET improves COCO mAP by +2% at the similar FLOPs of ResNet-50, and speeds up the inference by 1.54x times faster at the same accuracy as ResNet-50. <ref type="figure">Figure 4</ref> visualizes the comparison in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>Remark Please note that we did not copy the numbers of baseline methods reported in previous works in <ref type="table" target="#tab_2">Table 1</ref> because the mAP depends not only on the architecture, but also on the training schedule, such as training epochs, learning rate, pre-training, etc. Therefore, for a fair comparison, all models in <ref type="table" target="#tab_2">Table 1</ref> are trained by the same training schedule. For comparison with numbers reported in previous works, see Subsection 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with SOTA NAS Methods</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, we compare MAE-DET with SOTA NAS methods for the backbone design in object detection. We directly use the numbers reported in the original papers. Since each NAS method uses different design spaces and training settings, it is impossible to make an absolutely fair comparison for all methods that everyone agrees with. Nevertheless, we list the total search cost, mAP and FLOPs of the best models reported in each work. This gives us an overall impression of how each NAS method works in real-world practice. From  <ref type="table" target="#tab_4">Table 3</ref> shows that MAE-DET requires fewer parameters and has a faster inference speed on V100 when achieving competitive performance over DetNAS and SpineNet on COCO.    <ref type="table" target="#tab_6">Table 4</ref>, the coarse-to-fine mutation further enhances the performance of multi-scale entropy prior, and the overall improvement over ResNet-50 is +1.1% on ImageNet-1k, +2.5% on COCO with YOLOF and +2.0% on COCO with FCOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation between entropy and mAP</head><p>To further study the correlations between model entropy and model mAP, models during the search are trained, and the results are exhibited in <ref type="figure" target="#fig_1">Figure 5</ref>. The right part of <ref type="figure" target="#fig_1">Figure 5</ref> indicates that the mAP positively correlates with the multi-scale entropy. The left part of <ref type="figure" target="#fig_1">Figure 5</ref> reveals that the singlescale entropy cannot represent the mAP well, so multi-scale entropy is necessary for detection tasks. By analyzing the structures in Appendix G, the computation of single-scale models is concentrated in the last stage C5, ignoring the C3 and C4 stages, and leading to the worse multi-scale score. Instead, multi-scale models allocate more computation to the previous stages to enhance the expressivity of C3 and C4, which improve the multi-scale score.</p><p>Discussion about dataset We agree that the dataset is powerful for ranking the architectures in the training-based methods. However, the process of MAE-DET search is performed without data training. If we replace the Gaussian input directly with target data, the output after one convolution is also random due to the Gaussian initialized weights. On the other hand, the FPN framework has considered data distribution characteristics according to previous works <ref type="bibr" target="#b33">(Lin et al., 2017a;</ref><ref type="bibr">Tian et al., 2019;</ref><ref type="bibr" target="#b27">Li et al., 2021b)</ref>. Thus, the aim of MAE-Det is to provide a better multi-scale feature extractor under the given inference budgets. We believe the zero-shot method combined with target data without training can be a future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Transfer to Other Tasks</head><p>VOC and Cityscapes To evaluate the transferability of MAE-DET in different datasets, we transfer the FCOSbased MAE-DET-M to VOC and Cityscapes dataset, as shown in the upper half of <ref type="table" target="#tab_8">Table 5</ref>. The models are finetuned after being pre-trained on ImageNet. Comparing to ResNet-50, MAE-DET-M achieves +4.1% better mAP in VOC, +1.1% better mAP in Cityscape.</p><p>Instance Segmentation The lower half of <ref type="table" target="#tab_8">Table 5</ref> reports results of Mask R-CNN <ref type="bibr" target="#b17">(He et al., 2017)</ref> and SCNet (Vu et al., 2021) models for the COCO instance segmentation task with 6X training from scratch. Comparing to ResNet-50, MAE-DET-M achieves better AP and mask AP with similar model size and FLOPs on Mask RCNN and SCNet.  <ref type="bibr" target="#b12">(Du et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we revisit the Maximum Entropy Principle in zero-shot NAS for object detection. The proposed MAE-DET achieves competitive detection accuracy with search speed orders of magnitude faster than previous trainingbased NAS methods. While modern object detection backbones involve more complex building blocks and network topologies, the design of MAE-DET is conceptually simple and easy to implement, demonstrating the grace of "simple is better" philosophy. Extensive experiments and analyses on various datasets validate its excellent transferability.</p><p>Sun, S., Pang, J., Shi, J., Yi, S., and Ouyang, W. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Details</head><p>Searching Details In MAE-DET, the evolutionary population N is set as 256 while total iterations T = 96000. Residual blocks and bottleneck blocks are utilized as searching space when comparing with ResNet series backbone <ref type="bibr" target="#b16">(He et al., 2016)</ref>. Following the previous designs <ref type="bibr" target="#b8">(Chen et al., 2019b;</ref><ref type="bibr" target="#b22">Jiang et al., 2020;</ref><ref type="bibr" target="#b12">Du et al., 2020)</ref>, MAE-DET is optimized for the budget of FLOPs according to the target networks, i.e., ResNet-50 and ResNet-101. To balance the computational complexity and large resolution demand, the resolution in search is set as 384 ? 384 for MAE-DET. When starting the search, the initial structure is composed of 5 downsampling stages with small and narrow blocks to meet the reasoning budget. In the mutation, whether the coarse-mutation or fine-mutation, the width of the selected block is mutated in a given scale {1/1.5, 1/1.25, 1, 1.25, 1.5, 2}, while the depth increases or decreases 1 or 2. The kernel size is searched in set {3, 5}.</p><p>Note that blocks deeper than 10 will be divided into two blocks equally to enhance diversity.  <ref type="table" target="#tab_2">Table 1</ref>. When training on the COCO dataset, the initial learning rate is set to 0.02, and decays two times with the ratio of 0.1 during training. SGD is adopted as optimizer with momentum 0.9; weight decay of 10 ?4 ; batch size of 16 (on 8 Nvidia V100 GPUs); patch size of 1333 ? 800.</p><p>Additionally, multi-scale training and Synchronized Batch Normalization (SyncBN) are adopted to enhance the stability of the scratch training without increasing the complexity of inference. Training from scratch is used to avoid the gap between ImageNet pre-trained models, to ensure a fair comparison with baselines. 3X learning schedule is applied for the ablation study with a multi-scale range between [0.8, 1.0] (36 epochs, decays at 28 and 33 epochs), and 6X learning schedule for the SOTA comparisons with the range between [0.6, 1.2] (73 epochs, decays at 65 and 71 epochs). All object detection training is produced under mmdetection <ref type="bibr" target="#b4">(Chen et al., 2019a)</ref> for fair comparisons, and hyper-parameters not mentioned in the paper are always set to default values in mmdection.</p><p>For image classification, all models are trained on ImageNet-1K with a batch size of 256 for 120 epochs. When training on ImageNet-1K, we use SGD optimizer with momentum 0.9; cosine learning rate decay <ref type="bibr" target="#b36">(Loshchilov &amp; Hutter, 2017)</ref>; initial learning rate 0.1; weight decay 4 ? 10 ?5 . For mobile-size object detection, we explore building MAE-DET-MB with MobileNetV2 <ref type="bibr" target="#b45">(Sandler et al., 2018)</ref> blocks, using the inverted bottleneck block with expansion ratio of 1/3/6. The weight ratio ? is still set as 1:1:6, and other searching settings are the same as the Resnet-like searching. In <ref type="table">Table.</ref> 6, MAE-DET-MB use less computation and parameters, but outperform MobileNetV2 by 1% AP with similar inference time on Google Pixel 2 phone. Additionally, inserting SE modules to MAE-DET-MB-M can improve the mAP by 0.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MAE-DET for Mobile Device</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Object Detection with ImageNet Pre-train Models</head><p>In the main body of the paper, training from scratch is used to avoid the gap between ImageNet pre-trained models, to ensure a fair comparison with baselines <ref type="bibr" target="#b18">(He et al., 2019)</ref>. Since 6X training from scratch consumes 3 times more time than 2X pre-trained training, we use the ImageNet pre-trained model to initialize the MAE-DET-M in various heads, including RetinaNet, FCOS and GFLV2. As present in <ref type="table">Table.</ref> 1, 7, whether using training from scratch or ImageNet pre-training, MAE-DET can outperform ResNet-50 in the three popular FPN-based frameworks by large margins.   <ref type="figure">Figure 6</ref>. mAP vs. scores. All models are from <ref type="table" target="#tab_13">Table 8</ref> and the scores are computed with different weight ratios. When the ratio is equal to 1:1:6, the correlation between mAP and score is well fitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-scale Entropy Prior Ablation Study</head><p>In <ref type="table" target="#tab_13">Table 8</ref>, we tune the different arrangements of multi-scale weights in a wide range. Seven multi-scale weight ratios are used to search different models, and all models are trained on the COCO dataset with FCOS and 3X learning schedule. <ref type="table" target="#tab_13">Table 8</ref> shows that if the same weights are arranged to C3-C5, the performance of MAE-DET on COCO is worse than ResNet-50. Considering the importance of C5 (discussed in Section 4.2), we increase the weight of C5, and MAE-DET's performance continues to improve. To further explore the correlations between mAP and scores, we use the seven weight ratios to calculate the different scores of each model, along with the single-scale weight ratio of 0:0:1. The correlations between mAP and different scores are plotted in <ref type="figure">Figure 6</ref>. Taking the results in <ref type="table" target="#tab_13">Table 8</ref> and <ref type="figure">Figure 6</ref>, we confirm the ratio of 1:1:6 may be good enough for the current FPN structure. We compare MAE-DET with architectures designed by zero-shot proxies for image classification in previous works, including <ref type="bibr">SyncFlow (Tanaka et al., 2020)</ref>, NASWOT <ref type="bibr" target="#b37">(Mellor et al., 2021)</ref>, ZenNAS <ref type="bibr" target="#b31">(Lin et al., 2021)</ref>. For a fair comparison, all methods use the same search space, FLOPs budget 91 G, searching strategy and training schedule. All searched backbones are trained on COCO with the FCOS head and 3X training from scratch. The results are reported in <ref type="table">Table.</ref> 9.</p><p>Among these methods, SyncFlow and NASWOT perform worse than ResNet-50 on COCO albeit they show competitive performance in image classification tasks. Zen-NAS achieves competitive performance over ResNet-50. The MAE-DET outperforms Zen-NAS by +1.3% mAP with slightly fewer FLOPs and nearly one third of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Random search comparisons</head><p>Using the same search space as MAE-DET-M, we randomly sample models within 70-95G FLOPs and 20-30M parameters without MAE score. Due to the high cost of sampling, 17 random models are currently sampled and trained on GFLV2 with a 3X learning schedule. The correlations are shown in <ref type="figure">Fig. 7</ref>. Our MAE-DET model has better performance than the random searched models. The visualizations of the searching process on different entropies are shown in <ref type="figure">Figure 8</ref>, 9, 10. We compare the change of layers, FLOPs and parameters during different iterations. Visualizations prove our assumptions in the main body of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Detail Structure of MAE-DETs</head><p>We list detail structure in <ref type="table" target="#tab_2">Table 1</ref>, 4.</p><p>The 'block' column is the block type. 'Conv' is the standard convolution layer followed by BN and RELU. 'ResBlock' is the residual bottleneck block used in ResNet-50 and is stacked by two Blocks in our design. 'kernel' is the kernel size of kxk convolution layer in each block. 'in', 'out' and 'bottleneck' are numbers of input channels, output channels and bottleneck channels respectively. 'stride' is the stride of current block. '# layers' is the number of duplication of current block type. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>mAP (on FCOS) vs. entropy (scores) during the search with different search strategies. The scores (x-axis) on the left and the right are computed with the ratio of 0:0:1 and 1:1:6 respectively. Starting from the initial point, the dotted line indicates the evolution direction in the search process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .Figure 8 .Figure 9 .Figure 10 .</head><label>78910</label><figDesc>Correlations between random sample models. Visualization of single-scale entropy searching process. #layer is the number of each block of different levels.MAE-DET: Maximum Entropy Principle for Efficient Object DetectionVisualization of multi-scale entropy searching process (Coarse). #layer is the number of each block of different levels. Visualization of single-scale entropy searching process (Coarse-to-Fine). #layer is the number of each block of different levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2 MUTATE Require: Structure F t , search space S, fine-search flag Flag. Ensure: Randomly mutated structureF t . 1: Uniformly select a block h in F t . 2: if Flag equals to True then</figDesc><table><row><cell>3:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>MAE-DET and ResNet on the COCO. All results using the same training setting. FPS on V100 is benchmarked on the full model with NVIDIA V100 GPU, pytorch, FP32, batch size 32. AP val AP S AP M AP L AP test on V100Dataset and Training DetailsWe evaluate detection performance on COCO<ref type="bibr" target="#b32">(Lin et al., 2014)</ref> using the official training/testing splits. The mAP is evaluated on val 2017 by default, and GFLV2 is additionally evaluated on test-dev 2007 following common practice. All models are trained from scratch<ref type="bibr" target="#b18">(He et al., 2019)</ref> for 6X (73 epochs) on COCO.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">FLOPs</cell><cell>Params</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">val2017</cell><cell></cell><cell cols="2">test-dev</cell><cell>FPS</cell></row><row><cell cols="3">Backbone</cell><cell cols="4">Backbone Backbone</cell><cell>Head</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RetinaNet</cell><cell cols="2">40.2</cell><cell>24.3</cell><cell>43.3</cell><cell>52.2</cell><cell>-</cell><cell>23.2</cell></row><row><cell cols="3">ResNet-50</cell><cell>83.6G</cell><cell></cell><cell>23.5M</cell><cell></cell><cell>FCOS</cell><cell cols="2">42.7</cell><cell>28.8</cell><cell>46.2</cell><cell>53.8</cell><cell>-</cell><cell>27.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GFLV2</cell><cell cols="2">44.7</cell><cell>29.1</cell><cell>48.1</cell><cell>56.6</cell><cell cols="2">45.1</cell><cell>24.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RetinaNet</cell><cell cols="2">42.1</cell><cell>25.8</cell><cell>45.7</cell><cell>54.1</cell><cell>-</cell><cell>18.7</cell></row><row><cell cols="3">ResNet-101</cell><cell cols="2">159.5G</cell><cell>42.4M</cell><cell></cell><cell>FCOS</cell><cell cols="2">44.4</cell><cell>28.3</cell><cell>47.9</cell><cell>56.9</cell><cell>-</cell><cell>21.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GFLV2</cell><cell cols="2">46.3</cell><cell>29.9</cell><cell>50.1</cell><cell>58.7</cell><cell cols="2">46.5</cell><cell>19.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RetinaNet</cell><cell cols="2">40.0</cell><cell>23.9</cell><cell>43.3</cell><cell>52.7</cell><cell>-</cell><cell>35.5</cell></row><row><cell cols="3">MAE-DET-S</cell><cell>48.7G</cell><cell></cell><cell>21.2M</cell><cell></cell><cell>FCOS</cell><cell cols="2">42.5</cell><cell>26.8</cell><cell>46.0</cell><cell>54.6</cell><cell>-</cell><cell>43.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GFLV2</cell><cell cols="2">44.7</cell><cell>27.6</cell><cell>48.4</cell><cell>58.2</cell><cell cols="2">44.8</cell><cell>37.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RetinaNet</cell><cell cols="2">42.0</cell><cell>26.7</cell><cell>45.2</cell><cell>55.1</cell><cell>-</cell><cell>21.5</cell></row><row><cell cols="3">MAE-DET-M</cell><cell>89.9G</cell><cell></cell><cell>25.8M</cell><cell></cell><cell>FCOS</cell><cell cols="2">44.5</cell><cell>28.6</cell><cell>48.1</cell><cell>56.1</cell><cell>-</cell><cell>24.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GFLV2</cell><cell cols="2">46.8</cell><cell>29.9</cell><cell>50.4</cell><cell>60.0</cell><cell cols="2">46.7</cell><cell>22.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RetinaNet</cell><cell cols="2">43.0</cell><cell>27.3</cell><cell>46.5</cell><cell>56.0</cell><cell>-</cell><cell>17.6</cell></row><row><cell cols="3">MAE-DET-L</cell><cell cols="2">152.9G</cell><cell>43.9M</cell><cell></cell><cell>FCOS</cell><cell cols="2">45.9</cell><cell>30.2</cell><cell>49.4</cell><cell>58.4</cell><cell>-</cell><cell>19.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GFLV2</cell><cell cols="2">47.6</cell><cell>30.2</cell><cell>51.8</cell><cell>60.8</cell><cell cols="2">48.0</cell><cell>18.1</cell></row><row><cell></cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GFLV2+MAE-DET</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GFLV2+MAE-DET</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GFLV2+ResNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GFLV2+ResNet</cell></row><row><cell>mAP (%)~2</cell><cell>42 44 46</cell><cell></cell><cell cols="2">.0% mAP</cell><cell cols="3">FCOS+MAE-DET FCOS+ResNet RetinaNet+MAE-DET RetinaNet+ResNet</cell><cell>mAP (%)</cell><cell>46 42 44</cell><cell cols="2">1.54x speed-up</cell><cell></cell><cell cols="2">FCOS+MAE-DET FCOS+ResNet RetinaNet+MAE-DET RetinaNet+ResNet</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>400</cell><cell></cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Model FLOPs (G)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Inference time on V100 (ms/img)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">(a) mAP vs. FLOPs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(b) mAP vs. Speed Figure 4. mAP vs. FLOPs and inference speed on COCO val 2017 in Table 1. Note that FLOPs in (a) is the value of the full detector, containing backbone, FPN and head.5.1. Experiment Settings Search Settings In MAE-DET, the evolutionary popula- tion N is set to 256. The total EA iterations T = 96000. Following the previous designs (Chen et al., 2019b; Jiang et al., 2020; Du et al., 2020), MAE-DET is optimized for FLOPs. The resolution for computing entropy is 384 ? 384.Following the Spinenet (Du et al., 2020), we use multi-scale training and Synchronized Batch Normalization (SyncBN). For VOC dataset, train-val 2007 and train-val 2012 are used for training, and test 2007 for evaluation. For image classifi- cation, all models are trained on ImageNet-1k</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparisons with SOTA NAS methods for object detection. FLOPs are counted for full detector. SpineNet paper did not report the total search cost, only mentioned that 100 TPUv3 was used.</figDesc><table><row><cell>Method</cell><cell>Training-free</cell><cell>Search Cost GPU Days</cell><cell>Search Part</cell><cell>FLOPs All</cell><cell>Pretrain/ Scratch</cell><cell>Epochs</cell><cell>COCO (AP test )</cell></row><row><cell>DetNAS</cell><cell>?</cell><cell>68</cell><cell>backbone</cell><cell>289G</cell><cell>Pretrain</cell><cell>24</cell><cell>43.4</cell></row><row><cell>SP-NAS</cell><cell>?</cell><cell>26</cell><cell>backbone</cell><cell>655G</cell><cell>Pretrain</cell><cell>24</cell><cell>47.4</cell></row><row><cell>SpineNet</cell><cell>?</cell><cell cols="2">100x TPUv3 ? backbone+FPN</cell><cell>524G</cell><cell>Scratch</cell><cell>350</cell><cell>48.1</cell></row><row><cell>MAE-DET</cell><cell></cell><cell>0.6</cell><cell>backbone</cell><cell>279G</cell><cell>Scratch</cell><cell>73</cell><cell>48.0</cell></row><row><cell>?:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparisons between MAE-DET, DetNAS<ref type="bibr" target="#b8">(Chen et al., 2019b)</ref> and SpineNet<ref type="bibr" target="#b12">(Du et al., 2020)</ref> under the same training settings. All backbones are trained under GFLV2 head with 6X training epochs. FLOPs and parameters are counted for full detector.</figDesc><table><row><cell>Backbone</cell><cell>Search Part</cell><cell cols="7">Search Space FLOPs Params AP val AP S AP M AP L</cell><cell>FPS on V100</cell></row><row><cell>DetNAS-3.8G</cell><cell>backbone</cell><cell>ShuffleNetV2 +Xception</cell><cell>205G</cell><cell>35.5M</cell><cell>46.4</cell><cell>29.3</cell><cell>50.0</cell><cell>59.0</cell><cell>17.6</cell></row><row><cell>SpineNet-96</cell><cell cols="2">backbone+FPN ResNet Block</cell><cell>216G</cell><cell>41.3M</cell><cell>46.6</cell><cell>29.8</cell><cell>50.2</cell><cell>58.9</cell><cell>19.9</cell></row><row><cell>MAE-DET-M</cell><cell>backbone</cell><cell>ResNet Block</cell><cell>215G</cell><cell>34.9M</cell><cell>46.8</cell><cell>29.9</cell><cell>50.4</cell><cell>60.0</cell><cell>22.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>, MAE-DET is the only zero-shot (training-free)</cell></row><row><cell>method with 48.0% mAP on COCO, using 0.6 GPU days of</cell></row><row><cell>search. SpineNet (Du et al., 2020) achieves a slightly better</cell></row><row><cell>mAP with 2x more FLOPs. It uses 100 TPUv3 for search-</cell></row></table><note>ing, but the total search cost is not reported in the original paper. MAE-DET achieves better mAP than DetNAS (Chen et al., 2019b) and SP-NAS (Jiang et al., 2020) while being 50 ? 100 times faster in search. To further fairly compare different backbones under the same training settings, we train backbones designed by MAE-DET and previous backbone NAS methods in Ta- ble 3. Because the implementation of SP-NAS is not open- sourced, we retrain MAE-DET, DetNAS and SpineNet on COCO from scratch.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Single-scale entropy Compared to ResNet-50, the model searched by single-scale entropy obtains +0.7% accuracy gain on ImageNet, +2% mAP gain with FPN-free YOLOF and +0.8% mAP gain with FPN-based FCOS. Meanwhile, the model searched by Zen-Score achieves +0.9% accuracy gain on ImageNet, +1.1% mAP gain with YOLOF and +0.1% mAP gain with FCOS.</figDesc><table><row><cell>reports the MAE-DET backbones searched by dif-</cell></row><row><cell>ferent evolutionary strategies, and whether using multi-scale</cell></row><row><cell>entropy prior. The COCO mAPs of models trained in two</cell></row><row><cell>detection frameworks (YOLOF and FCOS) are reported in</cell></row><row><cell>the right big two columns. YOLOF models are trained for</cell></row><row><cell>12 epochs with ImageNet pre-trained initialization, while</cell></row><row><cell>FCOS models are trained with the 3X training epochs. We</cell></row><row><cell>also compare their image classification ability on ImageNet-</cell></row><row><cell>1k. All models are constrained by the FLOPs less than 4.4</cell></row><row><cell>G while the number of parameters is not constrained. More</cell></row><row><cell>details about the searching process and architectures can be</cell></row><row><cell>found in Appendix G, H.</cell></row></table><note>Multi-scale entropy When using multi-scale entropy, both single-scale model and multi-scale model get simi-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Comparison of different evolutionary searching strategies in MAE-DET. C-to-F: Coarse-to-Fine. Zen-Score is the proxy in Zen-NAS (Lin et al., 2021). AP val AP S AP M AP L AP val AP S AP M AP L lar accuracy on ImageNet. The single-scale model uses 2X more parameters than the multi-scale model under the same FLOPs constraint. In terms of mAP, multi-scale model outperforms single-scale model by +0.3% on COCO with YOLOF and 0.6% on COCO with FCOS. From the last row of</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ImageNet-1K</cell><cell></cell><cell></cell><cell cols="2">COCO with YOLOF</cell><cell></cell><cell></cell><cell cols="2">COCO with FCOS</cell><cell></cell></row><row><cell cols="5">Score Mutation FLOPs Params TOP-1 % ResNet-50 None 4.1G 23.5M 78.0</cell><cell>37.8</cell><cell>19.1</cell><cell>42.1</cell><cell>53.3</cell><cell>38.0</cell><cell>23.2</cell><cell>40.8</cell><cell>47.6</cell></row><row><cell>Zen-Score</cell><cell>Coarse</cell><cell>4.4G</cell><cell>67.9M</cell><cell>78.9</cell><cell>38.9</cell><cell>19.0</cell><cell>43.2</cell><cell>56.0</cell><cell>38.1</cell><cell>23.2</cell><cell>40.5</cell><cell>48.1</cell></row><row><cell>Single-scale</cell><cell>Coarse</cell><cell>4.4G</cell><cell>60.1M</cell><cell>78.7</cell><cell>39.8</cell><cell>19.9</cell><cell>44.4</cell><cell>56.5</cell><cell>38.8</cell><cell>23.1</cell><cell>41.4</cell><cell>50.1</cell></row><row><cell>Multi-scale</cell><cell>Coarse</cell><cell>4.3G</cell><cell>29.4M</cell><cell>78.9</cell><cell>40.1</cell><cell>21.1</cell><cell>44.5</cell><cell>55.9</cell><cell>39.4</cell><cell>23.7</cell><cell>42.3</cell><cell>50.0</cell></row><row><cell>Multi-scale</cell><cell>C-to-F</cell><cell>4.4G</cell><cell>25.8M</cell><cell>79.1</cell><cell>40.3</cell><cell>20.8</cell><cell>44.7</cell><cell>56.4</cell><cell>40.0</cell><cell>24.5</cell><cell>42.6</cell><cell>50.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Transferability of MAE-DET in multiple object detection and instance segmentation tasks. FLOPs reported are counted for full detector.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Head</cell><cell>Backbone</cell><cell>Resolution</cell><cell cols="4">Epochs FLOPs AP val AP mask val</cell></row><row><cell>Object Detection</cell><cell>VOC Citescapes</cell><cell>FCOS</cell><cell cols="2">ResNet-50 MAE-DET-M 1000 ? 600 1000 ? 600 ResNet-50 2048 ? 1024 MAE-DET-M 2048 ? 1024</cell><cell>12 12 64 64</cell><cell>120G 123G 411G 426G</cell><cell>76.8 80.9 37.0 38.1</cell><cell>----</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet-50</cell><cell>1333 ? 800</cell><cell>73</cell><cell>375G</cell><cell>43.2</cell><cell>39.2</cell></row><row><cell></cell><cell></cell><cell>MASK</cell><cell cols="2">MAE-DET-M 1333 ? 800</cell><cell>73</cell><cell>379G</cell><cell>44.5</cell><cell>40.3</cell></row><row><cell>Instance Segmentation</cell><cell>COCO</cell><cell>R-CNN</cell><cell>ResNet-50 ? SpineNet-49 ?</cell><cell>640 ? 640 640 ? 640</cell><cell>350 350</cell><cell>228G 216G</cell><cell>42.7 42.9</cell><cell>37.8 38.1</cell></row><row><cell></cell><cell></cell><cell>SCNet</cell><cell cols="2">ResNet-50 MAE-DET-M 1333 ? 800 1333 ? 800</cell><cell>73 73</cell><cell>671G 675G</cell><cell>46.3 47.1</cell><cell>41.6 42.3</cell></row><row><cell cols="3">?: Numbers are cited from SpineNet paper</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Dataset and Training Details For object detection, trainval35k with 115K images in the COCO dataset is mainly used for training. With the single-scale testing of resolution 1333 ? 800, COCO mAP results are reported on the val 2017 for most experiments and the test-dev 2007 for GFLV2 results in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>MAE-DET-MB and MobileNetV2 on the COCO with the SSDLite head, which are trained from scratch with 600 epochs at resolution 320. FPS on Pixel 2 is benchmarked on the full model with CPU, FP32, batch size 1. MAE-DET-MB-M-SE means inserting SE modules to MAE-DET-MB-M.</figDesc><table><row><cell></cell><cell>FLOPs</cell><cell>Params</cell><cell></cell><cell cols="2">val2017</cell><cell></cell><cell>FPS</cell></row><row><cell>Backbone</cell><cell cols="7">Backbone Backbone AP val AP S AP M AP L on Pixel 2</cell></row><row><cell>MobileNetV2-0.5</cell><cell>217M</cell><cell>0.7M</cell><cell>14.7</cell><cell>0.8</cell><cell>11.0</cell><cell>31.2</cell><cell>13.5</cell></row><row><cell>MobileNetV2-1.0</cell><cell>651M</cell><cell>2.2M</cell><cell>21.1</cell><cell>1.7</cell><cell>20.5</cell><cell>39.9</cell><cell>6.6</cell></row><row><cell>MAE-DET-MB-S</cell><cell>201M</cell><cell>0.6M</cell><cell>15.9</cell><cell>0.8</cell><cell>12.2</cell><cell>31.8</cell><cell>13.8</cell></row><row><cell>MAE-DET-MB-M</cell><cell>645M</cell><cell>2.0M</cell><cell>22.2</cell><cell>2.1</cell><cell>21.5</cell><cell>42.3</cell><cell>6.3</cell></row><row><cell>MAE-DET-MB-M-SE</cell><cell>647M</cell><cell>2.3M</cell><cell>22.6</cell><cell>2.3</cell><cell>22.0</cell><cell>42.5</cell><cell>5.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .</head><label>7</label><figDesc>Results between Scratch and Pretrain strategy on the COCO with single-scale testing. Training strategy on ImageNet is same asTable.4. AP val AP S AP M AP L ?: results in this line are reported in the official github<ref type="bibr" target="#b27">(Li et al., 2021b)</ref>.</figDesc><table><row><cell cols="6">Backbone Strategy Epochs ResNet-50 FLOPs Backbone Params Backbone Head GFLV2 Scratch 73 83.6G 23.5M GFLV2 Pretrain 24</cell><cell>44.7 44.0</cell><cell>29.1 27.1</cell><cell>48.1 47.8</cell><cell>56.6 56.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GFLV2</cell><cell>Pretrain</cell><cell>24</cell><cell>43.9 ?</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">RetinaNet Scratch</cell><cell>73</cell><cell>42.0</cell><cell>26.7</cell><cell>45.2</cell><cell>55.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">RetinaNet Pretrain</cell><cell>24</cell><cell>42.3</cell><cell>25.3</cell><cell>46.5</cell><cell>56.0</cell></row><row><cell>MAE-DET-M</cell><cell>89.9G</cell><cell>25.8M</cell><cell>FCOS FCOS</cell><cell>Scratch Pretrain</cell><cell>73 24</cell><cell>44.5 44.5</cell><cell>28.6 28.8</cell><cell>48.1 48.5</cell><cell>56.1 56.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GFLV2</cell><cell>Scratch</cell><cell>73</cell><cell>46.8</cell><cell>29.9</cell><cell>50.4</cell><cell>60.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>GFLV2</cell><cell>Pretrain</cell><cell>24</cell><cell>46.0</cell><cell>29.0</cell><cell>50.0</cell><cell>59.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 .</head><label>8</label><figDesc>Results between different ratios of weights alpha in MSEP on COCO.Backbone ? 3 :? 4 :? 5 FLOPs Params AP val AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell></cell><cell cols="2">ResNet-50</cell><cell>None</cell><cell>83.6G</cell><cell cols="2">23.5M</cell><cell>38.0</cell><cell>55.2</cell><cell>41.0</cell><cell>23.2</cell><cell>40.8</cell><cell>47.6</cell></row><row><cell></cell><cell cols="2">MAE-DET</cell><cell>1:1:1</cell><cell>84.4G</cell><cell cols="2">11.5M</cell><cell>37.4</cell><cell>54.6</cell><cell>40.0</cell><cell>23.6</cell><cell>39.8</cell><cell>46.6</cell></row><row><cell></cell><cell cols="2">MAE-DET</cell><cell>1:1:2</cell><cell>84.8G</cell><cell cols="2">13.4M</cell><cell>37.8</cell><cell>54.9</cell><cell>40.5</cell><cell>23.2</cell><cell>40.0</cell><cell>47.8</cell></row><row><cell></cell><cell cols="2">MAE-DET</cell><cell>1:1:4</cell><cell>85.9G</cell><cell cols="2">17.2M</cell><cell>38.6</cell><cell>56.0</cell><cell>41.4</cell><cell>23.4</cell><cell>41.3</cell><cell>48.6</cell></row><row><cell></cell><cell cols="2">MAE-DET</cell><cell>1:1:6</cell><cell>88.7G</cell><cell cols="2">29.4M</cell><cell>39.4</cell><cell>57.3</cell><cell>42.1</cell><cell>23.7</cell><cell>42.3</cell><cell>50.0</cell></row><row><cell></cell><cell cols="2">MAE-DET</cell><cell>1:1:8</cell><cell>89.9G</cell><cell cols="2">31.7M</cell><cell>39.4</cell><cell>57.2</cell><cell>42.0</cell><cell>23.7</cell><cell>42.5</cell><cell>49.5</cell></row><row><cell></cell><cell cols="2">MAE-DET</cell><cell>1:4:1</cell><cell>86.3G</cell><cell cols="2">10.9M</cell><cell>35.7</cell><cell>52.6</cell><cell>38.3</cell><cell>22.2</cell><cell>38.1</cell><cell>44.9</cell></row><row><cell></cell><cell cols="2">MAE-DET</cell><cell>4:1:1</cell><cell>86.1G</cell><cell cols="2">11.1M</cell><cell>33.9</cell><cell>50.2</cell><cell>36.7</cell><cell>20.4</cell><cell>36.1</cell><cell>43.4</cell></row><row><cell>mAP (%)</cell><cell>36 38</cell><cell></cell><cell>mAP (%)</cell><cell>36 38</cell><cell></cell><cell></cell><cell>mAP (%)</cell><cell>36 38</cell><cell></cell><cell>mAP (%)</cell><cell>36 38</cell></row><row><cell></cell><cell>34</cell><cell></cell><cell></cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell>34</cell><cell></cell><cell></cell><cell>34</cell></row><row><cell></cell><cell>220</cell><cell>240</cell><cell>260</cell><cell>500</cell><cell>550</cell><cell>600</cell><cell></cell><cell>750</cell><cell>800</cell><cell>850</cell><cell>1260</cell><cell>1280</cell><cell>1300</cell></row><row><cell></cell><cell cols="3">Scores with ratio 0:0:1</cell><cell cols="4">Scores with ratio 1:1:1</cell><cell cols="3">Scores with ratio 1:1:2</cell><cell cols="2">Scores with ratio 1:1:4</cell></row><row><cell>mAP (%)</cell><cell>36 38</cell><cell></cell><cell>mAP (%)</cell><cell>36 38</cell><cell></cell><cell></cell><cell>mAP (%)</cell><cell>36 38</cell><cell></cell><cell>mAP (%)</cell><cell>36 38</cell></row><row><cell></cell><cell>34</cell><cell></cell><cell></cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell>34</cell><cell></cell><cell></cell><cell>34</cell></row><row><cell></cell><cell>1700</cell><cell>1750</cell><cell>1800</cell><cell cols="2">2200</cell><cell>2300</cell><cell></cell><cell>1100</cell><cell>1200</cell><cell></cell><cell>600</cell><cell>800</cell><cell>1000 1200</cell></row><row><cell></cell><cell cols="3">Scores with ratio 1:1:6</cell><cell cols="4">Scores with ratio 1:1:8</cell><cell cols="3">Scores with ratio 1:4:1</cell><cell cols="2">Scores with ratio 4:1:1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 .</head><label>9</label><figDesc>Different zero-shot proxies on COCO with FCOS. All methods use the same search space, FLOPs budget, searching strategy and training schedule.</figDesc><table><row><cell>Proxy</cell><cell>FLOPs Backbone</cell><cell>Params Backbone</cell><cell cols="4">AP val AP S AP M AP L</cell></row><row><cell>ResNet-50</cell><cell>84G</cell><cell>23.5M</cell><cell>38.0</cell><cell>23.2</cell><cell>40.8</cell><cell>47.6</cell></row><row><cell>SyncFlow</cell><cell>90G</cell><cell>67.4M</cell><cell>35.6</cell><cell>21.8</cell><cell>38.1</cell><cell>44.8</cell></row><row><cell>NASWOT</cell><cell>88G</cell><cell>28.1M</cell><cell>36.7</cell><cell>23.1</cell><cell>38.8</cell><cell>45.9</cell></row><row><cell>Zen-NAS</cell><cell>91G</cell><cell>67.9M</cell><cell>38.1</cell><cell>23.2</cell><cell>40.5</cell><cell>48.1</cell></row><row><cell>MAE-DET</cell><cell>89G</cell><cell>25.8M</cell><cell>39.4</cell><cell>23.7</cell><cell>42.3</cell><cell>50.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 .</head><label>10</label><figDesc>Architecture of single-scale entropy with coarse mutation inTable. 4</figDesc><table><row><cell>block</cell><cell>kernel</cell><cell>in</cell><cell>out</cell><cell cols="4">stride bottleneck # layers level</cell></row><row><cell>Conv</cell><cell>3</cell><cell>3</cell><cell>96</cell><cell>2</cell><cell>-</cell><cell>1</cell><cell>C1</cell></row><row><cell cols="2">ResBlock 5</cell><cell>96</cell><cell>208</cell><cell>2</cell><cell>32</cell><cell>2</cell><cell>C2</cell></row><row><cell cols="2">ResBlock 5</cell><cell cols="2">208 560</cell><cell>2</cell><cell>56</cell><cell>1</cell><cell>C3</cell></row><row><cell cols="2">ResBlock 5</cell><cell cols="2">560 1264</cell><cell>2</cell><cell>112</cell><cell>2</cell><cell>C4</cell></row><row><cell cols="2">ResBlock 5</cell><cell cols="2">1264 1712</cell><cell>2</cell><cell>224</cell><cell>3</cell><cell>C5</cell></row><row><cell cols="2">ResBlock 5</cell><cell cols="2">1712 2048</cell><cell>1</cell><cell>224</cell><cell>3</cell><cell>C5</cell></row><row><cell cols="2">ResBlock 5</cell><cell cols="2">2048 2048</cell><cell>1</cell><cell>256</cell><cell>4</cell><cell>C5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Largescale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Science and information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brillouin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Courier Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Oncefor-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10446</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mmdetection</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoformer: Searching transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12270" to="12280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">You only look one-level feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13039" to="13048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detnas: Backbone search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6642" to="6652" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Darts-: Robustly stepping out of performance collapse without indicators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning scale-permuted backbone for recognition and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spinenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11592" to="11601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fna++: Fast network adaptation via parameter remapping and architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2990" to="3004" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Complexity of linear regions in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2596" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Information theory and statistical mechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="620" to="630" />
			<date type="published" when="1957-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sp-nas: Serial-to-parallel backbone search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11863" to="11872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Giraffedet: A heavy-neck paradigm for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Information theory and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Courier Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ds-Net++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10060</idno>
		<title level="m">Dynamic weight slicing for efficient inference in cnns and transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11632" to="11641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">One-shot path aggregation network architecture search for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Opanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10195" to="10203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tiny deep learning on iot devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcunet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zen-nas: A zero-shot nas for highperformance deep image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural architecture search without training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7588" to="7598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Information, sensation, and perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Norwich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="81" to="82" />
			<pubPlace>San Diego</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient neural architecture transformation search in channellevel for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exponential expressivity in deep neural networks through transient chaos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3360" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">An introduction to information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Reza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Courier Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the information bottleneck theory of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dapello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Tracey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bounding and counting linear regions of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tjandraatmadja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A mathematical theory of communication. The Bell system technical journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning diverse and discriminative representations via the principle of maximal coding rate reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9422" to="9434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10076" to="10085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
