<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Peng</surname></persName>
							<email>ruipeng@stu.pku.edu.cnrgwang@pkusz.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawen</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depth estimation is solved as a regression or classification problem in existing learning-based multi-view stereo methods. Although these two representations have recently demonstrated their excellent performance, they still have apparent shortcomings, e.g., regression methods tend to overfit due to the indirect learning cost volume, and classification methods cannot directly infer the exact depth due to its discrete prediction. In this paper, we propose a novel representation, termed Unification, to unify the advantages of regression and classification. It can directly constrain the cost volume like classification methods, but also realize the sub-pixel depth prediction like regression methods. To excavate the potential of unification, we design a new loss function named Unified Focal Loss, which is more uniform and reasonable to combat the challenge of sample imbalance. Combining these two unburdened modules, we present a coarse-to-fine framework, that we call UniMVSNet. The results of ranking first on both DTU and Tanks and Temples benchmarks verify that our model not only performs the best but also has the best generalization ability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-view stereo (MVS) is a vital branch to extract geometry from photographs, which takes stereo correspondence from multiple images as the main cue to reconstruct dense 3D representations. Although traditional methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> have achieved excellent performance after occupying researchers for decades, more and more learningbased approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref> are proposed to promote the effectiveness of MVS due to their more powerful representation capability in low-texture regions, reflections, etc. Concretely, they infer the depth for each view from the 3D cost volume, which is constructed from the warped feature according to a set of predefined depth hypotheses. Compared with hand-crafted similarity metrics in traditional methods, the 3D cost volume can capture more discriminative features to achieve more robust matching. Without loss of integrity, existing learning-based methods can be divided into two categories: Regression and Classification.</p><p>Regression is the most primitive and straightforward implementation of the learning-based MVS method. It's a group of approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">39]</ref> to regress the depth from the 3D cost volume through Soft-argmin, which softly weighting each depth hypothesis. More specifically, the model expects to regress greater weight for the depth hypothesis with a small cost. Theoretically, it can achieve the sub-pixel estimation of depth by weighted summation of discrete depth hypotheses. Nevertheless, the model needs to learn a complex combination of weights under indirect constraints performed on the weighted depth but not on the weight combination, which is non-trivial and tends to overfit. You can imagine that there are many weight combinations for a set of depth hypotheses that can be weighted and summed to the same depth, and this ambiguity also implicitly increases the difficulty of the model convergence.</p><p>Classification is proposed in R-MVSNet <ref type="bibr" target="#b36">[36]</ref> to infer the optimal depth hypothesis. Different from the weight estimation in regression, classification methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">36]</ref> predict the probability of each depth hypothesis from the 3D cost volume and take the depth hypothesis with the maximum probability as the final estimation. Obviously, these methods cannot infer the exact depth directly from the model like regression methods. However, classification methods directly constrain the cost volume through the cross-entropy loss executed on the regularized probability volume, which is the essence of ensuring the robustness of MVS. Moreover, the estimated probability distribution can directly reflect the confidence, which is difficult to derive from the weight combination intuitively.</p><p>In this paper, we seek to unify the advantages of regression and classification, that is, we hope that the model can accurately predict the depth while maintaining robustness. There is a fact that the depth hypothesis close to the groundtruth has more potential knowledge, while that of other remaining hypotheses is limited or even harmful due to the wrong induction of multimodal <ref type="bibr" target="#b42">[42]</ref>. Motivated by this, we present that estimating the weights for all depth hypotheses is redundant, and the model only needs to do regression on the optimal depth hypothesis that the representative depth interval (referring to the upper area until the next larger depth hypothesis) contains the ground truth depth. To achieve this, we propose a unified representation for depth, termed Unification. As shown in <ref type="figure">Fig. 1</ref>, unlike regression, the loss is executed on the regularized probability volume directly, and different from classification, our method estimates the Unity (What we call), whose label is composed by at most one non-zero continuous target (0 ? 1), to simultaneously represent the location of optimal depth hypothesis and its offset to the ground-truth depth. We take proximity (defined as the complement of the offset between groundtruth and optimal depth hypothesis) to characterize the nonzero target in unity label, which is more efficient than purely using offset. The detailed comparisons are in Supp. Mat. Moreover, we note that this unified representation faces an undeniable sample imbalance in both category and hardness. While Focal Loss (FL) <ref type="bibr" target="#b18">[19]</ref> is the common solution proposed in the detection field, which is tailored to the traditional discrete label, the more general form (GFL) is proposed in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">40]</ref> to deal with the continuous label. Even though GFL has demonstrated its performance, we hold the belief that it has an obvious limitation in distinguishing hard and easy samples due to ignoring the magnitude of groundtruth. To this end, we put forward a more reasonable and unified form, called Unified Focal Loss (UFL), after thorough analysis to better address these challenges. In this way, the traditional FL can be regarded as a special case of UFL, while GFL is its imperfect expression.</p><p>To demonstrate the superiority of our proposed modules, we present a coarse-to-fine framework termed UniMVSNet (or UnifiedMVSNet), named for its unification of depth representation and focal loss, which replaces the traditional representation of recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">34]</ref> with Unification and adopts UFL for optimization. Extensive experiments show that our model surpasses all previous MVS methods and achieves state-of-the-art performance on both DTU <ref type="bibr" target="#b0">[1]</ref> and Tanks and Temples <ref type="bibr" target="#b13">[14]</ref> benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Traditional MVS methods. Taking the output scene representation as an axis of taxonomy, there are mainly four types of classic MVS methods: volumetric <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>, point cloud based <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>, mesh based <ref type="bibr" target="#b5">[6]</ref> and depth map based <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31]</ref>. Among them, the depth map based method is the most flexible one. Instead of operating in the 3D domain, it degenerates the complex problem of 3D geometry reconstruction to depth map estimation in the 2D domain. Moreover, as the intermediate representation, the estimated depth maps of all individual images can be merged into a consistent point cloud <ref type="bibr" target="#b21">[22]</ref> or a volumetric reconstruction <ref type="bibr" target="#b22">[23]</ref>, and the mesh can even be further reconstructed.</p><p>Learning-based MVS methods. While the traditional MVS pipeline mainly relies on hand-crafted similarity metrics, recent works apply deep learning for superior performance on MVS. SurfaceNet <ref type="bibr" target="#b10">[11]</ref> and LSM <ref type="bibr" target="#b11">[12]</ref> are the first proposed volumetric learning-based MVS pipelines to regress surface voxels from 3D space. However, they are restricted to memory which is the common drawback of the volumetric representation. Most recently, MVSNet <ref type="bibr" target="#b35">[35]</ref> first realizes an end-to-end memory low-sensitive pipeline based on 3D cost volumes. This pipeline mainly consists of four steps: image feature extraction by 2D CNN, variancebased cost aggregation by homography warping, cost regularization through 3D CNN, and depth regression. To further excavate the potential capacity of this pipeline, some variants of MVSNet have been proposed, e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36]</ref> are proposed to reduce the memory requirement through RNN or coarse-to-fine manner, <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">38]</ref> are proposed to adaptively re-weight the contribution of different pixels in cost aggregation. Meanwhile, all existing methods are based on one of the two complementary of classification and regression to infer depth. In this paper, we propose a novel unified representation to integrate their advantages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section will present the main contributions of this paper in detail. We first review the common pipeline of the learning-based MVS approach in Sec. 3.1, then introduce the proposed unified depth representation in Sec. 3.2 and unified focal loss in Sec. 3.3, and finally describe the detailed network architecture of our UniMVSNet in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of Learning-based MVS</head><p>Most end-to-end learning-based MVS methods are inherited from MVSNet <ref type="bibr" target="#b35">[35]</ref>, which constructs an elegant and effective pipeline to infer the depth D ? R H ?W of the reference image I 1 . Given multiple im-</p><formula xml:id="formula_0">ages {I i ? R C?H?W } N i=1 of a scene taken from N dif- ferent viewpoints, image features of all images {F i ? R C ?H ?W } N i=1</formula><p>are first extracted through a 2D network with shared weights. As mentioned above, the learningbased method is based on the 3D cost volume, and the depth hypothesis of M layers {d i ? R H ?W } M i=1 is sampled from the whole known depth range to achieve this, where d 1 represents the minimum depth and d M represents the maximum depth. With this hypothesis, feature volumes</p><formula xml:id="formula_1">{V i ? R M ?C ?H ?W } N i=1</formula><p>can be constructed in 3D space via differentiable homography by warping 2D image features of source images to the reference camera frustum. The homography between the feature maps of i th view and the reference feature maps at depth d is expressed as:</p><formula xml:id="formula_2">H i (d) = dK i T i T ?1 1 K ?1 1<label>(1)</label></formula><p>where K and T refer to camera intrinsics and extrinsics respectively.</p><p>To handle arbitrary number of input views, the multiple feature volumes {V i } N i=1 need to be aggregated to one cost volume C ? R M ?C ?H ?W . The aggregation strategy consists two dominant groups: statistical and adaptive. The variance-based mapping is a typical statistical aggregation:</p><formula xml:id="formula_3">C = 1 N N i=1 (V i ?V) 2<label>(2)</label></formula><p>WhereV denotes the average feature volume. Furthermore, the adaptive aggregation is proposed to re-weight the contribution of different pixels, which can be modeled as:</p><formula xml:id="formula_4">C = 1 N ? 1 N i=2 W i (V i ? V 1 ) 2 (3)</formula><p>where W is the learnable weight generated by an auxiliary network, and denotes element-wise multiplication. The matching cost between the reference view and all source views under each depth hypothesis has been encoded into the cost volume, which is required to be further refineed to generate a probability volume P ? R M ?H ?W through a softmax-based regularization network. Concretely, the probability volume is treated as the weight of depth hypotheses in regression methods and the depth at pixel (x, y) is calculated as the sum of the weighted hypotheses as:</p><formula xml:id="formula_5">D x,y = d x,y M d=d x,y 1 dP(d) x,y<label>(4)</label></formula><p>and the model is constrained by the L1 loss between D and the ground-truth depth. In classification methods, P refers to the probability of depth hypotheses and the depth is estimated as the hypothesis whose probability is maximum:</p><formula xml:id="formula_6">D x,y = arg max d?{d x,y i } M i=1 P(d) x,y<label>(5)</label></formula><p>and the model is trained by the cross-entropy loss between P and the ground-truth one-hot probability volume.</p><p>In traditional one-stage methods, compared with original input images, the depth map is either downsized during feature extraction <ref type="bibr" target="#b35">[35]</ref> or before the input <ref type="bibr" target="#b38">[38]</ref> to save memory, while in the coarse-to-fine method <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">34]</ref>, it's a multiscale result {D i } L i=1 with incremental resolution generated by repeating the above pipeline L times. The multi-scale is realized by a FPN-like <ref type="bibr" target="#b17">[18]</ref> feature extraction network, and the depth hypothesis with decreasing depth range is sampled based on the depth map generated in the previous stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unified Depth Representation</head><p>As aforementioned, the regression method tends to overfit due to its indirect learning cost volume and ambiguity in the correspondence between depth and the weight combination. For the classification method, although it can constrain the cost volume directly, it cannot predict exact depth like regression methods due to its discrete prediction. In this paper, we found that they can complement each other, and we unify them successfully through our unified depth representation, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. We recast the depth estimation as a multi-label classification task, in which the model needs to classify which hypothesis is the optimal one and regress the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Unity Generation</head><formula xml:id="formula_7">Input: Ground-truth depth D gt ? R H ?W ; Depth hypotheses {d i ? R H ?W } M i=1 . Output: Ground-truth Unity {U i ? R H ?W } M i=1 . Initialization: Depth interval r = 0. 1 for i = 1 to M do 2 for (x, y) = (1, 1) to (H , W ) do 3 if i &lt; M then 4 r = d x,y i+1 ? d x,y i ; 5 end 6 if d x,y i ? D x,y gt and d x,y i + r &gt; D x,y gt then 7 U x,y i = 1 ? D x,y gt ?d x,y i r ; 8 else 9 U x,y i = 0 ; 10 end 11 end 12 end 13 return {U i } M i=1 .</formula><p>proximity for it. In other words, we first adopt classification to narrow the depth range of the final regression, but they are executed simultaneously in our implementation. Therefore, the model in our Unification representation is able to estimate an accurate depth like regression methods, and it also directly optimizes the cost volume like classification methods. Below, we will introduce how to generate the ground-truth unity from ground-truth depth (Unity generalization), and how to regress the depth from the estimated unity (Unity regression).</p><p>Unity generation: As shown in <ref type="figure">Fig. 1</ref>, ground-truth</p><formula xml:id="formula_8">unity {U i } M i=1</formula><p>is a more general form of one-hot label peaked at the optimal depth hypothesis whose depth interval contains ground-truth depth. The at most one non-zero target is a continuous number and represents the proximity of the optimal hypothesis to ground-truth depth. The detail of unity generation is shown in Algorithm 1, which is one more step of proximity calculation than the one-hot label generation in classification methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Unity Regression</head><formula xml:id="formula_9">Input: Estimated unity { U i ? R H ?W } M i=1 ; Depth hypotheses {d i ? R H ?W } M i=1 . Output: Regressed Depth D ? R H ?W . Initialization: Depth interval r = 0. 1 for (x, y) = (1, 1) to (H , W ) do 2 Optimal hypothesis index o = arg max i?{1,??? ,M } U x,y i ; 3 Optimal hypothesis d = d x,y o ; 4 if o &lt; M then 5 r = d x,y o+1 ? d x,y o ; 6 else 7</formula><p>// previous interval for the last hypothesis</p><formula xml:id="formula_10">8 r = d x,y o ? d x,y o?1 ; 9 end 10 Offset of f = (1 ? U x,y o ) ? r ; 11</formula><p>Depth D x,y = d + of f ; 12 end 13 return D.</p><p>Unity regression: Unlike the traditional way of predicting the probability volume by softmax operators, unification representation estimated it through sigmoid operators. Here, we disassemble the estimated probability volume P into the estimated unity { U i } M i=1 along the M dimension. To regress the depth, we first select the optimal hypothesis with the maximum unity at each pixel, then calculate the offset to ground-truth depth, and finally fuse the estimated depth. The detailed procedure is shown in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Unified Focal Loss</head><p>Generally, the depth hypothesis of MVS models will be sampled quite densely to ensure the accuracy of the estimated depth, which will cause obvious sample imbalance due to only one positive sample (the at most one nonzero target) among hundreds of hypotheses. Meanwhile, the model needs to pay more attention to hard samples to prevent overfitting. Relevant Focal Loss (FL) <ref type="bibr" target="#b18">[19]</ref> has been proposed to solve these two problems, which auto-  <ref type="figure">Figure 4</ref>. Illustration of UniMVSNet. This is a typical coarse-to-fine framework. The part with pink background is inherited from existing methods, and the green part is our novel modules. The depth hypothesis is represented by the red curve for convenience. matically distinguishes hard samples through the estimated unity u ? [0, 1] and rebalances the sample through tunable parameter ? and ?. Here, we discuss a certain pixel for convenience. The typical definition of FL is:</p><formula xml:id="formula_11">FL(u, q) = ??(1 ? u) ? log(u), q = 1 ?(1 ? ?)u ? log(1 ? u), else<label>(6)</label></formula><p>where q ? {0, 1} is the discrete target. Therefore, the traditional FL is not suitable for our continuous situation. To enable successful training under the case of our representation, we borrow the main idea from FL. Above all, the binary cross-entropy ? log(u) or ? log(1 ? u) needs to be extended to its complete form BCE(u, q) = ?q log(u) ? (1 ? q) log(1 ? u). Correspondingly, the scaling factor should also be adjusted appropriately. The generalized FL form (GFL) obtained through these two steps is:</p><formula xml:id="formula_12">GFL(u, q) = ?|q ? u| ? BCE(u, q), q &gt; 0 (1 ? ?)u ? BCE(u, q), else<label>(7)</label></formula><p>where q ? [0, 1] is the continuous target. This advanced version is currently adopted by some existing methods with different names, e.g., QFL in <ref type="bibr" target="#b16">[17]</ref> or VFL in <ref type="bibr" target="#b40">[40]</ref>. But in this paper, we point out that this implementation is not perfect in scaling hard and easy samples, because they ignore the magnitude of the ground-truth. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the first two samples will be considered the hardest under the absolute error |q ? u| measurement in GFL. However, the absolute error cannot distinguish samples with different targets. Even if the first two samples in <ref type="figure" target="#fig_2">Fig. 3</ref> have the same absolute error, this error obviously has a smaller effect on the first sample due to its larger ground-truth. To solve this ambiguity, we further improve the scaling factor in GFL through relative error and propose our naive version of Unified Focal Loss (UFL) just as:</p><formula xml:id="formula_13">UFL(u, q) = ?( |q?u| q + ) ? BCE(u, q), q &gt; 0 (1 ? ?)( u q + ) ? BCE(u, q), else<label>(8)</label></formula><p>where q + ? (0, 1] is the positive target. It can be seen from Eq. (8) that FL is a special case of UFL when the positive target is the constant 1.</p><p>Moreover, we noticed that the range of scaling factor |q?u| q + is [0, +?), which may lead to a special case like the last sample in <ref type="figure" target="#fig_2">Fig. 3</ref>. Even a small number of such samples will overwhelm the loss and computed gradients due to their huge scaling factor. In this paper, we solve this problem by introducing a dedicated function to control the range of the scaling factor. Meanwhile, to keep the precious positive learning signals, we adopt an asymmetrical scaling strategy. And the complete UFL can be modeled as:</p><formula xml:id="formula_14">UFL(u, q) = ? + (S + b ( |q?u| q + )) ? BCE(u, q), q &gt; 0 ? ? (S ? b ( u q + )) ? BCE(u, q), else<label>(9)</label></formula><p>where the dedicated function S b (x) is designed as the sigmoid-like function (1/(1 + b ?x )) with a base of b in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">UniMVSNet</head><p>It's straightforward to apply our Unification and UFL to existing learning-based MVS methods. To illustrate the effectiveness and flexibility of the proposed modules, we build the UniMVSNet, whose framework is depicted in <ref type="figure">Fig. 4</ref>, based on the coarse-to-fine strategy. This pipeline abides by the procedure reviewed in Sec. 3.1, except the depth representation and optimization.</p><p>Inherited from CasMVSNet <ref type="bibr" target="#b8">[9]</ref>, We adopt a FPN-like network to extract multi-scale features, and uniformly sample the depth hypothesis with a decreasing interval and a decreasing number. To better handle the unreliable matching in non-Lambertian regions, we adopt an adaptive aggregation method with negligible parameters increasing like <ref type="bibr" target="#b38">[38]</ref> to aggregate the feature volumes warped by the differentiable homography. Meanwhile, we also apply multi-scale 3D CNNs to regularize the cost volume, and the generated probability volume P at each stage is treated as the estimated Unity { U i } M i=1 here, which can be further regressed to accurate depth as shown in Algorithm 2. It can be seen from <ref type="figure">Fig. 4</ref> that UniMVSNet directly optimizes cost volume through UFL, which can effectively avoid the overfitting of indirect learning strategy in regression methods.</p><p>Training Loss. As shown in <ref type="figure">Fig. 4</ref>, we apply UFL to all stages and fuse them with different weights. The total loss can be defined as:</p><formula xml:id="formula_15">Loss = L i=1 ? i UFL i<label>(10)</label></formula><p>where UFL i is the average of UFL of all valid pixels at stage i and ? i denotes the weight of UFL at i th stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section demonstrates the start-of-the-art performance of UniMVSNet with comprehensive experiments and verifies the effectiveness of the proposed Unification and UFL through ablation studies. We first introduce the datasets and implementation and then analyze our results. <ref type="bibr">Method</ref> ACC.(mm) Comp.(mm) Overall(mm) Furu <ref type="bibr" target="#b6">[7]</ref> 0.613 0.941 0.777 Gipuma <ref type="bibr" target="#b7">[8]</ref> 0.283 0.873 0.578 COLMAP <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> 0.400 0.664 0.532 SurfaceNet <ref type="bibr" target="#b10">[11]</ref> 0.450 1.040 0.745 MVSNet <ref type="bibr" target="#b35">[35]</ref> 0.396 0.527 0.462 P-MVSNet <ref type="bibr" target="#b19">[20]</ref> 0.406 0.434 0.420 R-MVSNet <ref type="bibr" target="#b36">[36]</ref> 0.383 0.452 0.417 Point-MVSNet <ref type="bibr" target="#b3">[4]</ref> 0.342 0.411 0.376 AA-RMVSNet <ref type="bibr" target="#b29">[30]</ref> 0.376 0.339 0.357 CasMVSNet <ref type="bibr" target="#b8">[9]</ref> 0.325 0.385 0.355 CVP-MVSNet <ref type="bibr" target="#b34">[34]</ref> 0.296 0.406 0.351 UCS-Net <ref type="bibr" target="#b4">[5]</ref> 0 Datasets. We evaluate our model on DTU <ref type="bibr" target="#b0">[1]</ref> and Tanks and Temples <ref type="bibr" target="#b13">[14]</ref> benchmark and finetune on BlendedMVS <ref type="bibr" target="#b37">[37]</ref>. (a) DTU is an indoor MVS dataset with 124 different scenes scanned from 49 or 64 views under 7 different lighting conditions with fixed camera trajectories. We adopt the same training, validation, and evaluation split as defined in <ref type="bibr" target="#b35">[35]</ref>. (b) Tanks and Temples is collected in a more complex realistic environment, and it's divided into the intermediate and advanced set. While intermediate set contains 8 scenes with large-scale variations, advanced set has 6 scenes. (c) BlendedMVS is a large-scale synthetic dataset, which is consisted of 113 indoor and outdoor scenes and is split into 106 training scenes and 7 validation scenes. Implementation. Following the common practice, we first train our model on the DTU training set and evaluate on DTU evaluation set, and then finetune our model on Blend-edMVS before validating the generalization of our approach on Tanks and Temples. The input view selection and data pre-processing strategies are the same as <ref type="bibr" target="#b35">[35]</ref>. Meanwhile, we utilize the finer DTU ground-truth as <ref type="bibr" target="#b29">[30]</ref>. In this paper, UniMVSNet is implemented in 3 stages with 1/4, 1/2 ,and 1 of original input image resolution respectively. We follow the same configuration (e.g., depth interval) of the model at each stage as <ref type="bibr" target="#b8">[9]</ref> in both training and evaluation of DTU. When training on DTU, the number of input images is set to N = 5 and the image resolution is resized to 640 ? 512. To emphasize the contribution of positive signals, we set ? + = 1, and scale the range of S + 5 to [1, 3) and S ? 5 to [0, 1). The other tunable parameters in UFL are configured stage-wise, e.g., ? ? is set to 0.75, 0.5, and 0.25, and ? is set to 2, 1, and 0 from the coarsest stage to the finest stage.. We optimize our model for 16 epochs with Adam optimizer <ref type="bibr" target="#b12">[13]</ref>, and the initial learning rate is set to 0.001 and decayed by 2 after 10, 12, and 14 epochs. During the evaluation of DTU, we also resize the input image size to 1152 ? 864 and set the number of the input images to 5. We report the standard metrics (accuracy, completeness,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-MVSNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CasMVSNet</head><p>UniMVSNet (Ours) GT <ref type="figure">Figure 6</ref>. Qualitative results of scan 15 on DTU. The top row shows the point clouds generated by different methods and the ground-truth, and the bottom row shows a more detailed local region corresponding to the red rectangle. and overall) proposed by the official evaluation protocol <ref type="bibr" target="#b0">[1]</ref>. Before testing on Tanks and Temples benchmark, we finetune our model on BlendedMVS for 10 epochs. We take 7 images as the input with the original size of 768 ? 576. For benchmarking on Tanks and Temples, the number of depth hypotheses in the coarsest stage is changed from 48 to 64, and the corresponding depth interval is set to 3 times as the interval of <ref type="bibr" target="#b35">[35]</ref>. We set the number of input images to 11 and report the F-score metric</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on DTU</head><p>Similar to previous methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref>, we introduce photometric and geometric constraints for depth map filtering. The probability threshold and the number of consistent views are set to 0.3 and 3 respectively, which is the same as <ref type="bibr" target="#b36">[36]</ref>. The final 3D point cloud is obtained through the same depth map fusion method as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref>.</p><p>We compare our method to those traditional and recent learning-based MVS methods. The quantitative results on the DTU evaluation set are summarized in Tab. 1, which indicates that our method has made great progress in performance. While Gipuma <ref type="bibr" target="#b7">[8]</ref> ranks first in the accuracy metric, our method outperforms all methods on the other two metrics significantly. Depth map estimation and point reconstruction of a reflective and low-textured sample are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, which shows that our model is more robust on the challenge regions. <ref type="figure">Figure 6</ref> shows some qualitative results compared with other methods. We can see that our model can generate more complete point clouds with finer details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Tanks and Temples</head><p>As the common practice, we verify the generalization ability of our method on Tanks and Temples benchmark using the model finetuned on BlendedMVS. We adopt a depth map filtering strategy similar to that of DTU, except for the geometric constraint. Here, we follow the dynamic geometric consistency checking strategies proposed in <ref type="bibr" target="#b32">[33]</ref>. Through this dynamic method, those pixels with fewer consistent views but smaller reprojection errors and those with larger errors but more consistent views will also survive.</p><p>The corresponding quantitative results on both intermediate and advanced sets are reported in Tab. 2. Our method achieves state-of-the-art performance among all existing MVS methods and yields first place in most scenes. Notably, our model outperforms the previous best model by 2.68 points and 3.24 points on the intermediate and advanced sets. Such obvious advantages just show that our model not only has the best performance but also exhibits the strongest generalization and robustness. The qualitative point cloud results are visualized in <ref type="figure" target="#fig_5">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Family</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Francis</head><p>Museum Courtroom   <ref type="table" target="#tab_5">Table 3</ref>. Ablation results on DTU evaluation set. "AA" and "FGT" refer to adaptive aggregation and finer ground-truth respectively. "Baseline (Reg)" is the original CasMVSNet <ref type="bibr" target="#b8">[9]</ref>. We set the confidence threshold and the consistent views to 0.3 and 3 for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>As aforementioned, we adopt some extra strategies (e.g., adaptive aggregation and finer ground-truth) that have been adopted by recent methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">38]</ref> to train our model for a fair comparison with them. However, this may not be fair to those methods inherited only from MVSNet. In this section, we will prove through extensive ablation studies that even if these strategies are eliminated, our method still has a significant improvement. We use our baseline CasMVSNet <ref type="bibr" target="#b8">[9]</ref>, whose original representation is regression, as the backbone and changing various components, e.g., depth representation, optimization, aggregation, and ground-truth. And we adopt 5 input views for all models for a fair comparison. Benefits of Unification. As shown in Tab. 3, significant progress can be made even if purely replacing the traditional depth representation with our unification. Meanwhile, unification is more robust when the hypothesis range of the finer stage doesn't cover the ground-truth depth. In this case, the target unity of the unification representation generated by Algorithm 1 is all zero, which is a correct supervision signal anyway, and the traditional representation will generate an incorrect supervision signal to pollute the model training. Meanwhile, our Unification can also generate sharp depth on object boundaries like <ref type="bibr" target="#b27">[28]</ref>. Benefits of UFL. Applying Focal Loss to our representation can effectively overcome the sample imbalance problem. It can be seen from Tab. 3 that GFL has a huge benefit to accuracy, albeit with a slight loss of completeness. And our UFL can further improve the accuracy and completeness significantly on the basis of GFL. More ablation results about UFL are shown in Supp. Mat. Other strategies and less data. Tab. 4 shows that our Uni- fication performs better and is more concise compared to other strategies. Meanwhile, we still achieve excellent performance even with only 50% of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a unified depth representation and a unified focal loss to promote the effectiveness of multi-view stereo. Our Unification can recover finer 3D scene benefits from the direct learning cost volume, and UFL is able to capture more fine-grained indicators for rebalancing samples and deal with continuous labels more reasonably. What's more valuable is that these two modules don't impose any memory or computational costs. Each plug-and-play module can be easily integrated into the existing MVS framework and achieve significant performance improvements, and we have shown this through our UniMVSNet. In the future, we plan to explore the integration of our modules into stereo matching or monocular field and look for more concise loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Explanation of Unified Focal Loss</head><p>As shown in Equation <ref type="formula" target="#formula_14">(9)</ref>, the dedicated function to control the range of scaling factor is designed as the sigmoidlike function as:</p><formula xml:id="formula_16">S b (x) = 1 (1 + b ?x ) (a)</formula><p>where x = |q?u| q + in this paper and its range is [0, +?), therefore, the range of S b (x) is [0.5, 1). As aforementioned, we adopt an asymmetrical scaling strategy to protect the precious positive learning signals and scale the range of S + 5 to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3)</ref> and S ? 5 to [0, 1). The detailed implementation of S + 5 is:</p><formula xml:id="formula_17">S + 5 (x) = 4 ? ( 1 1 + 5 ?x ? 0.5) + 1 (b)</formula><p>and the detailed implementation of S ? 5 is:</p><formula xml:id="formula_18">S ? 5 (x) = 2 ? ( 1 1 + 5 ?x ? 0.5) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Finer DTU Ground-truth</head><p>As mentioned in our main paper, we adopt the finer ground-truth to train our model additionally for a fair comparison with the start-of-the-art methods <ref type="bibr" target="#b29">[30]</ref>. The refinement of each DTU ground-truth is achieved by crossfiltering with its neighbor viewpoints. For convenience, we directly adopt the processed results provided in <ref type="bibr" target="#b29">[30]</ref>, and we only adopt the mask which indicates the validity of each point. Concretely, we adopt the union of the mask provided in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">35]</ref> and the up-sampled mask provided in <ref type="bibr" target="#b29">[30]</ref> as the final mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Ablation Studies on DTU Dataset</head><p>Here, we perform more ablation studies on DTU to show you more information about our implementation. The scaling factor in UFL. The range of scaling factor in Eq. <ref type="formula" target="#formula_13">(8)</ref> is [0, +?). We count the average number and sum of scaling factors that fall in different intervals. As shown in <ref type="figure" target="#fig_6">Fig. A, most</ref> of the scaling factors are less than 4 (Left <ref type="figure">figure)</ref>. Even though, those small fractions of larger scaling factors take more weight (Right <ref type="figure">figure)</ref>. This results in those abnormally large scaling factors occupying the model's training, and lead to difficulty in model convergence and extremely poor performance. Therefore, we introduce a dedicated function to control the scaling factors' range.</p><p>The dedicated function in UFL. As described in our main paper, we design the positive dedicated function as Eq. (b) and negative dedicated function as Eq. (c). In fact, this final implementation is confirmed under our more experimental results. As shown in Tab. A, compared to adopting a common sigmoid function with a base e, it's better to use a dedicated function with a base 5. Meanwhile, scaling the range of dedicated function to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3)</ref> is better than <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2)</ref>. As shown in <ref type="figure" target="#fig_7">Fig. B</ref>, the base number controls the speed at which the function converges to the maximum value. The smaller the base number, the slower the convergence. In our experiments, we found that most of the points whose x = |q?u| q + is in the interval [0, 4], so the scaling value calculated by the dedicated function needs to be distinguishable for the points in this interval, and we set b = 5 in this paper. To be honest, we have only conducted a limited number of the base number the range of dedicated function as shown in Tab. A due to the time and resource considerations, and we believe there will be more powerful configurations. The tunable parameter in UFL. Tunable parameters in UFL like ? and ? are also important for rebalancing samples. As mentioned in our main paper, we always set ? + = 1 to protect the positive learning signals and configure other tunable parameters stage by stage due to the different number of depth hypotheses. In our implementation, we set the number of depth hypotheses to 48, 32, and 8 from stage1 to stage3. Apparently, the sample imbalance in stage1 is the most challenging, while it's the most relaxing or even negligible in stage3. As shown in Tab. B, applying the same configuration across all stages performs the worst which indicates that the imbalances faced by different stages are different. Proximity VS. Offset. Different from the Regression and Classification, we propose Unification to classify the opti-   <ref type="table" target="#tab_5">Table C</ref>. Comparison of proximity and offset. These experiments are conducted on the model only using our Unification. mal depth hypothesis and regress its offset to ground-truth depth simultaneously. As shown in <ref type="figure" target="#fig_8">Fig. C</ref>, there are two ways to regress the offset. The first is to predict proximity which is the complement of the offset and is also the method we adopt in this paper. The second is to directly estimate the offset. The comparison results between them are shown in Tab. C. We can see that adopting proximity to regress the offset indirectly is much more powerful than purely using offset. As mentioned in our main paper, our Unification can be decomposed into two parts: classify the optimal depth hypothesis first and then regress the proximity for it. We infer that the reason why proximity is better than offset is the positive relationship between the magnitude of proximity and the quality of the classified optimal depth hypothesis in the first step. Meanwhile, we think that there should be better settings to improve the performance of using offset, but we have not made more attempts here. For proximity, we use the interval above the depth hypothesis as its regression interval, and for offset, use the area where the depth hypothesis is the median value as the regression interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Comparisons between Unification, Classification and Regression</head><p>(1) Regression methods are harder to converge and have a greater risk of overfitting due to its indirect learning strategy, which has been studied in <ref type="bibr" target="#b42">[42]</ref>. Meanwhile, they tend to generate smooth depth in object boundaries, because they treat the depth as the expectation of the depth hypotheses. However, they can achieve sub-pixel depth estimation, therefore, they have better accuracy. (2) Classification methods cannot generate accurate depth due to their discrete prediction, but they constrain the cost volume directly and achieve better completeness. (3) Our unification is exactly the complement of these two approaches. Take the essence, get rid of the dross. On the one hand, We directly constrain the cost volume to keep the model robust and pick the regression interval with the maximum unity to maintain the sharpness of the object boundary. On the other hand, we regress the proximity in the picked regression interval to generate accurate depth. Therefore, our unification is hoped to achieve regression's accuracy and classification's completeness. The results shown in Tab. 3 just prove this.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Results on DTU Dataset</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of our Unification. The (m ? 1) th hypothesis is the optimal hypothesis for red point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Hardness measurement of different samples. Four typical samples with different ground-truth q and different estimation u.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Depth estimation and point reconstruction of scan 13 on DTU. Our model produces more accurate and complete results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results of some scenes on Tanks and Temples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure A .</head><label>A</label><figDesc>The statistics of scaling factor x = |q?u| q + in Eq.<ref type="bibr" target="#b7">(8)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure B .</head><label>B</label><figDesc>The scaling value obtained through the positive dedicated function under different base numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure C .</head><label>C</label><figDesc>Two available solutions to generate Unity (supervised signal) from the ground-truth depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure D shows our</head><label></label><figDesc>additional point clouds reconstruction results. It can be seen that the point cloud reconstructed by our method has excellent accuracy and completeness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure D .</head><label>D</label><figDesc>More qualitative results on DTU dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 arXiv:2201.01501v3 [cs.CV] 11 Mar 2022 Comparison with different representations at a certain pixel. The purple curve represents different weights, probabilities, and unity of each depth hypothesis obtained by regression, classification and unification respectively. While the regression representation requires the exact weight of each hypothesis to regress the depth, the classification representation only cares about which hypothesis has the maximum probability and unification only needs to know the proximity with the maximum unity. CE denotes cross-entropy, and U F L refers to Unified Focal Loss (Sec. 3.3).</figDesc><table><row><cell>10</cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell></row><row><cell>GT: 8.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell cols="2">0.85</cell></row><row><cell>6 8</cell><cell>0.30 0.35</cell><cell>8.42</cell><cell>8.3</cell><cell>8</cell><cell>0 0 0</cell><cell>0.8</cell><cell>8 + 1 ? 0.8 ? 2 = 8.4</cell><cell>0 0 0</cell></row><row><cell></cell><cell></cell><cell>Estimation</cell><cell>GT</cell><cell>Estimation</cell><cell cols="2">One-hot GT</cell><cell>Estimation</cell><cell cols="2">Unity GT</cell></row><row><cell>4</cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Interval: 2</cell><cell></cell><cell cols="2">= 8.42 ?</cell><cell>=</cell><cell>( ,</cell><cell>)</cell><cell>=</cell><cell>( ,</cell><cell>)</cell></row><row><cell>2</cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Weight</cell><cell></cell><cell>Probability</cell><cell></cell><cell></cell><cell>Unity</cell><cell></cell><cell></cell></row><row><cell>Depth Hypotheses</cell><cell cols="2">Regression</cell><cell cols="3">Classification</cell><cell></cell><cell>Unification</cell><cell></cell></row><row><cell>Figure 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on DTU evaluation set. Best results in each category are in bold. Our model ranks first in terms of Completeness and Overall metrics.</figDesc><table><row><cell></cell><cell>.338</cell><cell>0.349</cell><cell>0.344</cell></row><row><cell>UniMVSNet (ours)</cell><cell>0.352</cell><cell>0.278</cell><cell>0.315</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>76.37 58.45 46.26 55.81 56.11 54.06 58.18 49.51 31.12 19.81 38.46 29.10 43.87 27.36 28.11 ACMP [32] 58.41 70.30 54.06 54.11 61.65 54.16 57.60 58.12 57.25 37.44 30.12 34.68 44.58 50.64 27.20 37.43 D 2 HC-RMVSNet [33] 59.20 74.69 56.04 49.42 60.08 59.81 59.61 60.77.40 60.23 47.07 63.44 62.21 57.28 60.54 52.07 33.78 20.79 38.77 32.45 44.20 28.73 37.70 AA-RMVSNet [30] 61.51 77.77 59.53 51.53 64.02 64.05 59.47 60.85 55.50 33.53 20.96 40.15 32.05 46.01 29.28 32.71 EPP-MVSNet [21] 61.68 77.86 60.54 52.96 62.33 61.69 60.34 62.44 55.30 35.72 21.28 39.74 35.34 49.21 30.00 38.75 UniMVSNet (ours) 64.36 81.20 66.43 53.11 63.46 66.09 64.84 62.23 57.53 38.96 28.33 44.36 39.74 52.89 33.80 34.</figDesc><table><row><cell>Method</cell><cell>Mean Fam.</cell><cell>Fra.</cell><cell>Hor.</cell><cell>Intermediate Lig. M60</cell><cell>Pan.</cell><cell>Pla.</cell><cell>Tra.</cell><cell cols="2">Mean Aud.</cell><cell>Bal.</cell><cell cols="2">Advanced Cou. Mus.</cell><cell>Pal.</cell><cell>Tem.</cell></row><row><cell>Point-MVSNet [4]</cell><cell cols="7">48.27 61.79 41.15 34.20 50.79 51.97 50.85 52.38 43.06</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PatchmatchNet [29]</cell><cell cols="13">53.15 66.99 52.64 43.24 54.87 52.87 49.54 54.21 50.81 32.31 23.69 37.73 30.04 41.80 28.31 32.29</cell></row><row><cell>UCS-Net [5]</cell><cell cols="7">54.83 76.09 53.16 43.03 54.00 55.60 51.49 57.38 47.89</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CVP-MVSNet [34]</cell><cell cols="7">54.03 76.50 47.74 36.34 55.12 57.28 54.28 57.43 47.54</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>P-MVSNet [20]</cell><cell cols="7">55.62 70.04 44.64 40.22 65.20 55.08 55.17 60.37 54.29</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CasMVSNet [9]</cell><cell cols="7">56.84 04 53.92</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VisMVSNet [41]</cell><cell>60.03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>63 Table 2. Quantitative results of F-score on Tanks and Temples benchmark. Best results in each category are in bold. "Mean" refers to the mean F-score of all scenes (higher is better). Our model outperforms all previous MVS methods with a significant margin on both Intermediate and Advanced set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Some other Ablation results.</figDesc><table><row><cell>Method</cell><cell cols="5">Confidence Consistent View ACC.(mm) Comp.(mm) Overall(mm)</cell></row><row><cell>Baseline (Cla) + Regress finetune</cell><cell>0.3</cell><cell>3</cell><cell>0.371</cell><cell>0.295</cell><cell>0.333</cell></row><row><cell>Baseline (Uni) + UFL (50% data)</cell><cell>0.3</cell><cell>3</cell><cell>0.364</cell><cell>0.284</cell><cell>0.324</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A .</head><label>A</label><figDesc>Ablation results of dedicated function. While the base number is ablated for both the positive and negative dedicated function, we only ablate the range of positive dedicated function.</figDesc><table><row><cell cols="5">Base Number Range ACC.(mm) Comp.(mm) Overall(mm)</cell></row><row><cell>e</cell><cell>[1, 2)</cell><cell>0.354</cell><cell>0.282</cell><cell>0.318</cell></row><row><cell>5</cell><cell>[1, 2)</cell><cell>0.354</cell><cell>0.280</cell><cell>0.317</cell></row><row><cell>5</cell><cell>[1, 3)</cell><cell>0.352</cell><cell>0.278</cell><cell>0.315</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table B .</head><label>B</label><figDesc>Ablation results of tunable parameters. These experiments are conducted on the model with only our Unification and UFL, and don't adopt the finer DTU ground-truth or adaptive aggregation.</figDesc><table><row><cell cols="6">? ? stage1 stage2 stage3 stage1 stage2 stage3 ?</cell><cell cols="3">ACC.(mm) Comp.(mm) Overall(mm)</cell></row><row><cell>0.75</cell><cell>0.75</cell><cell>0.75</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>0.347</cell><cell>0.325</cell><cell>0.336</cell></row><row><cell>1</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0.366</cell><cell>0.282</cell><cell>0.324</cell></row><row><cell>0.75</cell><cell>0.50</cell><cell>0.25</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0.353</cell><cell>0.287</cell><cell>0.320</cell></row><row><cell cols="2">Method</cell><cell cols="7">ACC.(mm) Comp.(mm) Overall(mm)</cell></row><row><cell cols="2">Offset</cell><cell></cell><cell>0.429</cell><cell></cell><cell></cell><cell>0.336</cell><cell></cell><cell>0.383</cell></row><row><cell cols="2">Proximity</cell><cell></cell><cell>0.372</cell><cell></cell><cell></cell><cell>0.282</cell><cell></cell><cell>0.327</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Thanks to the National Natural Science Foundation of China 62072013 and U21B2012, Shenzhen Research Projects of JCYJ20180503182128089 and 201806080921419290, Shenzhen Cultivation of Excellent Scientific and Technological Innovation Talents RCJC20200714114435057,Shenzhen Fundamental Research Program (GXWD20201231165807007-20200806163656003). In addition, we thank the anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale data for multiple-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Rasmus Ramsb?l Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders Bjorholm</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using multiple hypotheses to improve depth-maps for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Hern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="766" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pointbased multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1538" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep stereo using adaptive thin volume representation with uncertainty awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2524" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Object-centered surface reconstruction: Combining multi-image stereo and shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leclerc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="35" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1362" to="1376" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepmvs: Learning multi-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2821" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Surfacenet: An end-to-end 3d neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A theory of shape by space carving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kiriakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="218" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A quasi-dense approach to surface reconstruction from uncalibrated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Lhuillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="418" to="433" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="10452" to="10461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Epp-mvsnet: Epipolar-assembling based depth prediction for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5732" to="5740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time visibility-based fusion of depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Merrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Akbarzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippos</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nist?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Richard A Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international symposium on mixed and augmented reality</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structurefrom-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="501" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparison and evaluation of multi-view stereo reconstruction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Photorealistic scene reconstruction by voxel coloring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles R</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smd-nets: Stereo mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolin</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8942" to="8952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Patchmatchnet: Learned multi-view patchmatch stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangjinhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Speciale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14194" to="14203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhuang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingtian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-scale geometric consistency guided multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5483" to="5492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Planar prior assisted patchmatch multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12516" to="12523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhuang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dense hybrid recurrent multi-view stereo net with dynamic consistency checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cost volume pyramid based depth inference for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4877" to="4886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mvsnet: Depth inference for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent mvsnet for high-resolution multiview stereo depth inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5525" to="5534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Blendedmvs: A largescale dataset for generalized multi-view stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1790" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pyramid multi-view stereo net with self-adaptive view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhuang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runze</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast-mvsnet: Sparse-todense multi-view stereo with learned propagation and gaussnewton refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1949" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Feras Dayoub, and Niko Sunderhauf. Varifocalnet: An iou-aware dense object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visibility-aware multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive unimodal cost volume filtering for deep stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suihanjin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
