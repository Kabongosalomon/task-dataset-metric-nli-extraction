<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><forename type="middle">Mohammad</forename><surname>Rostami</surname></persName>
							<email>[a.m.rostami@aut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Amirkabir University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Mehdi Homayounpour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Amirkabir University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Nickabadi</surname></persName>
							<email>nickabadi]@aut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Amirkabir University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Automatic speaker verification</term>
					<term>Spoof Detec- tion</term>
					<term>ASVspoof</term>
					<term>Efficient Attention Branch Network</term>
					<term>Com- bined Loss Function</term>
					<term>EfficientNet-A0</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many endeavors have sought to develop countermeasure techniques as enhancements on Automatic Speaker Verification (ASV) systems, in order to make them more robust against spoof attacks. As evidenced by the latest ASVspoof 2019 countermeasure challenge, models currently deployed for the task of ASV are, at their best, devoid of suitable degrees of generalization to unseen attacks. Upon further investigation of the proposed methods, it appears that a broader three-tiered view of the proposed systems; comprised of the classifier, feature extraction phase, and model loss function, may to some extent lessen the problem. Accordingly, the present study proposes the Efficient Attention Branch Network (EABN) modular architecture with a combined loss function to address the generalization problem. The EABN architecture is based on attention and perception branches; the purpose of the attention branch-also interpretable from a human's point of view-is to produce an attention mask meant to improve classification performance. The perception branch, on the other hand, is used for the primary purpose of the problem at hand, that is, spoof detection. The new EfficientNet-A0 architecture was employed for the perception branch, with nearly ten times fewer parameters and approximately seven times fewer floating-point operations than the top performing SE-Res2Net50 network. The final evaluation results on ASVspoof 2019 dataset suggest an EER = 0.86% and t-DCF = 0.0239 in the Physical Access (PA) scenario using the log-PowSpec input feature, the EfficientNet-A0 for the perception branch, and the combined loss function. Furthermore, using the LFCC input feature, the SE-Res2Net50 for the perception branch, and the combined loss function, the proposed model performed at figures of EER = 1.89% and t-DCF = 0.507 in the Logical Access (LA) scenario, which to the best of our knowledge, is the best single system ASV spoofing countermeasure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Remote authentication has excited great interests in various academic circles and otherwise, given the increasing reliance on online applications as well as the onset of certain conditions such as the Covid-19 pandemic. Such circumstances call for an easy-to-use, accurate, and efficient authentication system. Along this thread, Automatic Speaker Verification (ASV) system and other biometric systems such as face recognition, electronic signatures, iris-based, and hybrid methods have been proposed as a means to satisfy user needs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Nevertheless, virtually all of these systems are vulnerable to spoof attacks (i.e., spoofable). A system based on face recognition for example, may be spoofed by simply displaying a person's image (photo) to the system <ref type="bibr" target="#b2">[3]</ref>. Likewise, a fingerprint system can be spoofed by copying a fingerprint. In particular, ASV systems are also vulnerable in the face of four types of attacks, counting on recording and replaying the voice of the authorized person (replay attack), text-to-speech systems that are trained with the voice of the targeted person, voice conversion systems, and speaker imitation <ref type="bibr" target="#b3">[4]</ref>. The threats facing ASV systems in terms of spoof attacks are potentially high and may amass to serious implications <ref type="bibr" target="#b4">[5]</ref>. In consequence, since 2015, ASVspoof challenges were held for research communities worldwide to try and enhance ASV systems so as to make them robust against spoofing attacks.</p><p>A total of three ASVspoof challenges have thus far been held, with the first instance in 2015, covering only speech synthesis and voice conversion (also called logical access scenario) attacks <ref type="bibr" target="#b5">[6]</ref>. A variety of methods and systems were proposed and implemented by ASV organizers to produce spoof samples, exciting the interest of many researchers intrigued by both the challenge and the dataset provided therein. The second ASVspoof challenge held in 2017, focused more on the replay attack (also called physical access scenario) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. In order to be able to test the performance of countermeasure systems in real conditions, the organizers produced the dataset in different environmental conditions and using different devices. Further comprehensive conditions were investigated in 2019 to account for all three attacks considered in previous challenges <ref type="bibr" target="#b8">[9]</ref>, ushering in the development of an extensive dataset using state-ofthe-art voice conversion and speech synthesis systems. Spoofing samples in this challenge were more realistic and challenging in view of the improvements made to the spoof systems in previous years. For replay attacks, in particular, samples were produced with greater degrees of control, and a tandem detection cost function (t-DCF) metric was used as the primary metric to assess the efficiency of integrating countermeasures with ASV systems.</p><p>Models proposed to assess the ASVspoof 2019 dataset can be categorized into two main classes: methods based on extraction and engineering of features and methods based on classifier architecture. Methods of the first category incorporate features such as Mel-filter frequency Cepstral coefficients (MFCC), Inverted Mel-Filter Frequency Cepstral coefficients (IMFFC), Constant Q Cepstral Coefficients (CQCC), Group Delay (GD) gram, Instantaneous Amplitude (IA), Instantaneous Frequency (IF), X-vectors, and features from deep learning models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Some methods also use raw signals to extract features using methods such as SincNet <ref type="bibr" target="#b17">[18]</ref> or Variational Auto Encoder (VAE) <ref type="bibr" target="#b18">[19]</ref>. The second category deals with a variety of classifiers such as Neural networkbased methods including VGG <ref type="bibr" target="#b17">[18]</ref>, Squeeze-Excitation (SE), Residual network, Siamese networks <ref type="bibr" target="#b19">[20]</ref>, and recurrent networks <ref type="bibr" target="#b20">[21]</ref>, as well as other traditional GMM-based methods. Certain methods have also used end-to-end structures for this purpose.</p><p>Inquiries made into the 2019 ASVspoof dataset results are suggestive of two primary drawbacks of the proposed methods. The first points to a lack of generalization and high error rate against unseen attacks, which is clearly observed given the difference between errors obtained for the training, development, and evaluation sets. In addressing this lack of generalization, numerous studies have tried to improve generalization by means of fusing several models (ensemble models) <ref type="bibr" target="#b21">[22]</ref>. Such fusion and ensemble models and methods that use deep neural networks have led to considerable increases in model parameters as well as the necessary floating-point operations <ref type="bibr">(FLOPS)</ref>. Under such circumstances, it would be infeasible to use the proposed models in specific applications. This provides the required grounds for the integration of simple yet efficient countermeasure techniques with ASV systems to make ASV robust. Moreover, the proposed body of research fails to provide a detailed understanding for how models detect spoof attacks or handle generalization issue. This ambiguity can be interpreted in terms of the incapacity of humans or rather human-oriented decision making to differentiate between the spoofed and the bonafide samples detected by the final system. A detailed examination of this issue can provide further insight into the development of better system.</p><p>The primary purpose of this work is to provide a model for detecting spoofing attacks on ASV systems. An interpretable attention mask in a new modular architecture was used for this purpose via the introduction of perception and attention branches. Furthermore, for the first time in this domain, the EfficientNet-A0 <ref type="bibr" target="#b22">[23]</ref> architecture was employed to achieve a system with low number of parameters and FLOPS. The proposed architecture along with the newly combined loss function and masks that provide a more human-oriented perspective, was used to obtain comparable and, in some cases, top-performing results in these spoofing attacks.</p><p>The following section provides a brief review of relevant studies conducted in recent years. The proposed countermeasures and the loss function are introduced in Section 3 and Section 4 calls attention to the general configuration used for experiments. Section 5 gives the analysis results along with a summarization of the work. The study is finally concluded in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>This section reviews some of the research carried out on spoof attack detection, taking a look on the best-performing methods, as per results obtained on the ASVspoof 2019 dataset. Similar models and tasks were also investigated inclusive of new architecture and the application of attention mechanisms and attitude in the loss function.</p><p>Cheng-I Lai et al. proposed a deep model to obtain discriminative features in both time and frequency domains <ref type="bibr" target="#b23">[24]</ref>. The proposed design includes a filter-based attention mechanism used to improve or ignore commonly extracted features implemented in the ResNet architecture to classify attended input maps. The reserved classifier used in their study (Residual Network) consists of a convolution layer equipped with dilated mechanism instead of a fully connected layer, which runs as an attentive filtering network; i.e., masks input features. The obtained results were suggestive of the relatively high performance of the model given the use of an attention mechanism to produce attention masks as well as an appropriate classifier.</p><p>X. Li et al. attempted to use the Res2Net architecture, which has achieved significant results in various computer vi-sion tasks <ref type="bibr" target="#b24">[25]</ref>. They proposed a new Res2Net architecture by revisioning ResNet blocks to allow for multi-scale features. In a Res2Net architecture, input feature maps of a block are divided into several groups of channels with a similar residual structure to the original ResNet. Using channels, feature map sizes can be different, increasing the covered area, and thereby yielding features with different scales. This modification improves system performance and the model's generalization against unseen attacks. In addition, using this architecture could reduce the size or number of model parameters relative to the original ResNet structure while improving model performance. The obtained results show that the Res2Net50 model performs better than the ResNet34 and ResNet50 models in both physical and logical access scenarios. They also showed that integrating the block with Squeeze-and-excitation (SE), which produces SE-Res2Net blocks, leads to better performance. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the architecture and structure of these blocks. Significant results were also obtained in both scenarios for the proposed SE-Res2Net50 network based on SE-Res2Net blocks and CQT feature. The network proposed in this work has nearly 0.9 million parameters, which is relatively small compared to other architectures. However, the main drawback to the model is the high number of FLOPS, which leads to increased runtime in the inference phase due to the multiplicity of blocks and the structure of SE-Res2Net. Zhang et al. focused on logical attacks in their work <ref type="bibr" target="#b25">[26]</ref>, explaining the lack of model generalization against unseen attacks as caused by the formulation of the spoof detection problem as a binary classification. The difficulty with using a binary classifier can be interpreted in terms of the distribution of training and test data for spoof and bonafide samples as not being the same. More specifically, samples in the test set generated by new systems or conditions not found in training data cause differences in distribution; which, however, is not the case for bonafide samples. The problem was, therefore, redefined as a one-class classification problem, where the distribution of a target class for a specific problem should be the same in both training and test datasets, irrespective of whether other classes have similar distributions or not. In such cases, the primary objective is to obtain the bonafide distribution and define a rigid decision boundary around it so that unseen samples from other classes cannot cross that decision boundary. To this aim, a one-class softmax loss function was incorporated for learning a feature space that can map bonafide samples in a dense space, while maintaining a good margin with spoofing samples. Finally, by means of the ResNet-18 network and the LFCC features, the authors succeeded in attaining top-performing results for logical access attacks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>The overall architecture of the proposed network was designed with three main objectives in mind; that the architecture be small enough to explicate an appropriate number of parameters, while maintaining an acceptable runtime in order to achieve satisfactory performance in most ASV applications; the architecture was designed to be interpretable by humans. To put differently, the architecture was required to somehow express what discriminates bonafide speech from speech made in a spoof attack as a means to improve systems in the future; lastly, the model was configured to emulate comparable performance to relevant classifiers used for this purpose.</p><p>To achieve all these goals, the Efficient Attention Branch Network (EABN) was proposed in this study. The intended framework adopts a well-performed Attention Branch Network <ref type="bibr" target="#b26">[27]</ref> in computer vision as the main idea for the EABN architecture. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, this network consists of two branches of attention and perception. The attention branch seeks to improve the performance of the perception branch by means of producing an attention mask, which is then applied to make the discriminative parts of the input feature map more. In addition to improving the performance of the perception branch, masks produced by the attention branch are also interpretable from a human point of view; as these masks are used for dummy classification tasks. The primary work load is performed in the perceptual branch, where the probability output of each class is produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Branch</head><p>The attention branch itself comprises of two main parts, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. As can be observed, the input feature map is initially fed into the attention branch, which uses four consecutive Basic Blocks to extract the appropriate features and to convert the input features to 16-feature maps. The blocks consist of two convolution layers with 3?3 kernels, which are then linked to the batch normalization layer. In addition to feature extraction, the first convolution layer also increases the feature map size, while the second convolution layer exclusively handles the feature extraction process. The obtained feature maps are eventually transformed from a 16-size map to a single feature using a convolution layer with a 1?1 kernel, which then goes through a softmax layer to yield the final output attention mask.</p><p>The other branch produces a human-interpretable attention mask. This is carried out by using a convolution operation to transform the 16 feature maps into maps the same size as the number of classes for the problem-which in this study includes the bonafide and spoof classes. Then, using a Global average Pooling layer, these two feature maps are converted to a 2?1 tensor. Finally, by applying softmax, the probability of a feature map belonging to each class is obtained. These probabilities are later used in the optimization process for the proposed combined loss function. Through the process of optimization, feature maps are generated so that in addition to helping the perception branch, they can also be used for classification and be made interpretable from a human perspective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Perception Branch</head><p>The perception branch can constitute virtually any classifier. However, as the primary objectives of this study call for low number of parameters, low runtime, and good performance in network design, the EfficientNet architecture-commonly introduced as a high performing model in image classification tasks and speech processing tasks such as speech recognition and keyword spotting-was employed. The fundamental architecture of the EfficientNet family is called EfficientNet-B0, which has about 4 million parameters. This number of parameters is not suitable for the target applications of this study. Alternatively, the approach introduced in the EfficientNet-Absolute Zero work <ref type="bibr" target="#b22">[23]</ref>, which applies the reverse of the compound scaling method, was used. The scaling method (S) is designed to shrink a base model (M ) by decreasing the depth (?), width (?), and resolution of the input image (?), simultaneously. A formulation of this method is given below as an optimization problem, in which the goal is to satisfy the intended conditions so that the final model has the best performance. </p><formula xml:id="formula_0">s.t. 1 20 ? ? ? ? 2 ? ? 2 ? 1 16 0.2 ? ?, ? ? 0.6, ? = 2</formula><p>The two parameters ? and ? are set by applying a grid search on intervals [0.2 ? 0.6] with steps of 0.005. Eventually, 19 models were evaluated with a small subset of samples, with parameters ? and ? set at values 0.2 and 0.25, respectively. ? was also set at ? 2, given the input image size (513?400) and EfficieNet-B0 input-size of 256?256. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the final model obtained for the perception branch with 95,000 parameters. The input to this branch is m(xi), where xi is the input image for the i th sample and is calculated from the following equation:</p><formula xml:id="formula_1">m (xi) = (1 + g (xi)) ? xi where g(xi)</formula><p>is the attention mask produced for the i th sample by attention branch. The output of this network is a vector of length 256, which represents the embedded vector of the input image and is applied for two scenarios: the first uses the vector along with a fully connected layer and the softmax layer to yield probabilities for each individual sample; the second scenario uses the vector as input to a loss function. Thus, samples are embedded in a 256-dimensional space in the most distinctive way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss function</head><p>To train model parameters, a combined loss function was used to account for all study objectives. To train an attention branch capable of producing interpretable masks, the ABoutput was used as input to a weighted Cross-Entropy (CE) loss function. It should be noted that by introducing the proposed loss function with coefficient ?AB, values in equation 3 are altered. Proceeding forward, the Triplet Center Loss (TCL) function was used to train the embedding vectors. TCL works in the same way as the triplet loss function, except that it no longer needs to mine triplets for training, and this difference makes the training process faster and more stable. This loss function considers center points for each class in the problem, which are initially assigned random values. The loss function then approaches to make it so that samples of one class are close to the center of their class and away from the nearest center of other classes. In other words, each sample tries to be closer to the center of its class and away from the remaining centers. The two centers used in this study to represent spoof and bonafide samples were C spoof and C bonaf ide , respectively. The goal here was to ensure that bonafide samples be close to the center of their respective target class, C bonaf ide , and away from C spoof . As a result, samples of a specific class in a dense space are closer to each other; representing feature vectors embedded for each sample in the desired space. TCL can be obtained for the xi sample as follows: </p><formula xml:id="formula_2">Lfocal (pt) = ??t (1 ? pt) 0.005 log (pt) (1) LP B (xi) = Ltc (xi) + ?focal Lfocal (xi) (2) Ltotal = LP B + ?ABLAB<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Dataset and evaluation metrics</head><p>The proposed method was evaluated using the ASVspoof 2019 dataset, which includes two scenarios: physical access (PA) and logical access (LA). Details of this dataset are shown in <ref type="table" target="#tab_0">Table  1</ref>. Furthermore, considering that one of the objectives of this research is the simultaneous use of countermeasure and ASV system, the tandem-detection cost function (t-DCF) and the equal error rate (EER) metrics were used. This metric was introduced as the primary evaluation metric of the 2019 challenge, which is calculated as:</p><formula xml:id="formula_3">t ? DCF(s) = C1P cm miss (s) + C2P cm fa (s)</formula><p>where P cm fa (s) and P cm miss are the false acceptance error rate and the false rejection error rate of the countermeasure, respectively. Considering the threshold, s, values for the two error rates can be obtained as follows: The two constants C1 and C2 represent the predefined cost for the errors, which are determined based on prior probabilities as shown below:</p><formula xml:id="formula_4">C1 = ?tar (C cm miss ? C asv miss P asv miss ) ? ?nonC asv fa P asv fa C2 = C cm fa ? spoof 1 ? P asv miss,spoof</formula><p>Here, C asv miss represents the cost incurred by the error of the ASV system for the false rejection error rate of the genuine person, and C asv fa represents the false acceptance error rate when ASV authorizes the wrong person. Each countermeasure error also corresponds to two costs; C cm miss , which indicates the cost in recognizing a bonafide sample as a spoof, and C cm fa , which indicates a mistake in accepting a sample produced by a spoof system as bonafide. In addition, the probability of occurrence of any class of genuine (?tar), non-target or imposter (?non) and spoof attack (? spoof ) are also considered with the condition ?tar + ?non + ? spoof = 1. Cost and probability values are calculated as in <ref type="table" target="#tab_1">Table 2</ref>. Here, 20ms frames with 512 Fourier transform points and 20 filters were used along with their first and second derivatives. Finally, a two-dimensional tensor with dimensions of 60?400 was obtained.</p><p>As a further step, specAug <ref type="bibr" target="#b27">[28]</ref> techniques were applied for better training and generalization. This method has worked well for other speech tasks, such as speaker verification, speech recognition, and keyword potting <ref type="bibr" target="#b22">[23]</ref>. The method was implemented by applying zero masks on the time and frequency axis for each training sample with a probability of 0.25. The size of this band is randomly between 20 and 80 frames on the time axis. In the frequency axis; the size of this band is selected randomly between 5 and 20 for the LFCC and between 25 and 100 for the logPowSpec. As a result, the model is capable of accounting for all time frames and frequency bins and preventing overfitting at specific time or frequency points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Perception branch models</head><p>In addition to the proposed EfficientNet-A0 architecture, a SE-ResNet50 architecture was also used, which achieved significant results. The models were then compared in terms of both efficiency and performance, and the EABN modularity idea was evaluated accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training procedure</head><p>The final results obtained from experiments on small subsets of the ASVspoof 2019 dataset yielded values of 0.1, 0.005, and 32 for ?AB, ? f ocal and m, respectively. To optimize the loss function with assigned values, configurations for the SE-ResNet50 architecture were adopted. In the case of Adam optimization, ?1, ?2, and learning rate were obtained at 0.9, 0.98, and 10 ?9 , respectively. The learning rate initially drops linearly for the first 1000 steps and then decreases in proportion to the inverse of the square root of the number of steps. All models were trained with 40 epochs and the model with the lowest EER on the development set of the dataset was selected as the optimal choice. Batch-sizes were set at 64 and 128 when using EfficienNet-A0 as the perception branch module with LFCC and logPowSpec, respectively. Due to the relatively greater number of parameters for the SE-Res2Net50 model compared to EfficientNet-A0, a batch-size of 8 was used for LFCC and logPowSpec features. The models were implemented on a GTX-1080ti GPU on Linux OS. The source code of our implementations based on Python and Pytorch is publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Perception branch's models evaluation</head><p>This section evaluates the overall architectural EABN and the EfficientNet-A0 network as a classifier for spoof detection. To investigate EABN performance, the EfficientNet-A0 and SE-ResNet50 architectures were used for the perception branch, which have the lowest EER as a single model to the best of our knowled.ge. The results for both attacks are shown in <ref type="table" target="#tab_2">Table 3</ref>. In the PA scenario, EfficientNet-A0 showed better performance than SE-ResNet50 and nearly ten times fewer parameters and seven times fewer FLOPS. On the other hand, the SE-ResNet50 model performs better when using LFCC feature for the LA scenario. This can be explained in terms of the enhanced performance of the EfficientNet-A0 model in extracting features from the spectrogram. On the other hand, the SE-ResNet50 model works best when the feature input is processed, and the EfficientNet-A0 model converges very quickly or suffers from overfitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Loss function</head><p>The proposed combined loss function was used for the first time in this work to achieve a discriminative vector space to distinguish spoof samples from bonafide samples. More precisely, the triplet center loss was used to map input samples to a discriminative space. As shown in <ref type="figure" target="#fig_6">Figure 5</ref>, the training samples mapping space is suitable for the classification problem. Examining test samples that include unseen attacks also demonstrate that the resulting space is reasonably discriminative. It can therefore be said that the model shows good generalization </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Attention masks</head><p>One of the main concerns about the proposed architecture was to obtain attention masks that could be interpreted from a human point of view. This was investigated for the LFCC feature, with the averaged masks generated for all samples in the evaluation set shown in <ref type="figure">Figure 6</ref>. As for the logPowSpec mask, few samples from the evaluation set of the PA attack are shown in <ref type="figure">Figure 7</ref>. The raw input features and results of the applied mask on the input feature, which is input for the perception branch, are also shown in this figure. Examining the LFCC feature masks obtained for different attack systems reveals that higher-frequency domain information is more effective in detecting spoof patterns. This is consistent with the fact that models using LFCC features tend to outperform those that work with MFCCs in most studies. As presented by the findings, resolution values are lower at high frequencies for MFCC than LFCC. Examining the masks created for physical access attacks also show that the model emphasizes silent parts of speech. This is in all likelihood to the fact that the effects of record and play devices on recorded speech are more considerable during silences, and the countermeasure can more easily detect attacks accordingly. Convolution operations in the perception branch also tend to converge faster by blurring and dominating some values at different frequencies. This can in effect be achieved by decreasing the impacts of frequencies that show lower capacity to discriminate spoof attacks from bonafide samples. In this regard, it can be said that paying attention to silence intervals and reducing the impact of human speech frequencies lead to better detection of physical access attacks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with other single models</head><p>The proposed models have been compared with some of the single models and the baseline models according to the presented objectives. Some of the top-performing models used for relevant purposes are shown and compared with the proposed model in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>For the LA attack, the LFCC+SEResABNet+CombLoss model achieved an EER=1.89% and t-DCF=0.507, which outperforms the baseline model LFCC-GMM. The proposed model also improved by approximately 0.98% from its corresponding base model (LFCC+SEResNet50+CE). Also, by comparing the results obtained with other works, it can be seen that this model outper- forms LFCC+ResNet18+OCS, which to the best of our knowledge, shows state-of-the-art performance. For physical access attacks, the LogPowSpec+EFFA0+CombLoss model achieved EER=0.86% and t-DCF=0.0239. This result is significantly better than the base models. Compared to results reported in the 2019 challenge, the proposed model also appears to outperform 90% of methods which use fusion models. These results, and other favorable features such as fewer parameters and shorter runtime compared to other models, prove the efficiency of the proposed EABN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Spoof detection is considered a major security concern in authentication systems, particularly the ASV system, demonstrating a clear need for solutions to combat this issue. There are generally two approaches to detecting spoofing attacks on ASV systems: the first is to develop an appropriate classifier targeted specifically at detecting the mentioned attacks, while the second approach is conducted as a preliminary step for extracting discriminative features. In the case of the former, most classifiers fail to consider the issue of optimality in terms of number of parameters and runtime. On the other hand, most proposed models are not interpretable from a human perspective, and features are chosen according to expert's knowledge, and therefore lack generalization to unseen attacks. However, a modular architecture based on branches of attention and perception gives the system the ability to easily utilize any classifier or method to produce an interpretable attention mask and improve classification. To this end, the proposed combined loss function, particularly the triplet center loss, succeeded in yielding a discriminative feature space that can help achieve a more generalized model for unseen attacks. The proposed model and loss function were evaluated on ASVspoof 2019 data. Using LogPowSpec and LFCC features, along with the first-time use of the EfficientNet-A0 architecture and the well-performed SE-Res2Net50, this study provides a novel method for detecting spoofs. The findings show that, the LFCC+SEResNet50+CE model runs with an EER of 1.89% and t-DCF of 0.507 in the logical access scenario, which to the best of our knowledge, outperforms all state-of-the-art methods. The EFFA0+CombLoss also obtained an EER of 0.86% and t-DCF of 0.0239 for the physical access scenario, which is better than 90% of the models presented for the ASVspoof 2019 challenge. It worth noting that the EfficientNet-A0 consists of only 95,000 parameters. The findings also shed light on certain special cases observed for the produced attention masks. For example, LFCC features outperformed MFCCs in detecting logical access attacks. Alternatively, to detect replay attacks, focusing more on silent segments and frequencies in the human speech frequency range can improve the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>ResNet, Res2Net, and SE-Res2Net blocks<ref type="bibr" target="#b24">[25]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Proposed Efficient Attention Branch Network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Proposed architecture for Attention branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>max d,r,w Accuracy(S(M, d, r, w))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Proposed architecture for perception branch operating via the reverse compound scaling method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>P cm miss (s) = #{ bona fide trials with CM score ? s} #{ Total bona fide trials } P cm fa (s) = #{ spoof trials with CM score &gt; s} #{ Total spoof trials }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Feature embedding visualization of our proposed loss function for evaluation (a) and training (b) sets of the ASVspoof 2019 LA attack. Features were reduced to 2-D space using PCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Average of produced LFCC attention masks for some spoof attacks in ASVspoof 2019 evaluation. As can be seen, the attention branch tries to dominant high frequencies to detect spoof attacks. Input (Inp.) feature, produced attention (Att.) mask, and final input feature for perception (Perc.) branch of some samples in the evaluation set for logPowSpec feature (B is bonafide class). Red boxes are parts of input features that the attention branch wants to dominate and are interpretable from a human's perspective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the ASVspoof2019 dataset.</figDesc><table><row><cell>Partition</cell><cell cols="4">PA # Spoof # Bonafide # Spoof # Bonafide LA</cell></row><row><cell>Train</cell><cell>48600</cell><cell>5400</cell><cell>22800</cell><cell>2580</cell></row><row><cell>Dev</cell><cell>24300</cell><cell>5400</cell><cell>22296</cell><cell>2548</cell></row><row><cell>Eval</cell><cell>116640</cell><cell>18090</cell><cell>63882</cell><cell>7355</cell></row><row><cell cols="4">4. Experimental configuration</cell><cell></cell></row><row><cell>4.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>t-DCF hyperparameters value.</figDesc><table><row><cell>Attack Type</cell><cell>?tar</cell><cell>Probabilities ?non</cell><cell cols="3">ASV costs ? spoof C asv fa C asv miss</cell><cell cols="2">Countermeasure costs C cm fa C cm miss</cell></row><row><cell>PA</cell><cell cols="2">0.9405 0.0095</cell><cell>0.05</cell><cell>10</cell><cell>1</cell><cell>10</cell><cell>1</cell></row><row><cell>LA</cell><cell cols="2">0.9405 0.0095</cell><cell>0.05</cell><cell>10</cell><cell>1</cell><cell>10</cell><cell>1</cell></row><row><cell cols="2">4.2. Feature extraction and engineering</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Based on past researches and works, a single acoustic feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">was considered for each of the attacks. For the PA scenario,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">we used the logarithm power of the spectrogram (logPowSpec)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">with 25 ms frames, 10ms step size with 1024 samples (with</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">zero padding applied if needed), using Hamming window. All</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">the samples are first transformed into 4s voice segments. To</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">do this, samples that are less than 4 seconds are repeated to</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">achieve a 4s segment. Longer samples are also divided into 4s</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">segments with no overlap, and each segment is considered an</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">individual utterance. The final input form consists of a spec-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">trogram with 513?400 dimensions. For the LA scenario, the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">LFCC feature was extracted according to the procedure used</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">in the base model presented in the ASVspoof 2019 challenge.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Result of models used in perception branch and input features on ASVspoof 2019 evaluation dataset for PA and LA scenarios. K, M, and G represent Kilo, Mega, and Giga, respectively.</figDesc><table><row><cell cols="4"># Perception branch model Input feature #Parameters</cell><cell>#Flops</cell><cell cols="4">PA EER(%) t-DCF EER(%) t-DCF LA</cell></row><row><cell>1 2</cell><cell>EfficientNet-A0 EfficientNet-A0</cell><cell>LFCC LogPowSepc</cell><cell>95k</cell><cell>198M 1.696G</cell><cell>-0.86</cell><cell>-0.0239</cell><cell>3.68 -</cell><cell>0.0931 -</cell></row><row><cell>3 4</cell><cell>SE-Res2Net50 SE-Res2Net50</cell><cell>LFCC LogPowSepc</cell><cell>964k</cell><cell>1.519G 12.929G</cell><cell>-0.98</cell><cell>-0.2769</cell><cell>1.89 -</cell><cell>0.0597 -</cell></row><row><cell cols="4">for unseen attacks. The best value for margin 32 was obtained</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">in this study by testing three values of 16, 32, and 64.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of the proposed systems with known single systems tested on the ASVspoof 2019 PA and LA evaluation set. Models are named base on their input feature, the classification model, and the loss function.</figDesc><table><row><cell>Input feature + Classifier + Loss function</cell><cell cols="4">PA EER(%) t-DCF EER(%) t-DCF LA</cell></row><row><cell>(Baseline) CQCC+GMM+EM [9]</cell><cell>11.04</cell><cell>0.2454</cell><cell>9.57</cell><cell>0.2366</cell></row><row><cell>(Baseline) LFCC+GMM+EM [9]</cell><cell>13.54</cell><cell>0.3017</cell><cell>8.09</cell><cell>0.2116</cell></row><row><cell>Spect+ResNet+CE [29]</cell><cell>3.81</cell><cell>0.9940</cell><cell>9.68</cell><cell>0.2741</cell></row><row><cell>MFCC+ResNet+CE [29]</cell><cell>-</cell><cell>-</cell><cell>9.33</cell><cell>0.2042</cell></row><row><cell>Spect+ResNet+CE [20]</cell><cell>1.29</cell><cell>0.0360</cell><cell>11.75</cell><cell>0.2160</cell></row><row><cell>Joint-gram+ResNet+CE [10]</cell><cell>1.23</cell><cell>0.0305</cell><cell>-</cell><cell>-</cell></row><row><cell>LFCC+LCNN+A-softmax [30]</cell><cell>4.60</cell><cell>0.1053</cell><cell>5.06</cell><cell>0.1000</cell></row><row><cell>Spect+LCNN+A-softmax [30]</cell><cell>-</cell><cell>-</cell><cell>4.53</cell><cell>0.1028</cell></row><row><cell>FG-CQT+LCNN+CE [31]</cell><cell>-</cell><cell>-</cell><cell>4.07</cell><cell>0.1020</cell></row><row><cell>Spect+LCGRNN+GKDE-softmax [32]</cell><cell>1.06</cell><cell>0.0222</cell><cell>3.77</cell><cell>0.0842</cell></row><row><cell>Spect+LCGRNN+triplet</cell><cell>0.92</cell><cell>0.0198</cell><cell>-</cell><cell>-</cell></row><row><cell>Fbank&amp;CQT+ResNeWt+CE [33]</cell><cell>0.52</cell><cell>0.0134</cell><cell>-</cell><cell>-</cell></row><row><cell>CQTMGD+ResNeWt+CE [33]</cell><cell>0.94</cell><cell>0.0250</cell><cell>-</cell><cell>-</cell></row><row><cell>Spect+SE-Res2Net50+CE [25]</cell><cell>0.74</cell><cell>0.0207</cell><cell>8.73</cell><cell>0.2237</cell></row><row><cell>LFCC+SE-Res2Net50+CE [25]</cell><cell>1.46</cell><cell>0.434</cell><cell>2.87</cell><cell>0.0786</cell></row><row><cell>CQT+SE-Res2Net50+CE [25]</cell><cell>0.46</cell><cell>0.0116</cell><cell>2.50</cell><cell>0.0743</cell></row><row><cell>Raw signal+SincNet+CE [18]</cell><cell>-</cell><cell>-</cell><cell>20.11</cell><cell>0.3563</cell></row><row><cell>logCQT&amp;powSpect+VGG+CE [18]</cell><cell>2.11</cell><cell>0.527</cell><cell>-</cell><cell>-</cell></row><row><cell>LFCC+ResNet18+OCS [26]</cell><cell>-</cell><cell>-</cell><cell>2.19</cell><cell>0.0590</cell></row><row><cell>Proposed: LFCC+SE-ResABNet+CombLoss</cell><cell>-</cell><cell>-</cell><cell>1.89</cell><cell>0.0507</cell></row><row><cell>Proposed: logPowSpec+EABNet+CombLoss</cell><cell>0.86</cell><cell>0.0239</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/AmirmohammadRostami/ASV-anti-spoofingwith-EABN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust text-independent speaker identification using gaussian mixture speaker models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="83" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speaker recognition by machines and humans: A tutorial review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="74" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biometrics systems under spoofing attack: an evaluation methodology and lessons learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="20" to="30" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the vulnerability of speaker verification to realistic voice spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Erg?nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems (BTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introduction to voice presentation attack detection and recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Biometric Anti-Spoofing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="321" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Asvspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hanil?i</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sizov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Asvspoof 2017 version 2.0: metadata analysis and baseline enhancements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2018-The Speaker and Language Recognition Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The asvspoof 2017 challenge: Assessing the limits of replay spoofing attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vestman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2249</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-2249" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1008" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The DKU replay detection system for the asvspoof 2019 challenge: On data augmentation, feature representation, classification, and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-1230</idno>
		<ptr target="https://doi.org/10.21437/Interspeech.2019-1230" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1023" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A new feature for automatic speaker verification anti-spoofing: Constant q cepstral coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="283" to="290" />
			<pubPlace>Odyssey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Boosting the performance of spoofing detection systems on replay attacks using qlogarithm domain feature normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="393" to="398" />
			<pubPlace>Odyssey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modulation dynamic features for the detection of replay attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Suthokumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sethu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wijenayake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ambikairajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="691" to="695" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiple phase information combination for replay attacks detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Oo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in INTERSPEECH</title>
		<imprint>
			<biblScope unit="page" from="656" to="660" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spoofing speech detection using high dimensional magnitude and phase features: The ntu approach for asvspoof 2015 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Amplitude and frequency modulation-based features for detection of replay spoof speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Kamble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="114" to="127" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep features for automatic spoofing detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="43" to="52" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Detecting spoofing attacks using vgg and sincnet: but-omilia submission to asvspoof 2019 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeinali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Athanasopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohdin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gkinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?ernock?</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2892</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-2892" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1073" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep generative variational autoencoding for replay spoof detection in automatic speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chettri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Benetos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">101092</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Assert: Antispoofing with squeeze-excitation and residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01120</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Audio replay spoof attack detection by joint segment-based linear filter bank feature extraction and attention-enhanced densenet-bilstm network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1813" to="1825" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Impact of score fusion on voice biometrics and presentation attack detection in cross-database evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Korshunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="695" to="705" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficientnetabsolute zero for continuous speech keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Akhaee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15695</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attentive filtering networks for audio replay attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6316" to="6320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Replay and synthetic speech detection with res2net architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6354" to="6358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">One-class learning towards synthetic voice spoofing detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="937" to="941" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention branch network: Learning of attention mechanism for visual explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="705" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">09</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep Residual Neural Networks for Audio Spoofing Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-3174</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-3174" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1078" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">STC Antispoofing Systems for the ASVspoof2019 Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lavrentyeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Novoselov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tseren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorlanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kozlov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1033" to="1037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<idno type="DOI">10.21437/Interspeech.2019-1768</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2019-1768" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Light Convolutional Neural Network with Feature Genuinization for Detection of Synthetic Speech Attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1101" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<idno type="DOI">10.21437/Interspeech.2020-1810</idno>
		<ptr target="http://dx.doi.org/10.21437/Interspeech.2020-1810" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A kernel density estimation based loss function and its application to asv-spoofing detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez-Alanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez-Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="108" to="530" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Replay detection using cqt-based modified group delay feature and resnewt network in asvspoof 2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="540" to="545" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
