<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PERCEPTUAL LOSS BASED SPEECH DENOISING WITH AN ENSEMBLE OF AUDIO PATTERN RECOGNITION AND SELF-SUPERVISED MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kataria</surname></persName>
							<email>skatari1@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s</forename><surname>Villalba</surname></persName>
							<email>jvillal7@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
							<email>ndehak3@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PERCEPTUAL LOSS BASED SPEECH DENOISING WITH AN ENSEMBLE OF AUDIO PATTERN RECOGNITION AND SELF-SUPERVISED MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Speech Denoising</term>
					<term>Perceptual Loss</term>
					<term>Pre-trained Networks</term>
					<term>Multi-Task Learning</term>
					<term>Self-Supervised Features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning based speech denoising still suffers from the challenge of improving perceptual quality of enhanced signals. We introduce a generalized framework called Perceptual Ensemble Regularization Loss (PERL) built on the idea of perceptual losses. Perceptual loss discourages distortion to certain speech properties and we analyze it using six large-scale pre-trained models: speaker classification, acoustic model, speaker embedding, emotion classification, and two self-supervised speech encoders (PASE+, wav2vec 2.0). We first build a strong baseline (w/o PERL) using Conformer Transformer Networks on the popular enhancement benchmark called VCTK-DEMAND. Using auxiliary models one at a time, we find acoustic event and self-supervised model PASE+ to be most effective. Our best model (PERL-AE) only uses acoustic event model (utilizing AudioSet) to outperform state-of-the-art methods on major perceptual metrics. To explore if denoising can leverage full framework, we use all networks but find that our seven-loss formulation suffers from the challenges of Multi-Task Learning. Finally, we report a critical observation that state-of-the-art Multi-Task weight learning methods cannot outperform hand tuning, perhaps due to challenges of domain mismatch and weak complementarity of losses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>There is a growing focus on the perceptual and intelligibility quality of enhanced signals obtained from Deep Neural Network (DNN) based speech enhancement systems. Quantifying them through human Mean Opinion Score (MOS) is highly expensive and prone to error. Proxy objective metrics (reference/non-reference based) are used in enhancement like Perceptual Evaluation of Speech Quality (PESQ) but they are hard to improve upon. In <ref type="bibr" target="#b0">[1]</ref>, authors introduced perceptual loss (or Deep Feature Loss (DFL)) to speech denoising problem. This improves perceptual quality of audio and is successfully applied using Generative Adversarial Network (GAN) <ref type="bibr" target="#b1">[2]</ref> and also for task-specific enhancement <ref type="bibr" target="#b3">[3]</ref>.</p><p>Alternatively, various differentiable metrics are proposed like Perceptual Metric for Speech Quality Evaluation (PMSQE) <ref type="bibr" target="#b4">[4]</ref>, Semi-supervised Speech Quality Assessment (SESQA) <ref type="bibr" target="#b5">[5]</ref>, and a metric based on Just Noticeable Differences (JNDs) <ref type="bibr" target="#b6">[6]</ref>. In <ref type="bibr" target="#b5">[5]</ref>, authors proposed SESQA which is an eight-loss Multi-Task Learning (MTL) based semi-supervised framework for automated speech quality assessment. In addition to predicting MOS, they define various auxiliary tasks including predicting JND, pairwise comparison, degradation strength, etc. <ref type="bibr" target="#b6">[6]</ref> took a different approach by constructing a large corpus based on (binary) human preference in audio pairs. By capturing JND, they are able to train a differentiable metric well correlated with human MOS rating.</p><p>Our aim is to improve perceptual and intelligibility metrics using perceptual loss on small-scale supervised speech denoising. To compensate for low resource training data, we investigate if largescale pre-trained speech models can be utilized. Such models, when used frozen and in ensemble, can regularize enhancement training to minimize distortions to various speech properties. Furthermore, we explore if self-supervised models can help preserve speech representations since they are trained on generic pretext tasks. Our idea faces challenges of domain mismatch and weak complementarity of losses, which we explore in-depth with various MTL methods.</p><p>Our contributions are as follows. First, we establish a strong baseline based on Conformer Transformer <ref type="bibr" target="#b7">[7]</ref> with a simple l1 objective. Second, we propose a rich framework called Perceptual Ensemble Regularization Loss (PERL) which leverages six opensource large-scale pre-trained networks to analyze speech denoising with perceptual losses. Third, we rank various speech tasks in term of their effectiveness in PERL and, for the first time, we demonstrate the utility of self-supervised representations for perceptual loss training. Fourth, we show the importance of l1 enhancement loss and all Conformer components to achieve state-of-the-art (SOTA) performance. Fifth, using all seven losses of PERL, we make an important finding that SOTA multi-task weight learning methods cannot outperform hand-tuned weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SPEECH DENOISING WITH PERCEPTUAL ENSEMBLE REGULARIZATION LOSS (PERL)</head><p>We consider a simple signal model or Noise Corruption Process (NCP):</p><formula xml:id="formula_0">y(t) = x(t) + n(t), where x(t) is clean (time-domain) signal, n(t)</formula><p>is noise signal, and y(t) is the resultant noisy signal. In general, x(t) suffers distortions to its acoustic event, speaker characteristics, and acoustic content. In DNN based speech enhancement, a simple l1 loss is not able to restore such distortions <ref type="bibr" target="#b3">[3]</ref>. Moreover, enhancement may introduce such distortions <ref type="bibr" target="#b3">[3]</ref>. To counter this, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">3]</ref> used perceptual loss or deep feature loss. Such loss compares enhanced signals with reference clean signals in the activation space of a pre-trained auxiliary network (trained for task T ). Depending on the choice of the auxiliary network, we accomplish speech enhancement with minimal distortions to representations required to solve task T . For e.g., <ref type="bibr" target="#b3">[3]</ref> did DFL based speech enhancement while minimizing distortions to speaker identities.</p><p>To avoid distortions to various speech characteristics during enhancement and/or NCP, we propose PERL: an ensemble of varied types of pre-trained (and frozen) speech models for perceptual loss training. Specifically, we use acoustic event classification, speaker embedding, acoustic model, and Speech Emotion Recog-arXiv:2010.11860v1 [eess.AS] 22 Oct 2020 nition (SER). We also incorporate self-supervised speech encoders since they model general speech representations <ref type="bibr" target="#b8">[8]</ref>. Our scheme is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. When using all components, loss function is L = ?eventLevent,n 1 + ?acousticLacoustic,n 2 + ?speakerLspeaker,n 3 +?emotionLemotion,n 4 + ?paseLpase,n 5 + ?wav2vecLwav2vec,n 6 +?</p><formula xml:id="formula_1">l 1 L l 1 ,<label>(1)</label></formula><p>where the last term refers to the simple Short-Time Fourier Transform (STFT) feature-domain l1 enhancement loss and ni refers to the number of internal layers of the corresponding auxiliary network used for deriving perceptual loss. The perceptual losses derived from i-th network are summed and divided by ni. We term PERL as a regularization loss since the auxiliary networks are frozen and only help constrain the output space. Note that this loss is different from usual formulations of transfer learning, knowledge distillation, and even multi-task learning. <ref type="bibr" target="#b9">[9]</ref> appropriately terms such setting as singletask multi-loss. Our framework is flexible and, instead of speech denoising, it can be used for voice conversion and domain adaptation as well. In terms of framework richness, our work is close to SESQA <ref type="bibr" target="#b5">[5]</ref> and PASE+ <ref type="bibr" target="#b10">[10]</ref>. There are some inherent challenges with PERL. One, since the auxiliary networks are trained on different features and domain, there can be significant domain mismatch which hinder training. To avoid this, we prefer to use initial layers since they tend to be more generic. Two, our test set is not equipped with labels for all auxiliary tasks and hence evaluation is restricted to enhancement metrics only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODEL DESCRIPTIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline Conformer Transformer Enhancement Network</head><p>Following the success of Transformer-based modeling of speech features <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11]</ref>, we choose convolution-augmented Transformer or Conformer <ref type="bibr" target="#b7">[7]</ref> for the denoising network. For modeling long-term and short-term patterns, it relies on self-attention mechanism and specially designed convolution modules respectively. Moreover, it combines the power of relative Position Encoding (PE) scheme and Macaron-style half-step Feed-Forward Networks (FFNs) <ref type="bibr" target="#b7">[7]</ref>. We additionally include Squeeze-and-Excitation <ref type="bibr" target="#b13">[12]</ref> module (squeeze factor of 8) after the 1D Depthwise Convolution inside the convolution module of Conformer. To avoid down-sampling in time, for the first layer of our Conformer, we use a Batch Normalization (BN) layer followed by a linear transformation instead of the usual convolutional sub-sampling layer. The attention dimension of the network is 240, the number of conformer blocks are 4, and total parameters are 10M. The denoising network predicts a mask which is then multiplied with the noisy spectra to predict the clean spectra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Brief Overview of Classification Models</head><p>Acoustic Event Classification: In <ref type="bibr" target="#b14">[13]</ref>, authors train a large-scale audio event classification network called Pre-trained Audio Neural Network (PANN) and prove its transferability to six audio pattern recognition tasks. It is trained with 1.9M audio clips from Au-dioSet <ref type="bibr" target="#b15">[14]</ref> (5000hrs, 527 sound classes). We choose the wide-band 14-layer version which has 81M parameters. They also address data imbalance problem of AudioSet and do online data augmentation with MixUp and SpecAugment <ref type="bibr" target="#b17">[15]</ref>. We choose nevent = 4 by using the first four convolution blocks (or first eight convolution layers).</p><p>Acoustic Model: Deep Speech 2 (DS2) is a multi-lingual endto-end Automatic Speech Recognition (ASR) model proposed in <ref type="bibr" target="#b18">[16]</ref>. We choose the English model trained on LibriSpeech <ref type="bibr" target="#b19">[17]</ref> consisting of 13M parameters. The architecture follows the Recurrent Neural Network (RNN)-Connectionist Temporal Classification (CTC) style. It has a convolutional front-end which is followed by five Bi-directional Long Short Term Memory (BLSTM) layers and final classification/CTC layer for English graphemes. We choose nacoustic = 3 by using the output of the first two convolutional layers and first RNN layer for DFL.</p><p>Speaker Embedding: In <ref type="bibr" target="#b20">[18]</ref>, authors propose RawNet2. It is a time-domain speaker embedding network trained with Voxceleb <ref type="bibr" target="#b20">[18]</ref>, which contains over 1M utterances from 6112 speakers. The architecture followed is a Convolutional Neural Network (CNN) made of six residual blocks followed by Gated Recurrent Units (GRU). The first layer is a sinc-convolution layer of SincNet <ref type="bibr" target="#b21">[19]</ref>. Total number of parameters are 87M. We choose nspeaker = 3 by using the output of the first sinc layer and the next two convolutional blocks.</p><p>Speech Emotion Classification: <ref type="bibr" target="#b22">[20]</ref> trains a time-domain emotion identification network on a 12hr emotion corpus called IEMOCAP. The architecture follows a Time-Delay Neural Network (TDNN)-LSTM style and has 1M parameters. The first layer is a 1-D convolution based data dependent layer. It also consists of five LSTM layers followed by four TDNN layers and a timerestricted self-attention based pooling of embeddings. Each frame of speech is classified in four emotions: happy, sad, neutral, and angry. We train this network using the same training scheme used for Conformer as described in Sec. 4. We choose nemotion = 3 by using the output of the first three layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Brief Overview of Self-Supervised Speech Encoder Models</head><p>PASE+: PASE+ [10] is a self-supervised speech encoder trained with 12 self-supervised tasks. For regression tasks, the model learns to predict various known speech transformations. For binary tasks, a contrastive loss tries to bring close representations of speech chunks belonging to same utterance while pushing apart representations of chunks belonging to different utterances. PASE+ can model speech content and speaker properties, which are robust to noise perturbations and have good transferability. The architecture has a convolutional front-end with a Quasi-Recurrent Neural Network (QRNN) backbone. It is trained with 50hrs of Librispeech <ref type="bibr" target="#b19">[17]</ref> and number of trainable parameters are 8M. We interchangeably refer to this model as PASE. We use npase = 6 by using the output of the first six convolutional blocks. We interchangeably refer to this model as wav2vec. We choose nwav2vec = 5 by using the output of the convolutional front-end and the next four Transformer blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Brief Overview of Multi-Task Learning Methods</head><p>We use four MTL methods off-the-shelf whose some aspects are as follows. Uncertainty Weighting <ref type="bibr" target="#b23">[21]</ref> uses the notion of an inherent task-dependent uncertainty or homoscedastic uncertainty. Using a Gaussian likelihood formulation, it defines a loss function consisting of learnable variance parameters for each loss. Hence, it accounts for the dynamic range of loss terms in a principled way. Coefficient of Variation <ref type="bibr" target="#b9">[9]</ref> states that a loss term is satisfied when its variance has decreased to zero. Hence, it assigns weight for a loss term as its coefficient of variation, i.e., standard deviation divided by mean. This quantity can compensate for the different dynamic range of loss terms. For mean estimate, a robust formulation of the current loss is used. For variance estimate, an online statistics tracking algorithm is used. Finally, all weights are normalized to make sum equals to one. Dynamic Weight Averaging <ref type="bibr" target="#b24">[22]</ref> keeps track of instantaneous ratio of change of loss value. Using that and a temperature parameter T (fixed to 2), it assigns weight to each task via a softmax classification. GradCosine <ref type="bibr" target="#b25">[23]</ref> treats main and auxiliary loss separately. To update the shared parameters (in our case, full denoising network), cosine similarity of gradient w.r.t. auxiliary loss is checked against the gradient w.r.t. main loss. When auxiliary gradients align, they are used to update shared parameters, otherwise they are rejected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP</head><p>We evaluate speech denoising on VCTK-DEMAND <ref type="bibr" target="#b26">[24]</ref>. It is a 16KHz small corpus with fixed training (10hr, 30 speakers) and validation data (30m, 2 speakers). We use the predicted clean spectra to compute Signal Approximation (SA) loss or, simply, the l1 loss (in STFT domain). We experimented with several popular loss functions but found l1 loss to be the best and the most resilient to change in experimental setup. For compatibility with auxiliary networks, we do appropriate on-the-fly transformations. To convert to timedomain, we re-use the phase of the noisy signal. Due to the large number of experiments, instead of subjective evaluation, we aim to  <ref type="bibr" target="#b5">5]</ref>. Our seven-network based framework is memory intensive, and hence, we do gradient accumulation. We train for 10 epochs using Adam optimizer, batch size of 32, a simple learning rate scheduler, and an initial learning rate of 0.00075. Best scores on validation data are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison of perceptual losses of various speech models</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we first note that using a simple l1 loss in STFT domain with Conformers, we get close to SOTA performance <ref type="table" target="#tab_2">(Table 3</ref>). In <ref type="bibr" target="#b0">[1]</ref>, authors used acoustic event classifier to achieve PESQ score of 2.57. Using Conformers and AudioSet based auxiliary network, we drastically improve that score (3.09). We find other speech models also give competitive performance except for the emotion model, perhaps due to its low resource training data. Here, np refers to the number of parameters in auxiliary network (in M). PASE+ is the best model considering performance as well as parameter efficiency. We experimented with various popular loss functions based on the ideas of multi-resolution spectra, Time-Domain Reconstruction (TDR), Signal-to-Distortion Ratio (SDR), etc. but found them much inferior to the l1 loss. It is important to state that we expect acoustic event loss to be the best since during noise corruption process (and consequently during enhancement), for a simple test set like ours, acoustic event information of audio is adversely affected the most. However, its comparison with other speech models is novel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Combining l1 enhancement loss with perceptual losses</head><p>Previous work <ref type="bibr" target="#b0">[1]</ref> did not investigate the benefit of combining feature-domain enhancement loss with perceptual losses. In <ref type="table" target="#tab_1">Table 2</ref>, we combine l1 loss with six auxiliary losses individually as well in certain combinations. Hand-tuning of loss terms gave (?event, ?acoustic, ?speaker, ?emotion, ?pase, ?wav2vec, ? l 1 ) = (5e-03, 1e-04, 1.25e-04, 4e-05, 1.7e-04, 3.5e-05, 1.1e-01). Combining l1 with acoustic event (PERL-AE) yields better results than baseline and outperforms SOTA models ( <ref type="table" target="#tab_2">Table 3</ref>). Note that we can further improve by predicting phase too by using a time-domain model like DEMUCS <ref type="bibr" target="#b27">[25]</ref>. For all cases, combining l1 loss results in better performance than without it (Refer <ref type="table" target="#tab_0">Table 1</ref>). Second last row in <ref type="table" target="#tab_1">Table 2</ref> shows good performance by combining best three models: PASE, acoustic event, and speaker embedding. This suggests that if we use such networks with more data and less domain mismatch w.r.t. enhancement training data, further improvements can be observed. The final row uses all losses and, hence, is termed PERL. Using full framework, we get inferior results w.r.t. baseline. This is perhaps due to sub-optimal choice of loss weights (selected via  To emphasize the role of Conformer architecture, we do an ablation study similar to <ref type="bibr" target="#b7">[7]</ref>. In <ref type="table" target="#tab_3">Table 4</ref>, we progressively (1) replace SWISH activation by ReLU; (2) remove convolution module inside Conformer blocks; (3) replace Macaron-style FFN pairs with a single FFN layer; (4) replace relative position embedding with absolute.</p><p>The trend of performance degradation shows that all components of Conformer are important, especially the convolution module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Automatic weight learning versus hand-tuning of weights</head><p>PERL faces challenges of MTL. For e.g., loss terms can have different dynamic range, speed of convergence (for fixed learning rate), and complementarity with main loss. In this section, we investigate if SOTA automatic weight learning methods can outperform handtuned PERL system i.e. the system trained with all seven losses. We also want to investigate if detrimental losses can be automatically rejected, especially by GradCosine <ref type="bibr" target="#b25">[23]</ref>. In <ref type="table" target="#tab_4">Table 5</ref>, second row refers to PERL system fine-tuned with the best loss discovered in <ref type="table" target="#tab_1">Table  2</ref> i.e. "l1+AcousticEvent". Note that it is able to (almost) recover fully. Results with equal weighting shows that the choice of weights is paramount. We find GradCosine to be the worst and Uncertainty Weighting to be the best among four MTL methods. Our important finding is that learning weights or even dynamically adjusting (nonlearnable) weights cannot outperform well-tuned (by hand) results.  We suspect this because of two reasons. One, the domain mismatch of enhancement training data with auxiliary network data is hard to overcome. By leveraging more training data and learning a transformation layer between auxiliary networks and the main (denoising) network, this problem can perhaps be quelled. Two, the fundamental assumption of multi-task learning of complementarity of losses does not hold and even SOTA MTL methods cannot overcome it by, for example, (soft/hard) rejection of terms unaligned with main loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>To improve perceptual metrics on small-scale speech denoising, we propose a framework called Perceptual Ensemble Regularization Loss (PERL). PERL leverages an ensemble of variety of opensource large-scale pre-trained speech models for deriving perceptual loss or Deep Feature Loss. We show the efficacy of all models individually as well as in certain combinations. Acoustic event and self-supervised PASE+ models are found to be most effective. We also find combining regular feature-domain enhancement loss to be complementary. With an ablation study, we highlight the critical role of components of Conformer Transformer. When using all components of PERL, we find that our seven-loss framework suffers from the challenges of Multi-Task Learning (MTL) and we suffer a performance loss. To recover, we experiment with various MTL methods to conclude that state-of-the-art MTL methods cannot outperform greedy hand-tuning based weight selection but, interestingly, this system can be fine-tuned on the best loss (l1+AcousticEvent) to restore lost performance. In future, we can (1) build towards universal enhancement by analyzing PERL on a richer test setup, (2) incorporate automatic speech quality quantification networks like SESQA <ref type="bibr" target="#b5">[5]</ref>, and (3) learn bridges/adaptors between main and auxiliary networks to alleviate domain mismatch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of supervised framework called Perceptual Ensemble Regularization Loss (PERL) using seven losses: L1, . . . , L7. To extract perceptual losses, partial forward pass of enhanced and reference clean signals is done as illustrated by using temporal signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>wav2vec 2 . 0</head><label>20</label><figDesc>In<ref type="bibr" target="#b8">[8]</ref>, authors propose wav2vec 2.0, a selfsupervised model which learns generic speech representations by solving a contrastive loss in the (quantized) latent space using a masked Transformer. It is trained on 960hrs of LibriSpeech without transcriptions. The architecture consists of 12 transformer blocks, model dimension of 768, inner dimension (Feed-Forward Network (FFN)) of 3,072, eight attention heads, and 96M parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of perceptual losses of various speech models with simple l1 loss. np refers to #parameters of auxiliary network.</figDesc><table><row><cell></cell><cell cols="6">np PESQ CSIG CBAK COVL STOI</cell></row><row><cell>l1</cell><cell>-</cell><cell>3.01</cell><cell>4.27</cell><cell>3.48</cell><cell>3.65</cell><cell>94.9</cell></row><row><cell>AcousticEvent</cell><cell>81</cell><cell>3.09</cell><cell>4.38</cell><cell>3.43</cell><cell>3.75</cell><cell>94.8</cell></row><row><cell>AcousticModel</cell><cell>87</cell><cell>2.97</cell><cell>4.15</cell><cell>3.43</cell><cell>3.55</cell><cell>94.6</cell></row><row><cell cols="2">SpeakerEmbedding 13</cell><cell>2.86</cell><cell>3.63</cell><cell>3.32</cell><cell>3.23</cell><cell>94.5</cell></row><row><cell>SpeechEmotion</cell><cell>1</cell><cell>2.41</cell><cell>3.49</cell><cell>3.07</cell><cell>2.93</cell><cell>92.8</cell></row><row><cell>PASE</cell><cell>8</cell><cell>3.04</cell><cell>4.23</cell><cell>3.42</cell><cell>3.63</cell><cell>94.7</cell></row><row><cell>wav2vec</cell><cell>95</cell><cell>2.93</cell><cell>3.67</cell><cell>3.33</cell><cell>3.29</cell><cell>93.9</cell></row><row><cell cols="7">improve upon standardized perceptual and intelligibility metrics like</cell></row><row><cell cols="7">PESQ, CSIG, CBAK, COVL, and STOI. PESQ ? [?0.5, 4.5], STOI</cell></row><row><cell cols="5">(in %) ? [0, 100], while other metrics ? [0,</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effect of adding perceptual losses (individually and in certain combinations) to l1 loss. Last row uses all seven loss terms.</figDesc><table><row><cell></cell><cell cols="5">PESQ CSIG CBAK COVL STOI</cell></row><row><cell>l1</cell><cell>3.01</cell><cell>4.27</cell><cell>3.48</cell><cell>3.65</cell><cell>94.9</cell></row><row><cell>l1+AcousticEvent</cell><cell>3.17</cell><cell>4.43</cell><cell>3.53</cell><cell>3.83</cell><cell>95.0</cell></row><row><cell>l1+AcousticModel</cell><cell>2.97</cell><cell>4.15</cell><cell>3.43</cell><cell>3.55</cell><cell>94.6</cell></row><row><cell>l1+SpkEmbedding</cell><cell>2.91</cell><cell>4.18</cell><cell>3.39</cell><cell>3.55</cell><cell>94.4</cell></row><row><cell>l1+SpkEmotion</cell><cell>2.86</cell><cell>4.12</cell><cell>3.38</cell><cell>3.5</cell><cell>94.2</cell></row><row><cell>l1+PASE</cell><cell>3.03</cell><cell>4.24</cell><cell>3.43</cell><cell>3.63</cell><cell>94.8</cell></row><row><cell>l1+wav2vec</cell><cell>2.92</cell><cell>4.16</cell><cell>3.37</cell><cell>3.54</cell><cell>94.5</cell></row><row><cell>l1+AcousticEvent</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+AcousticModel +SpkEmbedding</cell><cell>2.88</cell><cell>4.15</cell><cell>3.40</cell><cell>3.52</cell><cell>94.5</cell></row><row><cell>+SpkEmotion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">l1+PASE+wav2vec 2.95</cell><cell>4.2</cell><cell>3.4</cell><cell>3.58</cell><cell>94.6</cell></row><row><cell>l1+PASE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+AcousticEvent</cell><cell>3.05</cell><cell>4.31</cell><cell>3.5</cell><cell>3.69</cell><cell>94.8</cell></row><row><cell>+SpkEmbedding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>l1+PASE +AcousticEvent</cell><cell>3.04</cell><cell>4.37</cell><cell>3.47</cell><cell>3.72</cell><cell>94.9</cell></row><row><cell>l1+ALL (PERL)</cell><cell>2.89</cell><cell>4.11</cell><cell>3.41</cell><cell>3.5</cell><cell>94.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the best denoising system (PERL-AE or l1+AcousticEvent) with the state-of-the-art methods</figDesc><table><row><cell></cell><cell cols="5">PESQ CSIG CBAK COVL STOI</cell></row><row><cell>Noisy</cell><cell>1.97</cell><cell>3.35</cell><cell>2.44</cell><cell>2.63</cell><cell>-</cell></row><row><cell>Weiner filtering</cell><cell>2.22</cell><cell>3.23</cell><cell>2.68</cell><cell>2.67</cell><cell>-</cell></row><row><cell>Deep Feature Loss [1]</cell><cell>2.57</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Hi-Fi GAN [2]</cell><cell>2.94</cell><cell>4.07</cell><cell>3.07</cell><cell>3.49</cell><cell>-</cell></row><row><cell>Self-Adapt MHSA [26]</cell><cell>2.99</cell><cell>4.15</cell><cell>3.46</cell><cell>3.51</cell><cell>-</cell></row><row><cell>T-GSA [11]</cell><cell>3.06</cell><cell>4.18</cell><cell>3.59</cell><cell>3.62</cell><cell>-</cell></row><row><cell>DEMUCS [25]</cell><cell>3.07</cell><cell>4.31</cell><cell>3.4</cell><cell>3.63</cell><cell>95</cell></row><row><cell>PERL-AE (ours) (l1+AcousticEvent)</cell><cell>3.17</cell><cell>4.43</cell><cell>3.53</cell><cell>3.83</cell><cell>95</cell></row><row><cell cols="6">greedy hand-tuning) or simply due to detrimental nature of some</cell></row><row><cell cols="3">loss terms (investigated in Sec. 5.3).</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Reproducing ablation study of<ref type="bibr" target="#b7">[7]</ref> by removing features from Conformer network to converge to vanilla Transformer.</figDesc><table><row><cell></cell><cell cols="5">PESQ CSIG CBAK COVL STOI</cell></row><row><cell>Full Conformer</cell><cell>3.17</cell><cell>4.43</cell><cell>3.53</cell><cell>3.83</cell><cell>95</cell></row><row><cell>-SWISH + ReLU</cell><cell>3.15</cell><cell>4.38</cell><cell>3.52</cell><cell>3.78</cell><cell>94.9</cell></row><row><cell>-Convolution Block</cell><cell>2.81</cell><cell>4.21</cell><cell>3.36</cell><cell>3.51</cell><cell>94.7</cell></row><row><cell>-Macaron FFN</cell><cell>2.74</cell><cell>4.14</cell><cell>3.31</cell><cell>3.44</cell><cell>94.8</cell></row><row><cell>-Relative Pos. Emb.</cell><cell>2.65</cell><cell>4.01</cell><cell>3.30</cell><cell>3.36</cell><cell>94.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of fixed hand-tuned weights and multi-task weights learning methods for training PERL model (seven losses)</figDesc><table><row><cell></cell><cell></cell><cell cols="5">PESQ CSIG CBAK COVL STOI</cell></row><row><cell>Hand</cell><cell>Tuning</cell><cell>2.89</cell><cell>4.11</cell><cell>3.41</cell><cell>3.5</cell><cell>94.6</cell></row><row><cell>(PERL)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Fine-Tuning on l1+AcousticEvent</cell><cell>3.12</cell><cell>4.4</cell><cell>3.51</cell><cell>3.78</cell><cell>95</cell></row><row><cell cols="2">Equal weights</cell><cell>2.74</cell><cell>4.02</cell><cell>3.31</cell><cell>3.37</cell><cell>94.3</cell></row><row><cell cols="2">Coefficient of</cell><cell>2.74</cell><cell>4.08</cell><cell>3.31</cell><cell>3.31</cell><cell>94.3</cell></row><row><cell cols="2">Variation [9]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Uncertainty</cell><cell>2.85</cell><cell>4.12</cell><cell>3.35</cell><cell>3.48</cell><cell>94.5</cell></row><row><cell cols="2">Weighting [21]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dynamic</cell><cell></cell><cell>2.8</cell><cell>4.14</cell><cell>3.34</cell><cell>3.47</cell><cell>94.3</cell></row><row><cell>Weight</cell><cell>Aver-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>age [22]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GradCosine [23]</cell><cell>2.54</cell><cell>3.92</cell><cell>3.15</cell><cell>3.22</cell><cell>93.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Francois G Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10522</idno>
		<title level="m">Speech denoising with deep feature losses</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">High-fidelity denoising and dereverberation based on speech deep features in adversarial networks</title>
		<idno type="arXiv">arXiv:2006.05694</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Analysis of deep feature loss based enhancement for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kataria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phani</forename><surname>Sankar Nidadavolu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00139</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A deep learning loss function based on the perceptual evaluation of the speech quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Manuel Mart?n-Do?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">Manuel</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1680" to="1684" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sesqa: semisupervised learning for speech quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00368</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A differentiable perceptual audio metric learned from just noticeable differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranay</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautham J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mysore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04460</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for selfsupervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-loss weighting with coefficient of variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><surname>Groenendijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sezer</forename><surname>Karaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01717</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised learning for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6989" to="6993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">T-gsa: Transformer with gaussian-weighted self-attention for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="6649" to="6653" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turab</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10211</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="776" to="780" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improved rawnet with filter-wise rescaling for text-independent speaker verification using raw waveforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Bin</forename><surname>Jee-Weon Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hye-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju-Ho</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00526</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with sincnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emotion identification from raw speech signals using dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mousmita</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandarpa</forename><forename type="middle">Kumar</forename><surname>Nagendra Kumar Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in Interspeech</title>
		<imprint>
			<biblScope unit="page" from="3097" to="3101" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-toend multi-task learning with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1871" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adapting auxiliary losses using gradient similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02224</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Investigating rnn-based speech enhancement methods for noise-robust text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassia</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
			<biblScope unit="page" from="146" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Real time speech enhancement in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Defossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12847</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech enhancement using selfadaptation and multi-head self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuma</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Yaiabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Maxuxama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="181" to="185" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
