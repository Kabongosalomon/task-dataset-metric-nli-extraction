<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DCSAU-Net: A Deeper and More Compact Split-Attention U-Net for Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-24">24 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">A R T I C L E I N F O</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">A R T I C L E I N F O</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">A R T I C L E I N F O</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">A R T I C L E I N F O</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DCSAU-Net: A Deeper and More Compact Split-Attention U-Net for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-24">24 Sep 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Medical image segmentation Multi-scale fusion attention Depthwise separable convolution Computer-aided diagnosis</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B S T R A C T</head><p>Deep learning architecture with convolutional neural network (CNN) achieves outstanding success in the field of computer vision. Where U-Net, an encoder-decoder architecture structured by CNN, makes a great breakthrough in biomedical image segmentation and has been applied in a wide range of practical scenarios. However, the equal design of every downsampling layer in the encoder part and simply stacked convolutions do not allow U-Net to extract sufficient information of features from different depths. The increasing complexity of medical images brings new challenges to the existing methods. In this paper, we propose a deeper and more compact split-attention u-shape network (DCSAU-Net), which efficiently utilises low-level and high-level semantic information based on two novel frameworks: primary feature conservation and compact split-attention block. We evaluate the proposed model on CVC-ClinicDB, 2018 Data Science Bowl, ISIC-2018 and SegPC-2021 datasets. As a result, DCSAU-Net displays better performance than other state-of-the-art (SOTA) methods in terms of the mean Intersection over Union (mIoU) and F1-socre. More significantly, the proposed model demonstrates excellent segmentation performance on challenging images. The code for our work and more technical details can be found at https://github.com/xq141839/DCSAU-Net.</p><p>Qing Xu et al.: Preprint submitted to Elsevier</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Common types of cancer such as colon cancer, multiple myeloma and melanoma, are still one of the major causes of human suffering and death globally. Medical image analysis plays an essential role in terms of diagnosing and treating these diseases <ref type="bibr" target="#b0">[1]</ref>. For example, numerous cells in a microscopy image are able to illustrate the stage of diseases, assist in discriminating tumour types, support in insight of cellular and molecular genetic mechanisms, and present valuable information to many other applications, such as cancer and chronic obstructive pulmonary disease <ref type="bibr" target="#b1">[2]</ref>. Traditionally, medical images are analysed by pathologists manually. In other words, the result of diagnosis is usually dominated by on the experience of medical experts, which can be timeconsuming, subjective, and error-prone <ref type="bibr" target="#b2">[3]</ref>. Computer-aided diagnosis (CAD) has received significant attention from both pathological researchers and clinical practice, which is mainly depend on the result of medical image segmentation <ref type="bibr" target="#b3">[4]</ref>. Different from classification and detection tasks, the target of biomedical image segmentation is to separate the specified object from the background in an image, which is able to provide patients with more detailed disease analysis <ref type="bibr" target="#b4">[5]</ref>. Existing classic segmentation algorithms are based on edge detection, thresholding, morphology, distances between two objects and pixel energy, such as Otsu thresholding <ref type="bibr" target="#b5">[6]</ref>, Snake <ref type="bibr" target="#b6">[7]</ref> and Fuzzy algorithms <ref type="bibr" target="#b7">[8]</ref>. Each algorithm has its own parameters to accommodate different requirements. However, these algorithms often show limited performance on the generalization of complex datasets <ref type="bibr" target="#b8">[9]</ref>. The segmentation performance of these methods is also affected by image acquisition quality. For example, some pathological images may be blurred or contain noises. Other situations could have negative influences too, including uneven illumination, ORCID(s): 0000-0001-6898-0269 (Q. <ref type="bibr">Xu)</ref> low image contrast between foreground and background, and complex tissue background <ref type="bibr" target="#b9">[10]</ref>. Therefore, it is essential to construct a powerful and generic model which can achieve adequate robustness on challenging images and works for different biomedical applications.</p><p>CNN-based encoder-decoder architectures have outperformed traditional image processing methods in various medical image segmentation tasks <ref type="bibr" target="#b10">[11]</ref>. The success of these models is largely due to the skip connection strategy that incorporates the low-level semantic information with highlevel semantic information to generate the final mask <ref type="bibr" target="#b11">[12]</ref>. However, many improved architectures only focus on optimising algorithms in terms of in-depth feature extraction, which ignores the loss of high-resolution information in the header of the encoder. The sufficient feature maps extracted from this layer is able to help to compensate for the spatial information lost during the pooling operations <ref type="bibr" target="#b12">[13]</ref>.</p><p>In this paper, we propose a novel encoder-decoder architecture for medical image segmentation, called DCSAU-Net. In the encoder part, our model first adopts a novel primary feature conservation (PFC) strategy that reduces the number of parameters, amount of computation and integrates the long-range spatial information of the network in the shadow layer. The rich primary feature obtained from this layer will be delivered to our novel constructed module: compact splitattention (CSA) block. The CSA module strengthens the feature representation of different channels using multi-path attention structure. Each path contains a different number of convolutions so that the CSA module can output mixed feature maps with different receptive field scales. Both new frameworks are designed with residual style in order to alleviate gradient vanishing problem with increasing layers. For the decoder, encoded features in every downsampling layer are concatenated with corresponding upsampled features by skip connection. we apply the same CSA block to complete efficient feature extraction from the combined features. The proposed DCSAU-Net is easy to train without any extra support samples (eg. Initialised mask or edge). The main contributions of this work can be summarized as follows: 1) A novel mechanism, PFC, is embedded in our DCSAU-Net to capture sufficient primary features from the input images. Compared with other common designs, PFS not only improves computational efficiency but also extends the receptive field of the network. 2) To enhance the multi-scale representation of DCSAU-Net, we build a CSA block that adopts multi-branch feature groups with attention mechanism. Each group is comprised of a different number of convolutions in order to output feature maps with the combination of different receptive field sizes. 3) Experimental analysis is conducted with four different medical image segmentation datasets, including 2018 Data Science Bowl <ref type="bibr" target="#b13">[14]</ref>, ISIC-2018 Lesion Boundary Segmentation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, CVC-ClinicDB <ref type="bibr" target="#b16">[17]</ref>, and a multi-class segmentation dataset: SegPC-2021 <ref type="bibr" target="#b17">[18]</ref>. Evaluation results demonstrate that our proposed DCSAU-Net shows better performance than other SOTA segmentation methods in terms of standard computer vision metrics -mIoU and F1 score, which can be a new SOTA method for medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Medical Image Segmentation</head><p>Deep learning methods based on Convolutional Neural Network (CNN) have indicated outstanding performance in medical image segmentation. U-Net, proposed by Ronneberger et al. <ref type="bibr" target="#b18">[19]</ref>, is comprised of two components: encoder and decoder. Upsampling operators are added in the decoder, which is used to recover the resolution of input images. Also, features extracted from the encoder are combined with upsampled results to achieve precise localisation. U-Net shows a favourable segmentation performance in different kinds of medical images. Inspired by this architecture, Zhou et al. <ref type="bibr" target="#b19">[20]</ref> presented a nested U-Net (Unet++) for medical image segmentation. To reduce the semantic information loss of feature fusion between encoder and decoder, a series of nested and skip pathways are added to the model. Huang et al. <ref type="bibr" target="#b20">[21]</ref> designed another full-scale skip connection method that combines low-resolution information and high-resolution information in different scales. Jha et al. <ref type="bibr" target="#b21">[22]</ref> constructed a DoubleU-Net network that organise two U-Net architectures sequentially. In the encoder part, Atrous Spatial Pyramid Pooling (ASPP) is constructed at the end of each downsample layer to obtain contextual information. The evaluation result demonstrates that DoubleU-Net performs well in polyp, lesion boundary and nuclei segmentation. The gradient vanishing issue has been discovered when trying to converge deeper networks. To address this problem, He et al. <ref type="bibr" target="#b22">[23]</ref> introduced a deep residual architecture (ResNet) that had been widely applied in different segmentation networks. For medical image segmentation, Jha et al. <ref type="bibr" target="#b23">[24]</ref> constructed an advanced u-shape architecture for polyp segmentation, called ResUNet++. This model involves residual style, squeeze and excitation module, ASPP, and attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention Mechanisms</head><p>In previous years, the attention mechanism has rapidly appeared in computer vision. SENet <ref type="bibr" target="#b24">[25]</ref>, one of channel attention, has been widely applied in medical image segmentation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. It uses a squeeze module, with global average pooling, to collect global spatial information, and an excitation module to obtain the relationship between each channel in feature maps. Spatial attention can be referred to as an adaptive spatial location selection mechanism. For instance, Oktay et al. <ref type="bibr" target="#b27">[28]</ref> introduced an attention U-Net using a novel bottom-up attention gate, which can precisely focus on a specific region that highlights useful features without extra computational costs and model parameters. Furthermore, Transformers <ref type="bibr" target="#b28">[29]</ref> have received lot of attention recently because its success in natural language processing (NLP). Dosovitskiy et al. <ref type="bibr" target="#b29">[30]</ref> developed a vision transformer (ViT) architecture for computer vision tasks and indicated comparable performance to CNN. Also, a series of ungraded ViT has been in a wider range of fields. Xu et al. <ref type="bibr" target="#b30">[31]</ref> proposed LeViT-UNet to collect distant spatial information from features. In addition, Transformers have demonstrated strong performance when incorporated with CNN. Chen et al. <ref type="bibr" target="#b31">[32]</ref>, provided a novel TransUNet that selects CNN as the first half of the encoder to obtain image patches and uses the Transformer model to extract the global contexts. The final mixed feature in the decoder can achieve more accurate localisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Depthwise Separable Convolution</head><p>Depthwise separable convolution is an efficient neural nework architecture proposed by Howard et al. <ref type="bibr" target="#b32">[33]</ref>. Each convolution filter in this architecture is responsible for one input channel. Compared with a standard convolution, depthwise convolution not only can achieve the same effects but also costs fewer number of parameters and computations. However, it only extracts features of every input channel. To combine the information between the channels and create new feature maps, a 1x1 convolution, called pointwise convolution, follows a depthwise convolution. The final Mo-bileNets model was established and considered as a new backbone in deep learning. In the image classification task, Chollet <ref type="bibr" target="#b33">[34]</ref> used depthwise separable convolution to construct an Xception model that outperformed previous SOTA methods and showed lower complexity. However, Sandler et al. <ref type="bibr" target="#b34">[35]</ref> observed that depthwise convolution performs poorly in the low-channel feature maps. To tackle aforementioned issues, they proposed a new MobileNetV2 model that adds a 1x1 convolution in front of the depthwise convolution in order to increase the dimension of features in advance. Compared with MobileNets, MobileNetV2 does not raise the number of parameters but decreases the degradation of performance. In medical image segmentation, Qi et al. <ref type="bibr" target="#b35">[36]</ref> introduced an Xnet model for 3D brain stroke lesion segmentation. A novel <ref type="figure">Figure 1</ref>: Comparing our PFC strategy with U-Net <ref type="bibr" target="#b18">[19]</ref>, Stem block <ref type="bibr" target="#b36">[37]</ref> and ResUNet++ <ref type="bibr" target="#b23">[24]</ref> designs used to extract the low-level semantic information from the input images.</p><p>feature similarity module (FSM) was created to capture distance spatial information in feature maps using depthwise separable convolution. The experiment results demonstrate the X-net model costs only half the number of parameters of other SOTA models to achieve higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Primary Feature Conservation</head><p>For most of medical image segmentation networks, the covolutions used in the first downsampling block operation is to extract low-level semantic information from images. The U-Net architecture <ref type="bibr" target="#b18">[19]</ref> in <ref type="figure">Fig.1</ref> (a) has been widely used in different models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>. The stem block <ref type="bibr" target="#b36">[37]</ref> in <ref type="figure">Fig.1</ref> (b) is usually designed to obtain the same receptive field as 7x7 convolution and reduce the number of parameters. The first feature scale downsampling layer in ResUNet++ <ref type="bibr" target="#b23">[24]</ref> adds skip connection strategy to mitigate the potential impact of the gradient vanish, which is shown in <ref type="figure">Fig.1 (c)</ref>. Although stacking more convolutional blocks can extend the receptive field of neural network, the number of parameters and the amount of computation will increase rapidly. The stability of the model may be destroyed. Also, recent research suggests that the valid receptive field will decrease to some extent when the number of stacked 3?3 convolutions keep increasing <ref type="bibr" target="#b37">[38]</ref>. To address this issue, we introduce a new primary feature conservation (PFC) strategy in the first downsampling block, which is provided in <ref type="figure">Fig.1 (d)</ref>.</p><p>The main refinement of our module adopts depthwise separable convolution, consisting of 7x7 depthwise convolution followed by 1x1 pointwise convolution. As depthwise separable convolution decreases the costs of computation and the number of parameters compared to the standard convolution <ref type="bibr" target="#b32">[33]</ref>, we have the opportunity to apply large kernel sizes for the depthwise convolution in order to merge distant spatial information and preserve primary features as much as possible in the low-level semantic layer. The 1x1 pointwise convolution is used to combine channel information. Also, 3x3 convolution is added to the head of this module for downsampling the input image and raising the channel because depthwise separable convolution shows degradation of performance on low-dimensional features <ref type="bibr" target="#b34">[35]</ref>. Every convolution is followed by a ReLU activation and BatchNorm. To avoid gradient vanish, PFC block is constructed with residual style. To this end, our proposed PFC module can improve the performance without increasing the number of parameters and computational costs. In addition, the reason for using depthwise convolution with 7x7 kernel size will be explained in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Compact Split-Attention block</head><p>The VGGNet <ref type="bibr" target="#b38">[39]</ref> and typical residual structures <ref type="bibr" target="#b22">[23]</ref> have been applied in many previous semantic segmentation networks, such as DoubleUnet <ref type="bibr" target="#b21">[22]</ref> and ResUnet <ref type="bibr" target="#b39">[40]</ref>. However, convolutional layers in VGGNet are stacked directly, which means every feature layer has a comparatively constant receptive field <ref type="bibr" target="#b40">[41]</ref>. In medical image segmentation, different lesions may have different sizes. Sufficient representation of multi-scale features is beneficial for the model to perceive data features. Recently various models learning the representation via cross-channel features have been proposed, such as ResNeSt <ref type="bibr" target="#b41">[42]</ref>. Inspired by these methods, we develop a new compact split-attention (CSA) architecture.</p><p>An overview of CSA block is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. The ResNeSt utilises a large channel-split groups for feature extraction, which is more efficient for general computer vision tasks with the adequate data and costs massive parameters. Furthermore, each group of this model adopts the same convolutional operations that receive an equal receptive field size. To optimise the structure and make it more suitable for medical image segmentation, our proposed block maintains two feature groups ( = 2) to reduce the number of parameters the entire network. These two groups split from the input features will be fed into different transformations . Both two groups involve one 1?1 convolution followed by one 3?3 convolution. To improve the representation across channels, the output feature maps of the other group ( 2 ) will combine with the result of the first group ( 1 ) and go through another 3?3 convolution, which can receive semantic information from both split groups and expand the receptive field of the network. Therefore, CSA block presents a stronger ability to extract both global and local information from feature maps. Mathematically, the fusion feature maps can be defined as:</p><formula xml:id="formula_0">= ? =1 ( ),? ? ? ? (1)</formula><p>Where H, W and C are the scales of output feature maps. The channel-wise statistics generated by global average pooling collect global spatial information, which is produced by compressing transformation output through spatial dimensions and the -th component calculated by:</p><formula xml:id="formula_1">= 1 ? ? =1 ? =1? ( , ) , ?<label>(2)</label></formula><p>The channel-wise soft attention is used for aggregating a weighted fusion represented by cardinal group representation, where a split weighted combination can catch crucial information in feature maps. Then the -th channel of feature maps is calculated as:</p><formula xml:id="formula_2">= ? =1 ( ) ( )<label>(3)</label></formula><p>Where is a (soft) assignment weight designed by:</p><formula xml:id="formula_3">( ) = (? ( )) ? =1 (? ( ))<label>(4)</label></formula><p>Here ? indicates the weight of global spatial information to the -th channel and is quantified using two 1x1 convolutions with BatchNorm and ReLU activation. As a result, the full CSA block is designed with a standard residual architecture that the output is calculated using a skip connection: = + , when the shape of output feature maps is the same as the input feature maps. Otherwise, an extra transformation will be applied on the skip connection to obtain the same shape. For instance, can be convolution with a stride or mix of convolution and pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">DCSAU-Net Architecture</head><p>For medical image segmentation, we establish a novel model using the proposed PFC strategy and CSA block following the encoder-decoder architecture, which is referred to as DCSAU-Net, and shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The encoder of DCSAU-Net first uses PFC strategy to extract low-level semantic information from the input images. The depthwise separable convolution with a large 7x7 kernel size is able to broaden the receptive field of the network and preserve primary features without increasing the number of parameters. The CSA block applies multi-path feature groups with a different number of convolutions and the attention mechanism, which incorporates channel information across different receptive field scales and highlights meaningful semantic features. Each of block is followed by a 2?2 max pooling with stride 2 for performing a downsampling operation. Every decoder sub-network starts with an upsampling operator to recover the original size of the input image step by step. The skip connections are used to concatenate these feature maps with the feature maps from the corresponding encoder layer, which mixes low-level and high-level semantic information to generate a precise mask. The skip connections are followed by CSA blocks to alleviate the gradient vanishing problem and capture efficient features. Finally, a 1 ? 1 convolution succeeded by a sigmoid or softmax layer is used to output the binary or multi-class segmentation mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>To evaluate the effectiveness of DCSAU-Net, we test it on four publicly available medical image datasets. ? CVC-ClinicDB <ref type="bibr" target="#b16">[17]</ref> is a frequently-used dataset for the polyp segmentation task. It is also the training database for the MICCAI 2015 Sub-Challenge on Automatic Polyp Detection Challenge. ? The second dataset used in this study is from the 2018 Data Science Bowl challenge <ref type="bibr" target="#b13">[14]</ref>, which is used for the nuclei segmentation task. The dataset labels every cell in microscopic images. ? Another dataset used in our experiment is from a subtask in the ISIC-2018 challenge <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. The target of training the dataset is to develop a model for lesion boundary segmentation. ? In order to assess the performance of the proposed architecture on the multi-class segmentation task, we add the SegPC-2021 dataset <ref type="bibr" target="#b17">[18]</ref> in our experiment. Each of image in the dataset includes two different Myeloma Plasma cells.</p><p>More details about data split are presented in <ref type="table" target="#tab_0">Table 1</ref>. All of these datasets are related to clinic diagnosis. Therefore, their segmentation result can be significant for patients. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Mean intersection over union (mIoU), Accuracy, Recall, Precision and F1-score are standard metrics for medical image segmentation, where mIoU is a common metric used in competitions to compare the performance between each of models. For the more exhaustive comparison between the performance of DCSAU-Net and other popular models, we calculate each of these metrics in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Augmentation</head><p>Medical image datasets usually have a limited number of samples to be available in the training phase due to obtaining and annotating images is expensive and time-consuming <ref type="bibr" target="#b42">[43]</ref>. Therefore, the model is prone to overfitting. To mitigate this issue, data augmentation methods are generally used in the training stage to extend the diversity of samples and enhance the model generalisation. In our experiment, we randomly apply horizontal flip, rotation and cutout with the probability of 0.25 to the training set of each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation Details</head><p>All experiments are implemented using PyTorch 1.10.0 framework on a single NVIDIA V100 Tensor Core GPU, 8-core CPU and 16GB RAM. We use a common segmentation loss function, Dice loss, and an Adam optimizer with a learning rate of 1e-4 to train all models. The number of batch sizes and epochs are set to 16 and 200 respectively. During training, we resize the images to 256?256 for CVC-ClinicDB and 2018 Data Science Bowl datasets. For ISIC-2018 and SegPC-2021 datasets, the input images are resized to 512?512. Also, we apply ReduceLROnPlateau to optimise the learning rate. All experiments on four datasets are conducted on the same train, validation, and test datasets. In addition, we train other SOTA models with default parameters, meanwhile, a pretrained ViT model is loaded when training the TransUNet and LeViT-UNet. The rest of models are trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results</head><p>In this section, we present quantitative results on four different biomedical image datasets and compare our proposed architecture with other SOTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">Comparison on CVC-ClinicDB Dataset</head><p>The quantitative results on CVC-ClinicDB dataset are shown in <ref type="table" target="#tab_1">Table 2</ref>. For medical image segmentation task, the performance of network on mIoU and F1-score metrics usually receives more attention. From <ref type="table" target="#tab_1">Table 2</ref>, DCSAU-Net achieves a F1-score of 0.916 and a mIoU of 0.861, which outperforms DoubleU-Net by 2.0% in terms of F1-score and 2.5% in mIoU. Particularly, our proposed model provides a significant improvement over the two recent transformerbased architectures, where the mIoU of DCSAU-Net is 6.2% and 10.7% higher than TransUNet and LeViT-UNet, and the F1-score of DCSAU-Net is 4.9% and 8.8% higher than these two models respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Comparison on SegPC-2021 Dataset</head><p>For medical image analysis, some of medical images may have multi-class objects that need to be segmented out. To satisfy this demand, we evaluate all models on SegPC-2021 dataset with two different kinds of cells. The quantitative results are provided in <ref type="table" target="#tab_2">Table 3</ref>. Compared with other SOTA models, DCSAU-Net displays the best performance in all defined metrics. Specifically, our proposed method produces a mIoU score of 0.8048 with a more significant rise of 3.6% over Unet++ and 2.8% in F1-score compared to the DoubleU-Net architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3.">Comparison on 2018 Data Science Bowl Dataset</head><p>Nuclei segmentation plays an important role in the biomedical image analysis. We use an open-access dataset from 2018 Data Science Bowl challenge to evaluate the performance of DSAU-Net and other SOTA networks. A comparison between each model is presented in <ref type="table" target="#tab_3">Table 4</ref> The results demonstrate that DCSAU-Net achieves a F1-score of 0.914 which is 1.9% higher than TransUNet and mIoU of 0.850, which is 2.5% higher than UNet3+. Overall, our proposed model demonstrates the highest score in the most of evaluation metrics, including precision and accuracy. <ref type="table" target="#tab_4">Table 5</ref> shows the quantitative results on ISIC-2018 dataset for the lesion boundary segmentation task. mIoU is an offi- cial evaluation metric for the challenge. According to <ref type="table" target="#tab_3">Table  4</ref>, DCSAU-Net has an increase of 2.4% over LeViT-UNet in this metric, and 1.8% over UNet3+ in F1-score. Within the rest of metrics, our model achieves a recall of 0.922 and an accuracy of 0.960, which is better than other baseline methods. Also, a high recall score is more favourable in clinic applications <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4.">Comparison on ISIC-2018 Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>In this section, we conduct an extensional ablation study on the DCSAU-Net. The number of parameters, floating point operations (FLOPs) and frames per second (FPS) are calculated to investigate the effectiveness of each module in more detail. <ref type="table" target="#tab_5">Table 6</ref> provides the ablation results of four configurations on all four datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1.">Significance of PFC Strategy</head><p>The PFC Strategy is an essential part of the proposed DCSAU-Net model. It uses residual depthwise separable architecture with a large kernel size to enrich low-level semantic information in the initial downsampling block and help to generate a more accurate segmentation mask. We compare the network configurations: U-Net and U-Net + PFC to evaluate the efficiency of the PFC strategy. From the mIoU metric in <ref type="table" target="#tab_5">Table 6</ref>, PFC shows an improvement of 1.9% on the CVC-ClinicDB dataset, 1.4% improvement on the SegPC-2021, 2.2% improvement on the 2018 Data Science Bowl dataset and 1.9% improvement on the ISIC 2018 dataset. Thus, it can be concluded that the PFC strategy enhances the performance of the original U-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2.">Effectiveness of CSA Block</head><p>The DCSAU-Net model uses the CSA block to combine multi-scale feature maps, which can perceive different sizes of lesions in medical images. The effectiveness of CSA block can be evaluated by comparing the configurations: U-Net and U-Net + CSA in <ref type="table" target="#tab_5">Table 6</ref>. On the mIoU, the CSA block achieves an improvement of 3.1% on the CVC-ClinicDB dataset, 1.5% improvement on the SegPC-2021, 3.1% improvement on the 2018 Data Science Bowl dataset and 2.8% improvement on the ISIC 2018 dataset. Therefore, we can argue that the CSA block performs better than the U-Net model and has a more significant impact than the PFC strategy. By taking advantage of both modules, the DCSAU-Net model (U-Net + PFC + CSA) can further improve the F1-score by 0.6% to 3.5% and the mIoU by 1.1% to 3.3% compared to the U-Net with a single PFC or CSA module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Semantic segmentation has been widely witnessed in the field of medical image analysis. Many deep learning models construct encoder-decoder architectures and fuse low-level to high-level semantic information through skip connection. These methods usually select the U-Net <ref type="bibr" target="#b18">[19]</ref> block as the header of the encoder to extract low-level semantic infor-  mation, which probably misses some momentous features in images. Our approach adopts the depthwise separable convolutions with a larger kernel size to build a novel PFC strategy that retains these primary features as much as possible. In addition, we explore the impact of depthwise convolution with different number of kernel sizes on the performance, which is presented in <ref type="table" target="#tab_6">Table 7</ref>. From the experiment results, we can observe that the DCSAU-Net model is able to achieve a similar performance when using 3x3, 5x5 and 7x7 kernel sizes. In practical scenarios, people probably select a small kernel size to reduce the number of parameters and computation costs. However, to display the best performance of our proposed architecture in the study, we use a 7x7 kernel size to train the model. Based on the efficiency of depthwise separable convolution, adding more such layers may improve the information capture capability of the PFC module in the low-level semantic layer, which is worth exploring in future work. We next establish the CSA block that not only enhances the connectivity across different channels but also strengthens the feature representation in different scales with the attention mechanism and completes the multi-scale combination in the end. The effectiveness of both modules has been shown in <ref type="table" target="#tab_5">Table 6</ref> and proved by the ablation study. Although U-Net performs a shorter inference time than the DCSAU-Net model, our approach uses a tiny number of parameters in the equal output feature channels and also expends acceptable inference time, which is more suitable for deployment on machines with limited memory.</p><p>To further demonstrate that there is a significant improvement of the DCSAU-Net model for the medical image segmentation task, we visualise some of segmentation results using all models on challenging images, which is provided in <ref type="figure" target="#fig_2">Fig. 4</ref>. From the qualitative results, the segmentation mask generated by our proposed model is able to capture more proper foreground information from low-quality images, such as incomplete staining or obscurity, compared to other SOTA methods. Although the segmentation result of DCSAU-Net is not completely correct, this imperfect mask with more shape information has the possibility to be fixed using image post-processing algorithms, such as applying conditional random fields. In our experiments, we train all models based on a standard dice loss function. We compared the convergence speed of each model on all four datasets, which is shown in <ref type="figure" target="#fig_3">Fig 5.</ref> It can be observed that our proposed model converges noticeably faster than other SOTA methods in the first 20 epochs, which means the DCSAU-Net model is able to reach reliable performance by training fewer epochs. Furthermore, Using other advanced methods in training, such as deep supervision or combined loss functions, may show higher performance in medical image segmentation. Therefore, DCSAU-Net shows its robustness and superior performance on various medical segmentation tasks and we believe it can be used as a new SOTA model for medical image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel encoder-decoder architecture for medical image segmentation, called DCSAU-Net. The presented model is comprised of the PFC strategy and the CSA block. The former enhances the ability to preserve primary features from images. The latter splits the input feature maps into two feature groups. Each group contains a different number of convolutions and highlights meaningful features using the attention mechanism. Therefore, the CSA block can combine feature maps in the different receptive fields. We evaluate our model on four different medical image segmentation datasets. The results show that the DCSAU-Net architecture achieves higher scores than other SOTA models in the F1-score and mIoU metrics. Especially, our model performs better on the multi-class segmentation task and complex images. In the future, we will focus on optimising the DCSAU-Net architecture to improve its performance and make it suitable for more medical image segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The framework of CSA block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The presentation of DCSAU-Net with PFC strategy and CSA block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison results between DCSAU-Net and other SOTA models on challenging images of four different medical segmentation datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Results of the first 20 epochs on the test dataset of four medical image segmentation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Details of the medical segmentation datasets used in our experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Images Input size Train Valid Test</cell></row><row><cell>CVC-ClinicDB</cell><cell>612</cell><cell cols="2">384?288 441 110 61</cell></row><row><cell cols="2">2018 Data Science Bowl 670</cell><cell cols="2">Variable 483 120 67</cell></row><row><cell>ISIC 2018</cell><cell cols="3">2594 Variable 1868 467 259</cell></row><row><cell>SegPC 2021</cell><cell>498</cell><cell>Variable 360 89</cell><cell>49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Results on the CVC-ClinicDB</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell><cell>mIoU</cell></row><row><cell>U-Net [19]</cell><cell cols="5">0.984?0.019 0.882?0.195 0.893?0.176 0.872?0.189 0.809?0.213</cell></row><row><cell>Unet++ [20]</cell><cell cols="5">0.984?0.022 0.919?0.139 0.859?0.197 0.876?0.184 0.811?0.196</cell></row><row><cell cols="6">Attention-UNet [28] 0.986?0.016 0.904?0.170 0.901?0.185 0.895?0.168 0.835?0.179</cell></row><row><cell>ResUNet++ [24]</cell><cell cols="5">0.982?0.021 0.870?0.191 0.853?0.213 0.854?0.196 0.781?0.213</cell></row><row><cell>R2U-Net [44]</cell><cell cols="5">0.978?0.028 0.880?0.185 0.847?0.223 0.841?0.205 0.765?0.224</cell></row><row><cell>DoubleU-Net [22]</cell><cell cols="5">0.986?0.017 0.892?0.179 0.912?0.197 0.896?0.173 0.836?0.196</cell></row><row><cell>UNet3+ [21]</cell><cell cols="5">0.984?0.022 0.907?0.152 0.885?0.155 0.892?0.171 0.827?0.191</cell></row><row><cell>TransUNet [32]</cell><cell cols="5">0.982?0.209 0.876?0.199 0.873?0.191 0.867?0.188 0.799?0.201</cell></row><row><cell>LeViT-UNet [45]</cell><cell cols="5">0.980?0.023 0.849?0.241 0.826?0.232 0.828?0.233 0.754?0.244</cell></row><row><cell>DCSAU-Net</cell><cell cols="5">0.990?0.015 0.917?0.148 0.920?0.143 0.916?0.141 0.861?0.156</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Results on the SegPC 2021 (Multiple Myeloma Plasma Cells Segmentation challenge)</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell><cell>mIoU</cell></row><row><cell>U-Net [19]</cell><cell cols="5">0.939?0.053 0.842?0.142 0.879?0.118 0.855?0.119 0.766?0.148</cell></row><row><cell>Unet++ [20]</cell><cell cols="5">0.942?0.058 0.855?0.142 0.876?0.141 0.857?0.127 0.770?0.163</cell></row><row><cell cols="6">Attention-UNet [28] 0.940?0.048 0.845?0.143 0.866?0.125 0.849?0.117 0.757?0.147</cell></row><row><cell>ResUNet++ [24]</cell><cell cols="5">0.934?0.051 0.838?0.118 0.858?0.101 0.840?0.086 0.736?0.121</cell></row><row><cell>R2U-Net [44]</cell><cell cols="5">0.933?0.056 0.852?0.122 0.831?0.136 0.834?0.112 0.744?0.128</cell></row><row><cell>DoubleU-Net [22]</cell><cell cols="5">0.937?0.052 0.833?0.120 0.896?0.084 0.858?0.089 0.763?0.130</cell></row><row><cell>UNet3+ [21]</cell><cell cols="5">0.939?0.051 0.848?0.119 0.866?0.078 0.852?0.083 0.766?0.131</cell></row><row><cell>TransUNet [32]</cell><cell cols="5">0.939?0.047 0.822?0.130 0.869?0.121 0.838?0.113 0.741?0.146</cell></row><row><cell>LeViT-UNet [45]</cell><cell cols="5">0.939?0.049 0.850?0.120 0.837?0.115 0.837?0.101 0.738?0.137</cell></row><row><cell>DCSAU-Net</cell><cell cols="5">0.950?0.045 0.871?0.113 0.910?0.067 0.886?0.078 0.806?0.121</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Results on the 2018 Data Science Bowl</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell><cell>mIoU</cell></row><row><cell>U-Net [19]</cell><cell cols="5">0.955?0.047 0.872?0.105 0.920?0.111 0.887?0.090 0.808?0.126</cell></row><row><cell>Unet++ [20]</cell><cell cols="5">0.955?0.047 0.874?0.122 0.918?0.141 0.886?0.132 0.814?0.150</cell></row><row><cell cols="6">Attention-UNet [28] 0.953?0.046 0.870?0.151 0.918?0.136 0.887?0.134 0.816?0.152</cell></row><row><cell>ResUNet++ [24]</cell><cell cols="5">0.954?0.048 0.900?0.120 0.903?0.104 0.894?0.104 0.822?0.138</cell></row><row><cell>R2U-Net [44]</cell><cell cols="5">0.956?0.047 0.884?0.135 0.911?0.140 0.891?0.135 0.822?0.156</cell></row><row><cell>DoubleU-Net [22]</cell><cell cols="5">0.955?0.045 0.876?0.111 0.927?0.131 0.889?0.133 0.817?0.150</cell></row><row><cell>UNet3+ [21]</cell><cell cols="5">0.957?0.044 0.889?0.149 0.909?0.135 0.893?0.133 0.825?0.150</cell></row><row><cell>TransUNet [32]</cell><cell cols="5">0.954?0.047 0.900?0.101 0.906?0.121 0.895?0.099 0.821?0.136</cell></row><row><cell>LeViT-UNet [45]</cell><cell cols="5">0.953?0.049 0.889?0.150 0.888?0.147 0.882?0.136 0.808?0.157</cell></row><row><cell>DCSAU-Net</cell><cell cols="5">0.959?0.045 0.914?0.098 0.924?0.077 0.914?0.077 0.850?0.114</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Results on the ISIC 2018 (Skin Lesion Segmentation challenge)</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell><cell>mIoU</cell></row><row><cell>U-Net [19]</cell><cell cols="5">0.952?0.079 0.883?0.152 0.906?0.180 0.874?0.158 0.802?0.182</cell></row><row><cell>Unet++ [20]</cell><cell cols="5">0.954?0.077 0.899?0.136 0.906?0.155 0.883?0.138 0.812?0.171</cell></row><row><cell cols="6">Attention-UNet [28] 0.954?0.078 0.915?0.140 0.890?0.171 0.883?0.149 0.814?0.180</cell></row><row><cell>ResUNet++ [24]</cell><cell cols="5">0.954?0.082 0.905?0.139 0.889?0.183 0.879?0.153 0.810?0.181</cell></row><row><cell>R2U-Net [44]</cell><cell cols="5">0.945?0.078 0.834?0.189 0.912?0.163 0.848?0.160 0.762?0.189</cell></row><row><cell>DoubleU-Net [22]</cell><cell cols="5">0.953?0.092 0.903?0.149 0.897?0.186 0.879?0.167 0.813?0.191</cell></row><row><cell>UNet3+ [21]</cell><cell cols="5">0.956?0.068 0.889?0.151 0.916?0.130 0.886?0.132 0.816?0.165</cell></row><row><cell>TransUNet [32]</cell><cell cols="5">0.945?0.085 0.847?0.186 0.898?0.185 0.849?0.178 0.770?0.203</cell></row><row><cell>LeViT-U [45]</cell><cell cols="5">0.954?0.089 0.896?0.152 0.908?0.176 0.883?0.161 0.817?0.185</cell></row><row><cell>DCSAU-Net</cell><cell cols="5">0.960?0.075 0.917?0.127 0.922?0.139 0.904?0.128 0.841?0.158</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Detailed ablation study of the DCSAU-Net architecture. Net + PFC + CSA (ours) 0.990?0.015 0.917?0.148 0.920?0.143 0.916?0.141 0.861?0.156 2.60M</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell><cell>mIoU</cell><cell cols="3">Parameters FLOPs FPS</cell></row><row><cell></cell><cell>U-Net [19]</cell><cell>0.984?0.019</cell><cell>0.882?0.195</cell><cell>0.893?0.176</cell><cell>0.872?0.189</cell><cell>0.809?0.213</cell><cell>13.40M</cell><cell>31.11</cell><cell>109.95</cell></row><row><cell>CVC-ClinicDB</cell><cell>U-Net + PFC U-Net + CSA</cell><cell>0.987?0.014 0.987?0.015</cell><cell>0.901?0.191 0.890?0.211</cell><cell>0.885?0.214 0.903?0.179</cell><cell>0.881?0.211 0.890?0.193</cell><cell>0.828?0.216 0.840?0.204</cell><cell>13.37M 2.62M</cell><cell>29.70 8.33</cell><cell>103.49 44.26</cell></row><row><cell></cell><cell cols="8">U-6.91</cell><cell>43.37</cell></row><row><cell></cell><cell>U-Net [19]</cell><cell>0.939?0.053</cell><cell>0.842?0.142</cell><cell>0.879?0.118</cell><cell>0.855?0.119</cell><cell>0.766?0.148</cell><cell>13.40M</cell><cell cols="2">124.58 48.46</cell></row><row><cell>SegPC-2021</cell><cell>U-Net + PFC U-Net + CSA</cell><cell>0.946?0.046 0.946?0.046</cell><cell>0.866?0.123 0.855?0.135</cell><cell>0.874?0.086 0.896?0.071</cell><cell>0.864?0.085 0.870?0.080</cell><cell>0.780?0.144 0.781?0.146</cell><cell>13.37M 2.62M</cell><cell cols="2">119.79 47.63 33.35 33.22</cell></row><row><cell></cell><cell cols="7">U-Net + PFC + CSA (ours) 0.950?0.045 0.871?0.113 0.910?0.067 0.886?0.078 0.806?0.121 2.60M</cell><cell>27.66</cell><cell>32.08</cell></row><row><cell></cell><cell>U-Net [19]</cell><cell>0.955?0.047</cell><cell>0.872?0.105</cell><cell>0.920?0.111</cell><cell>0.887?0.090</cell><cell>0.808?0.126</cell><cell>13.40M</cell><cell>31.11</cell><cell>125.30</cell></row><row><cell>2018 Data</cell><cell>U-Net + PFC</cell><cell>0.955?0.046</cell><cell>0.905?0.105</cell><cell>0.910?0.096</cell><cell>0.901?0.084</cell><cell>0.830?0.123</cell><cell>13.37M</cell><cell>29.70</cell><cell>117.09</cell></row><row><cell>Science Bowl</cell><cell>U-Net + CSA</cell><cell>0.957?0.045</cell><cell>0.903?0.105</cell><cell cols="2">0.925?0.090 0.908?0.082</cell><cell>0.839?0.122</cell><cell>2.62M</cell><cell>8.33</cell><cell>43.87</cell></row><row><cell></cell><cell cols="4">U-Net + PFC + CSA (ours) 0.959?0.045 0.914?0.098 0.924?0.077</cell><cell cols="3">0.914?0.077 0.850?0.114 2.60M</cell><cell>6.91</cell><cell>43.42</cell></row><row><cell></cell><cell>U-Net [19]</cell><cell>0.952?0.079</cell><cell>0.883?0.152</cell><cell>0.906?0.180</cell><cell>0.874?0.158</cell><cell>0.802?0.182</cell><cell>13.40M</cell><cell>31.11</cell><cell>115.85</cell></row><row><cell>ISIC-2018</cell><cell>U-Net + PFC U-Net + CSA</cell><cell>0.955?0.076 0.955?0.078</cell><cell>0.915?0.129 0.915?0.123</cell><cell>0.901?0.148 0.909?0.140</cell><cell>0.890?0.128 0.893?0.127</cell><cell>0.821?0.161 0.830?0.160</cell><cell>13.37M 2.62M</cell><cell>29.70 8.33</cell><cell>113.36 43.19</cell></row><row><cell></cell><cell cols="7">U-Net + PFC + CSA (ours) 0.960?0.075 0.917?0.127 0.922?0.139 0.904?0.128 0.841?0.158 2.60M</cell><cell>6.91</cell><cell>41.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>An investigation of different kernel size in the PFC block of the DCSAU-Net architecture.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Kernel Size Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell><cell>mIoU</cell><cell cols="3">Parameters FLOPs FPS</cell></row><row><cell></cell><cell>3x3</cell><cell>0.989?0.014</cell><cell>0.892?0.196</cell><cell cols="2">0.922?0.176 0.903?0.188</cell><cell>0.857?0.194</cell><cell>2.58M</cell><cell>6.24</cell><cell>43.02</cell></row><row><cell>CVC-ClinicDB</cell><cell>5x5 7x7</cell><cell cols="3">0.987?0.010 0.990?0.015 0.917?0.148 0.920?0.143 0.898?0.172 0.916?0.136</cell><cell cols="3">0.904?0.159 0.916?0.141 0.861?0.156 2.60M 0.858?0.174 2.59M</cell><cell>6.50 6.91</cell><cell>42.89 43.37</cell></row><row><cell></cell><cell>9x9</cell><cell>0.988?0.017</cell><cell>0.908?0.160</cell><cell>0.902?0.180</cell><cell>0.894?0.177</cell><cell>0.841?0.198</cell><cell>2.61M</cell><cell>7.44</cell><cell>43.39</cell></row><row><cell></cell><cell>3x3</cell><cell>0.946?0.058</cell><cell>0.866?0.118</cell><cell>0.882?0.091</cell><cell>0.869?0.075</cell><cell>0.790?0.145</cell><cell>2.58M</cell><cell>39.42</cell><cell>32.09</cell></row><row><cell>SegPC-2021</cell><cell>5x5 7x7</cell><cell cols="6">0.948?0.048 0.950?0.045 0.871?0.113 0.910?0.067 0.886?0.078 0.806?0.121 2.60M 0.863?0.122 0.901?0.070 0.877?0.080 0.800?0.131 2.59M</cell><cell>40.49 42.10</cell><cell>32.02 32.08</cell></row><row><cell></cell><cell>9x9</cell><cell>0.946?0.050</cell><cell>0.851?0.134</cell><cell>0.896?0.078</cell><cell>0.868?0.104</cell><cell>0.786?0.153</cell><cell>2.61M</cell><cell>44.25</cell><cell>31.45</cell></row><row><cell></cell><cell>3x3</cell><cell>0.958?0.045</cell><cell>0.911?0.101</cell><cell>0.920?0.076</cell><cell>0.911?0.077</cell><cell>0.845?0.115</cell><cell>2.58M</cell><cell>6.24</cell><cell>43.31</cell></row><row><cell>2018 Data</cell><cell>5x5</cell><cell>0.958?0.044</cell><cell cols="2">0.915?0.096 0.918?0.077</cell><cell>0.912?0.077</cell><cell>0.847?0.114</cell><cell>2.59M</cell><cell>6.50</cell><cell>43.12</cell></row><row><cell>Science Bowl</cell><cell>7x7</cell><cell cols="2">0.959?0.045 0.914?0.098</cell><cell cols="4">0.924?0.077 0.914?0.077 0.850?0.114 2.60M</cell><cell>6.91</cell><cell>43.42</cell></row><row><cell></cell><cell>9x9</cell><cell>0.957?0.045</cell><cell>0.908?0.106</cell><cell>0.921?0.083</cell><cell>0.908?0.081</cell><cell>0.841?0.119</cell><cell>2.61M</cell><cell>7.44</cell><cell>43.08</cell></row><row><cell></cell><cell>3x3</cell><cell>0.958?0.080</cell><cell>0.921?0.112</cell><cell>0.904?0.171</cell><cell>0.893?0.144</cell><cell>0.829?0.173</cell><cell>2.58M</cell><cell>6.24</cell><cell>42.17</cell></row><row><cell>ISIC-2018</cell><cell>5x5 7x7</cell><cell cols="2">0.959?0.077 0.960?0.075 0.917?0.127 0.919?0.127</cell><cell cols="4">0.913?0.149 0.922?0.139 0.904?0.128 0.841?0.158 2.60M 0.898?0.139 0.836?0.165 2.59M</cell><cell>6.50 6.91</cell><cell>42.12 41.91</cell></row><row><cell></cell><cell>9x9</cell><cell>0.958?0.080</cell><cell cols="2">0.922?0.117 0.903?0.164</cell><cell>0.893?0.146</cell><cell>0.830?0.172</cell><cell>2.61M</cell><cell>7.44</cell><cell>42.63</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding adversarial attacks on deep learning based medical image analysis systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107332</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tailoring therapies-improving the management of early breast cancer: St gallen international expert consensus on the primary therapy of early breast cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Winer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldhirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Gelber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gnant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccart-Gebhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Th?rlimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Senn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Members</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of oncology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1533" to="1546" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning active contour models for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Vallabhaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Czanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11632" to="11640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeply-supervised density regression for automatic cell counting in microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Minn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Solnica-Krezel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Anastasio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101892</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-resolution encoder-decoder networks for low-contrast medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="461" to="475" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on systems, man, and cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image thresholding using type ii fuzzy sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Tizhoosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2363" to="2372" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new unsupervised approach for segmenting and counting cells in high-throughput microscopy image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Riccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brancati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gragnaniello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="437" to="448" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cpfnet: Context pyramid fusion network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3008" to="3018" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>S?nchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task attention-based semi-supervised learning for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bortsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garc?a-Uceda Ju?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Tulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Bruijne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="457" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uctransnet: rethinking the skip connections in u-net from a channel-wise perspective with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2441" to="2449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nucleus segmentation across imaging experiments: the 2018 data science bowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Karhohs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Cimini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1247" to="1253" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th international symposium on biomedical imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
	<note>ISBI 2018</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vilari?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Segpc-2021: Segmentation of multiple myeloma plasma cells in microscopic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehlot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goswami</surname></persName>
		</author>
		<idno type="DOI">10.21227/7np1-2q42</idno>
		<idno>doi:10.21227/7np1-2q42</idno>
		<ptr target="https://dx.doi.org/10.21227/7np1-2q42" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation, in: Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unet 3+: A full-scale connected unet for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1055" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Doubleu-net: A deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 33rd International symposium on computer-based medical systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">De</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focusnet: An attention-based fully convolutional network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th international symposium on biomedical imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="455" to="458" />
		</imprint>
	</monogr>
	<note>ISBI 2019</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-net: A collaborative regioncontour-driven network for fine-to-finer medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1046" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<title level="m">Attention u-net: Learning where to look for the pancreas</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attention is all you need, Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11065</idno>
		<title level="m">Segmentation transformer: Object-contextual representations for semantic segmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08623</idno>
		<title level="m">Levit-unet: Make faster encoders with transformer for medical image segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<title level="m">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mo-bilenetv2: Inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">X-net: Brain stroke lesion segmentation based on depthwise separable convolution and long-range dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="247" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11963" to="11975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Road extraction by deep residual u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="749" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="652" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Resnest: Split-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2736" to="2746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Targeted gradient descent: A novel method for convolutional neural networks fine-tuning and onlinelearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Asma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06955</idno>
		<title level="m">Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Levitunet: Make faster encoders with transformer for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Head and neck tumor segmentation in pet/ct: the hecktor challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Oreiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jreige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boughdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elhalawani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valli?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">102336</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
