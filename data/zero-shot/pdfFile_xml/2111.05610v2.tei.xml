<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLIP2TV: Align, Match and Distill for Video-Text Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">OVBU</orgName>
								<orgName type="institution" key="instit2">PCG</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">OVBU</orgName>
								<orgName type="institution" key="instit2">PCG</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqi</forename><surname>Sun</surname></persName>
							<email>sunweiqi@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">OVBU</orgName>
								<orgName type="institution" key="instit2">PCG</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">OVBU</orgName>
								<orgName type="institution" key="instit2">PCG</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dedan</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">OVBU</orgName>
								<orgName type="institution" key="instit2">PCG</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Zhao</surname></persName>
							<email>lilillzhao@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">OVBU</orgName>
								<orgName type="institution" key="instit2">PCG</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CLIP2TV: Align, Match and Distill for Video-Text Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Video-Text Retrieval, Multi-Modal Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern video-text retrieval frameworks basically consist of three parts: video encoder, text encoder and the similarity head. With the success of both visual and textual representation learning, transformerbased encoders and fusion methods have also been adopted in the field of video-text retrieval. In this paper, We propose a new CLIP-based framework called CLIP2TV, which consists of a video-text alignment module and a video-text matching module. The two modules are trained end-toend in a coordinated manner, and boost the performance to each other. Moreover, to address the impairment brought by data noise, especially false negatives introduced by vague description in some datasets, we propose similarity distillation to alleviate the problem. Extensive experimental results on various datasets validate the effectiveness of the proposed methods. Finally, on common datasets of various length of video clips, CLIP2TV achieves better or competitive results towards previous SOTA methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal retrieval is a fundamental task in both research fields of multimodal learning and industrial applications. Specifically, visual-text retrieval is attracting more attention to meet the needs of increasing tons of uploaded texts, images and videos. This work focuses on the task of video-text retrieval. Recently, both the research community <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3]</ref> and the industry are putting more efforts into it. Given the textual input, the system is required to retrieve corresponding videos, which complements traditional search engines relying on pure textual labels. In reverse, given the video input, the system could also output textual descriptions, which is useful for videos without captions.</p><p>A modern video-retrieval system typically consists of three parts: text encoder, video encoder, and similarity head. With the success of transformers <ref type="bibr" target="#b39">[40]</ref> in natural language processing, visual representation learning <ref type="bibr" target="#b7">[8]</ref>, and multimodal learning <ref type="bibr" target="#b5">[6]</ref>, the three parts have all been benefited from transformer mechanisms. For text encoder, from BERT-like models pretrained on pure texts,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vague description typo misdescription</head><p>This is a cartoon a family is having coversation people are cheering players <ref type="figure">Fig. 1</ref>. Three types of noise exist in descriptions. Vague description, or referred to low-quality description, can describe multiple distinct videos. Typo will result in the loss of some key information. Misdescription is a false description of some video contents or an error caption that is completely irrelevant to the video.</p><p>to CLIP-like models pretrained on texts and images, downstream tasks can exploit them either via fine-tuning or prompt learning. For video encoder, transformerbased methods are also showing great potentials. For instance, ViT <ref type="bibr" target="#b7">[8]</ref> and video Swin-Transformer <ref type="bibr" target="#b25">[26]</ref> are used in action recognition and other related tasks. For similarity head, which is also interpreted as multi-modal fusion, can be implemented as multi-modal transformers. Heavy models like ViLBERT <ref type="bibr" target="#b26">[27]</ref> and UNITER <ref type="bibr" target="#b5">[6]</ref> fuse texts and images as input tokens, with self and mutual attention learning via heavy layers of attention networks. To fully realize the potential of transformer-based backbones, we adopt text and image encoders from CLIP <ref type="bibr" target="#b33">[34]</ref> as text and video encoders in our proposed CLIP2TV.</p><p>Basically, our work follows the line of recent CLIP-based models <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b9">[10]</ref>. A similarity head is added on two streams of visual and textual encoders. The outputs of these two modules are projected and aligned in the common multimodal space, to formulate the video-text alignment (vta) module. Besides it, we are different from <ref type="bibr" target="#b27">[28]</ref> in two respects:</p><p>(i) For similarity head, we propose a video-text matching (vtm) module initialized from multiple layers of transformers. The vtm module takes as inputs online positive and negative pairs of embeddings from text and video encoders, and formulates a matching loss like previous multi-modal transformers <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b27">[28]</ref>, a transformer head is also added on two streams, but shows worse results towards simpler heads. We claim that alignment in the first place is critical to make vtm work. On the other hand, by back propagating the gradients in an end-to-end manner, the vtm module can improve the accuracy of vta. This two modules work in a coordinated manner and boost performance to each other. Similar structure in image-text retrieval is <ref type="bibr" target="#b20">[21]</ref>, which also uses an image-text alignment module to input embeddings to the image-text matching module. Instead of <ref type="bibr" target="#b20">[21]</ref> using matching score of the similarity head of vtm as the final retrieval result, we still use nearest neighbors in the common space from vta as the retrieval results. Therefore CLIP2TV is efficient for inference.</p><p>(ii) In the training process, we observe that vtm is sensitive to noisy data thus oscillates in terms of validation accuracy. We attribute the cause of it to the data noise. There are mainly three types of noise in video-text retrieval datasets: vague description, typo and misdescription, as showed in <ref type="figure">Fig.1</ref>. Among them, vague descriptions are the most frequent. For instance, in MSR-VTT <ref type="bibr" target="#b43">[44]</ref>, some descriptions are too general like "This is a cartoon" for a cartoon video. Also in DiDeMo <ref type="bibr" target="#b0">[1]</ref>, texts like "we see only the sky" make the target video hardly stand out. This will impair the contrastive training process where negative samples might be semantically close to the anchor sample. Typo and misdescription also exist, which will result in inaccurate description of videos and impair the relation between the paired text and video. To alleviate this problem, we aim to find the metric that reflects more accurately the similarity between two modalities. Therefore we propose similarity distillation, which distills 3 types of similarities from inter-, intra-or joint space before vtm. Then we use them as additional soft labels to supervise vtm.</p><p>In summary, we propose CLIP2TV, a new CLIP-based framework to address video-text retrieval. Our contributions are threefold:</p><p>1. The framework is comprised of two modules: video-text alignment (vta) and video-text matching (vtm). They are trained end-to-end in a coordinated manner and benefit each other.</p><p>2. With data exploration and extensive experiments, we find that network training suffers impairment brought by data noise, especially videos with vague descriptions. Therefore we propose similarity distillation to alleviate the problem.</p><p>3. We conduct comprehensive experiments on multiple datasets, achieving significant improvements on the strong baseline of CLIP4Clip, and finally get better or competitive results to previous SOTA methods on common datasets. We believe this work can bring useful insights and practical expertise to both research community and industry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video-Language Modeling. VideoBERT <ref type="bibr" target="#b38">[39]</ref> firstly extends BERT <ref type="bibr" target="#b39">[40]</ref> to video representation learning and adapts it for modeling visual-linguistic relationship and joint representation learning. The following work of CBT <ref type="bibr" target="#b37">[38]</ref> lightens VideoBERT <ref type="bibr" target="#b38">[39]</ref> by combining video and text representations after separate modality training. Then HERO <ref type="bibr" target="#b21">[22]</ref> proposes a hierarchical structure for encoding video and subtitle/speech together. While ActBERT <ref type="bibr" target="#b50">[51]</ref> utilizes global action information to bridge the gap between video and language and introduces transformer block to encode global actions, local objects, and text descriptions. Different from canonical approaches, ClipBERT <ref type="bibr" target="#b18">[19]</ref> learns video and text representations by an end-to-end manner, and adopts the sparse sampling strategy and initializes model with pretrained 2D model for efficiency and lower computational burden. Further more, to fully exploit the advantage of transformer, Frozen <ref type="bibr" target="#b2">[3]</ref> leverages a dual transformer encoder structure with end-to-end training. Nowadays, models such as CLIP4Clip <ref type="bibr" target="#b27">[28]</ref> and CLIP2Video <ref type="bibr" target="#b9">[10]</ref> extended from a large-scale image-text pretrained model CLIP <ref type="bibr" target="#b33">[34]</ref> are continuously proposed. Image/Video-Text Retrieval. Multi-modal retrieval, as one of the most popular tasks in multi-modal learning, attracts lots of attention to produce valuable works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref>. ViLBERT <ref type="bibr" target="#b26">[27]</ref> and UNITER <ref type="bibr" target="#b5">[6]</ref> use a shared transformer for image-text joint representation learning and retrieval. CLIP <ref type="bibr" target="#b33">[34]</ref> uses two independent transformer to encode image and text separately, yet ALBEF <ref type="bibr" target="#b20">[21]</ref> adopts pyramid-like structure that images and texts are separately encoded in shallow layers and fused in deeper layers for matching. Recently, video-text retrieval task earns much more attention. HiT <ref type="bibr" target="#b23">[24]</ref> and TACo <ref type="bibr" target="#b44">[45]</ref> use multi-level feature alignment for better cross-modal learning. Moreover, TVR <ref type="bibr" target="#b19">[20]</ref> and HERO <ref type="bibr" target="#b21">[22]</ref> extend video-text retrieval task to video corpus moment retrieval task aiming to retrieval a whole video from a large video corpus and localize the related moment within the video by a query. Meanwhile, inspired by image-text retrieval, many studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref> try to transfer knowledge from large-scale image-text pretraining model into video-text retrieval or train models with both images and videos. Learning with Noise. Learning with noise is quite popular in recent years. Many works focus on supervised learning from noise <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref>. Since deep neural networks can easily overfit to noisy labels <ref type="bibr" target="#b47">[48]</ref>, which results in poor generalization. Several techniques have been developed to enhance the robustness to label noise, including losses that reduce the impact of outliers <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b11">12]</ref>, loss correction approaches that model the source of label noise <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b1">2]</ref>, and regularization procedures tailored to lower the impact of noise <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b31">32]</ref>. We refer readers to <ref type="bibr" target="#b36">[37]</ref> for a detailed survey of prior work on learning with label noise. In this work, we consider that existing video retrieval benchmarks are based on datasets collected for other tasks (e.g. video captioning) <ref type="bibr" target="#b42">[43]</ref>, which often leads to ambiguity of the captions (i.e. can be matched to many videos). <ref type="bibr" target="#b42">[43]</ref> proposes multi-query training and inference method to reduce the impact of vague or low quality queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Given a set of captions and a set of videos, video-text retrieval aims at seeking a matching function calculating similarities between captions and videos. Recent works like <ref type="bibr" target="#b27">[28]</ref> have showed the benefits of image-text retrieval pre-training and advantages of end-to-end training. Inspired by these, we adopt CLIP <ref type="bibr" target="#b33">[34]</ref> as our multi-modal encoder and base our work on <ref type="bibr" target="#b27">[28]</ref>. <ref type="figure">Fig.2</ref> depicts the framework of CLIP2TV. It is comprised of a video-text alignment (vta) module and a video-text matching (vtm) module. In the following, we show how vta and vtm interact with each other in detail, and how similarity distillation address the problem caused by noisy samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cosine Similarity Video-Text Align</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching Head</head><p>Matching Score Video-Text Match</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distilled Similarity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[eos]</head><p>Patch Embedding + Patch Position Embedding </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Modal Transformer</head><p>Hard Negatives ? ti m e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distilled Similarity</head><p>Video-Text Alignment Video-Text Matching <ref type="figure">Fig. 2</ref>. Framework of our method. Vta: Dual transformer encoders encode video and text representations, which are then projected and aligned in the common multimodal space. Vtm: The following multi-modal transformer and matching head take as input concatenated video-text embeddings and output the matching score. Similarity distillation: distilled similarity from intra-or inter-space serve as soft labels for videotext matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>Video Encoder: Given a video, we first split it into equal-length segments, then randomly extract one frame from each segment. For a video clip v = {v i } Nv i=1 , N v is number of frames. A transformer-based Video Encoder, i.e., ViT used in CLIP, encodes it into frame embeddings V emb ? R Nv?Dv , where D v is the dimension of frame embedding. After that, a temporal transformer with frame position embedding and residual connection is applied to frame embeddings to enhance temporal information. Then, a MLP projector projects contextualized frame embeddings into a multi-modal common space. For simplicity,we re-use V emb ? R Nv?D as enhanced temporal embeddings, where D is the dimension. The final video embedding is calculated as v = mean-pooling(V emb ) ? R D . Text Encoder: The bert-like transformer used in CLIP is adopted as our text encoder. The caption denoted as t = {t i } Nt i=1 is fed into encoder to acquire token embeddings T emb ? R Nt?Dt , where N t and D t are the number and dimension of token embeddings. Same with the procedure of encoding videos, a text projector projects token embeddings into a common sub space with dimension D. Finally, the representation of [EOS] token will serve as the embedding of the whole caption t ? R D . Contrastive Learning by Alignment (vta): To align video embeddings v and caption embeddings t in the common space, we adopt contrastive loss to maximize similarity between positive pairs and minimize similarity between negative pairs. The cosine similarity is calculated between normalized embeddings from each modality. Given a batch B with B video-text pairs, cross-entropy loss adds on softmax-normalized similarity gives the InfoNCE loss:</p><formula xml:id="formula_0">p v2t vta (v) ? exp(s(v, t)/? 1 ) t?B exp(s(v, t)/? 1 ) ,<label>(1)</label></formula><formula xml:id="formula_1">L v2t vta = E v?B H(p v2t vta (v), y v2t vta (v)) ,<label>(2)</label></formula><p>where s(?, ?) denotes cosine similarity, ? 1 represents a learnable temperature parameter, y v2t vta (?) is the ground-truth binary label that positive pairs and negative pairs are 1 and 0 respectively, and H(?, ?) is cross-entropy formulation. Meanwhile, L t2v is calculated vice versa. Then, the contrastive loss for alignment is formulated as:</p><formula xml:id="formula_2">L vta = 1 2 (L v2t vta + L t2v vta )<label>(3)</label></formula><p>Contrastive Learning by Matching (vtm): To fully exploit the abundant information between two modalities and enhance cross-modal interactions, we utilize a multi-modal fusion and matching strategy to predict whether a videotext pair is positive or negative. Similar with TACo <ref type="bibr" target="#b44">[45]</ref> and ALBEF <ref type="bibr" target="#b20">[21]</ref>, our multi-modal encoder consists of self-attention layers. It takes as input video frame embedding V emb ? R Nv?D and caption token embedding T emb ? R Nt?D and outputs fusion embedding F emb ? R (Nv+Nt)?D . We adopt embedding of [EOS] token as fusion feature of video-text pairs, which is f ? R D . Then, a matching head g(?) composed of LN-FC-ReLU-FC calculates the matching score.</p><p>The training objective of multi-modal fusion is also InfoNCE loss:</p><formula xml:id="formula_3">p vtm (v, t) ? exp(g(f (v,t) )) (v ? ,t ? )?P exp(g(f (v ? ,t ? ) )) ,<label>(4)</label></formula><formula xml:id="formula_4">L vtm = E (v,t)?B [H(p vtm (v, t), y vtm (v, t))] ,<label>(5)</label></formula><p>where P denotes the pair set that consists of a ground truth video-text pair and the negative sample pairs related to them. p vtm (?, ?) represents the probability whether a pair in the pair set is positive. And y vtm (?, ?) is the ground-truth one-hot label of the pair set, while H(?, ?) is cross-entropy formulation. In order to improve efficiency and reduce computational cost, we only select in-batch hard negative samples for multi-modal fusion. Concretely, for each video, we choose top-K negative text samples during real-time training according to eq.(1) and vice versa. Finally, B ? (2K + 1) pairs are fed into multi-modal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Similarity Distillation on vtm</head><p>As mentioned before, in the training process, we observe that vtm suffers severer oscillation than vta, and ends up with lower accuracy. We attribute this to two reasons: 1. Data noise including vague description, typo and misdescription. Among them vague description is the most frequent. Since most datasets are originally designed for video captioning, which are not particularly labeled to be distinct. This problem is also mentioned in <ref type="bibr" target="#b42">[43]</ref>. 2. The inherent nature of multi-modal transformer in vtm. For instance, anchor textual embedding and positive/negative video embedding are fused in an earlier stage, thus all the final outputs contain the information of the anchor embedding, which results in more difficulty in separation.</p><p>To seek for a more accurate metric on the similarity between positive-positive pairs and positive-negative pairs, we refer to the output embeddings from vta. There are three sub-spaces in vta: textual space, video space and the aligned multi-modal space. Each space has its own metric on the similarity of a pair of instances. This raises curiosity to us: can we refer to the similarity in them and which will be the best. Without loss of generality, we will try three variants of strategies.</p><p>1.Inter-modal : It is natural to adopt inter-modal similarity from vta as the soft label to guide video-text matching as:</p><formula xml:id="formula_5">y t2v inter = softmax (s(t, v ? )/? 2 ) and y v2t inter = softmax (s(v, t ? )/? 2 )<label>(6)</label></formula><p>where y inter denotes inter soft-label. ? 2 is temperature to control distillation. v and t denote video embedding and text embedding, respectively. 2.Intra-modal : We utilize video-video similarity in video space on textto-video matching, and text-text similarity in textual space on video-to-text matching. Formally,</p><formula xml:id="formula_6">y t2v intra = softmax (s(v, v ? )/? 2 ) and y v2t intra = softmax (s(t, t ? )/? 2 ) (7)</formula><p>where y intra denotes intra soft-label.</p><p>3.Joint-modal : Joint soft-label utilize intra-modal similarity distilled knowledge from video-modal space and text-modal space jointly to supervise vtm module. Concretely, we compute average intra-modal similarity of both modalities to obtain the joint soft-label:</p><formula xml:id="formula_7">y t2v,v2t joint = softmax (s(t, t ? ) + s(v, v ? )/2? 2 )<label>(8)</label></formula><p>where y joint denotes joint soft-label. Then we compute soft InfoNCE loss for multi-modal encoder:</p><formula xml:id="formula_8">L sof t?vtm = E (v,t)?B [H(p vtm (v, t), y * (v, t))]<label>(9)</label></formula><p>y * denotes soft-label in terms of intra, inter or joint. Finally, fuse L vtm by:</p><formula xml:id="formula_9">L f inal?vtm = (1 ? ?) ? L vtm + ? ? L sof t?vtm<label>(10)</label></formula><p>where ? is hyperparameter balancing vanilla vtm and soft vtm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Objective</head><p>Combining video-text alignment and video-text matching, we formulate the training objective as following:</p><formula xml:id="formula_10">L = L vta + L f inal?vtm .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metric</head><p>We report our experimental results on five public available video-text retrieval datasets. Following <ref type="bibr" target="#b27">[28]</ref>, we use recall at rank K (R@K), median rank (MdR) and mean rank (MnR) as metrics to evaluate our model. MSR-VTT <ref type="bibr" target="#b43">[44]</ref> is the most popular benchmark for video-text retrieval task. It consists of 10000 videos with 20 captions for each. We report our results on the standard full split <ref type="bibr" target="#b8">[9]</ref> and 1K-A split <ref type="bibr" target="#b45">[46]</ref>.  <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b2">[3]</ref>, we treat it as a videoparagraph retrieval task that all of the captions belonging to the same video will be concatenated to form a paragraph.</p><p>ActivityNet <ref type="bibr" target="#b17">[18]</ref> is comprised of 19,994 videos downloaded from YouTube. We follow <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b10">[11]</ref> to concatenate all of the video related descriptions to form a paragraph for retrieval and evaluate our model on 'val1' split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementations Details</head><p>The basic video encoder and text encoder are both initialized by CLIP <ref type="bibr" target="#b33">[34]</ref>. Besides basic encoders, frame position embedding is initialized with position embedding used in CLIP's text encoder. Both temporal transformer and multimodal transformer are initialized by the part of layers from text encoder in CLIP. Concretely, the width, heads, and layers are 512, 8 and 4, respectively. The fixed video length is 12 and caption length is 32 for MSR-VTT <ref type="bibr" target="#b43">[44]</ref>, MSVD <ref type="bibr" target="#b3">[4]</ref>, VATEX <ref type="bibr" target="#b40">[41]</ref>. As for DiDeMo <ref type="bibr" target="#b0">[1]</ref> and ActivityNet <ref type="bibr" target="#b17">[18]</ref>, we set both video length and caption length to 64. ? and ? 2 for similarity distillation are 0.5 and 0.5, respectively. Following <ref type="bibr" target="#b27">[28]</ref>, we finetune our model with Adam optimizer in 10 epochs with 256 batch size and 8 for K. The learning rate is 1e ? 7 for the basic video encoder and text encoder, and 1e ? 4 for new layers such as frame position embedding, temporal transformer and multi-modal encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Art Methods</head><p>We compare our model with other state-of-the-art methods. <ref type="table">Table.</ref>1 shows the text-to-video and video-to-text retrieval results of CLIP2TV on MSR-VTT 1kA. As non-CLIP-based methods are usually trained beyond feature-level, which means models adopt video frame features extracted by off-the-shelf pretrained visual models as inputs, they cannot exceed the zero-shot performance of CLIP which is trained end-to-end with much more data. We categorize these approaches into CLIP-based and others for a fair comparison. On MSR-VTT 1kA, we achieve state-of-the-art results on both text-to-video and video-to-text retrieval. Upon strong baseline of CLIP4clip, CLIP2TV improves R1 of text-tovideo to 45.6, and R1 of video-to-text to 43.9. CLIP2TV+SD shows the results with similarity distillation. It further improves CLIP2TV on nearly on all metrics. Particularly, when replace ViT-32 with ViT-16, we can acquire higher performance(with 3.2% improvement in text-to-video retrieval at R@1). <ref type="table" target="#tab_2">Table.2-Table.</ref>6 present the text-to-video retrieval results on MSR-VTT full, MSVD, DiDeMo, ActivityNet and VATEX. For DiDeMo and ActivityNet, we reproduce results based on the provided code of CLIP4Clip and denote as CLIP4Clip-meanP*. On MSR-VTT full, MSVD, and VATEX, we obtain comparable performance to previous SOTA CLIP2Video with ViT32. On MSVD, though the temporal transformer hardly learns temporal representation with very few frames <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10]</ref>, we still boost the performance close to CLIP2Video. We also note that on VATEX, similarity distillation boosts the performance with small margin. This is mainly due to the reason that captions in VATEX are relatively detailed and distinct, which just validates our claim. Our method outperforms previous  <ref type="table">Table 3</ref>. MSVD</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Text-to-Video R@1 R@5 R@10 MdR MnR CE <ref type="bibr" target="#b24">[25]</ref> 19.8 49.0 63.8 6.0 -SUPPORT-SET <ref type="bibr" target="#b29">[30]</ref> 28.4 60.0 72.9 4.0 -Frozen <ref type="bibr" target="#b2">[3]</ref> 33.7 64.7 76.3 3.0 -CLIP-straight <ref type="bibr" target="#b32">[33]</ref> 37.0 64.1 73.8 3.0 -CLIP4Clip-meanP <ref type="bibr" target="#b27">[28]</ref>    SOTA methods by a large margin on paragraph-to-video retrieval on DiDeMo and ActivityNet, which further demonstrates our superiority on longer videos with longer descriptions. Since our method is orthogonal to CLIP2Video, it is expected that combining CLIP2TV with CLIP2Video might further improve the results. We leave this for future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Why alignment with matching? To evaluate the effectiveness of alignment and matching, we test various combinations of vta and vtm. The ablation results are presented in <ref type="table">Table.</ref>7. The 1st row shows the result of vta without vtm. It can be seen as fine-tuning CLIP on other datasets. The result is still competitive due to the outstanding ability of CLIP pretrained on huge data. However, as shown in the 2nd row of <ref type="table">Table.</ref>7, when we remove the alignment part and train our model with only vtm, the result is severely degraded. Worse on ActivityNet, the training is totally decayed. We attribute this catastrophe to the nature of the dataset. On the one hand, videos in ActivityNet are usually longer than 60 seconds, and some videos can be as long as 100 seconds or even more than 200 seconds, so that input frames with fixed length of 64 cannot cover all the scenes described in paragraph. On the other hand, the paragraph paired with a video consists of several sentences only describing the corresponding video segments, while such video segments may be discontinuous in the video, which means some input frames sampled from the interval of two segments are invalid. These two aspects make it difficult for multi-modal encoder to model the relationship between text words and video frames. This justifies the necessity of aligning two modalities before sending them to multi-modal transformer. The 3rd row is CLIP2TV combining vta with vtm, retrieving the result from vta. The result shows that it consistently outperforms vta except on ActivityNet. The last row shares the same structure with the 3rd row, but retrieves the result from vtm. For inference efficiency, we only select top-200 candidates from vta as inputs to vtm for re-ranking. Since the ground truth target might not be in the 200 candidates, the mean ranking of ground truth will be absent from vtm in such case, therefore we neglect MnR results here. We can see that inference result from vtm is much worse than it from vta. This observation is different from <ref type="bibr" target="#b20">[21]</ref> on image-text retrieval. We speculate the reason is vtm pretrained on images and texts lacks enough videos for further pretrain.</p><p>To sum up, vta and vtm work in a coordinated manner and benefit each other. The alignment is essential and vta can generate hard negatives to vtm. In reverse, vtm tunes vta more finely via back propagating gradients. Moreover, since we retrieve the targets using vta, CLIP2TV avoids computing candidates in vtm with the heavy load, and is practically friendly. What is the best negative sampling strategy for vtm? As we know, negative samples plays a crucial role in contrastive learning. Will a larger number of negatives bring benefits as in self-supervised learning? With these questions, we design thorough experiments to find the answer. We choose MSR-VTT 1kA with noisy captions and VATEX with more detailed descriptions. We use K to denote number of negatives. Results shown in <ref type="table">Table.</ref>8 demonstrate that with the increase of K, it brings more noise and ambiguity in pairs which hurts the performance. The model decays or saturates when the number of negative samples reaches a certain amount. As MSR-VTT has more noisy samples, a small K is good for it, while VATEX can support larger K value. How to distill similarity for vtm? We show performance of three variants of similarity distillation on MSR-VTT 1kA in <ref type="table">Table.</ref>9. We can see that all three variants can improve the results above vanilla vtm. Specifically, results from joint and intra are nearly the same, and intra is slightly better than inter. This implies that intra-modal similarity reflects more accurate relative distances between an anchor representation of text/video, towards a group of video/text representations. Moreover, joint inner-modal metric from video and text is more accurate than single modal. To better illustrate the difference of intra-and inter-modal similarity, we show similarity matrices in <ref type="figure">Figure.</ref>3, we can see that overall similarity values are gradually increasing along (a)text-video, (b)video-video, and (c)text-text. (d) is the average similarity of text-text and video-video. In <ref type="table">Table.</ref>10, we show the results of vanilla vtm, soft-vtm and mean of them by changing the value of ?, across datasets of MSR-VTT, DiDeMo and Activ-ityNet. Since these datasets contain relatively more data noise, thus similarity distillation contributes more on them. Also, combining vanilla vtm and soft-vtm achieves the best results. We leave more choices of ? for future study.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>To illustrate the advantage of the proposed method and the impact of noise, we visualize some text-to-video retrieval results on MSR-VTT 1kA shown in <ref type="figure">Fig.4</ref>. The 1st row is the text for query, and the 2nd row is the corresponding groundtruth video followed by its retrieval rank obtained from CLIP4Clip, CLIP2TV, and CLIP2TV+SD, respectively. The 3rd-5th rows present the retrieved R@1 video by each method and its paired caption.</p><p>As shown in <ref type="figure">Fig. 4</ref>, our proposed CLIP2TV can perform better and obtain more accurate retrieval results than CLIP4Clip. With the help of similarity distillation from joint soft-label on vtm module, CLIP2TV+SD can make a great improvement when querying a detailed text. On the other hand, CLIP2TV+SD is more robust to vague (3rd column of <ref type="figure">Fig.4</ref>) and typo (4th column of <ref type="figure">Fig.4</ref>) query, which illustrates that similarity distillation can alleviate the negative effects of data noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose CLIP2TV, a new CLIP-based framework on video-text retrieval. CLIP2TV is composed of a video-text alignment module and a videotext matching module. The two modules are trained in a coordinated manner and benefit each other. To address the problem brought by data noise from popular datasets, we propose similarity distillation and explore different variants based  <ref type="figure">Fig. 4</ref>. Visualized text-to-video retrieval results on MSR-VTT 1kA. The 1st row is the query, and the 2nd row is the corresponding ground-truth video followed by ranks acquired from CLIP4Clip, CLIP2TV, and CLIP2TV+SD, respectively. The 3rd-5th rows are the retrieved top-1 video by each method and its paired caption. The word "judes" in red is a typo.</p><p>in intra-and inter-modal space. To validate our motivation and explore best practice in this field, we conduct extensive experiments across several datasets. Experimental results prove the effectiveness of ours and we achieve better or competitive results towards previous SOTA methods. CLIP2TV is fast in inference and orthogonal to current CLIP-based frameworks, thus can be easily combined with them. We believe our work can bring insights and practical expertise to both community and industry.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Visualized similarity matrix. (a) text-video similarity. (b) text-text similarity. (c) video-video similarity. (d) joint-modal similarity which is average of text-text and video-video similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>9 a</head><label>9</label><figDesc>clip from fox news on the shelby north carolina shooting a clip from fox news on the shelby north carolina shooting newscasters speak about a school shooting on the news program info wars someone is taking about a car jeremy is describing a car a man is showing off a new vehicle a rock band performs on stage a band plays on a stage a man is playing a guitar with a band in a live concert a couple dancing doing salsa a person is cooking on stage two people welcome people to an episode of their show</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Retrieval result on MSR-VTT 1kA split. SD means Similarity Distillation. And the CLIP2TV-ViT16 is always combined with SD. We copy the results of other methods from corresponding papers.</figDesc><table><row><cell>Recall at rank 1(R@1)?, rank 5(R@5)?, rank</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>MSR-VTT full</figDesc><table><row><cell>Method</cell><cell cols="2">Text-to-Video R@1 R@5 R@10 MdR MnR</cell></row><row><cell>CE [25]</cell><cell cols="2">10.0 29.0 41.2 16.0 86.2</cell></row><row><cell cols="2">HT-pretrained [29] 14.9 40.2 52.8 9.0</cell><cell>-</cell></row><row><cell cols="3">CLIP-straight [33] 21.4 41.1 50.4 10.0 -</cell></row><row><cell>MDMMT [9]</cell><cell cols="2">23.1 49.8 61.8 6.0 52.8</cell></row><row><cell cols="3">CLIP2Video [10] 29.8 55.5 66.2 4.0 45.5</cell></row><row><cell>CLIP2TV</cell><cell cols="2">29.6 55.4 66.0 4.0 48.2</cell></row><row><cell cols="3">CLIP2TV+SD 29.9 55.6 66.3 4.0 48.1</cell></row><row><cell cols="3">CLIP2TV-ViT16 32.4 58.2 68.6 3.0 43.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>DiDeMo</figDesc><table><row><cell>Method</cell><cell cols="4">Text-to-Video R@1 R@5 R@10 MdR MnR</cell></row><row><cell>CE [25]</cell><cell>16.1 41.1</cell><cell>-</cell><cell cols="2">8.3 43.7</cell></row><row><cell>ClipBERT [19]</cell><cell cols="3">20.4 48.0 60.8 6.0</cell><cell>-</cell></row><row><cell>Frozen [3]</cell><cell cols="3">34.6 65.0 74.7 3.0</cell><cell>-</cell></row><row><cell cols="5">CLIP4Clip-meanP [28] 43.4 70.2 80.6 2.0 17.5</cell></row><row><cell cols="5">CLIP4Clip-seqTransf [28] 43.4 69.9 80.2 2.0 17.5</cell></row><row><cell cols="5">CLIP4Clip-meanP* [28] 42.1 69.3 79.6 2.0 18.0</cell></row><row><cell>CLIP2TV</cell><cell cols="4">43.9 70.5 79.8 2.0 16.6</cell></row><row><cell>CLIP2TV+SD</cell><cell cols="4">45.5 69.7 80.6 2.0 17.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>ActivityNet</figDesc><table><row><cell>Method</cell><cell cols="4">Text-to-Video R@1 R@5 R@50 MdR MnR</cell></row><row><cell>CE [25]</cell><cell cols="4">18.2 47.7 91.4 6.0 23.1</cell></row><row><cell>ClipBERT [19]</cell><cell>21.3 49.0</cell><cell>-</cell><cell>6.0</cell><cell>-</cell></row><row><cell>SUPPORT-SET [30]</cell><cell cols="3">26.8 58.1 93.5 3.0</cell><cell>-</cell></row><row><cell cols="5">MMT-pretrained [11] 28.7 61.4 94.5 3.3 16.0</cell></row><row><cell>HIT-pretrained [24]</cell><cell cols="3">29.6 60.7 95.6 3.0</cell><cell>-</cell></row><row><cell cols="5">CLIP4Clip-meanP [28] 40.5 72.4 98.1 2.0 7.4</cell></row><row><cell cols="5">CLIP4Clip-seqTransf [28] 40.5 72.4 98.2 2.0 7.5</cell></row><row><cell cols="5">CLIP4Clip-meanP* [28] 40.0 71.2 98.0 2.0 7.7</cell></row><row><cell>CLIP2TV</cell><cell cols="4">40.8 72.9 98.0 2.0 7.3</cell></row><row><cell>CLIP2TV+SD</cell><cell cols="4">44.1 75.2 98.4 2.0 6.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>VATEX. CLIP4Clip-seqTransf* means there are no results about VATEX of HGR split reported in<ref type="bibr" target="#b27">[28]</ref> and<ref type="bibr" target="#b9">[10]</ref>, so we conduct it by ourselves.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell></cell><cell cols="7">R@1 R@5 R@10 MdR MnR</cell></row><row><cell></cell><cell cols="4">CLIP-straight [33]</cell><cell>39.7</cell><cell></cell><cell>72.3</cell><cell>82.2</cell><cell>2.0</cell><cell></cell><cell>12.8</cell></row><row><cell></cell><cell cols="4">SUPPORT-SET [30]</cell><cell>44.9</cell><cell></cell><cell>82.1</cell><cell>89.7</cell><cell>1.0</cell><cell></cell><cell>-</cell></row><row><cell cols="6">CLIP4Clip-seqTransf* [28] 60.3</cell><cell></cell><cell>89.1</cell><cell>94.4</cell><cell>1.0</cell><cell></cell><cell>4.5</cell></row><row><cell></cell><cell cols="3">CLIP2Video [10]</cell><cell></cell><cell>61.2</cell><cell></cell><cell>90.9</cell><cell>95.6</cell><cell>1.0</cell><cell></cell><cell>3.4</cell></row><row><cell></cell><cell></cell><cell cols="2">CLIP2TV</cell><cell></cell><cell>61.4</cell><cell></cell><cell>90.6</cell><cell>95.2</cell><cell>1.0</cell><cell></cell><cell>3.7</cell></row><row><cell></cell><cell cols="3">CLIP2TV+SD</cell><cell></cell><cell cols="3">61.5 90.9</cell><cell>95.6</cell><cell>1.0</cell><cell></cell><cell>3.7</cell></row><row><cell></cell><cell cols="4">CLIP2TV-ViT16</cell><cell cols="3">65.4 92.7</cell><cell>96.6</cell><cell>1.0</cell><cell></cell><cell>2.9</cell></row><row><cell cols="13">Table 7. Evaluation of Multi-Modal Fusion with vanilla vtm on different datasets.</cell></row><row><cell cols="13">Recall at rank 1 (R@1)? and Mean Rank (MnR)? of text-to-video retrieval are reported.</cell></row><row><cell cols="13">M-V and Anet mean MSR-VTT and ActivityNet, respectively. The underline indicates</cell></row><row><cell cols="8">that retrieval results are inferred from this module.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>vta vtm</cell><cell cols="12">M-V 1kA M-V full R@1 MnR R@1 MnR R@1 MnR R@1 MnR R@1 MnR R@1 MnR MSVD VATEX DiDeMo Anet</cell></row><row><cell>?</cell><cell cols="12">43.8 15.8 29.2 49.5 45.8 11.1 60.3 4.5 41.6 18.9 42.2 7.4</cell></row><row><cell cols="13">? 30.4 67.4 17.3 357.4 31.7 16.1 56.2 4.0 16.7 53.2 0.2 1851.2</cell></row><row><cell cols="13">? ? 45.6 15.0 29.6 48.2 46.3 10.0 61.4 3.7 43.9 16.6 40.8 7.3</cell></row><row><cell cols="2">? ? 39.9</cell><cell>-</cell><cell>26.2</cell><cell>-</cell><cell>37.2</cell><cell>-</cell><cell>59.0</cell><cell>-</cell><cell>34.6</cell><cell>-</cell><cell>33.5</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Evaluation of the number of negative pairs K. We report our experiment results on MSR-VTT 1kA and VATEX.</figDesc><table><row><cell>K</cell><cell cols="4">MSR-VTT 1kA R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR VATEX</cell></row><row><cell cols="2">4 45.1 71.1 81.7</cell><cell>2.0 15.3 60.7 90.4 95.4</cell><cell>1.0</cell><cell>3.7</cell></row><row><cell cols="2">8 45.6 71.1 80.8</cell><cell>2.0 15.0 61.4 90.6 95.2</cell><cell>1.0</cell><cell>3.7</cell></row><row><cell cols="2">16 44.5 71.0 81.1</cell><cell>2.0 14.5 61.3 90.6 95.4</cell><cell>1.0</cell><cell>3.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>Evaluation of soft-label variants. We report our experimental results on the MSR-VTT 1kA. Evaluation of soft vtm. We report our experimental results on MSR-VTT 1kA, DiDeMo and ActivityNet.</figDesc><table><row><cell>soft-label</cell><cell>R@1</cell><cell>R@5</cell><cell>R@10</cell><cell>MdR</cell><cell>MnR</cell></row><row><cell>inter</cell><cell>45.8</cell><cell>72.5</cell><cell>81.6</cell><cell>2.0</cell><cell>14.6</cell></row><row><cell>intra</cell><cell>45.9</cell><cell>72.3</cell><cell>81.5</cell><cell>2.0</cell><cell>15.3</cell></row><row><cell>joint</cell><cell>46.1</cell><cell>72.5</cell><cell>82.9</cell><cell>2.0</cell><cell>15.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00650</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10638" to="10647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04309</idno>
		<title level="m">Robust contrastive learning against noisy views</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Komkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petiushko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10699</idno>
		<title level="m">Mdmmt: Multidomain multimodal transformer for video retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11097</idno>
		<title level="m">Clip2video: Mastering video-text retrieval via image clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11736</idno>
		<title level="m">Ranking info noise contrastive estimation: Boosting contrastive learning via ranked positives</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning cross-modal retrieval with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5403" to="5413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning with noisy correspondence for cross-modal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tvr: A large-scale dataset for video-subtitle moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="447" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07651</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hero: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00200</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1910" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15049</idno>
		<title level="m">Hit: Hierarchical transformer with momentum contrast for video-text retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<title level="m">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m">Regularizing neural networks by penalizing confident output distributions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A straightforward framework for video retrieval using clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12443</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08199</idno>
		<title level="m">Learning from noisy labels with deep neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4581" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03639</idno>
		<title level="m">Multi-query video retrieval</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Taco: Token-aware cascade contrastive learning for video-text alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11562" to="11572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding deep learning (still) requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
