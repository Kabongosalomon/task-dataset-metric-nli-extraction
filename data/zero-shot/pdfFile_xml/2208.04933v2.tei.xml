<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simplified State Space Layers for Sequence Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">T H</forename><surname>Smith</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Warrington</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
						</author>
						<title level="a" type="main">Simplified State Space Layers for Sequence Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Efficiently modeling long sequences is a challenging problem in machine learning. Information crucial to solving tasks may be encoded jointly between observations that are thousands of timesteps apart. Specialized variants of recurrent neural networks (RNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b7">8]</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref>, and transformers <ref type="bibr" target="#b56">[57]</ref> have been developed to try to address this problem. In particular, many efficient transformer methods have been introduced <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b58">59]</ref> to address the standard transformer's quadratic complexity in the sequence length. However, these more efficient transformers still perform poorly on very long-range sequence tasks <ref type="bibr" target="#b54">[55]</ref>.</p><p>Gu et al. <ref type="bibr" target="#b16">[17]</ref> presented an alternative approach using structured state space sequence (S4) layers. An S4 layer defines a nonlinear sequence-to-sequence transformation via a bank of many independent single-input, single-output (SISO) linear state space models (SSMs) <ref type="bibr" target="#b17">[18]</ref>, coupled together with nonlinear mixing layers. Each SSM leverages the HiPPO framework <ref type="bibr" target="#b14">[15]</ref> by initializing with specially constructed state matrices. Since the SSMs are linear, each layer can be equivalently implemented as a convolution, which can then be applied efficiently by parallelizing across the sequence length. Multiple S4 layers can be stacked to create a deep sequence model. Such models have achieved significant improvements over previous methods, including on the long range arena (LRA) <ref type="bibr" target="#b54">[55]</ref> benchmarks specifically designed to stress test long-range sequence models. Extensions have shown good performance on raw audio generation <ref type="bibr" target="#b13">[14]</ref> and classification of long movie clips <ref type="bibr" target="#b25">[26]</ref>.</p><p>We introduce a new state space layer that builds on the S4 layer, the S5 layer, illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. S5 streamlines the S4 layer in two main ways. First, S5 uses one multi-input, multi-output (MIMO) SSM in place of the bank of many independent SISO SSMs in S4. Second, S5 uses an efficient and widely implemented parallel scan. This removes the need for the convolutional and frequency-domain approach used by S4, which requires a non-trivial computation of the convolution  kernel. The resulting state space layer has the same computational complexity as S4, but operates purely recurrently and in the time domain. We then establish a mathematical relationship between S4 and S5. This connection allows us to inherit the HiPPO initialization schemes that are key to the success of S4. Unfortunately, the specific HiPPO matrix that S4 uses for initialization cannot be diagonalized in a numerically stable manner for use in S5. However, in line with recent work on the DSS <ref type="bibr" target="#b21">[22]</ref> and S4D <ref type="bibr" target="#b18">[19]</ref> layers, we found that a diagonal approximation to the HiPPO matrix achieves comparable performance. We extend a result from Gu et al. <ref type="bibr" target="#b18">[19]</ref> to the MIMO setting, which justifies the diagonal approximation for use in S5. We leverage the mathematical relationship between S4 and S5 to inform several other aspects of parameterization and initialization, and we perform thorough ablation studies to explore these design choices.</p><p>The final S5 layer has many desirable properties. It is straightforward to implement (see Appendix A), 1 enjoys linear complexity in the sequence length, and can efficiently handle timevarying SSMs and irregularly sampled observations (which is intractable with the convolution implementation of S4). S5 achieves state-of-the-art performance on a variety of long-range sequence modeling tasks, with an LRA average of 87.4%, and 98.5% accuracy on the most difficult Path-X task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We provide the necessary background in this section prior to introducing the S5 layer in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Linear State Space Models</head><p>Continuous-time linear SSMs are the core component of both the S4 layer and the S5 layer. Given an input signal u(t) ? R U , a latent state x(t) ? R P and an output signal y(t) ? R M , a linear continuous-time SSM is defined by the differential equation:</p><formula xml:id="formula_0">dx(t) dt = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t),<label>(1)</label></formula><p>and is parameterized by a state matrix A ? R P ?P , an input matrix B ? R P ?U , an output matrix C ? R M ?P and a feedthrough matrix D ? R M ?U . For a constant step size, ?, the SSM can be discretized using, e.g. Euler, bilinear or zero-order hold (ZOH) methods to define the linear recurrence</p><formula xml:id="formula_1">x k = Ax k?1 + Bu k , y k = Cx k + Du k ,<label>(2)</label></formula><p>where the discrete-time parameters are each a function, specified by the discretization method, of the continuous-time parameters. We refer the reader to Iserles <ref type="bibr" target="#b24">[25]</ref> for more information on discretization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parallelizing Linear State Space Models With Scans</head><p>We use parallel scans to efficiently compute the states of a discretized linear SSM. Given a binary associative operator ? (i.e. (a ? b) ? c = a ? (b ? c)) and a sequence of L elements [a 1 , a 2 , ..., a L ], the scan operation (sometimes referred to as all-prefix-sum) returns the sequence</p><formula xml:id="formula_2">[a 1 , (a 1 ? a 2 ), ..., (a 1 ? a 2 ? ... ? a L )].<label>(3)</label></formula><p>Computing a length L linear recurrence of a discretized SSM, x k = Ax k?1 + Bu k as in <ref type="formula" target="#formula_1">(2)</ref>, is a specific example of a scan operation. As discussed in Section 1.4 of Blelloch <ref type="bibr" target="#b5">[6]</ref>, parallelizing the linear recurrence of the latent transitions in the discretized SSM above can be computed in a parallel time of O(T log L), assuming L processors, where T represents the cost of matrix-matrix multiplication. For a general matrix A ? R P ?P , T is O(P 3 ). This can be prohibitively expensive in deep learning settings. However, if A is a diagonal matrix, the parallel time becomes O(P log L) with L processors and only requires O(P L) space. Finally, we note that efficient parallel scans are implemented in a work-efficient manner, thus the total computational cost of the parallel scan with a diagonal matrix is O(P L) operations. Appendix H provides a self-contained summary of parallel scans for linear recurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">S4: Structured State Space Sequence Layers</head><p>The S4 layer <ref type="bibr" target="#b16">[17]</ref> defines a nonlinear sequence-to-sequence transformation, mapping from an input sequence u 1:L ? R L?H to an output sequence u 1:L ? R L?H . An S4 layer contains a bank of H independent single-input, single-output (SISO) SSMs with N -dimensional states. Each S4 SSM is applied to one dimension of the input sequence. This results in an independent linear transformation from each input channel to each preactivation channel. A nonlinear activation function is then applied to the preactivations. Finally, a position-wise linear mixing layer is applied to combine the independent features and produce the output sequence u 1:L . <ref type="figure">Figure 4a</ref> in the appendix illustrates the view of the S4 layer as a bank of independent SSMs. <ref type="figure" target="#fig_2">Figure 2a</ref> shows an alternative view of S4 as one large SSM with state size HN and block-diagonal state, input and output matrices. Each S4 SSM leverages the HiPPO framework for online function approximation <ref type="bibr" target="#b14">[15]</ref> by initializing the state matrices with a HiPPO matrix (most often the HiPPO-LegS matrix). This was demonstrated empirically to lead to strong performance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>, and can be shown as approximating long-range dependencies with respect to an infinitely long, exponentially-decaying measure <ref type="bibr" target="#b19">[20]</ref>. While the HiPPO-LegS matrix is not stably diagonalizable <ref type="bibr" target="#b16">[17]</ref>, it can be represented as a normal plus low-rank (NPLR) matrix. The normal component, referred to as HiPPO-N and denoted A Normal LegS , can be diagonalized. Thus, the HiPPO-LegS can be conjugated into a diagonal plus low-rank (DPLR) form, which S4 then utilizes to derive an efficient form of the convolution kernel. This motivates S4's DPLR parameterization.</p><p>Efficiently applying the S4 layer requires two separate implementations depending on context: a recurrent mode and a convolution mode. For online generation, the SSM is iterated recurrently, much like other RNNs. However, when the entire sequence is available and the observations are evenly spaced, a more efficient convolution mode is used. This takes advantage of the ability to represent the linear recurrence as a one-dimensional convolution between the inputs and a convolution kernel for each of the SSMs. Fast Fourier transforms (FFTs) can then be applied to efficiently parallelize this application. <ref type="figure">Figure 4a</ref> in the appendix illustrates the convolution approach of the S4 layer for offline processing. We note that while parallel scans could, in principle, allow a recurrent approach to be used in offline scenarios, applying the parallel scan to all H of the N -dimensional SSMs would in general be much more expensive than the convolution approach S4 actually uses.</p><p>The trainable parameters of each S4 layer are the H independent copies of the learnable SSM parameters and the O(H 2 ) parameters of the mixing layer. For each of the h ? {1, ..., H} S4 SSMs, given a scalar input signal u (h) (t) ? R, an S4 SSM uses an input matrix B (h) ? C N ?1 , a DPLR parameterized transition matrix A (h) ? C N ?N , an output matrix C (h) ? C 1?N , and feedthrough matrix D (h) ? R 1?1 , to produce a signal y (h) (t) ? R. To apply the S4 SSMs to discrete sequences, each continuous-time SSM is discretized using a constant timescale parameter ? (h) ? R + . The learnable parameters of each SSM are the timescale parameter ? (h) ? R + , the continuous-time parameters B (h) , C (h) , D (h) , and the DPLR matrix, parameterized by vectors ? (h) ? C N and p (h) , q (h) ? C N representing the diagonal matrix and low-rank terms respectively. For notational compactness we denote the concatenation of the H S4 SSM states at discrete time index k as x</p><formula xml:id="formula_3">(1:H) k = (x (1) k ) , . . . , (x (H) k )</formula><p>, and the H SSM outputs as y k = y</p><formula xml:id="formula_4">(1) k , . . . , y (H) k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The S5 Layer</head><p>In this section we present the S5 layer. We describe its structure, parameterization and computation, particularly focusing on how each of these differ from S4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">S5 Structure: From SISO to MIMO</head><p>The S5 layer replaces the bank of SISO SSMs (or large block-diagonal system) in S4 with a multi-input, multi-output (MIMO) SSM, as in <ref type="formula" target="#formula_0">(1)</ref>, with a latent state size P , and input and output dimension H. The discretized version of this MIMO SSM can be applied to a vector-valued input sequence u 1:L ? R L?H , to produce a vector-valued sequence of SSM outputs (or preactivations) y 1:L ? R L?H , using latent states x k ? R P . A nonlinear activation function is then applied to produce a sequence of layer outputs u 1:L ? R L?H . See <ref type="figure" target="#fig_2">Figure 2b</ref> for an illustration. Unlike S4, we do not require an additional position-wise linear layer, since these features are already mixed. We note here that compared to the HN latent size of the block-diagonal SSM in the S4 layer, S5's latent size P can be significantly smaller, allowing for the use of efficient parallel scans, as we discuss in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">S5 Parameterization: Diagonalized Dynamics</head><p>The parameterization of the S5 layer's MIMO SSM is motivated by the desire to use efficient parallel scans. As discussed in Section 2.2, a diagonal state matrix is required to efficiently compute the linear recurrence using a parallel scan. Thus, we diagonalize the system, writing the continuous-time state matrix as A = V?V ?1 , where ? ? C P ?P denotes the diagonal matrix containing the eigenvalues and V ? C P ?P corresponds to the eigenvectors. Therefore, we can diagonalize the continuous-time latent dynamics from (1) as</p><formula xml:id="formula_5">dV ?1 x(t) dt = ?V ?1 x(t) + V ?1 Bu(t).<label>(4)</label></formula><formula xml:id="formula_6">Definingx(t) = V ?1 x(t),B = V ?1 B, andC = CV gives a reparameterized system, dx(t) dt = ?x(t) +Bu(t), y(t) =Cx(t) + Du(t).<label>(5)</label></formula><p>Next k  Note D is omitted for simplicity. We view An S4 layer as a single block-diagonal SSM with a latent state of size HN , followed by a nonlinearity and mixing layer to mix the independent features. (b) In contrast, the S5 layer uses a dense, MIMO linear SSM with latent size P HN . This is a linear SSM with a diagonal state matrix. This diagonalized system can be discretized with a timescale parameter ? ? R + using the ZOH method to give another diagonalized system with parameters</p><formula xml:id="formula_7">? = e ?? , B = ? ?1 (? ? I)B, C =C, D = D.<label>(6)</label></formula><p>In practice, we find it beneficial to use a vector of learnable timescale parameters ? ? R P (see Section 4.3). We also note that in practice we restrict the feedthrough matrix D to be diagonal. The S5 layer therefore has the learnable parameters:</p><formula xml:id="formula_8">B ? C P ?H ,C ? C H?P , diag(D) ? R H , diag(?) ? C P , and ? ? R P .</formula><p>Initialization Prior work showed that the performance of deep state space models are sensitive to the initialization of the state matrix <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>. We discussed in Section 2.2 that state matrices must be diagonal for efficient application of parallel scans. We also discussed in Section 2.3 that the HiPPO-LegS matrix cannot be diagonalized stably, but that the HiPPO-N matrix can be. In Section 4 we connect the dynamics of S5 to S4 to suggest why initializing with HiPPO-like matrices may also work well in the MIMO setting. We support this empirically, finding that diagonalizing the HiPPO-N matrix leads to good performance, and perform ablations in Appendix E to compare to other initializations. We note that DSS <ref type="bibr" target="#b21">[22]</ref> and S4D <ref type="bibr" target="#b18">[19]</ref> layers also found strong performance in the SISO setting by replacing S4's DPLR parameterization with a diagonalization of the HiPPO-N matrix.</p><p>Conjugate Symmetry The complex eigenvalues of a diagonalizable matrix with real entries always occur in conjugate pairs. We enforce this conjugate symmetry by only using half the eigenvalues and latent state. This reduces the number of parameters required, and reduces the runtime and memory usage of the parallel scan by a factor of two. This idea is also discussed in Gu et al. <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">S5 Computation: Fully Recurrent</head><p>Compared to the large HN effective latent size of the block-diagonal S4 layer, the smaller latent dimension of the S5 layer (P ) allows the use of efficient parallel scans when the entire sequence is available. The S5 layer can therefore be efficiently used as a recurrence in the time domain for both online generation and offline processing. Parallel scans and the continuous-time parameterization also allow for efficient handling of irregularly sampled time series or time-varying SSMs, by simply supplying a different A k matrix at each step. We leverage this feature and apply S5 to an example with irregularly sampled data in Section 6.3. In contrast, the convolution of the S4 layer requires a time invariant system and regularly spaced observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Matching the Computational Efficiency of S4 and S5</head><p>A key design desiderata for S5 was matching the computational complexity of S4 for both online generation and offline recurrence. The following proposition guarantees that their complexities are of the same order if S5's latent size P = O(H). Proof. See Appendix C.1.</p><p>We also support this proposition with empirical comparisons in Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Relationship Between Sand S5</head><p>Having introduced the S5 layer, we now establish a relationship between the dynamics of S5 and S4. In Section 4.1 we show that, under certain conditions, the outputs of the S5 SSM can be interpreted as a projection of the latent states computed by a particular S4 system. This interpretation motivates the use of HiPPO initializations for S5, which we discuss in more detail in Section 4.2. Finally, we discuss in Section 4.3 how the conditions required to relate the dynamics further motivate initialization and parameterization choices. More detailed discussion, derivations and ablation experiments are included in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Different Output Projections of Equivalent Dynamics</head><p>We compare the dynamics of S4 and S5 under some simplifying assumptions:</p><p>Assumption 1. We consider only H-dimensional to H-dimensional sequence maps.</p><p>Assumption 2. We assume the state matrix of each S4 SSM is identical,</p><formula xml:id="formula_9">A (h) = A ? C N ?N .</formula><p>Assumption 3. We assume the timescales of each S4 SSM are identical, ? (h) = ? ? R + Assumption 4. We assume that the same state matrix A is used in S5 as in S4 (also cf. Assumption 2). Note this also specifies the S5 latent size P = N . We also assume the S5 input matrix is the horizontal concatenation of the column input vectors used by S4:</p><formula xml:id="formula_10">B B (1) | . . . | B (H) .</formula><p>We will discuss relaxing these assumptions shortly, but under these conditions it is straightforward to derive a relationship between the dynamics of S4 and S5: Proposition 2. Consider an S5 layer, with state matrix A, input matrix B and some output matrix C (cf. Assumption 1); and an S4 layer, where each of the H S4 SSMs has state matrix A (cf. Assumption 2, 4) and input vector B (h) (cf. Assumption 4). If the S4 and S5 layers are discretized with the same timescales (cf. Assumption 3), then the S5 SSM produces outputs, y k , equivalent to a linear combination of the latent states of the H S4 SSMs, y k = C equiv x</p><formula xml:id="formula_11">(1:H) k , where C equiv = [ C ? ? ? C ]. Proof. See Appendix D.2.</formula><p>Importantly, the S5 SSM outputs are not equal to the outputs of the block-diagonal S4 SSM. Instead they are equivalent to the outputs of the block-diagonal S4 SSM with modified output matrix C equiv . Under the assumptions, however, the underlying state dynamics are equivalent. Recalling that initializing the S4 dynamics with HiPPO was key to performance <ref type="bibr" target="#b16">[17]</ref>, the relationship established in Proposition 2 motivates using HiPPO initializations for S5, as we now discuss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Diagonalizable Initialization</head><p>Ideally, given the interpretation above, we would initialize S5 with the exact HiPPO-LegS matrix. Unfortunately, as discussed in Section 2.3, this matrix is not stably diagonalizable, as is required for the efficient parallel scans used for S5. However, Gupta et al. <ref type="bibr" target="#b21">[22]</ref> and Gu et al. <ref type="bibr" target="#b18">[19]</ref> showed empirically that removing the low rank terms and initializing with the diagonalized HiPPO-N matrix still performed well. Gu et al. <ref type="bibr" target="#b18">[19]</ref> offered a theoretical justification for the use of this normal approximation for single-input systems: in the limit of infinite state dimension, the linear ODE with HiPPO-N state matrix produces the same dynamics as an ODE with the HiPPO-LegS matrix. Using linearity, it is straightforward to extend this result to the multi-input system that S5 uses:</p><p>Corollary 1 (Extension of Theorem 3 in Gu et al. <ref type="bibr" target="#b18">[19]</ref>).</p><formula xml:id="formula_12">Consider A LegS ? R N ?N , A Normal LegS ? R N ?N , B LegS ? R N ?H , P LegS ? R N as defined in Appendix B.1.1. Given vector-valued inputs u(t) ? R H , the ordinary differential equation dx (t) dt = A Normal LegS x (t) + 1 2 B LegS u(t) converges to dx(t) dt = A LegS x(t) + B LegS u(t) as N ? ?.</formula><p>We include a simple proof of this extension in Appendix D.3. This extension motivates the use of HiPPO-N to initialize S5's MIMO SSM. Note that S4D (the diagonal extension of S4) uses the same HiPPO-N matrix. Thus, when under the assumptions in Proposition 2, an S5 SSM in fact produces outputs that are equivalent to a linear combination of the latent states produced by S4D's SSMs. Our empirical results in Section 6 suggest that S5 initialized with the HiPPO-N matrix performs just as well as S4 initialized with the HiPPO-LegS matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relaxing the Assumptions</head><p>We now revisit the assumptions required for Proposition 2, since they only relate a constrained version of S5 to a constrained version of S4. Regarding Assumption 2, Gu et al. <ref type="bibr" target="#b16">[17]</ref> report that S4 models with tied state matrices can still perform well, though allowing different state matrices often yields higher performance. Likewise, requiring a single scalar timescale across all of the S4 SSMs, per Assumption 3, is overly restrictive. S4 typically learns different timescale parameters for each SSM <ref type="bibr" target="#b19">[20]</ref> to capture different timescales in the data. To relax these assumptions, note that Assumption 4 constrains S5 to have dimension P = N , and N is typically much smaller than the dimensionality of the inputs, H. Proposition 1 established that S5 can match S4's complexity with P as large as O(H). By allowing for larger latent state sizes, Assumptions 2 and 3 can be relaxed, as discussed in Appendix D.4. We also discuss how this relaxation motivates a block-diagonal initialization with HiPPO-N matrices on the diagonal, which often empirically improves performance compared to just using one large HiPPO-N matrix. Finally, to further relax the tied timescale assumptions, we note that in practice, we find improved performance by learning P different timescales (one per state). See Appendix D.5 for further discussion of this empirical finding. Finally, see the ablations in Appendix E.1 related to the ideas in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>S5 is most directly related to S4 and its other extensions, which we have discussed thoroughly thus far. However, there is a large body of literature on sequence modeling that uses similar ideas to those developed here. For example, prior work studied approximating nonlinear RNNs with stacks of linear RNNs connected by nonlinear layers, while also using parallel scans <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b41">42]</ref>. Martin &amp; Cundy <ref type="bibr" target="#b41">[42]</ref> showed that several efficient RNNs, such as QRNNs <ref type="bibr" target="#b6">[7]</ref> and SRUs <ref type="bibr" target="#b33">[34]</ref>, fall into a class of linear surrogate RNNs that can leverage parallel scans. Kaul <ref type="bibr" target="#b27">[28]</ref> also used parallel scans for an approach that approximates RNNs with stacks of discrete-time single-input, multi-output (SIMO) SSMs. However, S4 and S5 are the only methods to significantly outperform other comparable state-of-the-art nonlinear RNNs, transformers and convolution approaches. Our ablation study in Appendix E.2 suggests that this performance gain over prior attempts at parallelized linear RNNs is likely due to the continuous-time parameterization and the HiPPO initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We now compare empirically the performance of the S5 layer to the S4 layer and other baseline methods. We use the S5 layer as a drop-in replacement for the S4 layer. The architecture consists of a linear input encoder, stacks of S5 layers, and a linear output decoder <ref type="bibr" target="#b16">[17]</ref>. For all experiments we choose the S5 dimensions to ensure similar computational complexities as S4, following the conditions discussed in Section 3.3, as well as comparable parameter counts. The results we present show that the S5 layer matches the performance and efficiency of the S4 layer. We include in the appendix further ablations, baselines and runtime comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Long Range Arena</head><p>The long range arena (LRA) benchmark <ref type="bibr" target="#b54">[55]</ref> is a suite of six sequence modeling tasks, with sequence lengths from 1,024 to over 16,000. The suite was specifically developed to benchmark the performance of architectures on long-range modeling tasks (see Appendix G for more details). <ref type="table">Table 1</ref> presents S5's LRA performance in comparison to other methods. S5 achieves the highest average score among methods that have linear complexity in sequence length (most notably S4, S4D, and the concurrent works: Liquid-S4 <ref type="bibr" target="#b22">[23]</ref> and Mega-chunk <ref type="bibr" target="#b39">[40]</ref>). Most significantly, S5 achieves the highest score among all models (including Mega <ref type="bibr" target="#b39">[40]</ref>) on the Path-X task, which has by far the longest sequence length of the tasks in the benchmark. <ref type="table">Table 1</ref>: Performance on the LRA benchmark tasks <ref type="bibr" target="#b54">[55]</ref>. indicates the model did not exceed random guessing. We include an expanded table, <ref type="table">Table 7</ref>, with full citations in the appendix. We follow the procedure reported in Gu et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> and report means across three seeds for S4, S4D (as reported by Gu et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>) and S5. Bold scores indicate highest performance, underlined scores indicate second placed performance. We also include the results for the concurrent methods Liquid-S4 <ref type="bibr" target="#b22">[23]</ref> and Mega <ref type="bibr" target="#b39">[40]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Raw Speech Classification</head><p>The Speech Commands dataset <ref type="bibr" target="#b59">[60]</ref> contains high-fidelity sound recordings of different human readers reciting a word from a vocabulary of 35 words. The task is to classify which word was spoken. We show in <ref type="table" target="#tab_2">Table 2</ref> that S5 outperforms the baselines, outperforms previous S4 methods and performs similarly to the concurrent Liquid-S4 method <ref type="bibr" target="#b22">[23]</ref>. As S4 and S5 methods are parameterized in continuous-time, these models can be applied to datasets with different sampling rates without the need for re-training, simply by globally re-scaling the timescale parameter ? by the ratio between the new and old sampling rates. The result of applying the best S5 model trained on 16kHz data, to the speech data sampled (via decimation) at 8kHz, without any additional fine-tuning, is also presented in <ref type="table" target="#tab_2">Table 2</ref>. S5 also improves this metric over the baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Variable Observation Interval</head><p>The final application we study here highlights how S5 can naturally handle observations received at irregular intervals. S5 does so by supplying a different ? t value to the discretization at each step. We use the pendulum regression example presented by Becker et al. <ref type="bibr" target="#b3">[4]</ref> and Schirmer et al. <ref type="bibr" target="#b52">[53]</ref>, illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. The input sequence is a sequence of L = 50 images, each 24 ? 24 pixels in size, that has been corrupted with a correlated noise process and sampled at irregular intervals from a continuous trajectory of duration T = 100. The targets are the sine and cosine of the angle of the pendulum, which follows a nonlinear dynamical system. The velocity is unobserved. We match the architecture, parameter count and training procedure of Schirmer et al. <ref type="bibr" target="#b52">[53]</ref>. <ref type="table" target="#tab_3">Table 3</ref> summarizes the results of this experiment. S5 outperforms CRU on the regression task, recovering a lower mean error. Furthermore, S5 is as much as 100-times faster than CRU on the same hardware.  where ? t is the angle of the pendulum at time t, that are used as the regression targets. <ref type="table" target="#tab_12">Table 10</ref> in Appendix F.4 shows results of S5 on other common benchmarks including sequential MNIST, permuted sequential MNIST and sequential CIFAR (color). We see that S5 broadly matches the performance of S4, and outperforms a range of state-of-the-art RNN-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Pixel-level 1-D Image Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduce the S5 layer for long-range sequence modeling. The S5 layer modifies the internal structure of the S4 layer, and replaces the frequency-domain approach used by S4 with a purely recurrent, time-domain approach leveraging parallel scans. S5 achieves high performance while retaining the computational efficiency of S4. S5 also provides further opportunities. For instance, as demonstrated in Section 6.3, parallel scans allow efficient handling of sequences sampled at variable sampling rates. The concurrently developed method, Liquid-S4 <ref type="bibr" target="#b22">[23]</ref>, uses an input dependent state matrix, which highlights the opportunities of time-varying SSMs. S5's parallel scan can allow for naturally handling time-varying SSMs. The more general MIMO SSM design will also enable connections to be made with classical probabilistic state space modeling. More broadly, we hope the simplicity and generality of the S5 layer can expand the use of state space layers in deep sequence has combined state space layers with an attention mechanism to achieve similarly high performance.</p><p>Combining such attention mechanisms with S5 is an exciting opportunity for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix for: Simplified State Space Layers for Sequence Modeling</head><p>Contents:</p><p>? Appendix A: JAX Implementation of S5 Layer.</p><p>? Appendix B: S5 Layer Details.</p><p>? Appendix C: Computational Efficiency of S5.</p><p>? Appendix D: Relationship Between S4 and S5.</p><p>? Appendix E: Ablations.</p><p>? Appendix F: Supplementary Results.</p><p>? Appendix G: Experiment Configurations.</p><p>? Appendix H: Background on Parallel Scans for Linear Recurrences.</p><p>A JAX Implementation of S5 Layer 1 import jax 2 import jax.numpy as np 3 parallel_scan = jax.lax.associative_scan Returns: <ref type="bibr" target="#b23">24</ref> new element ( A_out, Bu_out ) """ <ref type="bibr" target="#b24">25</ref> A_i, Bu_i = element_i <ref type="bibr" target="#b25">26</ref> A_j, Bu_j = element_j <ref type="bibr" target="#b26">27</ref> return A_j * A_i, A_j * Bu_i + Bu_j <ref type="bibr">28 29</ref> def apply_ssm(Lambda_bar, B_bar, C_tilde, D, input_sequence): <ref type="bibr" target="#b29">30</ref> """ Compute the LxH output of discretized SSM given an LxH input. Here we provide additional details to supplement the discussion of initialization in Section 3.2. Gu et al. <ref type="bibr" target="#b19">[20]</ref> explains the ability of S4 to capture long-range dependencies when using the HiPPO-LegS matrix via decomposing the input with respect to an infinitely long, exponentially decaying measure. The HiPPO-LegS matrix and corresponding SISO input vector are defined as</p><formula xml:id="formula_13">(A LegS ) nk = ? ? ? ? ? ? (2n + 1) 1/2 (2k + 1) 1/2 , n &gt; k n + 1, n = k 0, n &lt; k . (7) (b LegS ) n = (2n + 1) 1 2 .<label>(8)</label></formula><p>Note that in Section 4.2, the input matrix B LegS ? R N ?H used in Corollary 1 is formed by concatenating H copies of b LegS ? R N . Theorem 1 of Gu et al. <ref type="bibr" target="#b16">[17]</ref> then shows that the HiPPO matrices in Gu et al. <ref type="bibr" target="#b14">[15]</ref>, A HiPPO ? R N ?N can be represented with a normal plus low-rank (NPLR) form consisting of a normal matrix, A Normal HiPPO = V?V * ? R N ?N , and a low-rank term</p><formula xml:id="formula_14">A HiPPO = A Normal HiPPO ? PQ = V (? ? (V * P)(V * Q) * ) V * (9) for unitary V ? C N ?N , diagonal ? ? C N ?N , and low-rank factorization P, Q ? R N ?r .</formula><p>The right hand side of this equation shows HiPPO matrices can be conjugated into a diagonal plus low-rank (DPLR) form. The HiPPO-LegS matrix can therefore be written in terms of the normal HiPPO-N matrix and low-rank term P LegS ? R N <ref type="bibr" target="#b13">[14]</ref> as</p><formula xml:id="formula_15">A LegS = A Normal LegS ? P Legs P Legs<label>(10)</label></formula><p>where</p><formula xml:id="formula_16">A Normal LegS nk = ? ? ? ? ? ? (n + 1 2 ) 1/2 (k + 1 2 ) 1/2 , n &gt; k 1 2 , n = k (n + 1 2 ) 1/2 (k + 1 2 ) 1/2 , n &lt; k .<label>(11)</label></formula><p>P Legs n = (n + 1 2 )</p><formula xml:id="formula_17">1 2<label>(12)</label></formula><p>Our default is to set the S5 layer state matrix A = A Normal LegS ? R P ?P , and take the eigendecomposition of this matrix to recover the initial ?. We often find it beneficial to also use V and V ?1 = V * to initializeB andC, as described below.</p><p>As mentioned in Section 4.3, we also found that performance on many tasks benefited from initializing the S5 state matrix as block-diagonal, with each block on the diagonal equal to A Normal LegS ? R R?R , where R here is less than the state dimension P , e.g. R = P 4 when 4 blocks are used on the diagonal. We then take the eigendecomposition of this matrix to initialize ?, as well asB andC. We note that even in this case,B andC are still initialized in dense form and there is no constraint that requires A to remain block-diagonal during learning. In the hyperparameter table in Appendix G, the J hyperparameter indicates the number of these HiPPO-N blocks used on the diagonal for initialization, where J = 1 indicates we used the default case of initializing with a single HiPPO-N matrix. We discuss the motivation for this block-diagonal initialization further in Appendix D.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Initialization of Input, Output and Feed-through Matrices</head><p>In general we explicitly initialize the the input matrixB and output matrixC using the eigenvectors from the diagonalization of the initial state matrix. Specifically, we sample B and C and then initialize the (complex) learnable parametersB asB = V ?1 B andC = CV.</p><p>We initialize D ? R H by independently sampling each element from a standard normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3 Initialization of the Timescales</head><p>Prior work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref> found the initialization of this timescale parameter to be important. This is studied in detail in Gu et al. <ref type="bibr" target="#b19">[20]</ref>. We sample these parameters in line with S4 and sample each element of log ? ? R P from a uniform distribution on the interval [log ? min , log ? max ), where the default range is ? min = 0.001 and ? max = 0.1. The only the exception is the Path-X experiment, where we initialize from ? min = 0.0001 and ? max = 0.1 to account for the longer timescales as discussed in Gu et al. <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Comparison of S4 and S5 Computational Elements</head><p>In <ref type="figure">Figure 4</ref> we illustrate a comparison of the computational details of the S4 and S5 layers for efficient, parallelized offline processing.  Proof. We first consider the case where the entire sequence is available and compare the S4 layer's convolution mode to the S5 layer's use of a parallel scan. We then consider the online generation case where each method operates recurrently.</p><p>Parallelized offline processing We consider the application of both the S4 and S5 layer to a vector-valued sequence u 1:L ? R L?H . Note that there are multiple ways to compute the convolution kernel for S4/S4D and the exact computational complexity depends on the implementation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>. However, the overall complexity of computing the S4 SSM outputs given the inputs is lower bounded by the FFTs used to convert into the frequency domain to apply the convolutions. Therefore Thus, S4 and S5 have the same order computational complexity and memory requirements in both cases. <ref type="table">Table 4</ref> provides an empirical evaluation of the runtime performance, in terms of speed and memory, between S4, S4D and S5 across a range of sequence lengths from the LRA tasks. We compared the JAX implementation of S5 to a JAX implementation of S4 and S4D, based on the JAX implementation from Rush &amp; Karamcheti <ref type="bibr" target="#b50">[51]</ref>. For a fair comparison, we modified these existing JAX implementations <ref type="table">Table 4</ref>: Benchmarking the runtime performance of S4, S4D and S5 on three LRA tasks of varied sequence lengths using parameterizations described in Section C.2. For speeds, &gt; 1? indicates faster than the S4D baseline. For memory, &lt; 1? indicates less memory was used than the S4D baseline. The fifth line of each metric shows the performance of the actual S5 model used for the LRA result in <ref type="table">Table 1</ref> for each task using the architecture reported in <ref type="table" target="#tab_13">Table 11</ref>. As in <ref type="table" target="#tab_13">Table 11</ref> 0.7 ? 1.0 ? 0.9 ? of S4 and S4D to allow them both to enforce conjugate symmetry and use bidirectionality. For each task, models use bidirectionality and conjugate symmetry as reported in Gu et al. <ref type="bibr" target="#b18">[19]</ref>. All models, except for the italicised S5 row, use the same input/output features H and number of layers as reported in Gu et al. <ref type="bibr" target="#b18">[19]</ref>. The S4 and S4D layers also use the same S4 SSM latent size as reported in Gu et al. <ref type="bibr" target="#b18">[19]</ref>. All methods used the same batch size and all comparisons were made using a 16GB NVIDIA V100 GPU. Note we observed the JAX S4D implementation to in general be faster than the JAX S4 implementation (possibly due to this specific S4 implementation's use of the naive Cauchy kernel computation <ref type="bibr" target="#b16">[17]</ref>). For this reason, we consider S4D as the baseline. We consider three configurations of S5 for comparison. The first two configurations, corresponding to lines 3 and 4 for each metric in <ref type="table">Table 4</ref>, show how the runtime metrics vary as S5's latent size is adjusted, with all other architecture choices equal to those of S4. In line 3 of each metric, we denote the S5 "Architecture" as "(P=H) Matched to Gu et al. <ref type="bibr" target="#b18">[19]</ref>" to indicate that this configuration of S5 sets the latent size equal to the number of input/output features, P = H. This line empirically supports the complexity argument presented in Appendix C.1. In line 4 of each metric, we denote the S5 "Architecture" as " (P=N) Matched to Gu et al. <ref type="bibr" target="#b18">[19]</ref>" to indicate that this configuration of S5 sets the latent size P equal to the latent size N S4 uses for each of its SISO SSMs. This line also corresponds to the constrained version of S5 that performs similarly to S4/S4D as presented in the ablation study in <ref type="table">Table 5</ref>. The runtime results of both of these configurations supports the claim in Section 4.3 that the latent size of S5 can be increased while maintaining S4's computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Empirical Runtime Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Finally, we include a third configuration of S5, presented in the fifth line of each metric and italicized. This configuration of S5 uses the best architectural dimensions from <ref type="table" target="#tab_13">Table 11</ref> and was used for the corresponding LRA results in <ref type="table">Table 1</ref>.</p><p>Importantly, the broad takeaway from this empirical study is that the runtime and memory usage of S5 and S4/S4D are broadly similar, as suggested by the complexity analysis in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Relationship Between S4 and S5</head><p>We now describe in more detail the connection between the S4 and S5 architectures. This connection allowed us to develop more performant architectures and extend theoretical results from existing work.</p><p>We break this analysis down into three parts:</p><p>1. In Section D.2 we prove Proposition 2. We exploit the linearity of the systems to identify that the latent states computed by the S5 SSM are equivalent to a linear combination of latent states computed by the H SISO S4 SSMs, and that the outputs of the S5 SSM are a further linear transformation of these states. We then highlight how S4 and S5 effectively define different output matrices in the block-diagonal perspective shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>2. In Section D.3 we provide a simple extension of the proof provided by Gu et al. <ref type="bibr" target="#b18">[19]</ref>. The original proof shows that in the SISO case, in the limit of large N , the dynamics arising from a (non-diagonalizable) HiPPO-LegS matrix, are faithfully approximated by the (diagonalizable) normal component of the HiPPO-LegS matrix. We extend this proof to apply to the MIMO setting. This motivates initialization with the HiPPO-N matrix, which in-turn allows us to use parallel scans efficiently.</p><p>3. In Section D.4 we conclude by showing that, by judicious choice of initialization of the S5 state matrix, S5 can implement multiple independent S4 systems and relax the assumptions made.</p><p>In addition, these systems can become coupled during optimization. We also discuss the vector of timescale parameters, which we found to improve performance.</p><p>We note that many of these results follow straightforwardly from the linearity of the recurrence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Assumptions</head><p>For these following sections we will use the following assumptions, until otherwise stated: Assumption 1. We consider only H-dimensional to H-dimensional sequence maps.</p><p>Assumption 2. We assume the state matrix of each S4 SSM is identical,</p><formula xml:id="formula_18">A (h) = A ? C N ?N .</formula><p>Assumption 3. We assume the timescales of each S4 SSM are identical, ? (h) = ? ? R + Assumption 4. We assume that the same state matrix A is used in S5 as in S4 (also cf. Assumption 2). Note this also specifies the S5 latent size P = N . We also assume the S5 input matrix is the horizontal concatenation of the column input vectors used by S4, B B (1) | . . . | B (H) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Different Output Projections of Equivalent Dynamics</head><p>We provide a proof of Proposition 2. </p><formula xml:id="formula_19">= C equiv x (1:H) k , where C equiv = [ C ? ? ? C ].</formula><p>Proof. For a single S4 SSM, the discretized latent states can be expressed as a function of the input sequence u 1:</p><formula xml:id="formula_20">L ? R L?H x (h) k = k i=1 A k?i B (h) u (h) i .<label>(13)</label></formula><p>For an S5 layer, the latent states are expressible as</p><formula xml:id="formula_21">x k = k i=1 A k?i Bu i ,<label>(14)</label></formula><p>where we index as B</p><formula xml:id="formula_22">B (1) | . . . | B (H) and u i u (1) i , . . . , u (H) i</formula><p>Here we make the observation:</p><formula xml:id="formula_23">x k = H h=1 x (h) k ,<label>(15)</label></formula><p>where this result follows directly from the linearity of (13) and <ref type="bibr" target="#b13">(14)</ref>. This shows that (under the assumptions outlined above) the states of the MIMO S5 SSM are equivalent to the summation of the states across the H different SISO S4 SSMs. We can then consider the effect of the output matrix C. For S5, the output matrix is a single dense matrix</p><formula xml:id="formula_24">y k = Cx k .<label>(16)</label></formula><p>We can substitute the relationship in <ref type="bibr" target="#b14">(15)</ref> into <ref type="formula" target="#formula_0">(16)</ref> to cast the outputs of the MIMO S5 SSM in terms of the state of the H SISO S4 SSMs:</p><formula xml:id="formula_25">y k = C H h=1 x (h) k ,<label>(17)</label></formula><formula xml:id="formula_26">= H h=1 Cx (h) k .<label>(18)</label></formula><p>Denoting the vertical concatenation of the H S4 SSM state vectors x</p><formula xml:id="formula_27">(1:H) k = x (1) k , . . . , x (H) k</formula><p>, we see that the outputs of the S5 SSM are expressible as:</p><formula xml:id="formula_28">y k = C equiv x (1:H) k , where C equiv = [ C | ? ? ? | C ] ,<label>(19)</label></formula><p>and hence are equivalent to a linear combination of the HN states computed by the H S4 SSMs.</p><p>This shows the outputs of the constrained S5 SSM under consideration (cf. Assumption 4) can be interpreted as a linear combination of the latent states computed by H constrained S4 SSMs with the same state matrices and timescale parameters. Note however, it does not show that the outputs of the S5 SSM directly equal to the outputs of the effective block-diagonal S4 SSM. Indeed, they are not equal, and we can repeat this analysis for the S4 layer to concretely identify the difference. For comparison we assume that the output vector for each S4 SSM is given as a row in the S5 output matrix, i.e. C = C (1) | . . . | C <ref type="bibr">(H)</ref> . We can express the output of each S4 SSM as</p><formula xml:id="formula_29">y (h) k = C (h) x (h) k ,<label>(20)</label></formula><p>where y (h) k ? R. We can then define the effective output matrix that operates on the entire latent space (the dashed box labelled C in <ref type="figure" target="#fig_2">Figure 2a</ref>) in S4 as</p><formula xml:id="formula_30">y (h) k = C S4 x k (h)<label>(21)</label></formula><p>By inspecting <ref type="bibr" target="#b18">(19)</ref> and <ref type="formula" target="#formula_0">(21)</ref>, we can concretely express the difference in the equivalent output matrix used by both layers</p><formula xml:id="formula_31">C S4 = ? ? ? C (1) ? ? ? 0 . . . . . . . . . 0 ? ? ? C (H) ? ? ? , C equiv = ? ? ? C (1) ? ? ? C (1) . . . . . . . . . C (H) ? ? ? C (H) ? ? ? = [ C | ? ? ? | C ] .<label>(22)</label></formula><p>In S4, the effective output matrix consists of independent vectors on the leading diagonal (as is pictured in <ref type="figure" target="#fig_2">Figure 2a</ref>). In contrast, the effective output matrix used by S5 instead ties dense output matrices across the H S4 SSMs. As such, S5 can be interpreted as simply defining a different projection of the H independent SISO SSMs than is used by S4. Note that both projection matrices have the same number of parameters. Although the projection is different, the fact that the latent dynamics can still be interpreted as a linear projection of the same underlying S4 latent dynamics suggests that initializing the state dynamics in S5 with the HiPPO-LegS matrix may lead to good performance, similarly to what was observed in S4. We discuss this in the next section. We note that it is not obvious whether tying the dense output matrices is any more or less expressive than S4's use of a single untied output vector for each SSM, and it is unlikely that one approach is universally better than the other. We also stress that one would never implement S4 using the block diagonal matrix in <ref type="bibr" target="#b21">(22)</ref>, or, implement S5 using the repeated matrix in <ref type="bibr" target="#b21">(22)</ref>. These matrices are simply constructs for understanding the equivalence between S4 and S5.</p><p>Finally, we note an alternative view: the block-diagonal S4 with output matrix C equiv of Proposition 2 is equivalent to a version of S4 that uses a bank of single-input, multi-output (SIMO) SSMs with tied state matrices, timescales and multi-channel output matrices C (h) = C ? R H?N and sums the individual SSM outputs. See appendix of Gu et al. <ref type="bibr" target="#b17">[18]</ref> for a discussion of how the linear state space layer (LSSL), a predecessor of S4, can have multiple output channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Diagonalizable Initialization</head><p>Proposition 2 suggests that initializing with the HiPPO-LegS matrix may yield good performance in S5, just as it does in S4 (because the constrained version of S5 under consideration is effectively a different linear projection of the same latent dynamics). However, the HiPPO-LegS matrix is not stably diagonalizable. Corollary 1 allows us to initialize MIMO SSMs with the diagonalizable HiPPO-N matrix to approximate the HiPPO-LegS matrix and expect the performance to be comparable. Proof. Theorem 3 in Gu et al. <ref type="bibr" target="#b18">[19]</ref> shows the following relationship for scalar input signals as N ? ?:</p><formula xml:id="formula_32">dx (h) (t) dt = A Normal LegS x (h) (t) + 1 2 B (h) LegS u (h) (t),<label>(23)</label></formula><p>where the only modification we have made is introducing the (h) superscript to allow us to explicitly index dimensions later. We define B = B</p><p>(1)</p><formula xml:id="formula_33">LegS | . . . | B (H)</formula><p>LegS . We wish to extend this to the case of vector-valued input signals.</p><p>We first recall <ref type="bibr" target="#b14">(15)</ref>, which shows that the latent states of the MIMO S5 SSM are the summation of the latent states of the H SISO S4 SSMs (to which Theorem 3 from Gu et al. <ref type="bibr" target="#b16">[17]</ref> applies). Although we derived <ref type="bibr" target="#b14">(15)</ref> in discrete time, it applies equally in continuous time:</p><formula xml:id="formula_34">x(t) = H h=1 x (h) (t).<label>(24)</label></formula><p>We can therefore define the derivative of the S5 state as:</p><formula xml:id="formula_35">dx(t) dt = H h=1 dx (h) (t) dt .<label>(25)</label></formula><p>Substituting <ref type="bibr" target="#b22">(23)</ref> into this then yields:</p><formula xml:id="formula_36">dx(t) dt = H h=1 A Normal LegS x (h) (t) + 1 2 B (h) LegS u (h) (t) ,<label>(26)</label></formula><formula xml:id="formula_37">= A Normal LegS H h=1 x (h) (t) + 1 2 H h=1 B (h) LegS u (h) (t),<label>(27)</label></formula><formula xml:id="formula_38">= A Normal LegS x(t) + 1 2 B LegS u(t).<label>(28)</label></formula><p>This equivalence motivates initializing S5 state matrices with the diagonalizable HiPPO-N matrix, and suggests that we can expect to see similar performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Relaxing the Assumptions</head><p>Here we discuss how relaxing the constraint on S5's latent size from Assumption 4 helps to relax the assumptions on the tied S4 SSM state matrices (Assumption 2) and timescales (Assumption 3) as well as the tied output matrices that result from Proposition 2.</p><p>We start by considering the case when the S5 SSM state matrix is block-diagonal. Consider an S5 SSM with latent size JN = O(H) and block-diagonal A ? R JN ?JN , dense B ? R JN ?H , dense C ? R H?JN , and J different timescale parameters ? ? R J . As a result of the block-diagonal state matrix, this system has a latent state x k ? R JN that can be partitioned into J different states x (j) k ? R N . We can then partition this system into J different subsystems and discretize each subsystem with one of the ? (j) to get the following discretized system:</p><formula xml:id="formula_39">A = ? ? ? ? A (1) . . . A (J) ? ? ? ? , B = ? ? ? ? B (1) . . . B (J) ? ? ? ? , C = C (1) | ? ? ? | C (J) ,<label>(29)</label></formula><p>where A (j) ? R N ?N , B (j) ? R N ?H and C (j) ? R H?N . It follows that this partitioned system can be also be viewed as J independent N -dimensional S5 SSM subsystems and the output of the overall system is the sum of the output of the J subsystems</p><formula xml:id="formula_40">y k = Cx k (30) = J j=1 C (j) x (j) k .<label>(31)</label></formula><p>It follows from Proposition 2 that the dynamics of each of these J S5 SSM subsystems can be related to the dynamics of a different S4 system from Proposition 2. Each of these S4 systems has its own bank of tied S4 SSMs (cf. Assumptions 2, 3). Importantly, each of the J S4 systems can have its own state matrix, timescale parameter and output matrix shared across its H S4 SSMs. Thus, the outputs of a JN dimensional S5 SSM can be equivalent to the linear combination of the latent states of J different S4 systems from Proposition 2. This fact motivates the option to initialize a block-diagonal S5 state matrix with several HiPPO-N matrices on the blocks, rather than just initializing with one larger HiPPO-N matrix. Further, there is no constraint that requires the S5 SSM state matrix to remain block-diagonal during training. Learning off-diagonal elements can be interpreted as learning couplings between the J different banks of S4 SSMs. In practice we found the block-diagonal initialization to improve performance on many tasks, see Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Timescale Parameterization</head><p>Finally, we take a closer look at the parameterization of the timescale parameters ?. As discussed in Section 4.3, S4 can learn a different timescale parameter for each S4 SSM allowing it to more effectively capture different timescales of the data. Further, the initialization of the timescales can be important <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, and limiting to sampling a single initial parameter may lead to a poor initialization. The discussion in the previous section motivates potentially learning J different timescale parameters, one for each of the J subsystems. However, in practice we found better performance when using P different timescale parameters, one for each of the states. On the one hand, this can be viewed simply as learning a different scaling for each of the eigenvalues in the diagonalized system (see Eq. <ref type="formula" target="#formula_7">(6)</ref>). On the other hand, this could be viewed as increasing the number of timescale parameters sampled at initialization, helping to combat the possibility of a poor initialization. Of course, the system could learn to use just a single timescale by setting all of the timescales to be the same. See further discussion in the ablation study in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Ablations</head><p>We perform several ablations to empirically explore different aspects of S5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 S5 latent size, block-diagonal initialization, and timescale parameterization</head><p>The discussion in Section 4 and Appendix D raises several interesting questions: How does S5 perform when the latent size P is restricted to be equal to the latent size N used by each of S4's SSMs? How important is the timescale parameterization discussed in Appendix D.5? How important is the block-diagonal initialization? <ref type="table">Table 5</ref> displays the results of an ablation study performed on the LRA tasks to get a better sense of this. We consider 3 versions of S5. The first version uses the same general architecture (e.g. number of input/output features H, number of layers, etc) as reported for the S4/S4D variants in Gu et al. <ref type="bibr" target="#b18">[19]</ref>, sets the S5 SSM latent size P to be equal to the latent size N = 64 used by each of the S4 SSMs, and uses only a single scalar timescale parameter ? ? R. This is essentially the version of S5 we consider in Proposition 2. We observe that this constrained version of S5 actually performs well on most tasks, though struggles to perform comparably to the S4 baselines on Image and ListOps.</p><p>The second version of S5 is exactly the same as the first except we parameterize the timescale parameter as a vector ? ? R N . We observe uniform improvements over the scalar timescale parameterization and this reflects our general findings when training S5.</p><p>Finally, the complexity analysis and runtime comparison in Appendix C.2 suggests the latent size of S5 can be increased while maintaining similar complexity and practical runtimes as S4. We include the unconstrained version of S5 reported in our main results that uses the settings reported in the hyperparameter <ref type="table" target="#tab_13">Table 11</ref>. These models were allowed to be parameterized with fewer input/output features H (to ensure similar parameter counts to the S4 baselines) and generally used larger latent sizes P &gt; N . Further, we swept over the use of a block-diagonal initialization or not and the number of blocks to use (where J = 1 indicates no block-diagonal initialization was used). All models benefited from the block-diagonal initialization for the LRA tasks (See <ref type="table" target="#tab_13">Table 11</ref> ). <ref type="table">Table 5</ref>: Ablations on the LRA benchmark tasks <ref type="bibr" target="#b54">[55]</ref>. S4 results were taken from <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b16">17]</ref>. Note that the total parameter count for all models in this </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Importance of HiPPO-N and continuous-time parameterization</head><p>We perform a further ablation study to gain insight into the differences between S5 and prior attempts at parallelized linear RNNs (discussed in Section 5) focusing on what appears to be the distinguishing features: continuous-time parameterizations and Hippo initializations. We compare different initializations of the state matrix: random Gaussian, random antisymmetric, and HiPPO-N. The antisymmetric initialization is interesting because prior work considered these matrices in RNNs for long-range dependencies <ref type="bibr" target="#b7">[8]</ref>, and because the HiPPO-LegS matrix can be parameterized in a way related to antisymmetric matrices <ref type="bibr" target="#b16">[17]</ref>. Moreover, to compare to a setup more akin to the previous parallelized linear RNN work, we also consider a direct discrete-time parameterization of S5 that does not perform repeated discretization during training or learn the timescale parameter ?. We present the results of this ablation study in <ref type="table" target="#tab_8">Table 6</ref> (along with S5). We consider three of the LRA tasks that vary in length and difficulty.</p><p>The main takeaway is that the only approach that consistently performs well on all tasks, including the ability to solve Path-X, is the S5 approach that uses the continuous-time parameterization and HiPPO initialization. We also note that we observed the discrete time/HiPPO-N matrix configuration to be difficult to train due to stability issues, typically requiring a much lower learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 S4D Initialization ablations</head><p>Finally, Gu et al. <ref type="bibr" target="#b18">[19]</ref> propose several alternative diagonal matrices to the diagonalized HiPPO-N matrix, including the S4D-Inv and S4D-Lin matrices. They perform an ablation on the LRA tasks where they simply replace the diagonalized HiPPO-N matrix with the S4D-Inv and S4D-Lin matrices while keeping all other factors the same. We include these results in <ref type="table">Table 7</ref>. In <ref type="table">Table 7</ref>, we also include results for a similar ablation in S5 by using these matrices to initialize S5 in place of the HiPPO-N matrix while keeping all other factors constant. Both matrices perform well on most tasks with the exception of the S4D-Lin matrix on Path-X. Interestingly, one of these runs reached 96.79%, however the other runs did not exceed random guessing on this task. Future exploration of these and other matrices are an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Supplementary Results</head><p>We include further experimental results to supplement the results presented in the main text. <ref type="table">Table 7</ref>: Performance on the LRA benchmark tasks <ref type="bibr" target="#b54">[55]</ref>. indicates the model did not exceed random guessing. Citations refer to the original model; additional citation indicates work from which this baseline is reported. We follow the procedure reported in Gu et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> and report means across three seeds for S4, S4D (as reported by Gu et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>) and S5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Extended LRA Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Extended Speech Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Pendulum Extended Results</head><p>We also evaluate two ablations: S5-drop uses the same S5 architecture, but drops the dependence on the inter-sample interval, i.e. ? t 1.0. We expect this network to perform poorly as it has no knowledge of how long has elapsed between observations. S5-append uses the same S5 architecture, but appends the integration timestep to the thirty-dimensional image encoding, prior to being input into the dense S5 input layer. Hypothetically, we expect this network to perform as well as S5. However, to do so, requires the S5 network to learn to process time, which may be difficult, especially in more complex domains. We include these ablations in the bottom partition of <ref type="table">Table 9</ref>. <ref type="table">Table 9</ref>: Test MSE ?10 ?3 and runtimes on the pendulum regression task (* indicates numbers are replicated from Schirmer et al. <ref type="bibr" target="#b52">[53]</ref>, with mean and standard deviations across five random seeds). We re-ran CRU (CRU (our run)) and S5, and present mean and variances (across twenty runs) of the MSE error on the held-out test set, using a model selected using the validation set. We refer the reader to Schirmer et al. <ref type="bibr" target="#b52">[53]</ref> for full description of the baselines.  <ref type="table" target="#tab_12">Table 10</ref> presents results and citations of the pixel-level 1-D image classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Pixel-level 1-D Image Classification Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Experiment Configurations</head><p>In this section we describe the experimental details. This includes the model architecture, general hyperparameters, specifics for each task, and information about the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Deep Sequence Model Architecture</head><p>For the experiments, we use the S5 layer as a drop-in replacement for the S4 layer used in the sequence model architecture of <ref type="bibr" target="#b16">[17]</ref>. On a high level, this architecture consists of a linear encoder (to encode the input at each time step into H features), multiple S5 layers, a mean pooling layer, a linear decoder, and a Softmax operation for the classification tasks. The mean pooling layer compresses the output of the last S5 layer, of shape [batch size, sequence length, number of features (H)], across the sequence length dimension, so that a single H-dimensional encoding is available for softmax classification.</p><p>The baseline S4 models from Gu et al. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> apply a GLU activation <ref type="bibr" target="#b11">[12]</ref> function to the S4 SSM outputs. To take advantage of the fact that the S5 SSM outputs have already been mixed throughout the MIMO SSM we use what is essentially a weighted sigmoid gated unit <ref type="bibr" target="#b53">[54]</ref> (a GLU activation without an additional linear transform). Specifically, given an S5 SSM output y k ? R H and a dense matrix W ? R H?H , the layer output of the activation function we apply is u k = GELU(y k ) ?(W * GELU(y k )).</p><p>Hyperparameter options such as dropout rate, using either layer normalization or batch normalization, and using either pre-norm or post-norm are applied between the layers. Exceptions to the basic architecture described here are mentioned in the individual experiment sections below. <ref type="table" target="#tab_13">Table 11</ref> presents the main hyperparameters used for each experiment. For all experiments we ensure the number of layers and layer input/output features H are less than or equal to the number of layers and layer input/output features reported in Gu et al. <ref type="bibr" target="#b18">[19]</ref> as well as ensuring comparable parameter counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Default Hyperparameters</head><p>In general, the models for most tasks used batch normalization and pre-norm. Exceptions are noted in the individual experiment sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.1 Optimizers and learning rates</head><p>We follow the general optimization approach used by S4/S4D in <ref type="bibr" target="#b18">[19]</ref>. We use the AdamW optimizer <ref type="bibr" target="#b37">[38]</ref> with a global learning rate. However, in general, no weight decay and a smaller learning rate (the SSM learning rate) is applied to ?,B, ?. All experiments used cosine annealing. Exceptions to these points are noted in the individual experiment sections below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.2 Bidirectionality</head><p>We follow Gu et al. <ref type="bibr" target="#b18">[19]</ref> and use bidirectional models for the LRA and speech tasks. Unidirectional (causal) models were used for the pendulum, sequential and permuted MNIST for fair comparison with prior methods that used unidirectional models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Task Specific Hyperparameters</head><p>Here we specify any task-specific details, hyperparameter or architectural differences from the defaults outlined above. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.1 Listops</head><p>Weight decay and the global learning rate were applied toB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.2 Text</head><p>No exceptions to the defaults for this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.3 Retrieval</head><p>This document matching task requires a slightly different architecture from the other experiments, as discussed in Tay et al. <ref type="bibr" target="#b54">[55]</ref>. We use the same configuration as S4 <ref type="bibr" target="#b16">[17]</ref>. Each string is passed through the input encoder, S5 layers, and mean pooling layers. Denoting X 1 as the output for the first document and X 2 as the output for the second document, four features are created and concatenated together <ref type="bibr" target="#b54">[55]</ref> as</p><formula xml:id="formula_41">X = [X 1 , X 2 , X 1 * X 2 , X 1 ? X 2 ].<label>(32)</label></formula><p>This concatenated feature is then fed to a linear decoder and softmax function as normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.4 Image</head><p>Weight decay and the global learning rate were applied toB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.5 Pathfinder</head><p>No exceptions to the defaults for this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.6 Path-X</head><p>Weight decay was applied toB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.7 Speech Commands</head><p>No weight decay and the SSM learning rate were applied toC for this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.8 Pendulum Regression</head><p>We use the same encoder-decoder architecture as Schirmer et al. <ref type="bibr" target="#b52">[53]</ref>. The encoder has layers: convolution, ReLU, max pool, convolution, ReLU, max pool, dense, ReLU, dense. The first convolution layer has twelve features, a 5 ? 5 kernel, and a padding of (2, 2). The second convolution layer has twelve features, a 3 ? 3 kernel, a stride of 2, and a padding of (1, 1). Both max pools use a window size of 2 ? 2 and a stride of 2. The dense layer has thirty hidden units. The linear readout layer outputs H = 30 features. This is chosen to match the encoding size in Schirmer et al. <ref type="bibr" target="#b52">[53]</ref>, and is used for all layers. Separate mean and unconstrained variance decoders are used, defined as a one-layer MLP with a hidden size of thirty. An elu+1 activation function is used to constrain the variance to be positive. Layer normalization and post-norm were used for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.9 Sequential MNIST</head><p>No exceptions to the defaults for this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.10 Permuted Sequential MNIST</head><p>Weight decay and the global learning rate were applied toB. Post-norm was used for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3.11 Sequential CIFAR</head><p>We trained a model with the exact hyperparameter settings as used for the LRA-IMAGE (grayscale sequential CIFAR) task with no further tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 Dataset Details</head><p>We provide more context and details for each of the LRA <ref type="bibr" target="#b54">[55]</ref> and Speech Commands <ref type="bibr" target="#b59">[60]</ref> datasets we consider. Note that we follow the same data pre-processing steps as Gu et al. <ref type="bibr" target="#b16">[17]</ref>, which we also include here for completeness.</p><p>? ListOps: A lengthened version of the dataset presented by Nangia &amp; Bowman <ref type="bibr" target="#b42">[43]</ref>. Given a nested set of mathematical operations (such as min and max) and integer operands in the range zero to nine, expressed in prefix notation with brackets, compute the integer result of the mathematical expression, e. ? Path-X: An "extreme" version of the Pathfinder challenge. Instead, the images are 128?128 pixels, resulting in sequences that are a factor of sixteen times longer. Otherwise identical to the Pathfinder challenge.</p><p>? Speech Commands: Based on the dataset released by Warden <ref type="bibr" target="#b59">[60]</ref>. ? Speech Commands ? 0.5: Temporally sub-sampled version of Speech Commands, where the validation and test datasets only are sub-sampled by a factor of 1/0.5, and are therefore shortened to length 8, 000. No subsequent padding is applied. The training dataset is not subsampled.</p><p>? Sequential MNIST: (sMNIST) 10-way digit classification from a 28 ? 28 grayscale image of a handwritten digit, where the input image is flattened into a 784-length scalar sequence.</p><p>? Permuted Sequential MNIST: (psMNIST) 10-way digit classification from a 28 ? 28 grayscale image of a handwritten digit, where the input image is flattened into a 784-length scalar sequence. This sequence is then permuted using a fixed order.</p><p>? Sequential CIFAR: (sCIFAR): 10-way image classification using the CIFAR-10 dataset. Identical to image, except that full colour images are input as a 1, 024-length input sequence, where each input is an (R,G,B) triple.</p><p>? Pendulum Regression: Reproduced from Becker et al. <ref type="bibr" target="#b3">[4]</ref> and Schirmer et al. <ref type="bibr" target="#b52">[53]</ref>. The input sequence is a 24?24 grayscale rendering of a pendulum, driven by a random torque process. The images pixels are corrupted by a noise process that is correlated in time. The pendulum is simulated for 100 timesteps, and 50 frames are irregularly sampled without replacement from the simulation. The objective is to estimate the sine and cosine of the angle of the pendulum. A train/validation/test split of 2, 000/1, 000/1, 000 is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Background on Parallel Scans for Linear Recurrences</head><p>For the interested reader, this section provides more background on using a parallel scan for a linear recurrence, as well as a simple example to illustrate how it can compute the recurrence in parallel. The parallelization of scan operations has been well studied <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6]</ref>, and many standard scientific computing libraries contain efficient implementations. We note the linear recurrence we consider here is a specific instance of the more general setting discussed in Section 1.4 of Blelloch <ref type="bibr" target="#b5">[6]</ref>. Computing a general parallel scan requires defining two objects:</p><p>? The initial elements the scan will operate on.</p><p>? A binary associative operator ? used to combine the elements.</p><p>To compute a length L linear recurrence, x k = Ax k?1 + Bu k , we will define the L initial elements, c 1:L , such that each element c k is the tuple c k = (c k,a , c k,b ) := (A, Bu k ).</p><p>These c 1:L will be precomputed prior to the scan. Having created the list of elements for the scan to operate on, we define the binary operator ? for the scan to use on this linear recurrence as q i ? q j := (q j,a q i,a , q j,a ? q i,b + q j,b ) ,</p><p>where q k denotes an input element to the operator that could be the initial elements c k or some intermediate result, denotes matrix-matrix multiplication, ? denotes matrix-vector multiplication and + denotes elementwise addition. We show that this operator is associative at the end of this section.</p><p>Simple example using binary operator We can illustrate how ? can be used to compute a linear recurrence in parallel with a simple example. Consider the system x k = Ax k?1 + Bu k , and a length L = 4 sequence of inputs u 1:4 . Assuming x 0 = 0, the desired latent states from this recurrence are:</p><formula xml:id="formula_44">x 1 = Bu 1<label>(35)</label></formula><p>x 2 = ABu 1 + Bu 2 (36)</p><formula xml:id="formula_45">x 3 = A 2 Bu 1 + ABu 2 + Bu 3<label>(37)</label></formula><p>x 4 = A 3 Bu 1 + A 2 Bu 2 + ABu 3 + Bu 4</p><p>We first note that ? can be used to compute this recurrence sequentially. We can initialize the scan elements c 1:4 as in <ref type="bibr" target="#b32">(33)</ref>, and then sequentially scan over these elements to compute the output elements s i = s i?1 ? c i . Defining s 0 := (I, 0) where I is the identity matrix, we have for our example: </p><p>Note that the second entry of each of the output tuples, s i,b , contains the desired x i computed above.</p><p>Computing the scan in this way requires four sequential steps since each s i depends on s i?1 .</p><p>Now consider how we can use this binary operator to compute the recurrence in parallel. We will label the output elements of the parallel scan as r 1:4 and define r 0 = (I, 0). We will first compute the even indexed elements r 2 and r 4 , and then compute the odd indexed elements r 1 and r 3 . We start by applying the binary operator ? to adjacent pairs of our initial elements c 1:4 to compute r 2 and the intermediate result q 4 , and we then repeat this process to compute r 4 by applying ? to r 2 and q 4 : </p><p>Now we will compute the odd indexed elements r 1 and r 3 , using the even indexed r 0 and r 2 , as r k = r k?1 ? c k : </p><p>Note that the second entry of each of the output tuples, r k,b , corresponds to the desired x k . Inspecting the required dependencies for each application of ?, we see that r 2 and the intermediate result q <ref type="bibr" target="#b3">4</ref> can be computed in parallel. Once r 2 and q 4 are computed, r 1 , r 3 and r 4 can all be computed in parallel. We have therefore reduced the number of sequential steps required from four in the sequential scan version to two in the parallel scan version. This reduction in sequential steps becomes important when the sequence length is large since, given sufficient processors, the parallel time scales logarithmically with the sequence length.</p><p>Associativity of binary operator Finally, for completeness, we show that the binary operator ? is associative:</p><p>(q i ? q j ) ? q k = (q j,a q i,a , q j,a ? q i,b + q j,b ) ? q k (50) = (q k,a (q j,a q i,a ), q k,a ? (q j,a ? q i,b + q j,b ) + q k,b )</p><p>= ((q k,a q j,a ) q i,a , q k,a ? (q j,a ? q i,b ) + q k,a ? q j,b + q k,b )</p><p>= ((q k,a q j,a ) q i,a , (q k,a q j,a ) ? q i,b + q k,a ? q j,b + q k,b )</p><p>= q i ? (q k,a q j,a , q k,a ? q j,b + q k,b )</p><formula xml:id="formula_53">= q i ? (q j ? q k )<label>(54)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The computational components of an S5 layer for offline application to a sequence. The S5 layer uses a parallel scan on a diagonalized linear SSM to compute the SSM outputs y 1:L ? R L?H . A nonlinear activation function is applied to the SSM outputs to produce the layer outputs. A similar diagram for S4 is included in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Schematic of the internal structure of a discretized S4 layer<ref type="bibr" target="#b16">[17]</ref> (top) and S5 layer (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 1 .</head><label>1</label><figDesc>Given an S4 layer with H input/output features, an S5 layer with H input/output features and a latent size P = O(H) has the same order of magnitude complexity as an S4 layer in terms of both runtime and memory usage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the pendulum regression example. Shown in the top row are the images used as input at the time points indicated. Shown on the bottom are the values of sin(? t ) and cos(? t ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 5 6 " 12 discretized</head><label>4612</label><figDesc>def discretize(Lambda, B_tilde, Delta): "" Discretize a diagonalized, continuous-time linear SSM Lambda_bar (complex64), B_bar (complex64) (P,), (P,H) """ 13 Identity = np.ones(Lambda.shape[0]) 14 Lambda_bar = np.exp(Lambda * Delta) 15 B_bar = (1 / Lambda * (Lambda_bar -Identity))[..., None] * B_tilde 16 return Lambda_bar, B_bar 17 18 def binary_operator(element_i, element_j): 19 """ Binary operator for parallel scan of linear recurrence. Assumes a diagonal matrix A. 20 Args: 21 element_i: tuple containing A_i and Bu_i at position i (P,), (P,) 22 element_j: tuple containing A_j and Bu_j at position j (P,), (P,) 23</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>S5 layer offline processing. Duplicated from the main text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :Proposition 1 .</head><label>41</label><figDesc>The computational components of the S4 layer<ref type="bibr" target="#b16">[17]</ref> (top) and the S5 layer (bottom) for offline application to a sequence. (a) The S4 layer applies an independent SSM to each dimension of the input sequence u 1:L ? R L?H . This requires a Cauchy kernel computation to compute the convolution kernel coefficients in the frequency domain. Convolutions are computed using FFTs to produce the independent SSM outputs y 1:L ? R L?H . A nonlinear activation function that includes a mixing layer is applied to the SSM outputs to produce the layer outputs. (b) (Reproduced fromFigure 1) The S5 layer uses a parallel scan on a diagonalized linear SSM to compute the SSM outputs y 1:L ? R L?H . A nonlinear activation function is applied to the SSM outputs to produce the layer outputs. Given an S4 layer with H input/output features, an S5 layer with H input/output features and a latent size P = O(H) has the same order of magnitude complexity as an S4 layer in terms of both runtime and memory usage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>, we can lower bound the cost of applying a single SISO S4 SSM to a scalar input sequence as O(L log L) operations and O(L) space. The S4 layer then consists of H different S4 SSMs, and therefore requires O(HL log L) operations and O(HL) space. Finally, mixing the H activations at each timestep requires L independent matrix-vector multiplications. Thus, the S4 layer sequence-to-sequence transformation requires a total of O(H 2 L + HL log L) operations when run in the efficient convolution mode. Given H 2 L processors, these operations can be parallelized to a minimum of O(log H + log L) parallel time.We now consider the S5 layer. As discussed in Section 2.2, the parallel scan requires O(P L) operations and O(P L) space to perform the linear recurrence that computes the discretized statesx 1:L ? R L?P . In addition, O(P HL) operations are required for the independent matrix-vector multiplications that compute Bu 1:L ? R L?P and the SSM outputsCx 1:L ? R L?H . Therefore, the S5 layer requires O (P HL + P L) operations. Given P HL processors, these operations can be parallelized to a minimum of O(log P + log L) parallel time. Thus, we see that when the S5 state dimension P = O(H), the S5 layer requires O H 2 L + HL operations compared to the S4 layer's O(H 2 L + HL log L) operations. Crucially, when P = O(H), S4 and S5 each have parallel complexity of O(log H + log L) (when H 2 L processors are available). In addition, when P = O(H), the space complexity for the parallel scan is O(HL) which matches the space complexity of S4's FFTs. Both methods then also perform identical broadcasted matrix vector multiplications, and hence have the same space complexity. Online generation For online generation, both the S4 and S5 layers are run recurrently. The S4 layer requires O(H 2 + HN ) operations per step due to its O(HN ) DPLR-matrix-vector multiplication [17] and the O(H 2 ) matrix-vector multiplication of its mixing layer. The S5 layer requires O(P H + P ) operations per step due to its O(P ) matrix-vector multiplication with its diagonal matrix and its O(P H) matrix-vector multiplications to compute Bu k ? R P andCx k ? R H . Thus, we see the two approaches have the same per step complexity of O(H 2 ) when P = O(H) and the individual S4 SSM state sizes N are O(H).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Corollary 1 ( 2 B</head><label>12</label><figDesc>Extension of Theorem 3 in Gu et al. [19]). Consider A LegS ? R N ?N , A Normal LegS ? R N ?N , B LegS ? R N ?H , P LegS ? R N as defined in Appendix B.1.1. Given vector-valued inputs u(t) ? R H , the ordinary differential equation dx (t) dt = A Normal LegS x (t) + 1 LegS u(t) converges to dx(t) dt = A LegS x(t) + B LegS u(t) as N ? ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Readers recite one of 35 words. The task is then to classify which of the 35 words was spoken from a 16kHz one-dimensional audio recording. There are 35 different classes, each representing one of the words in the vocabulary. Sequences are all of the same length (16, 000). There are 24, 482 training examples, 5, 246 validation examples, and 5, 247 test examples. Data is normalized to be zero mean and have a standard deviation of 0.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>s 1 = 2 , 1 + 2 ) ( 40 )s 3 = s 2 ? c 3 = (A 2 , 1 +A 2 1 + 2 +A 2 1 + 3</head><label>1212403221212213</label><figDesc>s 0 ? c 1 = (I, 0) ? (A, Bu 1 ) = (AI, A0 + Bu 1 ) = (A, Bu 1 )(39)s 2 = s 1 ? c 2 = (A, Bu 1 ) ? (A, Bu 2 ) = (A ABu Bu ABu Bu 2 ) ? (A, Bu 3 ) = (A 3 , Bu ABu Bu 3 )(41)s 4 = s 3 ? c 4 = (A 3 , Bu ABu 2 + Bu 3 ) ? (A, Bu 4 ) Bu 1 + A 2 Bu 2 + ABu 3 + Bu 4 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>r 2 = 2 , 2 , 3 + 4 ) ( 45 )r 4 = r 2 ? q 4 = (A 2 , 2 , 3 1 + A 2 2 +</head><label>222344542223122</label><figDesc>c 1 ? c 2 = (A, Bu 1 ) ? (A, Bu 2 ) = (A ABu 1 + Bu 2 ) (44) q 4 = c 3 ? c 4 = (A, Bu 3 ) ? (A, Bu 4 ) = (A ABu Bu ABu 1 + Bu 2 ) ? (A ABu 3 + Bu 4 ) Bu Bu ABu 3 + Bu 4 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>r 1 =</head><label>1</label><figDesc>r 0 ? c 1 = (I, 0) ? (A, Bu 1 ) = (A, Bu 1 ) (48) r 3 = r 2 ? c 3 = (A 2 , ABu 1 + Bu 2 ) ? (A, Bu 3 ) = (A 3 , A 2 Bu 1 + ABu 2 + Bu 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Unlike S4 methods and S5, the best Mega model retains the transformer's O(L 2 ) complexity.</figDesc><table><row><cell>Model</cell><cell>ListOps</cell><cell>Text</cell><cell cols="5">Retrieval Image Pathfinder Path-X Avg.</cell></row><row><cell>(Input length)</cell><cell>(2,048)</cell><cell>(4,096)</cell><cell>(4,000)</cell><cell>(1,024)</cell><cell>(1,024)</cell><cell>(16,384)</cell><cell></cell></row><row><cell>Transformer</cell><cell>36.37</cell><cell>64.27</cell><cell>57.46</cell><cell>42.44</cell><cell>71.40</cell><cell></cell><cell>53.66</cell></row><row><cell>Luna-256</cell><cell>37.25</cell><cell>64.57</cell><cell>79.29</cell><cell>47.38</cell><cell>77.72</cell><cell></cell><cell>59.37</cell></row><row><cell>H-Trans.-1D</cell><cell>49.53</cell><cell>78.69</cell><cell>63.99</cell><cell>46.05</cell><cell>68.78</cell><cell></cell><cell>61.41</cell></row><row><cell>CCNN</cell><cell>43.60</cell><cell>84.08</cell><cell></cell><cell>88.90</cell><cell>91.51</cell><cell></cell><cell>68.02</cell></row><row><cell>Mega (O(L 2 ))</cell><cell>63.14</cell><cell>90.43</cell><cell>91.25</cell><cell>90.44</cell><cell>96.01</cell><cell>97.98</cell><cell>88.21</cell></row><row><cell>Mega-chunk (O(L))</cell><cell>58.76</cell><cell>90.19</cell><cell>90.97</cell><cell>85.80</cell><cell>94.41</cell><cell>93.81</cell><cell>85.66</cell></row><row><cell>S4D-LegS</cell><cell>60.47</cell><cell>86.18</cell><cell>89.46</cell><cell>88.19</cell><cell>93.06</cell><cell>91.95</cell><cell>84.89</cell></row><row><cell>S4-LegS</cell><cell>59.60</cell><cell>86.82</cell><cell>90.90</cell><cell>88.65</cell><cell>94.20</cell><cell>96.35</cell><cell>86.09</cell></row><row><cell>Liquid-S4</cell><cell>62.75</cell><cell>89.02</cell><cell>91.20</cell><cell>89.50</cell><cell>94.8</cell><cell>96.66</cell><cell>87.32</cell></row><row><cell>S5</cell><cell>62.15</cell><cell>89.31</cell><cell>91.40</cell><cell>88.00</cell><cell>95.33</cell><cell>98.58</cell><cell>87.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance on 35-way Speech Commands classification task<ref type="bibr" target="#b59">[60]</ref>. Training examples are one-second 16kHz audio waveforms. Last column indicates 0-shot testing at 8kHz (constructed by naive decimation). As in Gu et al.<ref type="bibr" target="#b18">[19]</ref>, the mean across three random seeds is reported. An expanded table with full citations is included in the appendix.</figDesc><table><row><cell>Model</cell><cell cols="2">Parameters 16kHz</cell><cell>8kHz</cell></row><row><cell>(Input length)</cell><cell>(16,000)</cell><cell>(8,000)</cell><cell></cell></row><row><cell>InceptionNet</cell><cell>481K</cell><cell>61.24</cell><cell>05.18</cell></row><row><cell>ResNet-1</cell><cell>216K</cell><cell>77.86</cell><cell>08.74</cell></row><row><cell>XResNet-50</cell><cell>904K</cell><cell>83.01</cell><cell>07.72</cell></row><row><cell>ConvNet</cell><cell>26.2M</cell><cell>95.51</cell><cell>07.26</cell></row><row><cell>S4-LegS</cell><cell>307K</cell><cell>96.08</cell><cell>91.32</cell></row><row><cell>S4D-LegS</cell><cell>306K</cell><cell>95.83</cell><cell>91.08</cell></row><row><cell>Liquid-S4</cell><cell>224K</cell><cell>96.78</cell><cell>90.00</cell></row><row><cell>S5</cell><cell>280K</cell><cell>96.52</cell><cell>94.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>MSE ?10 ?3 (mean ? std) and relative application speed on the pendulum regression task (* indicates result is from Schirmer et al.<ref type="bibr" target="#b52">[53]</ref> with five random seeds). We re-ran CRU (CRU (our run)), and present mean and variances of the MSE error on the held-out test set across twenty random seeds. Further details and descriptions are included in the appendix.</figDesc><table><row><cell>Model</cell><cell cols="2">Relative speed Regression MSE (?10 ?3 )</cell></row><row><cell>mTAND*</cell><cell>8.3?</cell><cell>65.64 ? 4.05</cell></row><row><cell>RKN*</cell><cell>1.3?</cell><cell>8.43 ? 0.61</cell></row><row><cell>RKN-? t *</cell><cell>1.3?</cell><cell>5.09 ? 0.40</cell></row><row><cell>ODE-RNN*</cell><cell>0.68?</cell><cell>7.26 ? 0.41</cell></row><row><cell>CRU*</cell><cell>0.68?</cell><cell>4.63 ? 1.07</cell></row><row><cell>CRU (our run)</cell><cell>1.00?</cell><cell>3.81 ? 0.28</cell></row><row><cell>S5</cell><cell>130?</cell><cell>3.38 ? 0.28</cell></row></table><note>modeling and lead to new formulations and extensions. Finally, the concurrent work on Mega [40]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Computes BxLxH output sequence of an S5 layer given BxLxH input sequence.</figDesc><table><row><cell>31</cell><cell></cell><cell>Args:</cell><cell></cell><cell></cell></row><row><cell>32</cell><cell></cell><cell cols="3">Lambda_bar (complex64): discretized diagonal state matrix</cell><cell>(P,)</cell></row><row><cell>33</cell><cell></cell><cell>B_bar</cell><cell cols="2">(complex64): discretized input matrix</cell><cell>(P, H)</cell></row><row><cell>34</cell><cell></cell><cell>C_tilde</cell><cell cols="2">(complex64): output matrix</cell><cell>(H, P)</cell></row><row><cell>35</cell><cell></cell><cell>D</cell><cell>(float32):</cell><cell>feedthrough matrix</cell><cell>(H,)</cell></row><row><cell>36</cell><cell></cell><cell cols="3">input_sequence (float32): input sequence of features</cell><cell>(L, H)</cell></row><row><cell>37</cell><cell></cell><cell>Returns:</cell><cell></cell><cell></cell></row><row><cell>38</cell><cell></cell><cell cols="3">ys (float32): the SSM outputs (S5 layer preactivations)</cell><cell>(L, H)</cell><cell>"""</cell></row><row><cell>39</cell><cell cols="4"># Prepare elements required to initialize parallel scan</cell></row><row><cell>40</cell><cell cols="5">Lambda_elements = np.repeat(Lambda_bar[None, ...], input_sequence.shape[0], axis=0)</cell></row><row><cell>41</cell><cell cols="4">Bu_elements = jax.vmap(lambda u: B_bar @ u)(input_sequence)</cell></row><row><cell>42</cell><cell cols="4">elements = (Lambda_elements, Bu_elements)</cell><cell># (L, P), (L, P)</cell></row><row><cell>43</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>44</cell><cell cols="5"># Compute latent state sequence given input sequence using parallel scan</cell></row><row><cell>45</cell><cell cols="4">_, xs = parallel_scan(binary_operator, elements)</cell><cell># (L, P)</cell></row><row><cell>46</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>47</cell><cell cols="3"># Compute SSM output sequence</cell><cell></cell></row><row><cell>48</cell><cell cols="5">ys = jax.vmap(lambda x, u: (C_tilde @ x + D * u).real)(xs, input_sequence)</cell></row><row><cell>49</cell><cell cols="2">return ys</cell><cell></cell><cell></cell></row><row><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">51 def apply_S5_layer(params, input_sequence):</cell></row><row><cell>53</cell><cell></cell><cell>Args:</cell><cell></cell><cell></cell></row><row><cell>54</cell><cell></cell><cell cols="3">params: tuple of the continuous time SSM parameters</cell></row><row><cell>55</cell><cell></cell><cell cols="3">input_sequence: input sequence of features</cell><cell>(L, H)</cell></row><row><cell>56</cell><cell></cell><cell>Returns:</cell><cell></cell><cell></cell></row><row><cell>57</cell><cell></cell><cell cols="3">The S5 layer output sequence</cell><cell>(L, H)</cell><cell>"""</cell></row><row><cell>58</cell><cell cols="4">Lambda, B_tilde, C_tilde, D, log_Delta = params</cell></row><row><cell>59</cell><cell cols="4">Lambda_bar, B_bar = discretize(Lambda, B_tilde, np.exp(log_Delta))</cell></row><row><cell>60</cell><cell cols="5">preactivations = apply_ssm(Lambda_bar, B_bar, C_tilde, D, input_sequence)</cell></row><row><cell>61</cell><cell cols="4">return jax.nn.gelu(preactivations)</cell></row><row><cell>62</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">63 def batch_apply_S5_layer(params, input_sequences):</cell></row><row><cell cols="2">64 """ 65</cell><cell>Args:</cell><cell></cell><cell></cell></row><row><cell>66</cell><cell></cell><cell cols="3">params: tuple of the continuous time SSM parameters</cell></row><row><cell>67</cell><cell></cell><cell cols="3">input_sequences: batch of input feature sequences</cell><cell>(B, L ,H)</cell></row><row><cell>68</cell><cell></cell><cell>Returns:</cell><cell></cell><cell></cell></row><row><cell>69</cell><cell></cell><cell cols="3">Batch of S5 layer output sequences</cell><cell>(B, L, H)</cell><cell>"""</cell></row></table><note>52 """ Computes LxH output sequence of an S5 layer given LxH input sequence.70 return jax.vmap(apply_S5_layer, in_axes=(None, 0))(params, input_sequences) Listing 1: JAX implementation to apply a single S5 layer to a batch of input sequences.B S5 Layer DetailsB.1 Initialization DetailsB.1.1 Initialization of the State Matrix</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Proposition 2 .</head><label>2</label><figDesc>Consider an S5 layer, with state matrix A, input matrix B and some output matrix C (cf. Assumption 1); and an S4 layer, where each of the H S4 SSMs has state matrix A (cf. Assumption 2, 4) and input vector B (h) (cf. Assumption 4). If the S4 and S5 layers are discretized with the same timescales (cf. Assumption 3), then the S5 SSM produces outputs, y k , equivalent to a linear combination of the latent states of the H S4 SSMs, y k</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>table are commensurate, and hence variations in performance cannot be attributed to a model having drastically more parameters.</figDesc><table><row><cell>Model</cell><cell>ListOps</cell><cell>Text</cell><cell cols="4">Retrieval Image Pathfinder Path-X</cell></row><row><cell>(Input length)</cell><cell>(2,048)</cell><cell>(4,096)</cell><cell>(4,000)</cell><cell>(1,024)</cell><cell>(1,024)</cell><cell>(16,384)</cell></row><row><cell>S4D-LegS</cell><cell>60.47</cell><cell>86.18</cell><cell>89.46</cell><cell>88.19</cell><cell>93.06</cell><cell>91.95</cell></row><row><cell>S4-LegS</cell><cell>59.60</cell><cell>86.82</cell><cell>90.90</cell><cell>88.65</cell><cell>94.20</cell><cell>96.35</cell></row><row><cell>S5 (P=N, J = 1, ? ? R)</cell><cell>57.20</cell><cell>87.60</cell><cell>90.53</cell><cell>82.01</cell><cell>94.14</cell><cell>96.59</cell></row><row><cell>S5 (P=N, J = 1, ? ? R N )</cell><cell>58.65</cell><cell>88.12</cell><cell>90.76</cell><cell>85.04</cell><cell>94.53</cell><cell>97.49</cell></row><row><cell>S5 (P: free, J?1, ? ? R P , see Table 11)</cell><cell>62.15</cell><cell>89.31</cell><cell>91.40</cell><cell>88.00</cell><cell>95.33</cell><cell>98.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>S5 Initialization and Parameterization Ablation Study. indicates the model did not improve over random guessing.</figDesc><table><row><cell>Model</cell><cell>Parameterization</cell><cell>Initialization</cell><cell>ListOps</cell><cell>Text</cell><cell>Path-X</cell></row><row><cell>(Input length)</cell><cell></cell><cell></cell><cell>(2,048)</cell><cell cols="2">(4,096) (16,384)</cell></row><row><cell>S5 (ablation)</cell><cell>Discrete</cell><cell>Gaussian</cell><cell>41.50</cell><cell>81.09</cell><cell></cell></row><row><cell>S5 (ablation)</cell><cell>Discrete</cell><cell>Antisymmetric</cell><cell>49.10</cell><cell>86.42</cell><cell></cell></row><row><cell>S5 (ablation)</cell><cell>Discrete</cell><cell>HiPPO-N</cell><cell>58.15</cell><cell>61.93</cell><cell></cell></row><row><cell>S5 (ablation)</cell><cell>Continuous</cell><cell>Gaussian</cell><cell>58.50</cell><cell>69.03</cell><cell></cell></row><row><cell>S5 (ablation)</cell><cell>Continuous</cell><cell>Antisymmetric</cell><cell>59.35</cell><cell>82.83</cell><cell></cell></row><row><cell>S5</cell><cell>Continuous</cell><cell>HiPPO-N</cell><cell>62.15</cell><cell>89.31</cell><cell>98.58</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Speech Commands classification [60]. Performance on 35-way keyword spotting. Training examples are one-second audio waveforms sampled at 16kHz, or a one-dimensional sequence of length 16000. Last column indicates zero-shot testing at 8kHz where examples are constructed by naive decimation. The mean across three random seeds is reported. Citations refer to the original model; additional citation indicates work from which this baseline is reported.</figDesc><table><row><cell>Model</cell><cell>Parameters</cell><cell>16000Hz</cell><cell>8000Hz</cell></row><row><cell>InceptionNet [19, 44]</cell><cell>481K</cell><cell>61.24 (0.69)</cell><cell>05.18 (0.07)</cell></row><row><cell>ResNet-18 [19, 44]</cell><cell>216K</cell><cell>77.86 (0.24)</cell><cell>08.74 (0.57)</cell></row><row><cell>XResNet-50 [19, 44]</cell><cell>904K</cell><cell>83.01 (0.48)</cell><cell>07.72 (0.39)</cell></row><row><cell>ConvNet [19, 44]</cell><cell>26.2M</cell><cell>95.51 (0.18)</cell><cell>07.26 (0.79)</cell></row><row><cell>S4-LegS [19, 17]</cell><cell>307K</cell><cell>96.08 (0.15)</cell><cell>91.32 (0.17)</cell></row><row><cell>S4-FouT [19, 20]</cell><cell>307K</cell><cell>95.27 (0.20)</cell><cell>91.59 (0.23)</cell></row><row><cell>S4-(LegS/FouT) [19, 20]</cell><cell>307K</cell><cell>95.32 (0.10)</cell><cell>90.72 (0.68)</cell></row><row><cell>S4D-LegS [19]</cell><cell>306K</cell><cell>95.83 (0.14)</cell><cell>91.08 (0.16)</cell></row><row><cell>S4D-Inv [19]</cell><cell>306K</cell><cell>96.18 (0.27)</cell><cell>91.80 (0.24)</cell></row><row><cell>S4D-Lin [19]</cell><cell>306K</cell><cell>96.25 (0.03)</cell><cell>91.58 (0.33)</cell></row><row><cell>Liquid-S4 [23]</cell><cell>224K</cell><cell cols="2">96.78 (0.05) 90.00 (0.25)</cell></row><row><cell>S5</cell><cell>280K</cell><cell cols="2">96.52 (0.16) 94.53 (0.10)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Pixel-level 1-D Image classification. Citations refer to the original model; additional citation indicates work from which this baseline is reported.</figDesc><table><row><cell>Model</cell><cell cols="3">sMNIST psMNIST sCIFAR</cell></row><row><cell>(Input length)</cell><cell>(784)</cell><cell>(784)</cell><cell>(1024)</cell></row><row><cell>Transformer [56, 57]</cell><cell>98.9</cell><cell>97.9</cell><cell>62.2</cell></row><row><cell>CCNN [48]</cell><cell>99.72</cell><cell>98.84</cell><cell>93.08</cell></row><row><cell>FlexTCN [47]</cell><cell>99.62</cell><cell>98.63</cell><cell>80.82</cell></row><row><cell>CKConv [49]</cell><cell>99.32</cell><cell>98.54</cell><cell>63.74</cell></row><row><cell>TrellisNet [3]</cell><cell>99.20</cell><cell>98.13</cell><cell>73.42</cell></row><row><cell>TCN [2]</cell><cell>99.0</cell><cell>97.2</cell><cell>-</cell></row><row><cell>LSTM [16, 24]</cell><cell>98.9</cell><cell>95.11</cell><cell>63.01</cell></row><row><cell>r-LSTM [56]</cell><cell>98.4</cell><cell>95.2</cell><cell>72.2</cell></row><row><cell>Dilated GRU [9]</cell><cell>99.0</cell><cell>94.6</cell><cell>-</cell></row><row><cell>Dilated RNN [9]</cell><cell>98.0</cell><cell>96.1</cell><cell>-</cell></row><row><cell>IndRNN [36]</cell><cell>99.0</cell><cell>96.0</cell><cell>-</cell></row><row><cell>expRNN [35]</cell><cell>98.7</cell><cell>96.6</cell><cell>-</cell></row><row><cell>UR-LSTM [16]</cell><cell>99.28</cell><cell>96.96</cell><cell>71.00</cell></row><row><cell>UR-GRU [16]</cell><cell>99.27</cell><cell>96.51</cell><cell>74.4</cell></row><row><cell>LMU [58]</cell><cell>-</cell><cell>97.15</cell><cell>-</cell></row><row><cell>HiPPO-RNN [15]</cell><cell>98.9</cell><cell>98.3</cell><cell>61.1</cell></row><row><cell>UNIcoRNN [50]</cell><cell>-</cell><cell>98.4</cell><cell>-</cell></row><row><cell>LMU-FFT [10]</cell><cell>-</cell><cell>98.49</cell><cell>-</cell></row><row><cell>LipschitzRNN [13]</cell><cell>99.4</cell><cell>96.3</cell><cell>64.2</cell></row><row><cell>LSSL [18]</cell><cell>99.53</cell><cell>98.76</cell><cell>84.65</cell></row><row><cell>S4 [19, 17]</cell><cell>99.63</cell><cell>98.70</cell><cell>91.80</cell></row><row><cell>S4D [19]</cell><cell>-</cell><cell>-</cell><cell>89.92</cell></row><row><cell>Liquid-S4 [23].</cell><cell>-</cell><cell>-</cell><cell>92.02</cell></row><row><cell>S5</cell><cell>99.65</cell><cell>98.67</cell><cell>90.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Hyperparameters used for the reported results. Depth: number of layers. H: number of input/output features. P: Latent size. J: number of blocks used for the initialization of A (see Section B.1.1). Dropout: dropout rate. LR: global learning rate. SSM LR: the SSM learning rate. B: batch size. Epochs: max epochs set for the run. WD: weight decay.</figDesc><table><row><cell></cell><cell>Depth</cell><cell>H</cell><cell>P</cell><cell>J</cell><cell>Dropout</cell><cell>LR</cell><cell>SSM LR</cell><cell>B</cell><cell cols="2">Epochs WD</cell></row><row><cell>ListOps</cell><cell>8</cell><cell>128</cell><cell>16</cell><cell>8</cell><cell>0.0</cell><cell>0.003</cell><cell>0.001</cell><cell>50</cell><cell>40</cell><cell>0.04</cell></row><row><cell>Text</cell><cell>6</cell><cell cols="3">256 192 12</cell><cell>0.1</cell><cell>0.004</cell><cell>0.001</cell><cell>50</cell><cell>35</cell><cell>0.07</cell></row><row><cell>Retrieval</cell><cell>6</cell><cell cols="3">128 256 16</cell><cell>0.0</cell><cell>0.002</cell><cell>0.001</cell><cell>32</cell><cell>20</cell><cell>0.05</cell></row><row><cell>Image</cell><cell>6</cell><cell cols="2">512 384</cell><cell>3</cell><cell>0.1</cell><cell>0.005</cell><cell>0.001</cell><cell>50</cell><cell>250</cell><cell>0.07</cell></row><row><cell>Pathfinder</cell><cell>6</cell><cell cols="2">192 256</cell><cell>8</cell><cell>0.05</cell><cell>0.005</cell><cell>0.0009</cell><cell>64</cell><cell>200</cell><cell>0.03</cell></row><row><cell>Path-X</cell><cell>6</cell><cell cols="3">128 256 16</cell><cell>0.0</cell><cell>0.002</cell><cell>0.0006</cell><cell>32</cell><cell>75</cell><cell>0.06</cell></row><row><cell>Speech</cell><cell>6</cell><cell>96</cell><cell cols="2">128 16</cell><cell>0.1</cell><cell>0.008</cell><cell>0.002</cell><cell>16</cell><cell>40</cell><cell>0.04</cell></row><row><cell>Pendulum</cell><cell>4</cell><cell>30</cell><cell>16</cell><cell>8</cell><cell>0.0</cell><cell>0.012</cell><cell>0.003</cell><cell>32</cell><cell>100</cell><cell>0.0</cell></row><row><cell>sMNIST</cell><cell>4</cell><cell>96</cell><cell>128</cell><cell>1</cell><cell>0.1</cell><cell>0.008</cell><cell>0.002</cell><cell>50</cell><cell>150</cell><cell>0.01</cell></row><row><cell>psMNIST</cell><cell>4</cell><cell cols="2">128 128</cell><cell>2</cell><cell>0.15</cell><cell>0.004</cell><cell>0.001</cell><cell>50</cell><cell>150</cell><cell>0.01</cell></row><row><cell>sCIFAR</cell><cell>6</cell><cell cols="2">512 384</cell><cell>3</cell><cell>0.1</cell><cell>0.0045</cell><cell>0.001</cell><cell>50</cell><cell>250</cell><cell>0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>g. [MAX29[MIN47]0] ? 9.Characters are encoded as one-hot vectors, with 17 unique values possible (opening brackets and operators are grouped into a single token). The sequences are of unequal length, and hence the end of shorter sequences is padded with a fixed indicator value, padded to a maximum length of 2, 000. A reserved end-of-sequence token is appended. There are 10 different classes, representing the integer result of the expression. There are 96, 000 training sequences, 2, 000 validation sequences, and 2, 000 test sequences. No normalization is applied.? Text: Based off of the iMDB sentiment dataset presented by Maas et al.<ref type="bibr" target="#b40">[41]</ref>. Given a movie review, where characters are encoded as a sequence of integer tokens, classify whether the movie review is positive or negative.Characters are encoded as one-hot vectors, with 129 unique values possible. Sequences are of unequal length, and are padded to a maximum length of 4, 096. There are two different classes, representing positive and negative sentiment. There are 25, 000 training examples and 25, 000 test examples. No validation set is provided. No normalization is applied. Given two textual citations, where characters are encoded as a sequence of integer tokens, classify whether the two citations are equivalent. The citations must be compressed separately, before being passed into a final classifier layer. This is to evaluate how effectively the network can represent the text. The decoder head then uses the encoded representation to complete the task. Characters are encoded into a one-hot vector with 97 unique values. Two paired sequences may be of unequal length, with a maximum sequence length of 4, 000. There are two different classes, representing whether the citations are equivalent or not. There are 147, 086 training pairs, 18, 090 validation pairs, and 17, 437 test pairs. No normalization is applied. ? Image: Uses the CIFAR-10 dataset presented by Krizhevsky [30]. Given a 32 ? 32 grayscale CIFAR-10 image as a one-dimensional raster scan, classify the image into one of ten classes. Sequences are of equal length (1, 024). There are ten different classes. There are 45, 000 training examples, 5, 000 validation examples, and 10, 000 test examples. RGB pixel values are converted to a grayscale intensities, which are then normalized to have zero mean and unit variance (across the entire dataset). ? Pathfinder: Based off of the Pathfinder challenge introduced by Linsley et al. [37]. A 32 ? 32 grayscale image image shows a start and an end point as a small circle. There are a number of dashed lines on the image. The task is to classify whether there is a dashed line (or path) joining the start and end point. There are two different classes, indicating whether there is a valid path or not. Sequences are all of the same length (1, 024). There are 160, 000 training examples, 20, 000 validation examples, and 20, 000 test examples. The data is normalized to be in the range [?1, 1].</figDesc><table /><note>? Retrieval: Based off of the ACL Anthology network corpus presented by Radev et al. [46].</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The full S5 implementation is available at: https://github.com/lindermanlab/S5.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements and Disclosure of Funding</head><p>We thank Albert Gu for his thorough and insightful feedback. This work was supported by grants from the Simons Collaboration on the Global Brain (SCGB 697092), the NIH BRAIN Initiative (U19NS113201 and R01NS113119), and the Sloan Foundation. Some of the computation for this work was made possible by Stanford Data Science Microsoft Education Azure cloud credits.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent kalman networks: Factorized inference in high-dimensional deep feature spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harit</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Gebhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="544" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Prefix Sums and Their Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>School of Computer Science, Carnegie Mellon</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rept. CMU-CS-90-190</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quasi-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">AntisymmetricRNN: A dynamical system view on recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallelizing Legendre memory unit training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narsimha</forename><surname>Reddy Chilkuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1898" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Benjamin</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lipschitz recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Benjamin Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Azencot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Hodgkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">It&apos;s raw! Audio generation with state-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato</editor>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HiPPO: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1474" to="1487" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving the gating mechanism of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3800" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state space layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the parameterization and initialization of diagonal state space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11893</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How to train your HiPPO: State space models with generalized orthogonal basis projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.12037</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gmat: Global memory augmentation for transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Diagonal state spaces are as effective as structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14343</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Liquid structural state-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makram</forename><surname>Chahine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.12951</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A first course in the numerical analysis of differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arieh</forename><surname>Iserles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Long movie clip classification with state-space video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohaiminul</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bertasius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01692</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transformers are RNNs: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Linear dynamical systems as a core computational primitive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiva</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16808" to="16820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Parallel prefix computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Ladner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="831" to="838" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Parallel computing using the prefix problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaramakrishnan</forename><surname>Lakshmivarahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sudarshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03824</idno>
		<title level="m">Fnet: Mixing tokens with Fourier transforms</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simple recurrent units for highly parallelizable recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4470" to="4481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lezcano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mart?nez-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3794" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (INDRNN): Building a longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning long-range spatial dependencies with horizontal gated recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Linsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Veerabadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Windolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Linear unified nested attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangke</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10655</idno>
		<title level="m">Mega: Moving average equipped gated attention</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parallelizing linear recurrent neural nets over sequence length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Listops: A diagnostic dataset for latent tree learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel R Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">In-depth benchmarking of deep neural network architectures for ecg diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Nonaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Seita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="414" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio</title>
		<meeting><address><addrLine>Alex Graves</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
		<title level="m">Pradeep Muthukrishnan, and Vahed Qazvinian. The ACL anthology network corpus</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Flexconv: Continuous kernel convolutions with differentiable kernel sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert-Jan</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Bruintjes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Mikolaj Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Towards a general purpose cnn for long range dependencies in nd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Knigge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoogendoorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03398</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Jakub Mikolaj Tomczak, and Mark Hoogendoorn. CKConv: Continuous kernel convolution for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unicornn: A recurrent model for learning very long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9168" to="9178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The Annotated S4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidd</forename><surname>Karamcheti</surname></persName>
		</author>
		<ptr target="https://srush.github.io/annotated-s4/" />
	</analytic>
	<monogr>
		<title level="m">Blog Track at ICLR 2022</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal parallelization of Bayesian smoothers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>S?rkk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>?ngel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garc?a-Fern?ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="299" to="306" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Modeling irregular time series with continuous recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Schirmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mazin</forename><surname>Eltayeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lessmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="19388" to="19405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Weighted sigmoid gate unit for an activation function of deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Long Range Arena : A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning longer-term dependencies in RNNs with auxiliary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trieu</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4965" to="4974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Legendre Memory Units: Continuous-time representation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Kaji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Belinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech Commands: A dataset for limited-vocabulary speech recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Nystr?mformer: A Nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">14138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">H-transformer-1d: Fast one-dimensional hierarchical attention for sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3801" to="3815" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
