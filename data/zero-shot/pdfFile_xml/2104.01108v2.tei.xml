<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Group Collaborative Learning for Co-Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Kwai Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of AI (IIAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of AI (IIAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Keung</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of AI (IIAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">HKUST</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Kwai Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Group Collaborative Learning for Co-Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel group collaborative learning framework (GCoNet) capable of detecting co-salient objects in real time (16ms), by simultaneously mining consensus representations at group level based on the two necessary criteria: 1) intra-group compactness to better formulate the consistency among co-salient objects by capturing their inherent shared attributes using our novel group affinity module; 2) inter-group separability to effectively suppress the influence of noisy objects on the output by introducing our new group collaborating module conditioning the inconsistent consensus. To learn a better embedding space without extra computational overhead, we explicitly employ auxiliary classification supervision. Extensive experiments on three challenging benchmarks, i.e., CoCA, CoSOD3k, and Cosal2015, demonstrate that our simple GCoNet outperforms 10 cutting-edge models and achieves the new state-ofthe-art. We demonstrate this paper's new technical contributions on a number of important downstream computer vision applications including content aware co-segmentation, co-localization based automatic thumbnails, etc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Co-salient object detection (CoSOD) targets at detecting common salient objects sharing the same attributes given a group of relevant images. CoSOD is more challenging than the standard salient object detection (SOD) task, because CoSOD needs to distinguish co-occurring objects across multiple images <ref type="bibr" target="#b0">[1]</ref> in presence of other objects. That is, both intra-class compactness and inter-class separability should be simultaneously maximized. With this favorable feature CoSOD is thus often employed as a pre-processing step for various computer vision tasks, such as image retrieval <ref type="bibr" target="#b1">[2]</ref>, image quality assessment <ref type="bibr" target="#b2">[3]</ref>, collection-based crops <ref type="bibr" target="#b3">[4]</ref>, co-segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, semantic segmentation <ref type="bibr" target="#b6">[7]</ref>, image surveillance <ref type="bibr" target="#b7">[8]</ref>, video analysis <ref type="bibr" target="#b8">[9]</ref>, video co-localization <ref type="bibr" target="#b9">[10]</ref>, etc.</p><p>Previous works attempt to leverage the consistency * Corresponding author: Deng-Ping Fan (dengpfan@gmail.com).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Method</head><p>Other Method d2 &gt;&gt; d1 d 1 d2 <ref type="figure">Figure 1</ref>. t-SNE <ref type="bibr" target="#b10">[11]</ref> visualization of consensuses, where each point represents one consensus of an image group. Highlighted here are two similar but different groups (guitar &amp; violin) to illustrate the effectiveness of GCoNet. The consensus strategy in traditional CoSOD model tends to cluster consensuses together even they belong to different groups, resulting in ambiguous co-saliency detection. In contrast, our consensus strategy with effective intergroup constraint enables higher diversity with a very large group variance (d2 d1) and thus better inter-group separability.</p><p>among relevant images to facilitate CoSOD within an image group by exploring different shared cues <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> or semantic connections <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Some of them <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> use predicted saliency maps by computing various inter-image cues to discover co-salient objects. Other works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref> exploit a unified network to jointly optimize co-saliency information and saliency maps. Despite their promising results, most current models only extract their CoSOD representations in an individual group, which introduces a number of limitations. First, images from the same group contain similar foregrounds (i.e., co-salient objects) only provide positive relations while lacking the negative relations between different objects. Training the model only using positive pairs may lead to overfitting and result in ambiguous results for outlier images. Moreover, the number of images in a group is typically limited (20 to 40 images for most CoSOD datasets), so using a single group cannot provide enough information for learning a discriminative representation. Finally, individual groups also fall short in offering high-level semantic information, which is necessary for distinguishing noisy objects during inference in complex real-world scenarios.</p><p>To address the above issues, we propose a novel group collaborative learning framework (GCoNet) to mine the semantic correlation between different image groups. The proposed GCoNet consists of three important components: group affinity module (GAM), group collaborating module (GCM) and auxiliary classification module (ACM), which simultaneously learn the intra-group compactness and inter-group separability. The GAM makes the network learn the consensus feature within the same image group, while the GCM discriminates target attributes between different groups, thus enabling the model to be trained on the existing large-scale SOD datasets. <ref type="bibr" target="#b0">1</ref> We further improve the feature representation at a global semantic level through our ACM on each image to learn a better embedding space. In summary, our contributions are:</p><p>? We introduce a novel group collaborative learning strategy to address the CoSOD problem, and validate its effectiveness with extensive ablation studies.</p><p>? We design a novel unified Group Collaborative Learning Network (GCoNet) for CoSOD by simultaneously considering intra-group compactness and inter-group separability to mine the consensus representation.</p><p>? Our group affinity module (GAM) and group collaborating module (GCM) collaborate with each other to achieve better intra-and inter-group collaborative learning. The auxiliary classification module (ACM) further promotes learning at a global semantic level.</p><p>? Extensive experiments on three challenging CoSOD benchmarks, i.e., CoCA, CoSOD3k, and Cosal2015, show that our GCoNet achieves the new state-of-theart. Furthermore, we present two downstream applications based on our technical contributions, i.e., cosegmentation and co-localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The traditional salient object detection task <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> targets at directly segmenting salient object in each image separately, while CoSOD aims to segment the common salient objects across several relevant images. Previous works mainly exploit inter-image cues to detect co-salient objects. Early CoSOD methods explore the inter-image correspondence between image-pairs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref> or a group of relevant images <ref type="bibr" target="#b26">[27]</ref> based on shallow handcrafted descriptors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. They employ different approaches to mine the inter-image relationships using constraints or heuristic characteristics. Several studies attempt to capture the interimage constraints by employing an efficient manifold ranking scheme <ref type="bibr" target="#b28">[29]</ref> to obtain guided saliency maps, or using a global association constraint with clustering <ref type="bibr" target="#b12">[13]</ref>, or translational alignment <ref type="bibr" target="#b3">[4]</ref>. Other works attempt to formulate the semantic attributes shared among images in a group from the high-level features in the heuristic characteristics, using multiple saliency cues and self-adaptive weights <ref type="bibr" target="#b13">[14]</ref>, regional histograms and constrasts <ref type="bibr" target="#b29">[30]</ref>, metric learning by optimizing a new objective function <ref type="bibr" target="#b15">[16]</ref>, or pairwise similarity ranking and linear programming <ref type="bibr" target="#b30">[31]</ref>.</p><p>Recently deep-based models simultaneously explore the intra-and inter-image consistency in a supervised manner with different approaches, such as graph convolution networks (GCN) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, self-learning methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref>, inter-image co-attention with PCA projection <ref type="bibr" target="#b0">[1]</ref> or recurrent units <ref type="bibr" target="#b35">[36]</ref>, correlation techniques <ref type="bibr" target="#b36">[37]</ref>, quality measurement <ref type="bibr" target="#b37">[38]</ref>, or co-clustering <ref type="bibr" target="#b38">[39]</ref>. Some methods exploit multi-task learning to simultaneously optimize the co-saliency detection and co-segmentation <ref type="bibr" target="#b39">[40]</ref> or co-peak search <ref type="bibr" target="#b5">[6]</ref>. Other works explore hierachical features from multi-scale <ref type="bibr" target="#b40">[41]</ref>, multi-stage <ref type="bibr" target="#b41">[42]</ref>, or multi-layer <ref type="bibr" target="#b42">[43]</ref> features. Another notable research line is to explore groupwise semantic representation (consensus) which is used to detect co-salient regions for each image. There are different methods to capture the discriminative semantic representation, such as group attentional semantic aggregation <ref type="bibr" target="#b43">[44]</ref>, gradient feedback <ref type="bibr" target="#b19">[20]</ref>, co-category association <ref type="bibr" target="#b44">[45]</ref>, united fully convolutional network <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, or integrated multilayer graph <ref type="bibr" target="#b47">[48]</ref>. Methods are proposed to solve the CoSOD problem in a semi-supervised <ref type="bibr" target="#b48">[49]</ref> or unsupervised manner <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>, and studies <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> are availalbe on co-saliency detection from a single image.</p><p>Previous works have focused on intra-group (intra-and inter-image) cues for capturing common attributes of cosalient objects. The inter-group information has received less attention, although CODW <ref type="bibr" target="#b54">[55]</ref> focuses on visually similar neighbor. Recently Zhang et al. <ref type="bibr" target="#b19">[20]</ref> utilized a jigsaw training to implicitly exploit other images to facilitate group training. But their model still targets intra-group learning. Our method differs from existing models in the exploration of inter-group relations for discriminating feature learning at a group level explicitly and semantically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Group Collaborative Learning Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture Overview</head><p>Given a group of N relevant images {I 1 , I 2 , ..., I n } containing common salient objects of a certain class, CoSOD aims to detect them simultaneously and output the co- </p><formula xml:id="formula_0">? # ? ! ? ! Decoder ? # $%&amp; ? # ? # Encoder Decoder ? ! $%&amp; ? ! NxCxHxW NxCxHxW 1xCx1x1 1xCx1x1</formula><p>"guitar" group "violin" group Feature map Group Consensus <ref type="figure">Figure 2</ref>. Pipeline of the proposed Group Collaborative Learning Network (GCoNet). Images in two groups are first processed by a weight-shared encoder. Then we employ the group affinity module (GAM, see <ref type="figure">Figure 3</ref> for more details) to conduct intra-group collaborative learning for each group to generate a consensus, which is collaborated with the original feature maps to segment co-salient objects using the decoder. In addition, the original feature maps and consensuses of both groups are fed to the group collaborating module (GCM, see <ref type="figure">Figure 4</ref>) to conduct the inter-group collaborative learning. Moreover an auxiliary classification module (ACM) is applied to obtain the high-level semantic representation. The GCM and ACM are only used for training and are removed at inference. saliency maps. Unlike existing CoSOD methods which only depend on the information within the image group, we propose a novel group collaborative learning network (GCoNet) to mine the consensus representations at both intra-and inter-group level. <ref type="figure">Figure 2</ref> illustrates the flowchart of our GCoNet. First, an encoder network is used to extract feature maps F 1 = {F 1,n } N n=1 , F 2 = {F 2,n } N n=1 ? R N ?C?H?W for two image groups, where C is the channel number and H ? W is the spatial size. Then, a group affinity module (GAM) is used to combine all single-image features to distill the consensus feature E a 1 , E a 2 ? R 1?C?1?1 from F 1 , F 2 , representing the common attributes of the co-salient objects for each group. Simultaneously, a group collaborating module (GCM) is applied to enhance the image representation for discriminating the target attributes between different image groups. Finally, we further improve the high-level semantic representation of images using an auxiliary classification module (ACM) to learn a better embedding space. The resulting collaborative features are then fed to a decoder network to produce the co-saliency maps M 1 , M 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Group Affinity Module</head><p>Intuitively, common objects from the same class always share some similarity in appearance and have high similarity in features, which have been widely employed in many tasks. Inspired by self-supervised video tracking methods <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>, which propagate the segmentation masks of target objects based on the pixel-wise correspondences between two adjacent frames, we extend this idea to the CoSOD task by computing the global affinity among all images in a group.</p><p>For any two image features {F 1,n , F 1,m } ? F 1 2 and without losing generality we drop the group subscript, we can use the inner product to compute their pixel-wise correlations:</p><formula xml:id="formula_1">S (n,m) = ?(F n ) T ?(F m ),<label>(1)</label></formula><p>where ?, ? are linear embedding functions. The affinity map S (n,m) ? R HW ?HW efficiently captures the commonality of co-salient objects in the image pair (n, m). Then we can generate F n 's affinity map A n?m ? R HW ?1 by finding the maxima for each of F n 's pixel conditioned on F m which alleviates the influence of noisy correlation values in the map. Similarly, we can extend the local affinity of two images to the global affinity of all images in the group. Specifically, we compute the affinity map S F ? R N HW ?N HW between all image features F using Eq. 1. Then, we find the maxima for each image A F ? R N HW ?N from S F , and average all the maxima of N images to generate the global affinity attention map A F ? R N HW ?1 . In this way, the affinity attention map is globally optimized on all images thus alleviating the influence of occasional co-occurring bias. Then, we use a softmax operation to normalize A F and reshape it to generate the attention map A S ? R N ?(1?H?W ) . We multiply A S with the original feature F to produce the attention feature maps F a ? R N ?C?H?W . Finally, all the attention feature maps F a for the whole group are used to produce the attention consensus E a by average pooling along both the batch and spatial dimensions, as shown in <ref type="figure">Figure 3</ref>. <ref type="figure">Figure 3</ref>. Group Affinity Module. We first exploit the affinity attention to generate the attention maps for the input features by collaborating all images in group. Subsequently, the maps are multiplied with the input features to generate the consensus for the group. Then the obtained consensus is used to coordinate the original feature maps and is also fed to the GCM for inter-group collaborative learning. <ref type="figure">Figure 4</ref>. Group collaborating module. The original feature maps and consensuses of both groups are fed to the GCM. The predicted output conditioned on the consistent feature and consensus (from the same group) is supervised with the available groundtruth labels. Otherwise, it is supervised by the all-zero maps.</p><formula xml:id="formula_2">N x (1 x H x W) ? # ? $ NHW x NHW NHW x N x HW NHW x N NHW x 1 ? ? ) ? R S R M A A A R Reshape M Maximize A Average S R Softmax &amp; Reshape A A Batch &amp; Spatial Average Multiplication * N x (C x H x W)</formula><formula xml:id="formula_3">! " # ! ? " " CNN ? # " CNN ? " # ? " $ ? " % CNN ? # # ? # % ? # $ CNN ? " " &amp; ? # # &amp;</formula><p>The global affinity module focuses on capturing the commonality among co-salient objects within the same group and therefore improves the intra-group compactness of the consensus representation. Such intra-group compactness alleviates the disturbance of co-occurring noise and enables the model to concentrate on the co-salient regions. This allows the shared attributes of co-salient objects to be better captured and therefore results in better consensus representation. The obtained attention consensus E a is combined with the original feature maps F through depth-wise correlation <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref> to achieve efficient information association. The resulting feature maps F out are fed to the decoder to predict co-saliency maps M n for each image. The loss function is:</p><formula xml:id="formula_4">L sal = 1 N N n L siou (M n , G n ),<label>(2)</label></formula><p>where L siou is the soft IoU loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">62]</ref> and G n denotes the ground-truth label for each image in the group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Group collaborating module (GCM)</head><p>Most CoSOD methods tend to focus on the intra-group compactness of the consensus, but the inter-group separability is equally crucial for distinguishing distracting ob-jects, especially when processing complex images with more than one salient objects. To enhance the discriminative representations between different groups, we propose a simple but effective module, i.e., the GCM, by learning to encode the inter-group separability.</p><p>Given two image groups with the corresponding features {F 1 , F 2 } and attention consensus {E a 1 , E a 2 } obtained from the GAM, we apply an intra-and inter-group crossmultiplication. Specifically, the intra-group multiplication deals with the features and their consensus:</p><formula xml:id="formula_5">F 1 1 = F 1 ? E a 1</formula><p>and F 2 2 = F 2 ? E a 2 for the intra-group collaboration, while the inter-group multiplication acts on the features and consensus of different groups, i.e., F 2 1 = F 1 ? E a 2 and F 1 2 = F 2 ? E a 1 , to express the inter-group interaction. The intragroup representation F + = {F 1 1 , F 2 2 } is exploited to predict the co-saliency maps, and the inter-group representation F ? = {F 2 1 , F 1 2 } is employed to provide a consensus with group separability. Specifically, we feed {F + , F ? } to a small convolutional network with an upsampling layer and produce the saliency map {M + , M ? } 3 with different supervision signals: we use ground-truth labels to supervise F + , while all-zero maps are used for F ? . The loss function is:</p><formula xml:id="formula_6">L ctm = 1 N N n L FL (&lt; M + n , M ? n &gt;, &lt; G n , G 0 n &gt;),<label>(3)</label></formula><p>where L FL is the focal loss <ref type="bibr" target="#b62">[63]</ref>, G n is the ground-truth, G 0 n is the all-zero map and &lt; ? &gt; denotes the concatenation operation. Our GCM thus encourages the consensus to distinguish different groups with high inter-group separability to identify distractors in complex environment. Another advantage is that this module enables the model to be trained on the existing SOD datasets, whose images typically contain only one dominating object. We can discard this module during inference without introducing additional computational overhead. <ref type="table">Table 1</ref>. Quantitative ablation studies of our GCoNet on the effectiveness of the GAM (group affinity module), GCM (group collaborating module), ACM (auxiliary classification module) and their combinations.</p><formula xml:id="formula_7">3 M + = {M + 1 , M + 2 } and M ? = {M ? 1 , M ? 2 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modules</head><p>CoCA <ref type="bibr" target="#b19">[20]</ref> CoSOD3k <ref type="bibr" target="#b0">[1]</ref> Cosal2015 <ref type="bibr" target="#b54">[55]</ref>  </p><formula xml:id="formula_8">ID GAM GCM ACM E max ? ? S? ? F max ? ? ? E max ? ? S? ? F max ? ? ? E max ? ? S? ? F max ? ? ? 1 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Auxiliary Classification Module (ACM)</head><p>To obtain more discriminative features for consensus, we also introduce an ACM to facilitate high-level semantic representation learning. Specifically, we add a classification predictor with a global average pooling layer and one fully connected layer to the backbone to classify F n to the corresponding class Y n . In the Euclidean feature space, the classification supervision can separate classes by introducing a large margin, and cluster samples belonging to the same class. Therefore, it enables the model to generate more representative features and benefits the consensus learning for intra-group compactness and inter-group separability. The loss function is:</p><formula xml:id="formula_9">L cls = L ce (Y n ,? n ),<label>(4)</label></formula><p>where L ce is the cross-entropy loss and? n is the groundtruth class label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">End-to-end Training</head><p>During training, the GAM, GCM, and ACM are jointly trained with the backbone in an end-to-end manner. The whole framework is optimized by integrating all the aforementioned loss functions:</p><formula xml:id="formula_10">L = ? 1 L sal + ? 2 L ctm + ? 3 L cls ,<label>(5)</label></formula><p>where ? 1 , ? 2 , and ? 3 are hyperparameter weights to balance the loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We use VGG-16 <ref type="bibr" target="#b63">[64]</ref> with Feature Pyramid Network (FPN) <ref type="bibr" target="#b64">[65]</ref> as our backbone. For fair comparison, we follow GICD <ref type="bibr" target="#b19">[20]</ref> and use the DUTS <ref type="bibr" target="#b65">[66]</ref> dataset as our training set. The group labels derived from GICD <ref type="bibr" target="#b19">[20]</ref> are used to group the images during training. In each training episode, we randomly pick two different groups with 16 samples 4 in each group to train the network. The images are all resized to 224x224 for training and testing, and the output saliency <ref type="bibr" target="#b3">4</ref> Due to limited computing resource. The larger the better. maps are resized to the original size for evaluation. The network is trained over 50 epochs in total with the Adam optimizer. The initial learning rate is set to 10e ? 4, ? 1 = 0.9 and ? 2 = 0.99. The whole training takes around four hours and the inference speed on the image pair groups 5 is 16 ms. The platform for training and inference is equipped with 56 Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz and a Nvidia GeForce GTX 1080Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Datasets and Metrics</head><p>We employ three challenging datasets for evaluation: CoCA <ref type="bibr" target="#b19">[20]</ref>, CoSOD3k <ref type="bibr" target="#b66">[67]</ref>, and Cosal2015 <ref type="bibr" target="#b54">[55]</ref>. The last is a large dataset widely used in the evaluation of CoSOD methods. The first two were recently proposed for challenging real-world co-saliency evaluation, with the images usually containing multiple common and non-common objects against a complex background. Following the advice of recent large-scale benchmark work <ref type="bibr" target="#b66">[67]</ref>, we do not use iCoseg <ref type="bibr" target="#b67">[68]</ref> and MSRC <ref type="bibr" target="#b68">[69]</ref> for evaluation, because tbey usually provide only one salient object in an image and are not very suitable for evaluating CoSOD models. We use maximum E-measure E max ? <ref type="bibr" target="#b69">[70]</ref>, S-measure S ? <ref type="bibr" target="#b70">[71]</ref>, maximum F-measure F max ? <ref type="bibr" target="#b71">[72]</ref>, and mean absolute error (MAE) <ref type="bibr" target="#b72">[73]</ref> to evaluate methods in our experiments. Evaluation toolbox: https://github.com/ DengPingFan/CoSODToolbox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>In this section, we study the effectiveness of each component in our approach <ref type="table">(Table 1)</ref> and investigate how they contribute to a good consensus feature. Effectiveness of GAM. The global co-attention module is a fundamental component of our model, which is designed to capture the common attributes of co-salient objects in an image group for better intra-group compactness. Compared to the baseline model with only the vanilla consensus extracted by an average pooling operation, GAM improves the performance on all metrics and datasets. To get a deeper understanding of our GAM module, we visualize the learned attention masks in <ref type="figure" target="#fig_1">Figure 5</ref>. We find that our global coattention effectively alleviates the influence of co-occurring <ref type="table">Table 2</ref>. Quantitative comparison results between our GCoNet and other methods. "?" ("?") means that the higher (lower) is better. Co = CoSOD models, Sin = Single-SOD models. The symbol * denotes traditional CoSOD algorithms. Online benchmark has been made publicly available at: http://dpfan.net/cosod3k.</p><p>CoCA <ref type="bibr" target="#b19">[20]</ref> CoSOD3k <ref type="bibr" target="#b0">[1]</ref> Cosal2015 <ref type="bibr" target="#b12">[13]</ref> TIP 2013 Co 0.641 0.523 0.313 0.180 0.637 0.528 0.466 0.228 0.656 0.544 0.532 0.233 GWD <ref type="bibr" target="#b46">[47]</ref> IJCAI 2017 Co 0.701 0.602 0.408 0.166 0.777 0.716 0.649 0.147 0.802 0.744 0.706 0.148 RCAN <ref type="bibr" target="#b35">[36]</ref> IJCAI 2019 Co 0.702 0.616 0.422 0.160 0.808 0.744 0.688 0.130 0.842 0.779 0.764 0.126 CSMG <ref type="bibr" target="#b40">[41]</ref> CVPR 2019 Co 0.733 0.627 0.499 0.114 0.804 0.711 0.709 0.157 0.842 0.774 0.784 0.130 BASNet <ref type="bibr" target="#b20">[21]</ref> CVPR 2019 Sin 0.644 0.592 0.408 0.195 0.804 0.771 0.720 0.114 0.849 0.822 0.791 0.096 PoolNet <ref type="bibr" target="#b21">[22]</ref> CVPR 2019 Sin 0.640 0.602 0.404 0.177 0.799 0.771 0.709 0.113 0.848 0.823 0.785 0.094 EGNet <ref type="bibr" target="#b22">[23]</ref> ICCV noise and focuses on co-salient regions in the image groups, e.g., in both the monkey and bicycle groups, there are some co-occurring persons in some images, but our GAM is not adversely influenced. The global view of GAM enables the most common objects to be detected, while the local pairwise co-attention cannot distinguish them in the local view.</p><formula xml:id="formula_11">[55] Method Pub. &amp; Year Type E max ? ? S? ? F max ? ? ? E max ? ? S? ? F max ? ? ? E max ? ? S? ? F max ? ? ? CBCD*</formula><p>Effectiveness of GCM. The group collaborating module is designed to enable the consensus inter-group separability to distinguish distracting objects from non-common objects. After equip the model with GCM, significant performance improvement (ID-1 versus ID-3) is obtained in <ref type="table">Table 1</ref> especially on the challenging CoCA <ref type="bibr" target="#b19">[20]</ref> dataset whose images usually contain multiple uncommon and common objects. To investigate the consensus characteristics when the model is trained with the GCM, we visualize the consensus using t-SNE <ref type="bibr" target="#b10">[11]</ref> on the CoCA dataset, and compare with the vanilla consensus without the GCM. As shown in <ref type="figure">Figure 1</ref>, the vanilla consensuses (top: other method) tend to cluster together, even if they belong to different groups, resulting in ambiguous co-saliency detection, especially for objects belonging to similar but different groups. In contrast, the consensuses trained with the GCM (bottom: our method) is more diverse with a higher group variance (d 2 d 1 ), for more effective inter-group separability. Effectiveness of ACM. As shown in <ref type="table">Table 1</ref>, the classification module introduces better backbone features for the consensus with the auxiliary classification supervision. The ACM improves the baseline performance on all metrics and datasets. This cost-free improvement does not change the network architecture and does not introduce extra computational overhead, thus has substantial potential to other models and tasks to take advantage of the multi-task learning and more representative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Competing Methods</head><p>Since not all CoSOD models have publicly released codes, we only compare our GCoNet with one representative traditional algorithm (CBCD) and five deepbased CoSOD models, including GWD <ref type="bibr" target="#b74">[75]</ref>, RCAN <ref type="bibr" target="#b35">[36]</ref>, CSMG <ref type="bibr" target="#b40">[41]</ref>, GICD <ref type="bibr" target="#b19">[20]</ref>, and CoEGNet <ref type="bibr" target="#b0">[1]</ref>. Following the current state-of-the-art model <ref type="bibr" target="#b19">[20]</ref>, we also compare with four cutting-edge deep salient object detection (SOD) 6 models: BASNet <ref type="bibr" target="#b20">[21]</ref>, PoolNet <ref type="bibr" target="#b21">[22]</ref>, EGNet <ref type="bibr" target="#b22">[23]</ref> and SCRN <ref type="bibr" target="#b73">[74]</ref>. More complete leaderboard can be found in recent standard benchmark works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b66">67]</ref>.  Quantitative Results. <ref type="table">Table 2</ref> tabulates the quantitative results of our model and state-of-the-art methods. Our model outperforms all of them in all metrics, especially on the challenging CoCA and CoSOD3k datasets. Among these three datasets, CoCA is the most challenging, since the images typically contain other multiple objects in addition to the co-salient objects which are even smaller in size. Our model capitalizes on our better consensus and significantly outperforms other methods especially the SOD methods which are trapped in distinguishing many distracting objects instead. CoSOD3k has similar attributes, and our model still performs much better than other models on this dataset. Cosal2015 is the easiest dataset because its images typically only contain one co-salient object, and therefore the SOD algorithms can easily handle this dataset. Our model cannot take full advantage of the better consensus on this dataset and the improvement is not as significant as on other datasets.</p><p>Qualitative Results. <ref type="figure" target="#fig_3">Figure 6</ref> shows the saliency maps generated by different methods for qualitative comparison. In these difficult examples, each image contains other multiple objects in addition to the co-salient objects. As aforementioned, the SOD methods can only detect salient objects and fail to distinguish co-salient objects due to their intrinsic limitation. The CoSOD methods perform better than the SOD methods owing to their consensus for distinguishing co-salient regions. However, limited by the their weak consensus, they are still unable to handle the challenging cases. Our model introduces an effective consensus through optimizing intra-group compactness and inter-group separability, and therefore performs much better on detecting cosalient objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion of Module Cooperation</head><p>Our three modules are closely interdependent and mutually reinforced for improving co-saliency detection performance. Combining the GAM and GCM can significantly improve the performance compared to the individual modules. Without the GAM the vanilla consensus is not robust against noise caused by uncommon objects and background, and the low-quality consensus cannot take full advantage of the GCM which heavily relies on the consensus for distinguishing different objects. On the other hand, although the consensus can capture common attributes with the help of the GAM, it is difficult to distinguish different groups without the GCM especially for similar groups. Overall, the GAM produces better   consensus with high intra-group compactness to detect cosaliency objects, while the GCM further endows the consensus with inter-group separability for better discriminative ability. Adding ACM, the consensus can benefit from more representative features leveraged by the multi-task learning. <ref type="figure" target="#fig_4">Figure 7</ref> qualitatively analyse their cooperation. The baseline model detects uncommon objects, while the GAM and GCM can slightly ameliorate their adverse influence. When combining the GAM and GCM, the model can effectively capture co-salient objects with the ACM further boosting the co-salient object detection result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Downstream Applications</head><p>Here, we show how the extracted co-saliency map can be utilized to generate high-quality segmentation masks for selected closely related downstream image processing tasks. Application #1: Content-Aware Co-Segmentation. Cosaliency maps have been previously used in pre-processing for unsupervised object segmentation. In our implementation, we first manually select a group of images from the internet by keyword search . Then, co-saliency maps are generated by our GCoNet to automatically mine the salient content of the specific group. Similar to Cheng et al. <ref type="bibr" target="#b18">[19]</ref>, we also utilize GrabCut <ref type="bibr" target="#b75">[76]</ref> to obtain the final segmentation results. To initialize GrabCut, we simply choose adaptive threshold <ref type="bibr" target="#b76">[77]</ref> to binarize the saliency maps. <ref type="figure" target="#fig_5">Figure 8</ref> shows the results of the content-aware object cosegmentation which should benefit existing e-commerce applications requiring background replacement. Application #2: Automatic Thumbnails.</p><p>The idea of paired-image thumbnails is derived from the seminal work <ref type="bibr" target="#b3">[4]</ref>. With the same goal 7 , we present a CNN-based photographic triage application which is valuable for sharing images with friends on the website. As shown in <ref type="figure" target="#fig_6">Figure 9</ref>, we first generate the yellow box based on the cosaliency map obtained by our GCoNet. Then, we simply enlarge the yellow box to get a larger red box. Finally, we adopt the collection-aware crops technique <ref type="bibr" target="#b3">[4]</ref> to produce the results (2 nd row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we investigate a novel group collaborative learning framework (GCoNet) for CoSOD. We find that group-level consensus can introduce effective semantic information to benefit the representation of both the intra-group compactness and inter-group separability for CoSOD. Our experiments quantitatively and qualitatively demonstrate the advantage of our GCoNet which outper-forms existing state-of-the-art models. In addition, our GCoNet achieves real-time speed (16ms) which can greatly benefit many applications such as co-segmentation, colocalization, and among others.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>2019 Sin 0.648 0.603 0.404 0.178 0.793 0.762 0.702 0.119 0.843 0.818 0.786 0.099 SCRN [74] ICCV 2019 Sin 0.642 0.612 0.413 0.164 0.805 0.771 0.716 0.113 0.850 0.817 0.783 0.098 GICD [20] ECCV 2020 Co 0.715 0.658 0.513 0.126 0.848 0.797 0.770 0.079 0.887 0.844 0.844 0.071 CoEGNet [1] TPAMI 2021 Co 0.717 0.612 0.493 0.106 0.825 0.762 0.736 0.092 0.882 0.836 0.832 0.077 GCoNet (Ours) CVPR 2021 Co 0.760 0.673 0.544 0.105 0.860 0.802 0.777 0.071 0.887 0.845 0.847 0.068 monkey camera bicycle Visualization of affinity attention maps learned by GAM using intra-group collaborative learning across all images in each group. Masks are sensitive to co-salient regions with shared attributes, which benefits the consensus representation learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparisons of our GCoNet and other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative ablation studies of our GCoNet on different modules and their combinations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Application 1. Content aware object co-segmentation visual results ("GT car") obtained by our GCoNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Application 2. Co-location based automatic thumbnails ("Raccoon") generated by our GCoNet.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the existing CoSOD datasets altogether contain about 6k images, while there are more than 12 SOD datasets, containing about 60k images. It may partially alleviate the insufficient training data issue in cosalient object detection.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">All analyses in section 3.2 on F 1 can be applied to F 2 . We omit the group subscript for notation simplicity, i.e., we use Fn to represent F 1,n .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">CoSOD task works for image groups. Therefore we use the basic image pair group to evaluate the speed rather than the single image.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">SOD methods can also be directly applied to the CoSOD task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Note that Jacobs et al.'s work [4] is limited to the case of image pairs.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Re-thinking co-salient object detection. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengpeng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salientshape: group saliency in image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVC</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">No-reference synthetic image quality assessment with convolutional neural network and local image saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick Wb</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="208" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cosaliency: Where people look when comparing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM UIST</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Higher-order image cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1011" to="1021" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepco3: Deep instance co-segmentation by co-peak search and co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint learning of saliency detection and weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Zhuge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Trustful internet of surveillance things based on deeply represented visual co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hugo C De Albuquerque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IoT-J</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4092" to="4100" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient video object co-localization with co-saliency activated tracklets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Koteswar Rao Jerripothula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="744" to="755" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cats: Co-saliency activated tracklet selection for video colocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Koteswar Rao Jerripothula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A co-saliency model of image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">King</forename><surname>Ngi Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3365" to="3375" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cluster-based co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-adaptively weighted co-saliency detection via rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4175" to="4186" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Co-saliency detection via a self-paced multiple-instance learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="865" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified metric learning-based framework for cosaliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2473" to="2483" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised cnn-based cosaliency detection with graphical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Segmentation driven low-rank matrix recovery for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kidiyo</forename><surname>Kpalma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Ronsin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="569" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-induced co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenda</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Basnet: Boundaryaware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for realtime salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Highly efficient salient object detection with 100k parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Qiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizing the F-measure for threshold-free salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Preattentive co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-saliency detection via base reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From co-saliency to co-segmentation: An efficient and fully unsupervised energy minimization model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Yueh</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Hong</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient saliencymodel-guided visual co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keren</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="588" to="592" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Co-saliency detection based on hierarchical segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liquan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="92" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cosalient object detection from multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanman</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">King</forename><surname>Ngi Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1896" to="1909" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unified multiple graph learning and convolutional network model for co-saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiple graph convolutional networks for cosaliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional network with attention graph clustering for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cosaliency detection based on intrasaliency prior transfer and deep intersaliency mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1163" to="1176" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting robust co-saliency with recurrent co-attention neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lv</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Icnet: Intra-saliency correlation network for cosaliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wen-Da Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Quality-guided fusion-based co-saliency estimation for image co-segmentation and colocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Koteswar Rao Jerripothula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TMM</publisher>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2466" to="2477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting co-saliency detection: A novel approach based on two-stage multi-view spectral rotation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3196" to="3209" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image co-saliency detection and co-segmentation via progressive joint optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chi</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="71" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cosaliency detection via mask-guided fully convolutional networks with multi-scale label smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep learning intraimage and inter-images features for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Co-saliency detection via integration of multi-layer convolutional features and inter-image propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangling</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="146" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Coadnet: Collaborative aggregation-anddistribution networks for co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust deep co-saliency detection with group semantic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep group-wise fully convolutional network for co-saliency detection with graph propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">El</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farouk</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5052" to="5063" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Group-wise deep co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">El</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farouk</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<editor>IJ-CAI</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Co-salient object detection based on deep saliency networks and seed propagation over an integrated graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Insung</forename><surname>Dong-Ju Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Ik</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5866" to="5879" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A feature-adaptive semi-supervised framework for co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoju</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liansheng</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Supervision by fusion: Towards unsupervised learning of deep salient object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Coattention cnns for unsupervised object co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Jui</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cosaliency detection based on hierarchical consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Co-saliency detection within a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An easy-to-hard learning strategy for within-image co-saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyue</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjiang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhou</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="166" to="176" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Detection of co-salient objects by looking deep and wide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Self-supervised learning for video correspondence flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mast: A memoryaugmented self-supervised tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Fewshot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Interactive image segmentation with latent diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Taking a deeper look at the co-salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">icoseg: Interactive co-segmentation with intelligent scribble guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Object categorization by learned universal visual dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Vibhav Vineet, and Nigel Crook. Efficient salient region detection with soft image abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Group-wise deep object co-segmentation with co-attention recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqi</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
