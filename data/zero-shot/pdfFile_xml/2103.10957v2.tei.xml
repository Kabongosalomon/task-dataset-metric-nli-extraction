<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Visual Pretraining with Contrastive Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Visual Pretraining with Contrastive Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised pretraining has been shown to yield powerful representations for transfer learning. These performance gains come at a large computational cost however, with state-of-the-art methods requiring an order of magnitude more computation than supervised pretraining. We tackle this computational bottleneck by introducing a new self-supervised objective, contrastive detection, which tasks representations with identifying object-level features across augmentations. This objective extracts a rich learning signal per image, leading to state-of-the-art transfer accuracy on a variety of downstream tasks, while requiring up to 10? less pretraining. In particular, our strongest ImageNetpretrained model performs on par with SEER, one of the largest self-supervised systems to date, which uses 1000? more pretraining data. Finally, our objective seamlessly handles pretraining on more complex images such as those in COCO, closing the gap with supervised transfer learning from COCO to PASCAL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the AlexNet breakthrough on ImageNet, transfer learning from large labeled datasets has become the dominant paradigm in computer vision <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">50]</ref>. While recent advances in self-supervised learning have alleviated the dependency on labels for pretraining, they have done so at a tremendous computational cost, with state-of-the-art methods requiring an order of magnitude more computation than supervised pretraining <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21</ref>]. Yet the promise of self-supervised learning is to harness massive unlabeled datasets, making its computational cost a critical bottleneck.</p><p>In this work, we aim to alleviate the computational burden of self-supervised pretraining. To that end we introduce contrastive detection, a new objective which maximizes the similarity of object-level features across augmentations. The benefits of this objective are threefold. First, it extracts separate learning signals from all objects in an image, enriching the information provided by each training example for free-object-level features are simply ob-  Self-supervised pretraining with SimCLR <ref type="bibr" target="#b8">[9]</ref> matches the transfer performance of supervised pretraining only when given 10? more training iterations. Our proposed DetCon objective surpasses both, while requiring 5? less computation than SimCLR. Transfer performance is measured by fine-tuning the representation on the COCO dataset for 12 epochs, using a Mask-RCNN.</p><p>tained from intermediate feature arrays. Second, it provides a larger and more diverse set of negative samples to contrast against, which also accelerate learning. Finally, this objective is well suited to learning from complex scenes with many objects, a pretraining domain that has proven challenging for self-supervised methods. We identify approximate object-based regions in the image through the use of unsupervised segmentation algorithms. Perceptual grouping <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41]</ref>-the idea that low and mid-level regularities in the data such as color, orientation and texture allow for approximately parsing a scene into connected surfaces or object parts-has long been theorized to be a powerful prior for vision <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">55]</ref>. We leverage these priors by grouping local feature vectors accordingly, and applying our contrastive objective to each object-level feature separately. We investigate the use of several unsupervised, image-computable masks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2]</ref>, and find our objective to work well despite their inaccuracies.</p><p>We test the ability of our objective to quickly learn transferable representations by applying it to the ImageNet dataset and measuring its transfer performance on challenging tasks such as COCO detection and instance segmentation, semantic segmentation on PASCAL and Cityscapes, and NYU depth estimation. Compared to representations obtained from recent self-supervised objectives such as SimCLR and BYOL <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21]</ref>, our representations are more accurate and can be obtained with much less training time. We also find this learning objective to better handle images of more complex scenes, bridging the gap with supervised transfer from the COCO dataset. In summary, we make the following contributions:</p><p>1. We formulate a new contrastive objective which maximizes the similarity across augmentations of all objects in a scene, where object regions are provided by a simple, unsupervised heuristic.</p><p>2. We find this objective to alleviate the computational burden of self-supervised transfer learning, reducing by up to 10? the computation required to match supervised transfer learning from ImageNet. Longer training schedules lead to state-of-the-art transfer to COCO detection and instance segmentation, and our best model matches the very recent state-of-the-art self-supervised system SEER <ref type="bibr" target="#b19">[20]</ref> which is trained on 1000? more-if less curated-images.</p><p>3. When transferring from complex scene datasets such as COCO, our method closes the gap with a supervised model which learns from human-annotated segmentations.</p><p>4. Finally, we assess to what extent the existing contrastive learning paradigm could be simplified in the presence of high quality image segmentations, raising questions and opening avenues for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Transferring the knowledge contained in one task and dataset to solve other downstream tasks (i.e. transfer learning) has proven very successful in a range of computer vision problems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>. While early work focused on improving the pretraining architecture <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b51">51]</ref> and data <ref type="bibr" target="#b52">[52]</ref>, recent work in self-supervised learning has focused on the choice of pretraining objective and task. Early selfsupervised pretraining typically involved image restoration, including denoising <ref type="bibr" target="#b59">[59]</ref>, inpainting <ref type="bibr" target="#b46">[46]</ref>, colorization <ref type="bibr" target="#b65">[65,</ref><ref type="bibr" target="#b35">36]</ref>, egomotion prediction <ref type="bibr" target="#b0">[1]</ref>, and more <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b66">66]</ref>. Higher-level pretext tasks have also been studied, such as predicting context <ref type="bibr" target="#b12">[13]</ref>, orientation <ref type="bibr" target="#b17">[18]</ref>, spatial layouts <ref type="bibr" target="#b43">[44]</ref>, temporal ordering <ref type="bibr" target="#b41">[42]</ref>, and cluster assignments <ref type="bibr" target="#b4">[5]</ref>.</p><p>Contrastive objectives, which maximize the similarity of a representation across views, while minimizing its similarity with distracting negative samples, have recently gained considerable traction <ref type="bibr" target="#b22">[23]</ref>. These views have been defined as local and global crops <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b27">28]</ref> or different input channels <ref type="bibr" target="#b53">[53]</ref>. Instance-discrimination approaches generate global, stochastic views of an image through dataaugmentation, and maximize their similarity relative to marginally sampled negatives <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b62">62]</ref>, although the need for negative samples has recently been questioned <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>. While the benefits of instance-discrimination approaches have mostly been limited to pretraining from simple datasets such as ImageNet, clustering-based pretraining has proven very successful in leveraging large amounts of uncurated images for transfer learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>While most work has focused on learning whole-image representations, there has been increasing interest in learning local descriptors that are more relevant for downstream tasks such as detection and segmentation. Examples of such work include the addition of auxiliary losses <ref type="bibr" target="#b54">[54]</ref>, architectural components <ref type="bibr" target="#b48">[48]</ref>, or both <ref type="bibr" target="#b63">[63]</ref>. While perceptual grouping has long been used for representation learning, often relying on coherent motion in videos <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b60">60]</ref>, it has only recently been combined with contrastive learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b67">67]</ref>. Most related to our work are <ref type="bibr" target="#b58">[58,</ref><ref type="bibr" target="#b67">67]</ref> that also leverage image segmentations for self-supervised learning, although both differ from ours in that they learn backbones that are specialized for semantic segmentation and employ different loss functions. Although these works arrive at impressive unsupervised segmentation accuracy, neither report gains in pretraining efficiency for transfer learning tasks such as COCO detection and instance segmentation, which we study next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We introduce a new contrastive objective which maximizes the similarity across views of local features which represent the same object ( <ref type="figure">Figure 2</ref>). In order to isolate the benefit of these changes, we make the deliberate choice of re-using elements of existing contrastive learning frameworks where possible. To test the generality of our approach, we derive two variants, DetCon S and DetCon B , based on two recent self-supervised baselines, SimCLR <ref type="bibr" target="#b8">[9]</ref> and BYOL <ref type="bibr" target="#b20">[21]</ref> respectively. We adopt the data augmentation procedure and network architecture from these methods while applying our proposed contrastive detection loss to each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The contrastive detection framework</head><p>Data augmentation. Each image is randomly augmented twice, resulting in two images: x, x . DetCon S and DetCon B adopt the augmentation pipelines of SimCLR and BYOL respectively, which roughly consist of random cropping, flipping, blurring, and point-wise color transformations. We refer the reader to appendix A.1 for more details. In all cases, images are resized to 224?224 pixel resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Image and Heuristic Masks</head><p>Augmented Views</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Features</head><p>Masked Pooling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DetCon Objective</head><p>Encoder <ref type="figure">Figure 2</ref>. The contrastive detection method. We identify objectbased regions with approximate, image-computable segmentation algorithms (bottom). These masks are carried through two stochastic data augmentations and a convolutional feature extractor, creating groups of feature vectors in each view (middle). The contrastive detection objective then pulls together pooled feature vectors from the same mask (across views) and pushes apart features from different masks and different images (top).</p><p>In addition, we compute for each image a set of masks which segment the image into different components. As described in Section 3.2, these masks can be computed using efficient, off-the-shelf, unsupervised segmentation algorithms. If available, human-annotated segmentations can also be used. In any case, we transform each mask (represented as a binary image) using the same cropping and resizing as used for the underlying RGB image, resulting in two sets of masks {m}, {m } which are aligned with the augmented images x, x (see <ref type="figure">Figure 2</ref>, augmented views).</p><p>Architecture. We use a convolutional feature extractor f to encode each image with a spatial map of hidden vectors: h = f (x) where h ? R H?W ?D . We use the output of a standard ResNet-50 encoder <ref type="bibr" target="#b26">[27]</ref>, before the final meanpooling layer, such that hiddens form a 7?7 grid of 2048dimensional vectors h[i, j]. For every mask m associated with the image, we compute a mask-pooled hidden vector</p><formula xml:id="formula_0">h m = 1 i,j m i,j i,j m i,j h[i, j],</formula><p>having spatially downsampled the binary mask to a 7?7 grid with average pooling. We then transform each of these vectors with a two-layer MLP, yielding non-linear projec-</p><formula xml:id="formula_1">tions z m = g(h m ) ? R d .</formula><p>For DetCon S we process both views with the same encoder f ? and projection network g ? , where ? are the learned parameters. For DetCon B one view is processed with f ? and g ? and the other with f ? and g ? , where ? is an exponential moving average of ?. The first view is further transformed with a prediction network q ? . Here again we reuse the details of SimCLR and BYOL for DetCon S and DetCon B respectively in the definition of the projection and prediction networks (see appendix A.2). In summary, we represent each view and mask as latents v m and v m where</p><formula xml:id="formula_2">v m = g ? (h m ), v m = g ? (h m ) for DetCon S and v m = q ? ? g ? (h m ), v m = g ? (h m )</formula><p>for DetCon B . We rescale all latents with a temperature hyperparameter ? , such that their norm is equal to 1/ ? ? , with ? = 0.1. Note that for downstream tasks, we retain only the feature extractor f ? and discard all other parts of the network (the prediction and projection heads, as well as any exponential moving averages).</p><p>Objective: contrastive detection. Let v m , v m be the latents representing masks m, m in the views x, x . The contrastive loss function</p><formula xml:id="formula_3">m,m = ? log exp(v m ?v m ) exp(v m ?v m ) + n exp(v m ?v n )<label>(1)</label></formula><p>defines a prediction task: having observed the projection v m , learn to recognize the latent v m in the presence of negative samples {v n }. We include negative samples from different masks in the image and different images in the batch. Note that we make no assumptions regarding these masks, allowing negative masks to overlap with the positive one. A natural extension of this loss would be to jointly sample paired masks m, m which correspond to the same region in the original image, and maximize the similarity of features representing them</p><formula xml:id="formula_4">L = E (m,m )?M m,m .<label>(2)</label></formula><p>We make a few practical changes to this objective. First, in order to facilitate batched computation we randomly sample at each iteration a set of 16 (possibly redundant) masks from the variable-sized sets of masks {m} and {m }. Second, we densely evaluate the similarity between all pairs of masks and all images, such that each image contributes 16 negative samples to the set {v n } in equation <ref type="formula" target="#formula_3">(1)</ref>, rather than a single one. We aim to makes these negatives as diverse as possible by choosing masks that roughly match different objects in the scene (Section 3.2). Finally, we mask out the loss to only maximize the similarity of paired locations, allowing us to handle cases where a mask is present in one  <ref type="bibr" target="#b1">[2]</ref>. 5 th column: "oracle" masks used to assess potential improvements from higher-quality segmentations.</p><p>view but not another (see <ref type="figure">Figure 2</ref>). Together, these simple modifications bring us to the DetCon objective:</p><formula xml:id="formula_5">L = m m 1 m,m m,m<label>(3)</label></formula><p>where the binary variable 1 m,m indicates whether the masks m, m correspond to the same underlying region.</p><p>Optimization. When pretraining on ImageNet we adopt the optimization details of SimCLR and BYOL for training DetCon S and DetCon B respectively. When pretraining on COCO we make minor changes to the learning schedule to alleviate overfitting (see appendix A.3).</p><p>Computational cost. The computational requirements of self-supervised learning are largely due to forward and backward passes through the convolutional backbone. For the typical ResNet-50 architecture applied to 224?224resolution images, a single forward pass requires approximately 4B FLOPS. The additional projection head in Sim-CLR and DetCon S requires an additional 4M FLOPS. Since we forward 16 hidden vectors through the projection head instead of 1, we increase the computational cost of the forward pass by 67M FLOPS, less than 2% of total. Together with the added complexity of the contrastive loss, this increase is 5.3% for DetCon S and 11.6% for DetCon B (see appendix A.2). Finally, the cost of computing image segmentations is negligible because they can be computed once and reused throughout training. Therefore the increase in complexity of our method relative to the baseline is sufficiently small for us to interchangeably refer to "training iterations" and "computational cost".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unsupervised mask generation</head><p>To produce masks required by the DetCon objective, we investigate several segmentation procedures, from simple spatial heuristics to graph-based algorithms from the literature.</p><p>Spatial heuristic. The simplest segmentation we consider groups locations based on their spatial proximity only. Specifically, we divide the image into an n ? n grid of nonoverlapping, square sub-regions ( <ref type="figure" target="#fig_2">Figure 3</ref>, 2 nd column). Note that when using a single, global mask (n = 1), the DetCon S objective reverts to SimCLR.</p><p>Image-computable masks: FH. We also consider the Felzenszwalb-Huttenlocher algorithm <ref type="bibr" target="#b16">[17]</ref>, a classic segmentation procedure which iteratively merges regions using pixel-based affinity <ref type="figure">(</ref> Image-computable masks: MCG. Multiscale Combinatorial Grouping <ref type="bibr" target="#b1">[2]</ref> is a more sophisticated algorithm which groups superpixels into many overlapping object proposal regions <ref type="bibr" target="#b7">[8]</ref>, guided by mid-level classifiers <ref type="figure" target="#fig_2">(Figure 3</ref>, 4 th column). For each image we use 16 MCG masks with the highest scores. Note that the fact that these masks can overlap is supported by our formulation.</p><p>Human annotated masks. Throughout this work we consider the benefits afforded by the use of the unsupervised masks detailed above. In the final section, we ask whether higher quality masks (provided by human annotators; <ref type="figure">Fig</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation protocol</head><p>Having trained a feature extractor in an unsupervised manner, we evaluate the quality of the representation by finetuning it for object detection and instance segmentation on COCO, segmentatic segmentation on PASCAL and Cityscapes, and depth estimation on NYU v2.</p><p>Object detection and instance segmentation. We use the pretrained network to initialize the feature extractor of a Mask-RCNN <ref type="bibr" target="#b25">[26]</ref> equipped with feature pyramid networks <ref type="bibr" target="#b37">[38]</ref> and cross-replica batch-norm <ref type="bibr" target="#b47">[47]</ref>. We adopt the Cloud TPU implementation 1 and use it without modification. We fine-tune the entire model on the COCO train2017 set, and report bounding-box AP (AP bb ) and mask AP (AP mk ) on the val2017 set. We use two standard training schedules: 12 epochs and 24 epochs <ref type="bibr" target="#b23">[24]</ref>.</p><p>Semantic segmentation. Following <ref type="bibr" target="#b23">[24]</ref> we use our network to initialize the backbone of a fully-convolutional network <ref type="bibr" target="#b38">[39]</ref>. For PASCAL, we fine-tune on the train aug2012 set for 45 epochs and report the mean intersection over union (mIoU) on the val2012 set. For Cityscapes, we finetune on the train fine set for 160 epochs and evaluate on the val fine set.</p><p>Depth estimation. Following <ref type="bibr" target="#b20">[21]</ref> we stack the deconvolu-1 https://github.com/tensorflow/tpu/tree/master/ models/official/detection tional network from <ref type="bibr" target="#b34">[35]</ref> on top of our feature extractor, and fine-tune on the NYU v2 dataset. We report accuracy as the percentage of errors below 1.25 (pct&lt;1.25).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our main self-supervised learning experiments employ FH masks, because as we will show, with DetCon they outperform simple spatial heuristics and approach the performance of MCG masks while being fast and easy to apply to large datasets such as ImageNet, given their availability in scikit-image <ref type="bibr" target="#b57">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Transfer learning from ImageNet</head><p>We first study whether the DetCon objective improves the pretraining efficiency of transfer learning from ImageNet.</p><p>Pretraining efficiency. We train SimCLR and DetCon S models on ImageNet for 100, 200, 500 and 1000 epochs, and transfer them to several datasets and tasks. Across all downstream tasks and pretraining regimes, DetCon S substantially outperforms SimCLR <ref type="figure" target="#fig_1">(Figures 1 and 4</ref>, blue and orange curves). When fine-tuning on COCO, the performance afforded by 1,000 epochs of SimCLR pretraining is surpassed by only 200 epochs of DetCon S pretraining (i.e. a 5? gain in pretraining efficiency). We found similar results when transferring to other downstream tasks: DetCon S yields a 2? gain in pretraining efficiency for PAS- CAL semantic segmentation <ref type="figure">(Figure 4</ref>, 2 nd column), and 10? gains for Cityscapes semantic segmentation and NYU depth prediction. <ref type="figure">(Figure 4</ref>, 3 rd , and 4 th columns). We also evaluated the transfer performance of a supervised ResNet-50 trained on ImageNet <ref type="figure" target="#fig_1">(Figures 1 and 4</ref>, red curve). While supervised pretraining performs well with small computational budgets (e.g. 100 pretraining epochs), it quickly saturates, indicating that ImageNet labels only partially inform downstream tasks. This is emphasized for Cityscapes semantic segmentation and NYU depth prediction, which represent a larger shift in the domain and task.</p><p>From BYOL to DetCon B . How general is DetCon? We tested this by comparing DetCon B to the BYOL framework, upon which it is based. We adopt the underlying framework details (e.g. data-augmentation, architecture, and optimization) without modification, possibly putting the Det-Con objective at a disadvantage. Despite this, DetCon B outperforms BYOL across pretaining budgets and downstream tasks. In particular, DetCon B yields a 3? gain in pretraining efficiency when transferring to COCO, PASCAL, and Cityscapes detection and segmentation, and a 10? gain when transferring to NYU depth prediction ( <ref type="table" target="#tab_1">Table 1)</ref>.</p><p>Comparison with prior art. We now compare to other works in self-supervised transfer learning, and use fullytrained DetCon S and DetCon B models for the comparison.</p><p>Here we focus on transfer to COCO as it is more widely studied. Note that other methods use a slightly different implementation of the Mask-RCNN <ref type="bibr" target="#b61">[61]</ref> however their results for supervised ImageNet pretraining and SimCLR match our own <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b63">63]</ref>, enabling a fair comparison. <ref type="table" target="#tab_2">Table 2</ref> shows that DetCon outperforms all other methods for supervised and self-supervised transfer learning.</p><p>Scaling model capacity. Prior works in self-supervised learning have been shown to scale very well with model capacity <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b8">9]</ref>. Could the gains afforded by DetCon  We went a step further and trained a ResNet-200 with a 2? width multiplier <ref type="bibr" target="#b32">[33]</ref>, containing 250M parameters. Surprisingly, despite only being trained on Ima-geNet, this model's transfer performance matches that of a very recently proposed large-scale self-supervised model, SEER <ref type="bibr" target="#b19">[20]</ref>, having 693M parameters and trained on 1000? more data <ref type="table" target="#tab_3">(Table 3)</ref>. While the comparison is imperfect (large-scale data is necessarily more noisy), it highlights the potential of improvements from the self-supervised learning objective alone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transfer learning from COCO</head><p>We next investigate the ability of the DetCon objective to handle complex scenes with multiple objects. For this we pretrain on the COCO dataset and compare to SimCLR.</p><p>Training efficiency. We train SimCLR and DetCon S for a range of schedules (324-5184 epochs), and transfer all models to semantic segmentation on PASCAL. We find DetCon S to outperform SimCLR across training budgets ( <ref type="figure">Figure 6</ref>). As before, the maximum accuracy attained by SimCLR is reached with 4? less pretraining time.</p><p>Surpassing supervised transfer from COCO. We also evaluated the transfer performance of representations trained on COCO in a supervised manner. Specifically, we trained a Mask-RCNN with a long schedule (108 epochs, a "9?" schedule), and use the learned feature extractor (a ResNet-50, as for SimCLR and DetCon pretraining) as a representation for PASCAL segmentation. Unlike SimCLR, DetCon pretraining surpasses the performance of this fully supervised baseline ( <ref type="figure">Figure 6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations and analysis</head><p>We now dissect the components of the DetCon objective and assess the benefits of each. For this we pretrain on COCO as it contains complex scenes with many objects and associated ground-truth masks, allowing us to measure the impact of segmenting them accurately. We evaluate learned representations with a frozen-feature analysis, in which the feature extractor is kept fixed while we train the other layers of a Mask-RCNN also on COCO. This controlled setting is analogous 2 to the linear classification protocol used to evaluate the quality of self-supervised representations for image recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b65">65]</ref> What makes good masks? The DetCon objective can be used with a variety of different image segmentations, which ones lead to the best representation? We first consider spatial heuristics which partition the image into 2?2, 5?5, or 10?10 grids, a 1?1 grid being equivalent to using the Sim-CLR objective. We find downstream performance increases with finer grids, a 5?5 grid being optimal <ref type="figure">(Figure 7</ref>). Next we consider image-computable FH and MCG masks, both of which outperform the spatial heuristic masks, MCG masks leading to slightly better representations. Interestingly, the quality of the representation corre-  <ref type="figure">Figure 7</ref>. Effect of type of masks used in DetCon objective. We train DetCon models on COCO using unsupervised masks (blue), or the ground-truth COCO masks (grey). Using a single, global mask (i.e. a "1?1" grid) is equivalent to SimCLR (orange). We compute the Average Best Overlap (ABO) by measuring the IoU between each ground-truth mask and the closest pretraining mask, and averaging over all ground truth instances and images (x-axis). We evaluate the accuracy of each model on COCO detection using the frozen-feature paradigm (y-axis).</p><p>lates very well with the overlap between pretraining masks and ground-truth-the better each ground truth object is covered by some mask, the better DetCon performs.</p><p>Contrastive detection vs contrastive recognition. How does the DetCon objective benefit from these image segmentations? We assess the impact of each of its components by incrementally adding them to the SimCLR framework. As mentioned previously, we recover SimCLR when using a single, global mask in the DetCon objective. As a sanity check, we verify that duplicating this mask several times and including the resulting (identical) features in the Det-Con objective makes no difference in the quality of the representation <ref type="table" target="#tab_3">(Table 4</ref>, row a). Interestingly, using FH masks but only sampling a single mask per image slightly deteriorates performance, presumably because the model only learns from part of the image at every iteration ( <ref type="table" target="#tab_3">Table 4</ref>, row b). By densely sampling object regions DetCon S learns from the entire image, while also benefiting from a diverse set of positive and negative samples, resulting in increased detection and segmentation accuracy ( Pretraining image resolution <ref type="figure">Figure 8</ref>.</p><p>Better segmentations benefit from higher resolutions. We pretrain backbone networks on COCO using SimCLR and DetConS (with FH or GT masks) at various resolutions. We report frozen-feature performance with a fixed resolution of 1024?1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">What if segmentation were solved?</head><p>The DetCon objective function leads to fast transfer learning and strong performance despite using fairly approximate segmentation masks. In Section 4.3 we found higher quality segmentations (such as those computed using MCG, or obtained from human annotators) to improve representational quality. How might we improve the learning objective given more accurate segmentations? We assessed this question by revisiting our design choices for the contrastive objective, when given ground-truth masks from the COCO dataset as opposed to the approximate FH masks.</p><p>Scaling image resolution. We hypothesized that higher image resolutions might enable the network to benefit more from these more informative segmentations. To preserve fine-grained information we sample local features from within each mask and optimize them using the Det-Con objective. We pretrain SimCLR and DetCon S models equipped with FH or ground-truth (GT) masks, given 384?384 or 512?512 resolution images. While DetCon with FH masks only modestly benefited and SimCLR's performance deteriorated with high-resolution images, DetCon with GT masks improves substantially <ref type="figure">(Figure 8</ref>). Note that this is solely due to an improved representation quality; the image resolution used for downstream evaluation is maintained at 1024?1024 for all models.</p><p>Revisiting the contrastive framework. Finally, we asked whether the current contrastive learning paradigm-which utilizes large numbers of negatives and predictions across stochastic augmentations-remains optimal in the context of the DetCon objective with high-quality segmentations.</p><p>Are large numbers of negative samples necessary? Not with high-quality masks. When dividing the total number of negative samples by 128 (by only gathering negatives from within a worker) the performance of DetCon with FH masks drops ( <ref type="table" target="#tab_3">Table 5</ref>, row a), consistently with other contrastive learning frameworks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>. In contrast, DetCon S using GT masks improves despite this limitation.</p><p>Are positive pairs sampled across augmented views necessary? Not with high-quality masks. We run DetCon  <ref type="table" target="#tab_3">Table 5</ref>. Simplifying the contrastive framework. We train DetConS models on COCO using approximate FH masks or higher-quality ground-truth (GT) masks, and evaluate them in the frozen-feature setting. "all neg": Negative samples are collected from the entire batch as opposed to only within a worker (out of 128 workers). "two views": Contrastive predictions are made across augmentations, as opposed to within a view. models while sampling a single augmentation for each image and maximizing the similarity of mask-based features within this view. Here again, the DetCon objective suffers from this handicap when using approximate FH masks, but not with high-quality segmentations ( <ref type="table" target="#tab_3">Table 5</ref>, row b). How can this be? One interpretation is that other images give us clean negative examples because images in COCO depict different scenes. However it appears that negatives from the same image provide a stronger learning signal (in that they share features such as lighting, background, etc) as long as they are clean-i.e., we are not pushing features from the same object apart. Positives from the same image are also at least as good as those across augmentations if again they are clean-i.e., we are not pulling together features from different objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We have proposed DetCon, a simple but powerful selfsupervised learning algorithm. By exploiting low-level cues for organizing images into entities such as objects and background regions, DetCon accelerates pretraining on large datasets while improving accuracy on a variety of downstream tasks. Our best models achieve state-of-the-art performance among self-supervised methods pretrained on Im-ageNet and match recent state-of-the-art methods training larger models on a much larger dataset <ref type="bibr" target="#b19">[20]</ref>.</p><p>We showed that the power of DetCon strongly correlates with how well the masks used align with object boundaries. This seems intuitive-the DetCon objective can only leverage independent learning signals from each image region if they contain distinct content. Similarly, the resulting negative samples are genuinely diverse only if they represent different objects. This opens exciting prospects of research in jointly discovering objects and learning to represent them. Given the improved performance of DetCon representations for instance segmentation, a natural question is whether they could be used to perform better unsupervised segmentations than the ones used during pretraining. If so, these might be used to learn better representations still, leading to a virtuous crescendo of unsupervised scene understanding.</p><p>1. random cropping: a random patch of the image is selected, whose area is uniformly sampled in [0.08 ? A, A], where A is the area of the original image, and whose aspect ratio is logarithmically sampled in The augmented images x, x result from augmentations sampled from distributions T and T respectively. These distributions apply the primitives described above with different probabilities, and different magnitudes. The following table specifies these parameters for the SimCLR <ref type="bibr" target="#b8">[9]</ref> and BYOL frameworks <ref type="bibr" target="#b20">[21]</ref>, which we adopt for DetCon S and DetCon B without modification. Transfer to NYU-Depth v2. The original 640?480 frames are down-sampled by a factor of 2 and center-cropped to 304?228 pixels. For training, images are randomly flipped horizontally and color jittered with the same grayscale, brightness, saturation, and hue settings as <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation: architecture</head><p>Our default feature extractor is a ResNet-50 <ref type="bibr" target="#b26">[27]</ref>. In Section 4.1 we also investigate deeper architectures (ResNet-101, -152, and -200), and a wider model (ResNet-200 ?2) obtained by scaling all channel dimensions by a factor of 2. As detailed in Section 3.1, this encoder yields a grid of hidden vectors which we pool within masks to obtain a set of vectors h m representing each mask. These are then transformed by a projection head g (and optionally a prediction head q) before entering the contrastive loss.</p><p>DetCon S . Following SimCLR, the projection head is a twolayer MLP whose hidden and output dimensions are 2048 and 128. The network uses the learned parameters ? for both views.</p><p>DetCon B . Following BYOL, the projection head is a twolayer MLP whose hidden and output dimensions are 4096 and 256. The network uses the learned parameters ? for processing one view, and an exponential moving average of these parameters ? for processing the second. Specifically, ? is updated using ? ? ? ? ? + (1 ? ?) ? ?, where the decay rate ? is annealed over the course of training from ? 0 to 1 using a cosine schedule <ref type="bibr" target="#b20">[21]</ref>. ? 0 is set to 0.996 when training for 1000 epochs and 0.99 when training for 300 epochs. The projection of the first view is further transformed with a prediction head, whose architecture is identical to that of the projection head.</p><p>Computational cost. The forward pass through a ResNet-50 encoder requires roughly 4B FLOPS. Ignoring the cost of bias terms and point-wise nonlinearities, the projection head in DetCon S requires 4.4M FLOPS (i.e. 2048?2048 + 2048?128). Since this is calculated 16 times rather than once, it results in an overhead of 67M FLOPS compared to SimCLR. For DetCon B the combined cost of evaluating the projection and prediction heads results in an additional 173M FLOPS compared to BYOL. Finally, the cost of evaluating the contrastive loss is 134M FLOPS for DetCon S (i.e. 128?4096?16 2 ) and 268M FLOPS for DetCon B . In total DetCon S requires 201M additional FLOPS and DetCon B 441M which represent 5.3% and 11.6% of the cost of evaluating the backbone. This overhead is sufficiently small compared to the gain in training iterations required to reach a given transfer performance (e.g. a 500% gain for DetCon S over SimCLR, and a 333% for DetCon B over DetCon) for us not further distinguish between gains in computation and training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Implementation: optimization</head><p>Self-supervised pretraining. We train using the LARS optimizer <ref type="bibr" target="#b64">[64]</ref> with a batch size of 4096 split across 128 Cloud TPU v3 workers. When training on ImageNet we again adopt the optimization details of SimCLR and BYOL for DetCon S and DetCon B , scaling the learning rate linearly with the batch size and decaying it according to a cosine schedule. For DetCon S the base learning rate is 0.3 and the weight decay is 10 ?6 . DetCon B also uses these values when training for 300 epochs; when training for 1000 epochs they are 0.2 and 1.5 ? 10 ?6 .</p><p>When pretraining on COCO, we replace the cosine learning rate schedule with a piecewise constant, which has been found to alleviate overfitting <ref type="bibr" target="#b24">[25]</ref>, dropping the learning rate by a factor of 10 at the 96 th and 98 th percentiles. For fair comparison we use the same schedules when applying Sim-CLR to the COCO dataset, which we also find to perform better than the more aggressive cosine schedule.</p><p>Transfer to COCO. We fine-tune with stochastic gradient descent, increasing the learning rate linearly for the first 500 iterations and dropping twice by a factor of 10, after 2 3 and 8 9 of the total training time, following <ref type="bibr" target="#b61">[61]</ref>. We use a base learning rate of 0.3 for ResNet-50 models and 0.2 for larger ones, a momentum of 0.9, a weight decay of 4?10 ?5 , and a batch size of 64 images split across 16 workers.</p><p>Transfer to PASCAL. We fine-tune for 45 epochs with stochastic gradient descent, with a batch size of 16 and weight decay of 10 ?4 . The learning rate is 0.02 and dropped by a factor of 10 at the 70 th and 90 th percentiles.</p><p>Transfer to Cityscapes. We fine-tune for 160 epochs with stochastic gradient descent and a Nesterov momentum of 0.9, using a batch size of 2 and weight decay of 10 ?4 . The initial learning rate is 0.005 and dropped by a factor of 10 at the 70 th and 90 th percentiles.</p><p>Transfer to NYU-Depth v2. We fine-tune for 7500 steps with a batch size of 256, weight decay of 5?10 ?4 , and a learning rate of 0.16 scaled linearly with the batch size <ref type="bibr" target="#b20">[21]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Results: larger models</head><p>In <ref type="table" target="#tab_2">Table 2</ref> we compare to prior works on self-supervised learning which transfer to COCO. Here we provide additional comparisons which use larger models (ResNet-101, -152, and -200). We find DetCon to continue to outperform prior work in this higher capacity regime <ref type="table" target="#tab_1">(Table A.1)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Efficient self-supervised pretraining with DetCon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 ,</head><label>3</label><figDesc>3 rd column). We generate a diverse set of masks by varying two hyperparameters, the scale s and minimum cluster size c, using s ? {500, 1000, 1500} and c = s when training on COCO and s = 1000 when training on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>ure 3, 5 th column) can improve our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>Figure 4. Efficient ImageNet pretraining with DetConS. We pretrain networks with SimCLR, DetConS, or supervised learning on ImageNet for different numbers of epochs, and fine-tune them for COCO detection and instance segmentation (for 12 epochs), semantic segmentation on PASCAL or Cityscapes, or depth estimation on NYU v2. DetConS outperforms SimCLR, with up to 10? less pretraining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Scaling DetCon to larger models. We pretrain ResNet-50, ResNet-101, ResNet-152, and ResNet-200 feature extractors on ImageNet using supervised learning, SimCLR, BYOL, or DetConB and fine-tune them on COCO for 12 epochs. disappear with larger models? We trained SimCLR, BYOL, and DetCon B models on ImageNet, using ResNet-101, -152, and -200 feature extractors instead of ResNet-50. Figure 5 and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>[3/4, 4/3]. The patch is then resized to 224 ?224 pixels using bicubic interpolation;2. horizontal flipping; 3. color jittering: the brightness, contrast, saturation and hue are shifted by a uniformly distributed offset; 4. color dropping: the RGB image is replaced by its greyscale values; 5. gaussian blurring with a 23?23 square kernel and a standard deviation uniformly sampled from [0.1, 2.0]; 6. solarization: a point-wise color transformation x ? x ? 1 x&lt;0.5 + (1 ? x) ? 1 x?0.5 with pixels x in [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc><ref type="bibr" target="#b3">4</ref>. Efficient ImageNet pretraining with DetConS. We pretrain networks with SimCLR, DetConS, or supervised learning on ImageNet for different numbers of epochs, and fine-tune them for COCO detection and instance segmentation (for 12 epochs), semantic segmentation on PASCAL or Cityscapes, or depth estimation on NYU v2. DetConS outperforms SimCLR, with up to 10? less pretraining. Efficient ImageNet pretraining with DetConB. We pretrain networks on ImageNet with BYOL or DetConB, and fine-tune them for COCO detection and instance segmentation (for 12 epochs), semantic segmentation on PASCAL or Cityscapes, or depth estimation on NYU v2. DetConB outperforms BYOL, with up to 10? less pretraining (colors highlight gains in pretraining efficiency).</figDesc><table><row><cell></cell><cell cols="2">Detection</cell><cell cols="2">Instance Segmentation</cell><cell cols="3">Semantic Segmentation</cell><cell></cell><cell cols="2">Depth Estimation</cell></row><row><cell></cell><cell cols="2">COCO</cell><cell></cell><cell>COCO</cell><cell cols="2">PASCAL</cell><cell cols="2">Cityscapes</cell><cell cols="2">NYU v2</cell></row><row><cell>Pretrain epochs</cell><cell>300</cell><cell>1000</cell><cell>300</cell><cell>1000</cell><cell>300</cell><cell>1000</cell><cell>300</cell><cell>1000</cell><cell>100</cell><cell>1000</cell></row><row><cell>BYOL</cell><cell>41.2</cell><cell>41.6</cell><cell>37.1</cell><cell>37.2</cell><cell>74.7</cell><cell>75.7</cell><cell>73.4</cell><cell>74.6</cell><cell>83.7</cell><cell>84.2</cell></row><row><cell>DetConB</cell><cell>42.0</cell><cell>42.7</cell><cell>37.8</cell><cell>38.2</cell><cell>75.6</cell><cell>77.3</cell><cell>75.1</cell><cell>77.0</cell><cell>85.1</cell><cell>86.3</cell></row><row><cell>Efficiency Gain</cell><cell>&gt; 3?</cell><cell></cell><cell></cell><cell>&gt; 3?</cell><cell>? 3?</cell><cell></cell><cell>&gt; 3?</cell><cell></cell><cell cols="2">&gt; 10?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison to prior art: all methods are pretrained on ImageNet then fined-tuned on COCO for 12 epochs (1? schedule) or 24 epochs (2? schedule).</figDesc><table><row><cell></cell><cell cols="2">Fine-tune 1?</cell><cell cols="2">Fine-tune 2?</cell></row><row><cell>method</cell><cell>AP bb</cell><cell>AP mk</cell><cell>AP bb</cell><cell>AP mk</cell></row><row><cell>Supervised</cell><cell>39.6</cell><cell>35.6</cell><cell>41.6</cell><cell>37.6</cell></row><row><cell>VADeR [48]</cell><cell>39.2</cell><cell>35.6</cell><cell>-</cell><cell>-</cell></row><row><cell>MoCo [24]</cell><cell>39.4</cell><cell>35.6</cell><cell>41.7</cell><cell>37.5</cell></row><row><cell>SimCLR [9]</cell><cell>39.7</cell><cell>35.8</cell><cell>41.6</cell><cell>37.4</cell></row><row><cell>MoCo v2 [11]</cell><cell>40.1</cell><cell>36.3</cell><cell>41.7</cell><cell>37.6</cell></row><row><cell>InfoMin [54]</cell><cell>40.6</cell><cell>36.7</cell><cell>42.5</cell><cell>38.4</cell></row><row><cell>PixPro [63]</cell><cell>41.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BYOL [21]</cell><cell>41.6</cell><cell>37.2</cell><cell>42.4</cell><cell>38.0</cell></row><row><cell>SwAV [7]</cell><cell>41.6</cell><cell>37.8</cell><cell>-</cell><cell>-</cell></row><row><cell>DetConS</cell><cell>41.8</cell><cell>37.4</cell><cell>42.9</cell><cell>38.1</cell></row><row><cell>DetConB</cell><cell>42.7</cell><cell>38.2</cell><cell>43.4</cell><cell>38.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table A</head><label>A</label><figDesc>.1 show that DetCon continues to outperform other methods in this higher-capacity regime.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 ,Table 4</head><label>44</label><figDesc>final row).</figDesc><table><row><cell>model</cell><cell>masks</cell><cell>#latents</cell><cell>AP bb</cell><cell>AP mk</cell></row><row><cell>SimCLR</cell><cell>global</cell><cell>1</cell><cell>31.6</cell><cell>29.2</cell></row><row><cell>(a)</cell><cell>global</cell><cell>16</cell><cell>31.5</cell><cell>29.3</cell></row><row><cell>(b)</cell><cell>FH</cell><cell>1</cell><cell>31.2</cell><cell>28.8</cell></row><row><cell>DetConS</cell><cell>FH</cell><cell>16</cell><cell>33.4</cell><cell>30.6</cell></row></table><note>. Ablation: from SimCLR to DetConS. We pretrain on COCO and evaluate frozen feature accuracy also on COCO.masks: specifies whether hidden vectors are pooled globally, or within individual FH masks. #latents: number of masks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Transfer to COCO. When fine-tuning, image are randomly flipped and resized to a resolution of u ? 1024 pixels on the longest side, where u is uniformly sampled in [0.8, 1.25], then cropped or padded to a 1024?1024 image. The aspect ratio is kept the same as the original image. During testing, images are resized to 1024 pixels on the longest side then padded to 1024?1024 pixels.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Transfer to PASCAL. During training, images are ran-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>domly flipped and scaled by a factor in [0.5, 2.0]. Training</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>and testing are performed with 513?513-resolution images.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Transfer to Cityscapes. During training, images are</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>randomly horizontally flipped and scaled by a factor in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[0.5, 2.0], with minimum step size 0.25 within that range.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Training is performed on 769?769-resolution images and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>testing is performed on 1025?2049-resolution images.</cell></row><row><cell></cell><cell cols="2">DetConS</cell><cell cols="2">DetConB</cell></row><row><cell>Parameter</cell><cell>T</cell><cell>T</cell><cell>T</cell><cell>T</cell></row><row><cell>Random crop probability</cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell>Flip probability</cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell></row><row><cell>Color jittering probability</cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>Color dropping probability</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell></row><row><cell>Brightness adjustment max</cell><cell>0.8</cell><cell></cell><cell>0.4</cell><cell></cell></row><row><cell>Contrast adjustment max</cell><cell>0.8</cell><cell></cell><cell>0.4</cell><cell></cell></row><row><cell>Saturation adjustment max</cell><cell>0.8</cell><cell></cell><cell>0.2</cell><cell></cell></row><row><cell>Hue adjustment max</cell><cell>0.2</cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell cols="5">Gaussian blurring probability 1.0 0.0 1.0 0.1</cell></row><row><cell>Solarization probability</cell><cell cols="4">0.0 0.0 0.0 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A .</head><label>A</label><figDesc>1. Comparison to prior art: all methods are pretrained on ImageNet then fined-tuned on COCO for 12 epochs (1? schedule) or 24 epochs (2? schedule).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">But note that the Mask-RCNN contains several non-linear layers due to the additional complexity of the output space relative to classification.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors are grateful to Carl Doersch, Raia Hadsell, and Evan Shelhamer for insightful discussions and feedback on the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Appendix A.1. Implementation: data augmentation Self-supervised pretraining. Each image is randomly augmented twice, resulting in two images: x, x . The augmentations are constructed as compositions of the following operations, each applied with a given probability:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<title level="m">Learning representations by maximizing mutual information across views</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Leveraging large-scale uncurated data for unsupervised pre-training of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Leveraging Large-Scale Uncurated Data for Unsupervised Pre-training of Visual Features</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cpmc: Automatic object segmentation using constrained parametric min-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1312" to="1328" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A simple framework for contrastive learning of visual representations</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of visual features in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01988</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image denoising using mixtures of gaussian scale mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jose A Guerrero-Col?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Portilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="565" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4918" to="4927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sm Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<title level="m">Data-efficient image recognition with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14613</idno>
		<title level="m">Space-time correspondence as a contrastive random walk</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Principles of Gestalt psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Koffka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">44</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Routledge</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Revisiting self-supervised visual representation learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
	<note>Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning of edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Perceptual organization and visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision. ICCV</title>
		<meeting>Eighth IEEE International Conference on Computer Vision. ICCV</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>T Nathan Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9339" to="9348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06370</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6181" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unsupervised learning of dense visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Pedro O Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Benmaleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.05499</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">High-level vision: Object recognition and visual cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Emmanuelle Gouillart, Tony Yu, and the scikit-image contributors. scikit-image: image processing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>St?fan Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Boulogne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Unsupervised semantic segmentation by contrasting object mask proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06191</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10043</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1058" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Self-supervised visual representation learning from hierarchical grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03044</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
