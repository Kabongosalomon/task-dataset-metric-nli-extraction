<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinda</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Virtual Reality Technology and Systems</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fine-grained image recognition</term>
					<term>food recognition</term>
					<term>knowledge guidance</term>
					<term>peak suppression</term>
					<term>vision transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained image recognition is challenging because discriminative clues are usually fragmented, whether from a single image or multiple images. Despite their significant improvements, the majority of existing methods still focus on the most discriminative parts from a single image, ignoring informative details in other regions and lacking consideration of clues from other associated images. In this paper, we analyze the difficulties of fine-grained image recognition from a new perspective and propose a transformer architecture with the peak suppression module and knowledge guidance module, which respects the diversification of discriminative features in a single image and the aggregation of discriminative clues among multiple images. Specifically, the peak suppression module first utilizes a linear projection to convert the input image into sequential tokens. It then blocks the token based on the attention response generated by the transformer encoder. This module penalizes the attention to the most discriminative parts in the feature learning process, therefore, enhancing the information exploitation of the neglected regions. The knowledge guidance module compares the imagebased representation generated from the peak suppression module with the learnable knowledge embedding set to obtain the knowledge response coefficients. Afterwards, it formalizes the knowledge learning as a classification problem using response coefficients as the classification scores. Knowledge embeddings and image-based representations are updated during training simultaneously so that the knowledge embedding includes a large number of discriminative clues for different images of the same category. Finally, we incorporate the acquired knowledge embeddings into the image-based representations as comprehensive representations, leading to significantly higher recognition performance. Extensive evaluations on the six popular datasets demonstrate the advantage of the proposed method in performance. The source code and models will be available online after the acceptance of the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Aiming to distinguish the objects belonging to multiple sub-categories of the same meta-category, fine-grained image recognition has been one of the most fundamental problems in the computer vision and multimedia communities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. It is essential for a wide range of downstream applications such as rich image captioning <ref type="bibr" target="#b5">[6]</ref>, image generation <ref type="bibr" target="#b6">[7]</ref>, machine teaching <ref type="bibr" target="#b7">[8]</ref>, fine-grained image retrieval <ref type="bibr" target="#b8">[9]</ref>, food recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, and food recommendation <ref type="bibr" target="#b11">[12]</ref>.</p><p>Fine-grained image recognition is challenging due to subtle inter-class differences and significant intra-class variances. Most existing methods only consider the problem of fine-grained recognition from the perspective of obtaining the discriminative characteristics of a single image but ignore the clues provided by multiple images. In order to take the clues of multiple images into consideration, we try to explain the difficulty of fine-grained image recogni- * Corresponding author tion with a new perspective and attribute it to fragmented discriminative clues.</p><p>The fragmentation here has two implications: (1) From the perspective of a single image, discriminative clues appear in different local areas since the inter-class differences could be subtle; As shown in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref> and (b), the long-tailed jaeger is similar to the black tern overall, but the beak of long-tailed jaeger is curved, and the beak of the black tern is straight. This kind of discriminative part is usually tiny and distributed in different image regions, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (c). <ref type="bibr" target="#b1">(2)</ref> From the perspective of multiple images, each image contains only a part of the discriminative information about the category depending on different poses, scales, and rotations, due to significant intra-class variances. As shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref> and (c), these two pictures are long-tailed jaeger, the beak of the bird in (c) is difficult to distinguish, and there are no bird claws in <ref type="bibr">(b)</ref>. Therefore, the discriminative information contained in each image is incomplete.</p><p>To find and aggregate fragmented clues is the key to fine-grained image recognition. Despite their impressive results, existing methods usually consider fine-grained image recognition only from few regions, ignoring many informative details in other regions and other associated images. For instance, if the beak of a specific bird is very different from other birds, the model may pay much attention to the beak of the bird while ignoring the claws and tail of the bird. When this happens, the model could easily make mistakes when the beak of the bird is not visible.</p><p>Given this challenge, we propose a Transformer with Peak Suppression and Knowledge Guidance (TPSKG) for fine-grained image recognition. The proposed Peak Suppression (PS) module uses the transformer architecture to integrate the local information and explores a training routine to increase the diversity of discriminative features. This PS module is designed to obtain as many discriminative clues as possible from a single image. Simultaneously the proposed Knowledge Guidance (KG) module incorporates the learnable knowledge embedding into the imagebased representation for a comprehensive representation. This KG module is used to aggregate discriminative information from multiple images.</p><p>Specifically, we are inspired by ViT <ref type="bibr" target="#b12">[13]</ref> and use a transformer architecture to tackle the fine-grained image recognition problems. The input image is reshaped into a patch sequence without overlap and then linearly mapped to the sequential tokens. The transformer encoder uses the selfattention mechanism to integrate the information of the different tokens to obtain a global representation. Instead of integrating all the token information like the original ViT, we deliberately remove the most discriminative token based on the value of the attention weight map in training to penalize strongly discriminative learning and enforce the network to pay attention to other neglected informative areas for keeping the fine-grained representation diversity.</p><p>After that, we use a knowledge embedding set to explicitly express the discriminative clues of the same category from different images and formalize the learning of knowledge embedding as a classification task. The knowledge guidance module measures the similarity of the knowledge embeddings and image-based representations generated from the peak suppression module to obtain the knowledge response coefficients. We use the knowledge response coefficients as the classification scores directly and use the category label as ground truth to supervise the knowledge learning. The image-based representations become more discriminative through the joint training of finegrained classification and knowledge learning tasks, and the knowledge embeddings are also concurrently updated through iterations. The learning procedure of knowledge embeddings covers the entire training dataset, therefore these embeddings become the comprehensive representations containing various subtle and slight characteristics of all categories. Finally, we obtain the knowledge-based representations computed from the knowledge embedding set along with the knowledge response coefficients, and inject them into the image-based representations. The proposed knowledge embedding learning and exploitation lead to a significant boost for recognition performance.</p><p>To verify the effectiveness of our method, we conduct extensive experiments on the six popular benchmarks for the fine-grained image recognition task. Quantitative experimental results demonstrate that the proposed method can achieve competitive performance compared to the stateof-the-art approaches. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, qualitative experimental results demonstrate the advantages of our method in covering more informative areas and increasing the diversity of expression at the same time. The quantitative analysis and visualization of knowledge embedding also illustrate the effectiveness of category-related knowledge embedding learning.</p><p>In summary, we make the following main contributions: <ref type="bibr" target="#b0">(1)</ref> We provide a new perspective that the difficulty of fine-grained recognition lies in fragmented discriminative clues. This perspective helps consider not only multiple regions from a single image but also multiple images.</p><p>(2) We propose a vision transformer architecture with peak suppression and knowledge guidance for the finegrained image recognition task. Peak suppression effectively increases the diversity of image representations via aggregating the local features from multiple regions from a single image. Knowledge guidance optimizes the final representations with the knowledge embeddings learning from multiple images.</p><p>(3) We formalize the knowledge learning as a classification problem and directly use the similarity between knowledge embeddings and image-based representations as the classification score to update the knowledge embeddings related to the category.</p><p>(4) We conduct extensive quantitative and qualitative experiments to demonstrate the effectiveness of the proposed method, which achieves competitive performance compared to the state-of-the-art approaches on six public datasets.</p><p>The rest of this paper is organized as follows. Section 2 reviews the related works. Section 3 elaborates on the proposed framework. Experimental results and analysis are reported in Section 4. Finally, we conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>This section introduces the most related researches into the following categories: the fine-grained image recognition task and the vision transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fine-grained Image Recognition</head><p>There are two prevailing paradigms in the current research in fine-grained image recognition. One is the local identification, and the other is the global discrimination.</p><p>Local-identification approaches focus on locating the discriminative semantic parts of fine-grained objects to identify the subtle differences among different object categories and construct mid-level representations corresponding to these parts for the final classification. Early works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> used strong supervised mechanisms with part bounding box annotations to learn localizing the discriminative parts. However, the part annotation is timeconsuming. Recent researches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> focused on weakly supervised recognition methods with only imagelevel labels to obtain accurate part localization to solve this problem. Some patch-based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> first initialize abundant region proposals and select the discriminative parts based on a specific strategy. There are also attention-based ways to localize the corresponding high areas related to the image label, such as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Global-discrimination approaches generally learn the embeddings using a specific distance metric so that samples from the same category can be pulled close to each other while samples from different categories are pushed apart. For example, a bilinear model is used in <ref type="bibr" target="#b25">[26]</ref> to learn the interacted feature of two independent CNNs, which achieves remarkable fine-grained recognition performance. However, the exceptionally high dimensionality of bilinear features still makes it impractical for realistic applications. Chen et al. <ref type="bibr" target="#b26">[27]</ref> enforced the classification network to pay more attention to discriminative regions for spotting the differences by destructing and reconstructing the input image. Sun et al. <ref type="bibr" target="#b27">[28]</ref> masked the most salient features for the input images to force the network to use more subtle clues for its correct classification. Zhuang et al. <ref type="bibr" target="#b28">[29]</ref> learned a mutual feature vector to capture semantic differences in the input image pair.</p><p>Unlike the methods described above, we consider the discriminative but not the most significant part of a single image, but also emphasize the discriminative information aggregation in different images. Hence, we propose a vision transformer with peak suppression and knowledge guidance, which can effectively increase the richness of finegrained representations in a local area and effectively aggregate patch features. Simultaneously, it emphasizes the learning and utilization of the knowledge of distinguishing characteristics between different image samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Vision Transformer</head><p>The transformer architecture by <ref type="bibr" target="#b29">[30]</ref> is proposed to deal with the sequential data in the field of natural language processing <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Inspired by the breakthroughs of transformer architectures in the field of natural language processing, researchers have recently applied transformer to computer vision tasks, such as image recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>, object detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, segmentation <ref type="bibr" target="#b35">[36]</ref>, image superresolution <ref type="bibr" target="#b36">[37]</ref>. For example, Cordonnier et al. <ref type="bibr" target="#b37">[38]</ref> proved that a multi-head self-attention layer with a sufficient number of heads is at least as expressive as any convolutional layers. They extracted patches from the input image and applied full self-attention on top. iGPT <ref type="bibr" target="#b38">[39]</ref> applies transformers to image pixels after reducing the image resolution and color space. It is worth noting that the Vision Transformer (ViT) <ref type="bibr" target="#b12">[13]</ref> is a pure transformer that performs well on the image classification task when applied directly to the sequences of image patches. Based on ViT, Touvron et al. <ref type="bibr" target="#b32">[33]</ref> transferred the model to the fine-grained visual categorization and achieved competitive performance.</p><p>To sum up, the vision transformer maps group pixels into a small number of visual tokens, representing a semantic concept in the image. These visual tokens are used directly for image classification, with the transformers being used to model the relationships among tokens. Our work is inspired by ViT and adopts the same method as ViT to build a transformer. Despite the efficiency of iGPT, ViT, and DeiT, these works only fine-tune the model on the fine-grained datasets directly to evaluate the effectiveness of the model for transfer learning and ignore the characteristics of the fine-grained image recognition task. Different from the above methods, we consider the characteristics of the fine-grained image and focus on the specific recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Framework</head><p>This section introduces the proposed framework, which is a transformer architecture for the fine-grained recognition task. As shown in <ref type="figure">Fig. 3</ref>, this framework mainly consists of two components, namely Peak Suppression (PS) and Knowledge Guidance (KG). PS takes images as input and outputs the suppressed image-based representations to the KG module. The KG module takes the image-based representations and learns the knowledge embeddings, finally uses the fusion representations for the recognition task. Section 3.1 introduces PS and Section 3.2 details KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Peak Suppression</head><p>Inspired by the effectiveness of the diversification block on convolutional neural networks, we proposed the peak suppression module on the transformer architecture to pay more attention to the other informative parts and obtain more diverse expressions by suppressing the most discriminative regions. Different from the CNNs-based method of directly operating feature maps using the category-specific activation maps, the transformer-based method cannot achieve the goal by directly removing the most significant corresponding token in the last layer because all tokens have interacted during the feedforward process in multihead attention layers. Therefore, we can only use the attention map to backtrack to the input image space and then mask salient image regions in the image space.</p><p>Formally, we follow the settings of <ref type="bibr" target="#b12">[13]</ref> and use the ViT as the backbone. Let x ? R H?W ?C denotes a given training image where (H, W ) is the resolution of the image, C is the number of channels. The image x is reshaped into a sequence of flattened 2D patches x p ? R N ?P 2 ?C , the resolution of each image patch is (P, P ), and N = HW/P 2 is the resulting number of patches. These patches are converted to D dimensions embedding x p E ? R N ?D as input tokens through a trainable linear projection. Attaching the learnable embedding class token z 0 0 , there are a total of N + 1 tokens. Position embeddings E pos are added to the patch embeddings E pos ? R (N +1)?D to retain the positional information. The transformer encoder takes the z 0 as input and outputs z L and the attention weight M L , where L means the transformer encoder is composed of a stack of L identical layers. Each layer consists of multihead self-attention (MSA) and MLP blocks. Layernorm (LN) is applied before every block and residual connections after every block.</p><formula xml:id="formula_0">z 0 = [x class ; x 1 p E; x 2 p E; ...; x N p E] + E pos , z l = M SA(LN (z l?1 )) + z l?1 , l = 1...L, z l = M LP (LN (z l )) + z l , l = 1...L.<label>(1)</label></formula><p>We use the Attention Rollout technique <ref type="bibr" target="#b39">[40]</ref> to acquire the attention map from the output token to the input space. Given a transformer with L layers, we need to flow the attention from all positions in the final layer L to all positions in layer 1. At every transformer layer, we average attention weights at each layer over all heads and get the weight matrix M l that defines the attention value flows from all tokens in the previous layer to all tokens in the l layer. Considering that there are residual connections in the backbone, we deal with them by adding the identity matrix I to the attention matrices and re-normalize the attention weights to keep the total attention in the range of 0 to 1. Finally, we recursively multiply the weight matrices of all layers.</p><formula xml:id="formula_1">M l = (M l + I)M l?1 if l &gt; 1, M l + I if l = 1.<label>(2)</label></formula><p>In this equation,M l is attention rollout of the l layer, M l is raw attention of the l layer, and the multiplication operation is matrix multiplication. TheM L illustrates the mixing of attention among tokens across all layers. Let B ? R N +1 denote the binary suppressing mask for the input tokens. Each element in mask B is in the domain {0, 1}, where 0 indicates the corresponding location will be suppressed while 1 means that no suppression will take place. Note that the B 0 is always 1 because it corresponds to the class token.</p><p>After obtaining the attention mapM L ? R N , we compute the B by traversing the entire attention map and finding the position of the largest response.</p><formula xml:id="formula_2">B i = 0 ifM i?1 L = max(M L ) 1 otherwise.<label>(3)</label></formula><p>where i ? {1, 2, ..., N }. In order to remove the influence of peaky token, we remove it from the forward process.</p><formula xml:id="formula_3">z 0 = [x class ; x 1 p E; x 2 p E; ...; x N p E] * B + E pos ,<label>(4)</label></formula><p>where * denotes element-wise multiplication. After the forward process, the transformer encoder outputs y as the image-based representation.</p><formula xml:id="formula_4">z l = M SA(LN (? l?1 )) +? l?1 , l = 1...L, z l = M LP (LN (? l )) +? l , l = 1...L. y = LN (? 0 L ),<label>(5)</label></formula><p>where? 0 L denotes the class token vector of the output of L layer transformer encoder after peak suppression. We implement the remove of Equation (4) by setting ?? to the suppressed token vectors. After the softmax layer of Equation <ref type="formula" target="#formula_4">(5)</ref>, the responses of these tokens will be close to 0.</p><p>To sum up, we suppress the peaky token based on the attention maps in the training phase. By suppressing the tokens, the network is forced to find the other informative regions instead of the most discriminative regions in the image. The increase of the feature diversity can improve the performance of the network in the test phase.  <ref type="figure">Figure 3</ref>: Overview of our framework. Here we visualize the case of peak suppression and knowledge guidance given a training batch with a image and its corresponding label Yellow-breasted Chat. S(?) means the similarity function. Only the presentation label is used to predict in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Knowledge Guidance</head><p>After obtaining the diversified discriminative clues of a single image, the knowledge guidance module fuses the information of multiple images to get a more comprehensive feature representation. The knowledge guidance module first learns the knowledge embeddings related to the category. To this end, we propose a novel learning method that abstracts the learning of knowledge embeddings as a classification problem and directly uses the similarity between knowledge embeddings and image-based representations as the classification score. Subsequently, the knowledge guidance module injects the obtained knowledge embeddings into the image-based representations to get a more comprehensive expression. Therefore, the knowledge guidance module contains two tasks, one is knowledge learning, and the other is knowledge exploiting. We first introduce the knowledge learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Knowledge Learning</head><p>To enforce the networks to learn the knowledge embedding for each category, we treat the knowledge learning task as a classification task. Considering the multiclass fine-grained image recognition task, let F = {f g } G g=1 denotes the fine-grained label set containing all G finegrained labels and X = {x j } J j=1 denotes the image training dataset containing the total J images. Through the peak suppression module of transformer encoder, we can acquire the representation set Y = {y j } J j=1 , y j ? R D from the training dataset. Randomly initializing a knowledge embedding set of the D-dimension knowledge embedding</p><formula xml:id="formula_5">K = {k g } G g=1 , k g ? R D , each k g means the knowledge em- bedding of the category f g .</formula><p>Given a image x j with the corresponding ground-truth fine-grained label f g , the transformer encoder outputs its representation y j ? R D . The knowledge embedding set tries to distinguish which category this representation y j belongs to by judging the similarity between this representation y j and every knowledge embedding k g in the K. We obtain a knowledge response coefficients r j ? R G .</p><formula xml:id="formula_6">r j = Sof tmax(S(y j , K)),<label>(6)</label></formula><p>where S(?) is a similarity function. We have tried a variety of methods to calculate similarity, such as the neural networks and element-wise multiplication, and the results are not significantly different. In addition, the computational complexity of element-wise multiplication is much smaller. Therefore, without loss of generality, the elementwise multiplication is adopted in our experiments. We then directly convert the knowledge response coefficients r j into one-hot for a supervised learning. We define the knowledge learning loss as Loss kl = CrossEntropy(Onehot(r j ), f g ).</p><p>We use Loss kl to supervise knowledge learning processing to update the knowledge embedding set. The knowledge embeddings have gradually become the common ground of different instances of the same category during the training procedure. As the representations become more discriminative in training, the knowledge embeddings become better to express the category. Furthermore, in this training procedure, the knowledge learning task considers the representations of all training images, making the knowledge embeddings more comprehensive in the expression of corresponding categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Knowledge Exploiting</head><p>In order to effectively use knowledge, we first obtain a knowledge-based representation based on the knowledge response coefficients and the knowledge embeddings. The knowledge-based representation ? j is computed as</p><formula xml:id="formula_8">? j = g?G r j k g .<label>(8)</label></formula><p>This knowledge-based representation includes a summary of the fine-grained features of different instances in the same category and a summary of the differences in different categories.</p><p>We use the knowledge to guide the classification by injecting the knowledge-based representation into the final fusion fine-grained representation:</p><formula xml:id="formula_9">u j = F C(LN (y j + ? j )),<label>(9)</label></formula><p>where FC is the fully connective layer. We define a representation learning loss to supervise the training to obtain the fusion representation:</p><formula xml:id="formula_10">Loss rep = CrossEntropy(u j , f g ).<label>(10)</label></formula><p>We optimize the knowledge learning task and the knowledge exploiting task simultaneously so that the image representation and the knowledge can be updated iteratively and promote each other during the training process.</p><p>Thus, the total loss function of the whole network can be defined as</p><formula xml:id="formula_11">Loss = Loss kl + ? ? Loss rep ,<label>(11)</label></formula><p>where ? is a hyperparameter used to adjust the different emphasis of the two tasks. During the training, our method explicitly obtains the knowledge embeddings of the different fine-grained categories. These knowledge embeddings are distinguishable and comprehensive, so we insert them into the image-based representations to increase the comprehensiveness of features for the fine-grained recognition task.</p><p>In summary, the peak suppression module aims to consider more regions in a single image to obtain more diverse expressions. Based on the peak suppression module, the knowledge guidance module aims to extract and exploit the category-related embeddings based on the expression of multiple images. Therefore, these two modules are complementary in aggregating fragmented information at different levels. In the next Section 4, we conduct sufficient experiments to prove the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup 4.1.1. Datasets</head><p>We conduct our experiments on six fine-grained image recognition datasets, including two publicly available bird datasets CUB-200-2011 <ref type="bibr" target="#b40">[41]</ref> and NABirds <ref type="bibr" target="#b41">[42]</ref>, one flower dataset Oxford 102 Flowers <ref type="bibr" target="#b42">[43]</ref>, one dog dataset Stanford Dogs <ref type="bibr" target="#b43">[44]</ref>, and two food datasets ISIA Food-200 <ref type="bibr" target="#b9">[10]</ref> and ISIA Food-500 <ref type="bibr" target="#b44">[45]</ref>. The detailed statistics about these six datasets including class numbers and train/test distributions are summarized in <ref type="table" target="#tab_1">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Implementation Details and Comparison</head><p>Methods Our method is implemented on the Pytorch platform with four Nvidia V100 GPUs. The input image size is 448 ? 448 as most state-of-the-art fine-grained image recognition approaches. By following the settings of NTS-NET <ref type="bibr" target="#b19">[20]</ref>, we use data augmentations, including random cropping and horizontal flipping during the training procedure. Only the center cropping is involved in inference. The model is trained with the stochastic gradient descent (SGD) with a batch size of 8 and momentum of 0.9 for all datasets. The learning rate is set to 3e-2 initially, and the schedule applies a cosine decay function to an optimizer step. In all our experiments, we use ViT-B-16 pre-trained on ImageNet21k as the backbone. For all experiments, we adopt the top-1 accuracy as the evaluation metric. To demonstrate the advantages of our model, we list the some methods for comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? DVAN [4]: Diversified visual attention network which</head><p>relieves the dependency on supervised information.</p><p>? MaxEnt <ref type="bibr" target="#b45">[46]</ref>: Maximum entropy approach which provides a training routine to maximize the entropy of the output probability distributions.</p><p>? NTS-NET <ref type="bibr" target="#b19">[20]</ref>: Navigator-teacher-scrutinizer network which finds consistent informative regions through multi-agent cooperation.</p><p>? Cross-X <ref type="bibr" target="#b46">[47]</ref>: Multi-scale feature learning with exploiting the relationships between different images and layers.</p><p>? GHNS <ref type="bibr" target="#b47">[48]</ref>: A framework that generates features of hard negative samples.</p><p>? CSC-Net <ref type="bibr" target="#b48">[49]</ref>: Category-specific semantic coherency network which semantically aligns the discriminative regions of the same subcategory.</p><p>? CDL <ref type="bibr" target="#b49">[50]</ref>: Correlation-guided discriminative learning model which mines and exploits the discriminative potentials of correlations.</p><p>? MLA-CNN <ref type="bibr" target="#b50">[51]</ref>: Multi-level attention model which uses the neural activations to generate multi-scale regions which are helpful for the fine-grained categorization.</p><p>? BiM-PMA <ref type="bibr" target="#b51">[52]</ref>: Progressive mask attention model by leveraging both visual and language modalities.</p><p>? CIN <ref type="bibr" target="#b52">[53]</ref>: Channel interaction network models channel-wise interplay within an image and across images.</p><p>? DB <ref type="bibr" target="#b27">[28]</ref>: End-to-end network with a diversification block to use more subtle clues.</p><p>? FDL <ref type="bibr" target="#b21">[22]</ref>: Filtration and distillation learning model with region proposing and feature learning.</p><p>? PMG <ref type="bibr" target="#b53">[54]</ref>: Progressive multi-granularity method exploiting information based on the smaller granularity information found at the last step and the previous stage.</p><p>? API-NET <ref type="bibr" target="#b28">[29]</ref>: Pairwise interaction network which can progressively recognize a pair of images by interaction.</p><p>? CPM <ref type="bibr" target="#b16">[17]</ref>: Complementary part model in a weakly supervised manner to retrieve information suppressed by dominant object parts detected by CNNs.</p><p>? GaRD <ref type="bibr" target="#b54">[55]</ref>: Graph-based relation discovery approach to grasp the stronger contextual details.</p><p>? SnapMix <ref type="bibr" target="#b55">[56]</ref>: Semantically proportional mixing which exploits CAM to lessen the label noise in augmenting fine-grained data.</p><p>? HGNet <ref type="bibr" target="#b56">[57]</ref>: Hierarchical gate network to exploit the interconnection among hierarchical categories.</p><p>? SCAPNet <ref type="bibr" target="#b57">[58]</ref>: Scale-consistent attention part network to guide part selection across multi-scales and keep the selection scale consistent.</p><p>? CTF-CapsNet <ref type="bibr" target="#b58">[59]</ref>: Coarse-to-fine capsule network to shape an increasingly specialized description.</p><p>? MSEC [60]: Multi-Scale Erasure and Confusion which realizes confusion at different scales in images and sub-regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Analysis</head><p>In this section, we conduct a series of ablation studies on the CUB-200-2011, Stanford Dog, Oxford 102 Flowers, NABirds, ISIA Food-200, and ISIA Food-500 datasets to better understand the designation of the proposed TP-SKG. We use the performance of the original ViT-B-16 as the ablation baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Impact of different components</head><p>To investigate the contribution of each component in the proposed method, we omit different components of TPSKG and report the corresponding top-1 recognition accuracy. From the results reported in <ref type="table" target="#tab_2">Table 2</ref>, we can draw the following conclusions:</p><p>(1) The recognition accuracy on the CUB-200-2011 dataset drops from 91.3% to 91.0% and 90.9% when omitting the PS module and the KG module respectively, which demonstrates the effectiveness of both of the components for the fine-grained image recognition task. The experimental results on the other five datasets also have similar trends to the results of the CUB-200-2011 dataset, indicating that both the PS module and the KG module can effectively improve the recognition performance.</p><p>(2) The network with only the PS module improves the recognition accuracy of baseline by 0.5% (90.4% vs. 90.9%) on CUB-200-2011 dataset, shows that details other than the most significant part are helpful for the finegrained image recognition task. At the same time, the network with the KG module improves 0.6% (90.4% vs. 91.0%) on the CUB-200-2011 dataset, showing that integrating the discriminative information of multiple images can effectively improve the recognition performance of the network. This result is consistent with our analysis of discriminative information fragmentation in fine-grained image recognition tasks.</p><p>(3) The PS module improves the recognition accuracy of baseline by 1.3% (59.9% vs. 61.2%) and the KG module improves the recognition accuracy of baseline by 2.1% (59.9% vs. 62.0%) on the ISIA Food-500 dataset, the combination of these two modules can improve the recognition accuracy of baseline by 5.5% (59.9% vs. 65.4%). This experimental phenomenon shows that the PS module and the KG module are complementary and can promote each other. The more diverse the expression obtained by the peak suppression module, the stronger the expression ability of embedding learned by the knowledge guidance module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Visualizations of knowledge embedding</head><p>The knowledge embedding is a category-related feature representation learning with the similarity as the classification score directly. The embedding becomes the most similar to the feature expression in the corresponding category and the most dissimilar to the feature expression in other categories. As shown in <ref type="figure" target="#fig_0">Fig. 4: (1)</ref> The knowledgebased representation and the image-based representation  are in the same subspace, which is convenient for feature fusion.</p><p>(2) These two representations of the same category are close in the subspace, indicating that knowledge-based representation can express category-related information.</p><p>(3) The knowledge-based representation is separable in the subspace, so it is helpful for recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Choice of ?:</head><p>Since Equation 11 requires selecting a hyperparameter ?, it is essential to study the influence of classification performance on the choice of ?. We conduct this experiment for four different ? on the CUB-200-2011 dataset. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, the experimental results show that (1) The performance is relatively robust to the choice of ? generally.</p><p>(2) The model performs better when the weight of Loss rep is slightly larger than Loss kl . The probable reason is that the learning of knowledge relies on the combined effect of representation and label. A slightly larger weight of Loss rep allows the network to learn the discriminative feature representation preferentially. Because the performance of the model is not sensitive to the choice of ?, the weighting coefficient in Equation 11 is empirically chosen to be ?=2 in all the following experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Computation complexity</head><p>Since the computation and memory cost can be heavy for global attention in vision transformer architecture, we compare the proposed modules to the original ViT, both in terms of the number of parameters and MACs (Multiply-Accumulate Operations). As shown in <ref type="table" target="#tab_3">Table 3</ref>, the proposed KG module achieves better performance compared to the original ViT without significantly increasing computational complexity. Although the proposed PS module increases the computational complexity in the training phase, it does not increase the computational complexity at all in the inference phase, and at the same time improves the recognition performance. The number of parameters has not increased significantly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-art</head><p>For further verification for the TPSKG, we compare our method to the state-of-the-art methods on the six publicly available fine-grained datasets in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">CUB-200-2011</head><p>We compare the proposed method against many stateof-the-art fine-grained recognition models on CUB-200-2011, as shown in <ref type="table" target="#tab_4">Table 4</ref>. The results show the following conclusions.</p><p>(1) Overall, the proposed TPSKG performs better than the state-of-the-art fine-grained methods, including the global-discrimination approaches methods MaxEnt <ref type="bibr" target="#b45">[46]</ref> and DB <ref type="bibr" target="#b27">[28]</ref>, and the part-based methods NTS-NET <ref type="bibr" target="#b19">[20]</ref> and CPM <ref type="bibr" target="#b16">[17]</ref>.</p><p>(2) Images with higher resolutions usually contain richer information and subtle details that are important for the fine-grained image recognition task. According to the literature <ref type="bibr" target="#b60">[61]</ref>, higher resolution input images will produce better performance generally. Our method uses a smaller resolution than PMG <ref type="bibr" target="#b53">[54]</ref>, API-NET <ref type="bibr" target="#b28">[29]</ref>, and CPM <ref type="bibr" target="#b16">[17]</ref> but achieves a better performance. At the same time, our method is possible to perform better with higher resolution.</p><p>(3) CPM <ref type="bibr" target="#b16">[17]</ref> has good performance using the stacked BiLSTMs to integrate the patch features. The performance of the original ViT method is equivalent to the CPM, which proves the effectiveness of the transformer in feature aggregation and its potential in the fine-grained recognition task.</p><p>(4) Although the original ViT model has satisfactory performance, we have improved it by 0.9%.</p><p>(5) Both CIN <ref type="bibr" target="#b52">[53]</ref> and API-NET <ref type="bibr" target="#b28">[29]</ref> have achieved good results based on the contrastive learning mechanism. It is worth noting that the improvement from our framework is orthogonal to those works, so the proposed TPSKG can also benefit from these methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Stanford Dog</head><p>As can be seen from <ref type="table" target="#tab_5">Table 5</ref>, our method shows more performance improvement on the Stanford Dog dataset, which is 2.2% higher than the current state-of-the-art method API-NET <ref type="bibr" target="#b28">[29]</ref> without using the contrastive learning mechanism and the high-resolution input. It is worth noting that the performance of the original ViT also exceeds API-NET by 1.1%, which reflects that the transformer architecture can be well migrated to fine-grained recognition task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Oxford 102 Flowers</head><p>Unlike BiM-PMA <ref type="bibr" target="#b51">[52]</ref>, which uses all 2040 images in the training set and validation set for training, we follow the settings of PC-CNN <ref type="bibr" target="#b61">[62]</ref> and PBC <ref type="bibr" target="#b62">[63]</ref> and only use 1020 images in training set for training to ensure a relatively fair comparison. As can be seen from <ref type="table" target="#tab_6">Table 6</ref>, although using fewer images, our method still achieves a 2.1% performance improvement compared to BiM-PMA. At the same time, our method still improves the recognition performance when the recognition performance of ViT is excellent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">NABirds</head><p>The NABirds dataset is a larger fine-grained dataset than the CUB-200-2011 dataset containing 48,562 North American bird images. Many methods with complex operations are not easy to experiment on a data set of this order of magnitude. If an image generates thousands of proposals, it means that tens of millions of proposals need to be processed. <ref type="table" target="#tab_7">Table 7</ref> reports the performance of several methods on the NABirds dataset. DSTL <ref type="bibr" target="#b60">[61]</ref> uses the transfer learning strategy for the fine-grained image recognition task consisting of more than one dataset. The result of our method trained on a separate dataset exceeds DSTL by 2.2% in accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Resolution Accuracy(%) PC-CNN <ref type="bibr" target="#b61">[62]</ref> DenseNet-161 224 ? 224 82.8 MaxEnt <ref type="bibr" target="#b45">[46]</ref> DenseNet-161 -83.0 Cross-X <ref type="bibr" target="#b46">[47]</ref> ResNet </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5.">ISIA Food-200</head><p>In order to further verify the effectiveness and explore the scope of application of our method, we explored the food recognition task on the ISIA Food-200 dataset. Unlike other fine-grained objects, many types of food are nonrigid and lack a fixed spatial structure and semantic pattern. Therefore, it is challenging to capture specific semantic information from food images. Our method attempts to increase the diversity of representations to cover more distinguished areas, which is more effective for non-rigid objects. Simultaneously, our method injects the extracted knowledge into the image-based representation so that a more comprehensive understanding of food categories can be used in the recognition task. <ref type="table" target="#tab_9">Table 8</ref> reports the performance of several methods on the ISIA Food-200 dataset. The ViT is also mediocre on this task. IG-CMAN <ref type="bibr" target="#b9">[10]</ref> is a patch-based method sequentially localizing multiple informative image regions with multi-scale from category level to ingredient-level guidance in a coarse-to-fine manner. Our method achieves the best 69.5% without using the multi-scale strategy and outperforms the state-of-the-art method IG-CMAN by 2.0% in accuracy. This result proves that our method has a more significant performance improvement in complex recognition problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6.">ISIA Food-500</head><p>The ISIA Food-500 is a more comprehensive food dataset than the ISIA Food-200 with a larger data volume and higher diversity. We evaluated the proposed TPSKG against different fine-grained methods in <ref type="table" target="#tab_10">Table 9</ref>. The performance of ViT-ResNet-50 and ViT show a pronounced decline. The possible reason is that the volume and complexity of the ISIA Food-500 dataset are much higher than that of the ISIA Food-200 dataset. This increase in com-plexity makes the impact of the loss of local region semantics more significant. It can also be seen that the proposed method exceeds the original ViT significantly, with a gain of 5.5% in accuracy. When the performance of the original ViT is poor, our method still achieves competitive performance compared to the state-of-the-art method, which proves that our method does not rely heavily on the performance of ViT. The proposed method obtains better accuracy than the SGLANet without a complicated multi-scale mechanism and spatial-channel attention. We will try to leverage the multi-scale information to improve performance in future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.7.">Overall</head><p>We summarize the results of all datasets to obtain an overall understanding of the proposed method. We can find that (1) The recognition results of the different methods for the six different datasets show a very high degree of correspondence, indicating the strong reproducibility.</p><p>(2) The performance of the hybrid model directly generated by the simple combination of ViT and ResNet-50 generally performs poorly on fine-grained image recognition task and even has performance degradation compared to the ViT model. A possible explanation for these results may be the lack of adequate semantic information in small regions. <ref type="bibr">(</ref>3) The ViT model is generally suitable for simple fine-grained recognition tasks and obtains close to state-of-the-art results on multiple datasets, but it does not perform well for more complex food recognition tasks. (4) The proposed PS module and KG module have effectively improved the recognition performance, and the proposed method has achieved very superior performance on all datasets. <ref type="bibr" target="#b4">(5)</ref> The KG module improves the model performance more significantly than the PS module. The possible explanation for this might be that the benefits of integrating discriminative information in multiple images are more significant than the coverage of more discriminative information areas in one single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Visualization</head><p>In order to show the effectiveness of the method more intuitively, we visualize the attention maps of the original  ViT and TPSKG models for sample images from six different datasets. As shown in <ref type="figure">Fig. 6</ref>, we find that although the original ViT can also perform localization and recognition, our method is better in both aspects. The attention map of the proposed TPSKG can not only locate the essential parts well but also cover more discriminative areas, which shows that the method is more robust than the original ViT. For the CUB-200-2011, Stanford Dog, Oxford Flowers and NABirds datasets with relatively simple scenes, features without the diversity can complete the recognition task. For relatively complex food recognition tasks on the ISIA Food-200 and ISIA Food-500 datasets, the diversity of features is more important, and our method has improved more obviously, which is consistent with the results of quantitative analysis.</p><p>To visually analyze the influence of the knowledge guidance module, we visualize the feature representations before/after injecting the knowledge embedding by employing t-SNE, on the six fine-grained image datasets as shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. The visualized data includes all test set images of CUB-200-2011, Stanford Dogs, Oxford Flowers, NAbirds, ISIA Food-200 datasets and 50 sample categories from the ISIA Food-500 dataset due to excessive data volume. Since the classification performance of the first four datasets is very high, the visualized images are not much different. But the latter two more complex food recognition datasets show significant differences. The visualization results of food recognition datasets in this figure show obvious intra-class clustering. This proves that the proposed KG module has a strong intra-class aggregation ability. Although the t-SNE method cannot prove the method's ability for the recognition task, it can be seen from the local structure that our method has a more vital ability to aggregate features within the class. This investigation confirms that feature representations will get into more separable clusters after injecting the category-related knowledge embedding.</p><p>In addition, we further show the confusion matrix of our method on the CUB-200-2011 and ISIA Food-200 datasets in <ref type="figure" target="#fig_5">Fig. 8</ref>, where the vertical axis shows the ground-truth classes, and the horizontal axis shows the predicted classes.</p><p>Yellower colors indicate better performance. We can see that our method still does not provide perfect performance for some bird and food categories. We enlarge specific regions to highlight the misclassified results and show some samples with low recognition rates. As shown in <ref type="figure" target="#fig_5">Fig. 8 (a)</ref>, some birds of the same meta-category are extremely difficult to distinguish, such as California Gull and Western Gull, Artic Tern and Common Tern. There are also images with similar poses that cannot be recognized well, such as Olive Sided Flycatcher and Wester Wood Pewee, requiring further study and exploration. From <ref type="figure" target="#fig_5">Fig. 8 (b)</ref> we can see that these food categories are very similar in visual appearances, such as Chow mein, Mie Goreng, and Fried noodles. Even humans cannot easily distinguish these food categories based on images. Some food categories have the same ingredients and different cooking techniques, which are difficult to distinguish, such as pork knuckle and the Schweinshaxe. Many types of food are difficult to classify based on images alone. A possible solution is to combine multiple media formats of information for the recognition task, such as ingredient lists and cooking processes.</p><p>The transformer architecture has promising applications in multimedia, including the visual field and the NLP field, and provides the possibility for a unified framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Fine-grained image recognition is an interesting and fundamental topic. In this paper, we investigate the problem of fine-grained image recognition from the perspective of fragmented information integration. Furthermore, we present a transformer with peak suppression and knowledge guidance (TPSKG) for the fine-grained image recognition task. Our method learns the diverse fine-grained representations by the peak suppression module penalizing the most discriminative parts. It then learns the knowledge embedding including a large number of discriminative clues for different images of the same category, and injects them into fine-grained representations leading to significantly higher recognition performance. The proposed network can be trained end-to-end in one stage, requiring no bounding box/part annotations. Qualitative and quantitative evaluations on six public fine-grained datasets demonstrate that the proposed TPSKG can achieve competitive performance compared to the state-of-the-art approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Some fine-grained bird images sampling from the CUB-200-2011 dataset. (a) is Black Tern, both (b) and (c) are Longtailed Jaeger. Discriminative parts are annotated by red boxes. The details in the red boxes of (a) and (b) are magnified next to the image. Discriminative clues are distributed in multiple regions of the image, and the clues of a single image are usually incomplete.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The effects of the proposed approach for some samples from the CUB-200-2011 and Stanford Dogs datasets. (a) is the original fine-grained image, (c) and (b) are the attention weights obtained from the vision transformer with and without the proposed method (TPSKG). The parts of the proposed method that are significantly different from the original method are annotated by red boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The t-SNE visualizations of image representations and knowledge embedding of 50 sample categories from the CUB-200-2011 dataset. Each dot stands for an image representation, and each star represents the knowledge embedding of a category. The color indicates the categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Performance on the choice of ? on the CUB-200-2011 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>The t-SNE visualization of fine-grained image feature representations of (top row) before injecting the knowledge embedding, (bottom row) after injecting the knowledge embedding on the six fine-grained datasets. Each color represents a different class. The upper right corner shows the accuracy of the corresponding method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Confusion matrix of our method on the (a) CUB-200-2011 and (b) ISIA Food-200 datasets. Some instances of low recognition rate categories are annotated by red boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Fine-grained image dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Class # Training # Testing</cell></row><row><cell>CUB-200-2011 [41]</cell><cell>200</cell><cell>5,994</cell><cell>5,794</cell></row><row><cell>Oxford Flowers [43]</cell><cell>102</cell><cell>1,020</cell><cell>6,149</cell></row><row><cell>Stanford Dogs [44]</cell><cell>120</cell><cell>12,000</cell><cell>8,580</cell></row><row><cell>NABirds [42]</cell><cell>555</cell><cell>23,929</cell><cell>24,633</cell></row><row><cell>ISIA Food-200 [10]</cell><cell>200</cell><cell>118,210</cell><cell>59,287</cell></row><row><cell>ISIA Food-500 [45]</cell><cell>500</cell><cell>239,379</cell><cell>120,143</cell></row><row><cell>4.1.2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation results of the proposed TPSKG on the fine-grained image datasets.</figDesc><table><row><cell>Model</cell><cell cols="6">CUB-200-2011 Stanford Dog Oxford Flowers NABirds ISIA Food-200 ISIA Food-500</cell></row><row><cell>ViT-B-16</cell><cell>90.4</cell><cell>91.4</cell><cell>99.2</cell><cell>89.6</cell><cell>67.4</cell><cell>59.9</cell></row><row><cell>w/o Peak Suppression</cell><cell>91.0</cell><cell>91.8</cell><cell>99.3</cell><cell>89.9</cell><cell>69.3</cell><cell>62.0</cell></row><row><cell>w/o Knowledge Guidance</cell><cell>90.9</cell><cell>91.8</cell><cell>99.3</cell><cell>89.8</cell><cell>68.3</cell><cell>61.2</cell></row><row><cell>All TPSKG</cell><cell>91.3</cell><cell>92.5</cell><cell>99.5</cell><cell>90.1</cell><cell>69.5</cell><cell>65.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison results on Cub-200-2011 dataset.</figDesc><table><row><cell>Models</cell><cell>Input Size</cell><cell>Params (M)</cell><cell>Training MACs(G)</cell><cell>Inference MACs(G)</cell><cell>Accuracy (%)</cell></row><row><cell>ViT-B-16</cell><cell>448?448</cell><cell>86.4</cell><cell>67.14</cell><cell>67.14</cell><cell>90.4</cell></row><row><cell cols="2">ViT w/ PS 448?448</cell><cell>86.4</cell><cell>134.4</cell><cell>67.14</cell><cell>90.9</cell></row><row><cell cols="2">ViT w/ KG 448?448</cell><cell>86.6</cell><cell>67.14</cell><cell>67.14</cell><cell>91.0</cell></row><row><cell>TPSKG</cell><cell>448?448</cell><cell>86.6</cell><cell>134.4</cell><cell>67.14</cell><cell>91.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison results on CUB-200-2011 dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Resolution Accuracy(%)</cell></row><row><cell>MaxEnt [46]</cell><cell>DenseNet-161</cell><cell>-</cell><cell>84.9</cell></row><row><cell>MLA-CNN [51]</cell><cell>VGG-19</cell><cell>448 ? 448</cell><cell>85.7</cell></row><row><cell>DVAN [4]</cell><cell>VGG-16</cell><cell>224 ? 224</cell><cell>87.1</cell></row><row><cell>NTS-NET [20]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.5</cell></row><row><cell>Cross-X [47]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.7</cell></row><row><cell>HGNet [57]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.9</cell></row><row><cell>CIN [53]</cell><cell>ResNet-101</cell><cell>448 ? 448</cell><cell>88.1</cell></row><row><cell>MSEC [60]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>88.3</cell></row><row><cell>CDL [50]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>88.4</cell></row><row><cell>DB [28]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>88.6</cell></row><row><cell>HGNet [57]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>88.7</cell></row><row><cell>CTF-CapsNet [59]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>88.9</cell></row><row><cell>GHNS [48]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>89.1</cell></row><row><cell>FDL [22]</cell><cell>DenseNet-161</cell><cell>448 ? 448</cell><cell>89.1</cell></row><row><cell>CSC-Net [49]</cell><cell>ResNet-50</cell><cell>224 ? 224</cell><cell>89.2</cell></row><row><cell>SCAPNet [58]</cell><cell>ResNet-50</cell><cell>224 ? 224</cell><cell>89.5</cell></row><row><cell>PMG [54]</cell><cell>ResNet-50</cell><cell>550 ? 550</cell><cell>89.6</cell></row><row><cell>GaRD [55]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>89.6</cell></row><row><cell>SnapMix [56]</cell><cell>ResNet-101</cell><cell>448 ? 448</cell><cell>89.6</cell></row><row><cell>CTF-CapsNet [59]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>89.7</cell></row><row><cell>API-NET [29]</cell><cell>DenseNet-161</cell><cell>512 ? 512</cell><cell>90.0</cell></row><row><cell>CPM [17]</cell><cell>GoogLeNet</cell><cell>over 800</cell><cell>90.4</cell></row><row><cell>ViT-ResNet-50</cell><cell cols="2">ViT&amp;ResNet-50 448 ? 448</cell><cell>89.2</cell></row><row><cell>ViT [13]</cell><cell>ViT-B-16</cell><cell>448 ? 448</cell><cell>90.4</cell></row><row><cell>TPSKG</cell><cell>ViT-B-16</cell><cell>448 ? 448</cell><cell>91.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison results on Stanford Dog dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Resolution Accuracy(%)</cell></row><row><cell>DVAN [4]</cell><cell>VGG-16</cell><cell>224 ? 224</cell><cell>81.5</cell></row><row><cell>MaxEnt [46]</cell><cell>DenseNet-161</cell><cell>-</cell><cell>83.6</cell></row><row><cell>PC-CNN [62]</cell><cell>DenseNet-161</cell><cell>224 ? 224</cell><cell>83.8</cell></row><row><cell>FDL [22]</cell><cell>DenseNet-161</cell><cell>448 ? 448</cell><cell>84.9</cell></row><row><cell>MSEC [60]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>85.6</cell></row><row><cell>MLA-CNN [51]</cell><cell>VGG-19</cell><cell>448 ? 448</cell><cell>86.8</cell></row><row><cell>DB [28]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>87.7</cell></row><row><cell>Cross-X [47]</cell><cell>ResNet-50</cell><cell>448 ? 448</cell><cell>88.9</cell></row><row><cell>API-NET [29]</cell><cell>DenseNet-161</cell><cell>512 ? 512</cell><cell>90.3</cell></row><row><cell cols="3">ViT-ResNet-50 ViT&amp;ResNet-50 448 ? 448</cell><cell>87.7</cell></row><row><cell>ViT [13]</cell><cell>ViT-B-16</cell><cell>448 ? 448</cell><cell>91.4</cell></row><row><cell>TPSKG</cell><cell>ViT-B-16</cell><cell>448 ? 448</cell><cell>92.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison results on Oxford 102 Flowers dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Resolution Accuracy(%)</cell></row><row><cell>PC-CNN [62]</cell><cell>DenseNet-161</cell><cell>224 ? 224</cell><cell>93.6</cell></row><row><cell>PBC [63]</cell><cell>GoogleNet</cell><cell>224 ? 224</cell><cell>96.1</cell></row><row><cell>BiM-PMA [52]</cell><cell>VGG-16</cell><cell>448 ? 448</cell><cell>97.4</cell></row><row><cell cols="3">ViT-ResNet-50 ViT&amp;ResNet-50 448 ? 448</cell><cell>98.5</cell></row><row><cell>ViT [13]</cell><cell>ViT-B-16</cell><cell>448 ? 448</cell><cell>99.2</cell></row><row><cell>TPSKG</cell><cell>ViT-B-16</cell><cell>448 ? 448</cell><cell>99.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison results on NABirds dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison results on ISIA Food-200 dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Resolution Accuracy(%)</cell></row><row><cell>ResNet-152</cell><cell>ResNet-152</cell><cell>224 ? 224</cell><cell>61.1</cell></row><row><cell>DenseNet-161</cell><cell>DenseNet-161</cell><cell>224 ? 224</cell><cell>62.6</cell></row><row><cell>IG-CMAN [10]</cell><cell>DenseNet-161</cell><cell>224 ? 224</cell><cell>67.5</cell></row><row><cell cols="3">ViT-ResNet-50 ViT&amp;ResNet-50 448 ? 448</cell><cell>62.5</cell></row><row><cell>ViT [13]</cell><cell>ViT-B-16</cell><cell>448 ? 448</cell><cell>67.4</cell></row><row><cell>TPSKG</cell><cell>ViT-B-16</cell><cell>448 ? 448</cell><cell>69.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparison results on ISIA Food-500 dataset.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Resolution Accuracy(%)</cell></row><row><cell>ResNet-152</cell><cell>ResNet-152</cell><cell>224 ? 224</cell><cell>57.0</cell></row><row><cell>WRN-50 [64]</cell><cell>WRN-50</cell><cell>224 ? 224</cell><cell>60.1</cell></row><row><cell>WS-DAN [65]</cell><cell>Inception-v3</cell><cell>299 ? 299</cell><cell>60.7</cell></row><row><cell>NAS-NET [66]</cell><cell>ResNet-152</cell><cell>331 ? 331</cell><cell>60.7</cell></row><row><cell>NTS-NET [20]</cell><cell>ResNet-152</cell><cell>448 ? 448</cell><cell>63.7</cell></row><row><cell>SENet-154</cell><cell>SENet-154</cell><cell>224 ? 224</cell><cell>63.8</cell></row><row><cell>DCL [27]</cell><cell>ResNet-152</cell><cell>448 ? 448</cell><cell>64.1</cell></row><row><cell>SGLANet [45]</cell><cell>SENet-154</cell><cell>224 ? 224</cell><cell>64.7</cell></row><row><cell cols="3">ViT-ResNet-50 ViT&amp;ResNet-50 448 ? 448</cell><cell>52.7</cell></row><row><cell>ViT [13]</cell><cell>ViT-B-16</cell><cell>448 ? 448</cell><cell>59.9</cell></row><row><cell>TPSKG</cell><cell>ViT-B-16</cell><cell>448 ? 448</cell><cell>65.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>The attention map comparison between our method and the baseline in different datasets. Top to below: original image, attention map of the ViT, attention map of our method. The yellow means high weights and the blue means relatively low weights.</figDesc><table><row><cell>CUB-200-2011</cell><cell>Stanford Dog</cell><cell>Oxford Flowers</cell><cell>NABirds</cell><cell>ISIA Food-200</cell><cell>ISIA Food-500</cell></row><row><cell>Figure 6: CUB-200-2011</cell><cell>Stanford Dog</cell><cell>Oxford Flowers</cell><cell>NABirds</cell><cell>ISIA Food-200</cell><cell>ISIA Food-500</cell></row><row><cell>90.4%</cell><cell>91.4%</cell><cell>99.2%</cell><cell>89.6%</cell><cell>67.4%</cell><cell>59.9%</cell></row><row><cell>91.3%</cell><cell>92.5%</cell><cell>99.5%</cell><cell>90.1%</cell><cell>69.5%</cell><cell>65.4%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attribute-aware attention model for fine-grained representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2040" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Only learn one sample: Fine-grained visual categorization with one sample training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1372" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">I read, I saw, I tell: Texts assisted fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="663" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two-view fine-grained classification of plant species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Ara?jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Britto</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Koerich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">467</biblScope>
			<biblScope unit="page" from="427" to="441" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep compositional captioning: Describing novel object categories without paired training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cvae-Gan</surname></persName>
		</author>
		<title level="m">finegrained image generation through asymmetric training, in: International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2764" to="2773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching categories to human learners with visual explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3820" to="3828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Solving mixed-modal jigsaw puzzle for fine-grained sketch-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10344" to="10352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ingredient-guided cascaded multi-attention network for food recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1331" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Being a supercook: Joint food attributes and multimodal content modeling for recipe retrieval and exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1100" to="1113" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Food recommendation: Framework, existing solutions, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2659" to="2671" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale, in: International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Part-stacked CNN for finegrained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on food computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-scale multi-view deep feature aggregation for food recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="265" to="276" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-order-interaction for weakly supervised fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">464</biblScope>
			<biblScope unit="page" from="27" to="36" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="438" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Plant disease recognition: A large-scale benchmark dataset and a visual region and loss reweighting approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2003" to="2015" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11555" to="11562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object-part attention model for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning rich part hierarchies with progressive attention networks for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="476" to="488" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised fine-grained image classification via guassian mixture model oriented discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9746" to="9755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilinear convolutional neural networks for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1309" to="1322" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Finegrained recognition: Accounting for subtle differences between similar classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12047" to="12054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence, AAAI, 2020</title>
		<imprint>
			<biblScope unit="page" from="13130" to="13137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10502" to="10511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
	<note>Generative pretraining from pixels</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics, Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4190" to="4197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR- 2011-001</idno>
		<title level="m">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ISIA Food-500: A dataset for large-scale food recognition via stacked global-local attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Maximum-entropy fine grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="635" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8241" to="8250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The feature generator of hard negative samples for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">439</biblScope>
			<biblScope unit="page" from="374" to="382" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Category-specific semantic coherency learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="174" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Weakly supervised fine-grained image classification via correlation-guided discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1851" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-level dictionary learning for fine-grained images categorization with attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page" from="403" to="412" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bi-modal progressive mask attention for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7006" to="7018" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Channel interaction networks for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence, AAAI, 2020</title>
		<imprint>
			<biblScope unit="page" from="10818" to="10825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification via progressive multigranularity training of jigsaw patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12365</biblScope>
			<biblScope unit="page" from="153" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graph-based high-order relation discovery for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15079" to="15088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Snapmix: Semantically proportional mixing for augmenting fine-grained data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence, AAAI, 2021</title>
		<imprint>
			<biblScope unit="page" from="1628" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Hierarchical gate network for finegrained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<pubPlace>Neurocomputing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning scale-consistent attention part network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A coarse-to-fine capsule network for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">456</biblScope>
			<biblScope unit="page" from="200" to="219" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">MSEC: multi-scale erasure and confusion for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">449</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11216</biblScope>
			<biblScope unit="page" from="71" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">PBC: polygon-based classifier for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="684" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Wide residual networks</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<idno>CoRR abs/1901.09891</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
