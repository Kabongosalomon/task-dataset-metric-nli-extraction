<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-view 3D Body and Cloth Reconstruction under Complex Poses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ugrinovic</surname></persName>
							<email>nugrinovic@iri.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institut de Rob?tica i Inform?tica Industrial</orgName>
								<orgName type="institution">CSIC-UPC</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
							<email>apumarolara@iri.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institut de Rob?tica i Inform?tica Industrial</orgName>
								<orgName type="institution">CSIC-UPC</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sanfeliu</surname></persName>
							<email>asanfeliu@iri.upc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institut de Rob?tica i Inform?tica Industrial</orgName>
								<orgName type="institution">CSIC-UPC</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut de Rob?tica i Inform?tica Industrial</orgName>
								<orgName type="institution">CSIC-UPC</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single-view 3D Body and Cloth Reconstruction under Complex Poses</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D human reconstruction</term>
					<term>augmented/virtual really</term>
					<term>deep networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in 3D human shape reconstruction from single images have shown impressive results, leveraging on deep networks that model the so-called implicit function to learn the occupancy status of arbitrarily dense 3D points in space. However, while current algorithms based on this paradigm, like PiFuHD <ref type="bibr" target="#b44">(Saito et al., 2020)</ref>, are able to estimate accurate geometry of the human shape and clothes, they require high-resolution input images and are not able to capture complex body poses. Most training and evaluation is performed on 1k-resolution images of humans standing in front of the camera under neutral body poses. In this paper, we leverage publicly available data to extend existing implicit function-based models to deal with images of humans that can have arbitrary poses and self-occluded limbs. We argue that the representation power of the implicit function is not sufficient to simultaneously model details of the geometry and of the body pose. We, therefore, propose a coarse-to-fine approach in which we first learn an implicit function that maps the input image to a 3D body shape with a low level of detail, but which correctly fits the underlying human pose, despite its complexity. We then learn a displacement map, conditioned on the smoothed surface and on the input image, which encodes the high-frequency details of the clothes and body. In the experimental section, we show that this coarse-to-fine strategy represents a very good trade-off between shape detail and pose correctness, comparing favorably to the most recent state-of-the-art approaches. Our code will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>While the 3D reconstruction of the human pose <ref type="bibr" target="#b24">(Martinez et al., 2017;</ref><ref type="bibr" target="#b28">Moreno-Noguer, 2017;</ref><ref type="bibr" target="#b35">Pavlakos et al., 2017;</ref><ref type="bibr" target="#b41">Rogez et al., 2019;</ref><ref type="bibr" target="#b25">Mehta et al., 2018;</ref><ref type="bibr" target="#b19">Kinauer et al., 2018)</ref> and shape of the naked body <ref type="bibr" target="#b18">(Kanazawa et al., 2017;</ref><ref type="bibr" target="#b36">Pavlakos et al., 2018;</ref><ref type="bibr" target="#b51">Varol et al., 2018;</ref><ref type="bibr" target="#b52">Varol et al., 2017)</ref> from single images has been extensively studied over the past few years and led to very accurate results, doing this with clothed humans remains a difficult challenge. There exist recent works that provide very good body and cloth reconstructions, but are methods limited to mild human poses, typically standing up in front of the camera <ref type="bibr" target="#b44">(Saito et al., 2020;</ref><ref type="bibr" target="#b43">Saito et al., 2019;</ref><ref type="bibr" target="#b31">Natsume et al., 2019;</ref><ref type="bibr" target="#b3">Alldieck et al., 2019b;</ref><ref type="bibr" target="#b17">Jackson et al., 2018)</ref>. A challenge that still remains open is thus to capture diverse poses while maintaining a dea https://orcid.org/0000-0002-1823-3780 b https://orcid.org/ 0000-0003-4185-6991 c https://orcid.org/0000-0003-3868-9678 d https://orcid.org/0000-0002-8640-684X tailed geometry of clothes and body.</p><p>PiFu  and very recently <ref type="bibr" target="#b44">(Saito et al., 2020)</ref> are the most relevant works on clothed human reconstruction, and builds upon the representation capacity of implicit functions, shown to be very effective for estimating the geometry of rigid 3D objects <ref type="bibr" target="#b26">(Mescheder et al., 2019;</ref><ref type="bibr" target="#b7">Chen and Zhang, 2019;</ref><ref type="bibr" target="#b56">Xu et al., 2019)</ref>. PiFu learns a per-pixel feature vector aligned with the 3D surface to get an implicit function based on local information. However, while this strategy provides a lot of detail, it cannot generalize to arbitrary human poses.</p><p>Other works are able to capture diverse poses but lack details of human clothing <ref type="bibr" target="#b11">(Genova et al., 2020)</ref>. There exist methods that do not use implicit functions, but introduce an additional step to the estimation of a parametric naked body model. For instance, <ref type="bibr" target="#b3">(Alldieck et al., 2019b</ref>) learns a displacement map over the SMPL model <ref type="bibr" target="#b22">(Loper et al., 2015)</ref>, although, this approach is also limited to a small range of body poses and it needs high-quality 1024 ? 1024 input images.</p><p>In this paper, we use implicit functions and propose an approach that, given a single image, is able to predict detailed meshes of clothed 3D humans for a wide range of poses and can work with but it is not limited to 224 ? 224 input images.</p><p>We argue that one of the reasons why <ref type="bibr" target="#b44">Saito et al., 2020)</ref> does not generalize well to difficult poses is that it strongly relies on local pixel features to guide the reconstruction and, thus, has no awareness of the overall topology of the mesh and therefore struggle to model unseen parts of the body.</p><p>To address this, we exploit global image features and alleviate their inherent lack in details using two strategies: First, we introduce a coarse-to-fine architecture with two modules, one building on an implicit function and global features that learns a coarse 3D shape, but with a correct body pose; and another network that learns a displacement map to add extra detail (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Second, we take into account the structure of the human body by including 2D joints as inputs of our system. This enables to have overall mesh consistency and retain the details of body and clothing in complex poses.</p><p>We quantitatively evaluate our method on synthetic data and qualitatively on real and synthetic images and demonstrate that our approach can capture a wide range of poses better than previous state-ofthe-art methods based on implicit functions. Thus, we claim that global reasoning combined with a refinement step leads to coherent human meshes with no disconnected body parts, even in difficult poses, while maintaining a good level of detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Single-view 3D Reconstruction of Rigid Objects is a well studied topic in computer vision and computer graphics. The works in this realm can be mainly categorized by the representation they use, whether it is a voxel grid <ref type="bibr" target="#b8">(Choy et al., 2016;</ref><ref type="bibr" target="#b49">Tulsiani et al., 2017;</ref><ref type="bibr" target="#b55">Wu et al., 2017)</ref>, pointcloud <ref type="bibr" target="#b38">(Pumarola et al., 2020;</ref><ref type="bibr" target="#b10">Fan et al., 2016)</ref>, mesh <ref type="bibr" target="#b54">(Wang et al., 2018;</ref><ref type="bibr" target="#b12">Gkioxari et al., 2019)</ref> or implicit function <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref>. Voxels usually require extensive memory and are time consuming to train while usually leading to reconstructions with very restricted resolution. Pointclouds require additional non-trivial post processing steps to generate the final mesh. <ref type="bibr" target="#b54">(Wang et al., 2018;</ref><ref type="bibr" target="#b12">Gkioxari et al., 2019)</ref> directly work on the mesh using a graph based CNN <ref type="bibr" target="#b46">(Scarselli et al., 2008)</ref>, although they are only able to generate overly smoothed meshes with simple topology which can be genus-0 only. In contrast, we choose to work with implicit function representation due to the well known fact that they require relatively simple architectures and have the ability to obtain a greater level of detail without requiring vast amounts of memory.</p><p>Several works <ref type="bibr" target="#b26">(Mescheder et al., 2019;</ref><ref type="bibr" target="#b34">Park et al., 2019;</ref><ref type="bibr" target="#b56">Xu et al., 2019;</ref><ref type="bibr" target="#b7">Chen and Zhang, 2019)</ref> have shown that implicit functions can be learned by means of deep neural networks, and it is possible to get high resolution reconstruction by applying the marching cubes (MC) algorithm. Most recent approaches for image 3D reconstruction use implicit functions. For example, <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref> conditions the learning of occupancy probabilities to an input image, being able to reconstruct a high resolution mesh. However, they rely solely on global image features which hinders the model to learn high frequency details. We, instead, use local information about the joints and learn a displacement map to improve the reconstruction details as a result of the MC algorithm. <ref type="bibr" target="#b7">(Chen and Zhang, 2019)</ref> also uses global features suffering from the same lack of detail needed to capture clothed humans.</p><p>Single-View 3D Human Reconstruction. While the problem of localizing the 3D position of the joints from a single image has been extensively studied <ref type="bibr" target="#b24">(Martinez et al., 2017;</ref><ref type="bibr" target="#b28">Moreno-Noguer, 2017;</ref><ref type="bibr" target="#b41">Rogez et al., 2019;</ref><ref type="bibr" target="#b27">Moon et al., 2019;</ref><ref type="bibr" target="#b25">Mehta et al., 2018)</ref> 3D human body shape reconstruction still remains an open problem. Single-view human reconstruction requires strong priors due to the inherent ambiguity of the problem. This has been addressed by using parametric models learned from body scan repositories such as SCAPE <ref type="bibr" target="#b4">(Anguelov et al., 2005)</ref> and SMPL <ref type="bibr" target="#b22">(Loper et al., 2015)</ref> to represent the human body geometry by a reduced number of parameters. These parameters are then optimized to match image characteristics. For example, methods that use deep neural networks input additional information such as silhouettes <ref type="bibr" target="#b9">(Dibra et al., 2017;</ref><ref type="bibr" target="#b36">Pavlakos et al., 2018)</ref> and other types of manual annotations <ref type="bibr" target="#b21">(Lassner et al., 2017;</ref><ref type="bibr" target="#b32">Omran et al., 2018)</ref>. Furthermore, (Vince Tan and Cipolla, 2017) uses a differential renderer along with a deep neural network to predict SMPL body parameters by directly estimating and minimizing the error of image features. Despite the usefulness of parametric models, they can only reproduce the geometry of the naked human body.</p><p>Monocular reconstruction of cloth geometry has been traditionally addressed under the Shape-from-Template (SfT) paradigm <ref type="bibr" target="#b29">(Moreno-Noguer and Fua, 2013;</ref><ref type="bibr" target="#b45">Sanchez et al., 2010;</ref><ref type="bibr" target="#b30">Moreno-Noguer and Porta, 2011;</ref><ref type="bibr" target="#b0">Agudo et al., 2016)</ref>, requiring 3D-to-2D point correspondences between a template mesh and the input. More recently <ref type="bibr" target="#b37">(Pumarola et al., 2018)</ref> introduced a deep network which alleviated the need for estimating correspondences. In any event, the clothes reconstructed by these approaches, were focused to simple rectangular-like shapes, and were not applicable to reconstruct the shape of the garments worn by humans.</p><p>To overcome this limitation, <ref type="bibr" target="#b3">(Alldieck et al., 2019b)</ref> proposes to learn a displacement map on top of the SMPL body model and is able to represent certain type of clothing, short hair details and hands details. However, it fails for more complex topologies such as dresses and skirts and it is limited to mild human body poses (people standing in front of the camera and looking at it). Also others use displacement maps for this purpose <ref type="bibr" target="#b59">(Zhu et al., 2019;</ref><ref type="bibr" target="#b33">Onizuka et al., 2020)</ref>, although mostly from videos or few image frames <ref type="bibr" target="#b2">(Alldieck et al., 2018;</ref><ref type="bibr" target="#b1">Alldieck et al., 2019a)</ref>. In this paper, while we also learn a displacement map, we are capable of capturing dresses and skirts while including a large diversity of body pose.</p><p>To address the limitations of parametric models, template-free methods have been used, some based on voxel representations <ref type="bibr" target="#b51">(Varol et al., 2018;</ref><ref type="bibr" target="#b58">Zheng et al., 2019;</ref><ref type="bibr" target="#b17">Jackson et al., 2018)</ref>, others based on different representations <ref type="bibr" target="#b39">(Pumarola et al., 2019;</ref><ref type="bibr" target="#b43">Saito et al., 2019)</ref>. BodyNet <ref type="bibr" target="#b51">(Varol et al., 2018)</ref> infers the volumetric body shape, although, due to resolution constrains and the use of SMPL as a final fitting, it cannot recover clothing geometry. DeepHuman <ref type="bibr" target="#b58">(Zheng et al., 2019)</ref> uses a volume-to-volume translation approach showing impressive results to capture pose and certain type of clothing, but it fails to correctly capture complex cloth geometry such as skirts and also suffers from high memory requirements of voxel representation, limiting its resolution and requiring and initial estimation of template-based model SMPL. To tackle the resolution limitation of voxels, GimNet <ref type="bibr" target="#b39">(Pumarola et al., 2019)</ref> uses geometry images to represent the body shape and is able to capture complex poses and geometries such as dresses, although with a lack of details. Finally, PIFu  and PI-FuHD <ref type="bibr" target="#b44">(Saito et al., 2020)</ref> use implicit function representation which is memory efficient and results in impressive level of details even for complex cloth geometries and accessories. However, this approach can not generalize to arbitrary human poses. We also use an implicit function representation, but in contrast to previous approaches we are able to capture a large range of arbitrary poses. This is made possible thanks to a first module of our model, which is general enough and reasons in a global manner generating realistic human meshes.</p><p>Finally, most similar in concept but very different implementation from our work, <ref type="bibr" target="#b14">(He et al., 2020)</ref> demonstrate that in order to have a better detailed reconstruction, it is first necessary to have a solid geometric prior which can be learned from a coarse voxel representation of the human body.</p><p>3D datasets. Even though 3D reconstruction has become a popular topic in the field, there are very few publicly available datasets that contain 3D information of human body. Obtaining the 3D body shape is a complex task that requires vast amounts of effort. BUFF dataset <ref type="bibr" target="#b57">(Zhang et al., 2017)</ref> is one of the few that contains high-quality 3D scans, nevertheless, it only includes 6 different subjects and although it has a good human body pose variation, only captures restricted actions. As an alternative, datasets with synthetically photo-realistic images have appeared in the scene <ref type="bibr" target="#b52">(Varol et al., 2017;</ref><ref type="bibr" target="#b39">Pumarola et al., 2019)</ref>. SURREAL <ref type="bibr" target="#b52">(Varol et al., 2017)</ref> is the largest dataset, containing 6 million frames generated by projecting synthetic textures of clothes onto random SMPL body shapes. However, given that clothes are projected onto a naked body model, they are only textures and have no shape of their own, making it impossible to learn clothing details from this dataset. On the contrary, 3DPeople <ref type="bibr" target="#b39">(Pumarola et al., 2019)</ref> contains models of 80 different 3D dressed subjects that perform 70 actions and 2.5 million photorealistic rendered images in which every action sequence is captured by from 4 camera views. For this work we use 3DPeople dataset. Most recently <ref type="bibr" target="#b5">(Caliskan et al., 2020)</ref> announced a similar dataset containing images of synthetic humans and their corresponding 3D human mesh annotations. We don't use this dataset, however, because it has not yet been made public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>We aim to solve the problem of single image 3D reconstruction applied to human bodies with clothing. Our goal is to make sure that not only the inferred pose of the mesh representing the person is correct but also that we recover geometry details of the clothing.</p><p>Let I ? R H?W ?3 be an input RGB image of a single clothed person at an arbitrary pose. Our aim is to learn a mapping M to reconstruct the mesh M which is a detailed 3D representation of the clothed body of the person. We represent M as a mesh with N vertices v i , where v i = (x i , y i , z i ) are the 3D coordinates of each vertex that explains the body of the person in the image, taking into account the body shape, pose and clothing details. We train M in a supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>We next describe our network to generate detailed meshes under complex poses from a single image. Given the high complexity of the task, we use a coarse-to-fine approach and divide our method into two main modules, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>The first module, denoted coarse network, outputs a smoothed mesh M smooth provided an input set of query points p and an observation of the 3D object, the image I. This mesh intentionally lacks the level of detail we are looking for but it is enforced to accurately fit the body pose.</p><p>The second module, which we call displacement network, adds details to the mesh by estimating vertex displacement d i over the direction of the normal vector n i for each vertex v i of M smooth , yielding to M det . For this, we learn a network that takes as inputs I and a set of vertices randomly sampled from M smooth , which we shall denote v smooth .</p><p>It is worth noting that, as an additional input to guide the learning of both networks, we use the 2D joints of the person in I. Next, we explain both networks in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Coarse Network</head><p>Given the input image I, we use J ground truth 2D body joint locations and represent them as heatmaps y ? R H?W ?J . We use J = 17 body joints. This joint representation is then concatenated with I and fed into the network. Additionally, the network has as input a set of query points in the 3D space p xyz = {p i } K i=1 . Our goal is to learn the occupancy probability for each p i given I and y. Formally, we seek to estimate the mapping:</p><formula xml:id="formula_0">M : I ? y, p i ? [0, 1]<label>(1)</label></formula><p>This mapping takes the form of an implicit function and can be learned by a neural network f ? s (p i , I, y). Estimating M to account for high frequency details is, however, significantly challenging for the network, and indeed we found out that training this network to learn details resulted in meshes with incorrect body poses. For this reason, we force it to learn a smoothed version of the occupancy field of the ground truth mesh, hence its name coarse network. To enforce this, instead of using the detailed mesh as ground truth, we train this network with a pseudo ground truth that results from applying Laplacian smoothing <ref type="bibr" target="#b48">(Sorkine et al., 2004)</ref>.</p><p>Finally, at inference, to recover the mesh we first evaluate f ? s (p, I, y) for all p of a discretized volumetric space. We then use an octree based algorithm MISE <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref> and mark each p as occupied if f ? s (p, I, y) is bigger or equal than some threshold ?. After the evaluation is complete, we apply the MC algorithm <ref type="bibr" target="#b23">(Lorensen and Cline, 1987)</ref> to extract and approximate isosurface and estimate the faces topology of M smooth . Note that although we intentionally train f ? s to produce a smooth mesh, the body pose is expected to be correct. Also note that we build on <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref> and, therefore, follow their formulation, however, any other reconstruction model could be used instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Displacement Network</head><p>This network has a similar architecture as the previous one with two main differences: instead of estimating occupancy probability, it regresses the magnitude for displacements d i and takes an additional conditioning value that also serves as a query input, the vertices v smooth . In the same fashion as before, we learn a new encoding for the image and joints representation ? I but we use v smooth to generate a point encoding ? p and concatenate this to ? I . This way we are able to condition the learning of the displacements on I, y and M smooth . This network, denoted h ? d (p, I, y, v), regresses the magnitude of the displacement d i which is then applied to v smooth in the direction of the normals n i of M smooth . This reduces the complexity of the problem by forcing the regressor to learn only a scalar value and not a 3-dimensional vector, helping the network to learn the proper displacements.</p><p>The final result is obtained by adding the learned displacements to the vertices estimated by the first module:</p><p>v</p><formula xml:id="formula_1">det = v smooth + d ,<label>(2)</label></formula><p>where v det are the vertices that correspond to M det and share the same faces as M smooth and d is the estimated displacement over the direction of the normal vector.</p><p>Finally, at inference, to obtain the detailed mesh M det we first evaluate h ? d (p, I, y, v), that in this case are all vertices v smooth . Then, using equation 2 we get the detail vertices for M det .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning the Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Smooth Reconstruction Loss</head><p>To learn the parameters ? s of the neural network f ? s (p, I, y), we randomly sample points in the 3D bounding volume of the mesh representing the person. We sample these points in three ways: (a) uniformly over the bounding volume, (b) densely over the face and hands, and (c) densely over the surface. For b and c we sample several points (much more than a) near the surface of the mesh, that is why we say it is a dense sampling. To automatically obtain sampling points for face and hands we only sample points within a radius r of a sphere centered at the 3D joints corresponding to hands and face. We found that hands and face require higher level of detail to be better reconstructed than feet, hence, we do not include sampling specifically corresponding to feet. For each sample image i in a training batch we sample K points p i j ? R 3 , j = 1, ..., K. The minibatch loss L B is then is evaluated at those locations:</p><formula xml:id="formula_2">L B (? s ) = 1 B |B| ? i=1 K ? j=1 L( f ? s (p i j , I, y), o i j ) ,<label>(3)</label></formula><p>where o i j ? o(p i j ) denotes the true occupancy at point p i j , and |B| is the minibatch size. The loss L(?,?), different from <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref>, is a weighted binary cross-entropy (wBCE) classification loss that takes into account the unbalanced number of points that lay inside the mesh in contrast to those that are outside. This avoids losing important body parts, especially the limbs, when extracting the mesh.</p><p>In a similar fashion as in <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref> we also introduce a generative loss that helps us capture the rich distribution of complex clothing. We do this by adding an encoder network g ? (?) that takes as inputs the points and occupancies to predict the mean u ? and standard deviation ? ? of a Gaussian distribution q ? (z|(p i j , o i j ) j=1:K ) on a latent space z ? R L as output and then optimizing the KL divergence. This way, the new loss becomes:</p><formula xml:id="formula_3">L B (?) = 1 B |B| ? i=1 [ K ? i= j L( f ? s (p i j , I, y), o i j )+ KL(q ? (z|(p i j , o i j ) j=1:K )||p 0 (z))]<label>(4)</label></formula><p>where p 0 (z) is a prior distribution on the latent variable z i and z i is sampled according to q ? (z|(p i j , o i j ) j=1:K ). We train this as a conditional variational autoencoder <ref type="bibr" target="#b47">(Sohn et al., 2015)</ref>.</p><p>To generate M smooth we use a hierarchical isosurface extraction algorithm <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref>, that incrementally builds an octree to efficiently obtain a high resolution mesh, that is then forwarded to the second stage of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Displacement Loss</head><p>In order to learn the parameters ? d of the neural network h ? d (p, I, y, v), in a similar manner as we did with the coarse network, we randomly sample N points p i j from v smooth and evaluate the minibatch loss L B . Yet, instead of using a wCBE loss, we use an L2 loss:</p><formula xml:id="formula_4">L D (? d ) = 1 B |B| ? i=1 N ? j=1 f ? d (p i j , I, y, v smooth ) ? d i j 2 (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION DETAILS</head><p>Our model builds upon the ONet network architecture <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref>. For the coarse network f ? s (p, I, y) we use 5 ResNet blocks <ref type="bibr" target="#b13">(He et al., 2016)</ref> which are conditioned on the input using conditional batch normalization <ref type="bibr" target="#b16">(Ioffe and Szegedy, 2015)</ref>. For the image and joint encoding we use a ResNet18 architecture. For the displacement network we modify the architecture by adding 5 ResNet blocks (yielding to a total of 10 blocks) and changing the last layer to regress the displacement value. We observed that for less amount of layers, the network is not able to capture the complexities of clothes and other details. It is important for the network to understand the 3D structure of the body in order to regress the desired displacements, for this reason we also modify the conditioning input of the architecture to be able to include mesh vertices as a prior. For this we use a similar encoder as in PointNet <ref type="bibr" target="#b40">(Qi et al., 2016)</ref> and for the network we use 10 ResNet blocks. We plan to release our code.</p><p>The model is trained with 60,000 synthetic images of cropped clothed people resized to 224 x 224 pixels as needed by the image encoder, however, this resolution could be easily changed. These images correspond to 15,000 different meshes of varying number of vertices taken from the 3DPeople dataset <ref type="bibr" target="#b39">(Pumarola et al., 2019)</ref> and projected to 4 camera views. We use 44 subjects out of 80 to reduce training time.</p><p>In order to train f ? s we generate occupancy annotations, i.e determine which points lie in the interior of the mesh. This step requires a watertight mesh. To do this we use code provided by <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref>. We train the coarse network during 645 epochs, K=2048 and Adam <ref type="bibr" target="#b20">(Kingma and Ba, 2014)</ref> optimizer with initial learning rate of 1e ? 4, beta1 0.9, beta2 0.999. For weighted-BCE we use a positive weight of 25. For reconstructing the mesh, we use a threshold parameter ?=0.96 for all cases. For this network to better capture complex poses, we first normalize each mesh w.r.t. three points: hips, upper left leg and upper right leg.</p><p>To train h ? d (p, I, y, v) we generate ground truth data using the results obtained from our coarse network and compute the displacement over the normal by first densely sampling the surface of the ground truth mesh and then finding the distance over the normal direction from a mesh vertex to the nearest point in the ground truth mesh. We train during 1700 epochs with batch size 14, K=2,048 and N=10,000. As for the optimizer we use Adam <ref type="bibr" target="#b20">(Kingma and Ba, 2014)</ref> with initial learning rate of 1e ? 4, beta1 as 0.9, beta2 as 0.999. At epoch 170 we change the learning rate to 1e ? 5 and, again, at epoch 1,200 to 1e ? 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>This section provides an evaluation of our proposed method. We present quantitative and qualitative results on synthetic images from 3DPeople <ref type="bibr" target="#b39">(Pumarola et al., 2019)</ref> and qualitative results on images in the wild. We evaluate our approach on 3,200 images randomly chosen for 5 subjects (2 female/ 3 male) from <ref type="bibr" target="#b39">(Pumarola et al., 2019)</ref>.</p><p>We compare our approach quantitatively (see <ref type="table" target="#tab_0">Table 1</ref>) with other two prominent implicit function models for 3D reconstruction, namely, Occu-pancyNets (ONets) <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref> and PiFu . Note that to ensure fair comparison both a re-trained with the same training data as our model and we test all models with the same test set as ours. Although one could argue that numerical comparison with SOTA should include PI-FuHD <ref type="bibr" target="#b44">(Saito et al., 2020)</ref>, this was not possible as the authors have not released the training code. However, we believe that the methods in question are good representatives of powerful implicit function models for 3D reconstruction. In this sense, being ONet a good candidate for global consistency models and PIFu for hi-detail local consistent models. Qualitative comparison with both these methods on synthetic images can be found in <ref type="figure">Fig. 4</ref>.</p><p>Additionally, in <ref type="table" target="#tab_1">Table 2</ref> we present a quantitative ablation study to validate all the components propose in this paper and used by our final method. The table compares our method and several baselines built upon the Occupancy Net <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref> and the losses we have defined in our system. <ref type="table" target="#tab_1">Table 2</ref> reports the errors for all methods and shows that our approach consistently improves all baselines. Also, notice how the addition of the wBCE and KL losses over the ONet baseline, gracefully reduce the errors.</p><p>As evaluation metrics we use volumetric IoU, Chamfer distance (CD), normal consistency score and point to surface score (P2S). Volumetric IoU is defined as the quotient of the volume of the two meshes union and the volume of their intersection. We use the same procedure as in <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref> to obtain this value. We calculate the CD by randomly sampling 100,000 points from both the watertight ground truth and the estimated meshes. We <ref type="figure">Figure 2</ref>: Comparison between the baselines used on 3DPeople. Baseline is <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref> retrained on 3DPeople and subsequent columns are results of added components to that model validated in <ref type="table" target="#tab_1">Table 2</ref>. The figure displays the reconstructed meshes from the camera viewpoint. The color of the meshes encodes the normal directions of the surface. Note how our approach captures the global consistency of the mesh, as the previous, and additionally presents certain clothing details. define a normal consistency score as the mean absolute dot product of the normals in one mesh and the normals at the corresponding nearest neighbors in the other mesh. As in , we measure the average point-to-surface Euclidean distance (P2S) in cm from the vertices on the reconstructed surface to the ground truth. <ref type="figure">Fig. 2</ref> shows three samples of the meshes reconstructed with each of the baselines and our final method. Regarding the three ONet baselines, note how the introduction of the losses tend to produce better reconstructions, although the sharper geometry details are more evident in our approach <ref type="figure">(Fig. 2(ours)</ref>), which includes all previous losses plus the refinement of the geometry estimated with the displacement network. Also the effect of other components of our model and the proposed training scheme if depicted qualitatively in <ref type="figure">Fig. 3</ref>.</p><p>Qualitative comparison on synthetic and real images is also presented. <ref type="figure">Fig. 4</ref> presents sample synthetic images from our test set that none of the models have seen before. Here we present results of our method along with ONet and PIFu, note that all are re-trained with 3DPeople dataset. As shown in <ref type="figure">Fig. 4</ref> one can note that PiFu if capable of reconstructing in a very acceptable manner all the front-view parts of the meshes, however, it fails to give a global consistency to the mesh. This can be seen in the columns depicting the side-view. We argue that this is due to PIFu's heavy reliance on local aligned features. Also we argue that PIFu is penalized by the relatively lowresolution of the input images, whereas our methods in not that sensitive to low-resolution failures. Additionally, since the 2D joints are not exploited by PiFu, the structure of the body it produces is not always consistent. Qualitative comparison with other SOTA <ref type="figure">Figure 3</ref>: Visual ablation study. Note that these are outputs of the coarse network so they lack finer details. (a) Difference in reconstructions when using 2D joints as inputs vs. not using them. (b) Effect of enforcing smoothness. Green meshes are ground truth, pink ones are reconstructions. Here we present reconstruction when the network tries to learn the detailed ground truth mesh vs. reconstruction when forcing coarse network to learn a smooth version of the ground truth. (c) Impact of using a generative loss and "dense" sampling, meaning, sampling more heavily on the surface of the mesh. Adding generative loss helps to capture the richness of the 3D shape distribution. Here we can see that by adding KL loss and then sampling near the surface, especially around face and hands (dense), we obtain better results both in hands, face and skirts.(+KL=wBCE+KL, +Dense=wBCE+KL+Dense) methods on real images can be found in <ref type="figure">Fig. 5</ref>. Here we compare our method with <ref type="bibr" target="#b44">Saito et al., 2020;</ref><ref type="bibr" target="#b14">He et al., 2020)</ref>. Note that non of these methods nor ours have been train with real images and inference, in this case, is done with the trained weights provided by the authors of each method. All PIFu methods, except Geo-PIFu (to a lesser extent) show the same problem addressed before: shockingly good front views, however lacking global consistency and human body coherence. Geo-PIFu works better in these cases as this model specifically aims for global coherence just as our method does.</p><p>Impact of using 2D joints. We found out that using 2D joints as additional input to our model improves the reconstruction quality. By adding joint information we prevent the network from generating incomplete human bodies, especially in cases where the image presents self-occlusions (see <ref type="figure">Fig. 3(a)</ref>).</p><p>Impact of enforcing smoothness. As stated before, we enforce the coarse network to learn a smooth version of the ground truth mesh. This reliefs the network from learning a more complex mapping to account for high-level details which has an impact on the correctness of the reconstructed human pose. As shown in <ref type="figure">Fig. 3(b)</ref>, one can clearly see that when we do not enforce to learn a smooth version of the mesh, the pose deviates considerably from the ground truth.</p><p>Impact of using a generative model. The use of generative loss (equation 4) helps the model to better capture the richness and variability of the distribution of human clothing and body details such as hands and face. As it can be seen in <ref type="figure">Fig. 3(c)</ref>, when adding the KL loss term to the model the skirt and hands are better reconstructed. Moreover, this is improved when combining this with the dense sampling strategy that was mentioned before.</p><p>Impact of dense sampling. When combined with the KL loss, the dense sampling strategy (near surface and around face and hands) helps the model to better capture the correct structure of clothing and human body. In the case of <ref type="figure">Fig. 3(c)</ref>, we show how adding this sampling strategy results in better hands and skirts. Although not shown here, we also observed slight improvement in the face area.</p><p>Real images. We finally show in Figures 4 and 5 the reconstructed shapes on synthetic and real images, respectively. Note, specially in the synthetic examples, how we are able to capture very complex body poses together with the details of the clothing (e.g. skirts). Also, note that for test and real images (given that we do not have ground truth for 2D joints) we use an off-the-shelf 2D pose detector such as . Another alternative is to use <ref type="bibr" target="#b42">(Rong et al., 2021)</ref> and get all necessary joints by projecting them into the 2D space.</p><p>Although we get good results on real images, it can be perceived, in some cases, that the results are not as good as on synthetic ones. We hypothesize that this is due to a slight difference in appearance of real images in contrast to synthetic ones, especially due to lighting conditions, shadows and color. It is known that there is domain gap between real and synthetic images. We believe that by training with real images or paying more attention to the photo-realism of synthetic images we would get even better results. While we are able to capture skirts, where most of other methods fail, there is still room for improvement. However, we believe that combining global reasoning with a refinement step to add details is the right direction to obtain coherent human meshes in a wide range of poses with high enough detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper we have made the following contributions to the problem of reconstructing the shape of dressed humans. As far as we can tell we are the first ones to do 3D reconstruction of clothed human body from single image in a wide range of poses including complex ones. In doing so, we do not require high resolution images. We demonstrate that different sampling schemes can improve the details with im- <ref type="figure">Figure 4</ref>: Results on synthetic images of the 3DPeople dataset. For every row we display the input RGB image and the mesh reconstructed using our approach and comparative approaches seen from two different viewpoints, Onets <ref type="bibr" target="#b26">(Mescheder et al., 2019)</ref> and PIFu . The color of the meshes encodes the normal directions of the surface. <ref type="figure">Figure 5</ref>: Qualitative results of our approach on real images. We compare with PIFu , PIFuHD <ref type="bibr" target="#b44">(Saito et al., 2020)</ref> and Geo-PIFu <ref type="bibr" target="#b14">(He et al., 2020)</ref> plicit function representation. Finally, we are able to capture details such as dresses and skirts while main-taining consistency of the body from all directions and not only the observed view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work is supported by the Spanish government with the projects MoHuCo PID2020-120049RB-I00 and Mar?a de Maeztu Seal of Excellence MDM-2016-0656.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our pipeline to reconstruct clothed people under complex poses. Given an input RGB image, an implicit function-based network initially predicts a smoothed version of the geometry, but with an accurate body pose. The fine details of the mesh are recovered by a second network that computes a displacement field over the smooth mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative evaluation on 3DPeople. Numerical comparison of our approach with other methods that use implicit functions retrained with the same data as ours. We measure IoU, Chamfer distance, Normal Consistency and Point to Surface (see main text) to validate the different components of our model. ?: higher the better. ?: lower the better.</figDesc><table><row><cell cols="3">Method IoU ? Chamfer ?</cell><cell cols="2">Normal Consistency ? P2S ?</cell></row><row><cell>ONet</cell><cell>0.516</cell><cell>0.280</cell><cell>0.793</cell><cell>18.135</cell></row><row><cell>PiFu</cell><cell>0.244</cell><cell>1.550</cell><cell>0.601</cell><cell>70.200</cell></row><row><cell>Ours</cell><cell>0.610</cell><cell>0.100</cell><cell>0.821</cell><cell>16.200</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative ablation study on 3DPeople dataset. Note that dense sampling denotes sampling strongly in the surface of the mesh (meaning several more points than in uniform sampling)</figDesc><table><row><cell>Components</cell><cell>Metrics</cell><cell></cell></row><row><cell cols="2">Occupancy wBCE KL Joints Uniform Samp. Dense Samp. Displacement CD ? IoU? Normal Consistency?</cell><cell>P2S?</cell></row><row><cell>2.752 0.516</cell><cell>0.793</cell><cell>18.135</cell></row><row><cell>1.689 0.576</cell><cell>0.808</cell><cell>18.698</cell></row><row><cell>1.496 0.579</cell><cell>0.811</cell><cell>18.353</cell></row><row><cell>1.422 0.579</cell><cell>0.814</cell><cell>18.265</cell></row><row><cell>1.051 0.612</cell><cell>0.829</cell><cell>16.397</cell></row><row><cell>1.082 0.606</cell><cell>0.821</cell><cell>16.200</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time 3d reconstruction of non-rigid shapes with a single moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Montiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="37" to="54" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to reconstruct people in clothing from a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3d people models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tex2shape: Detailed full human body geometry from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Graph</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view consistency loss for improved single-image 3d reconstruction of clothed people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Imre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Openpose: Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">T-PAMI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d-R2n2: A Unified Approach for Single and Multi-view 3d Object Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human shape from silhouettes using generative hks descriptors and cross-modal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dibra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>?ztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Point Set Generation Network for 3d Object Reconstruction from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local deep implicit functions for 3d shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mesh r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9785" to="9795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geometry and pixel aligned implicit functions for single-view human reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geo-Pifu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (Conferenceon Neural Information Processing Systems (NeurIPS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift. International Conference on International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d Human Body Reconstruction from a Single Image via Volumetric Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manafas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end Recovery of Human Shape and Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structured output prediction and learning for deep monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kinauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<editor>Pelillo, M. and Hancock, E.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="34" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unite the People: Closing the Loop Between 3d and 2d Human Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SMPL: a skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Marching cubes: A high resolution 3d surface construction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Singleshot multi-person 3D pose estimation from monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV 2018 , International Conference on 3D Vision</title>
		<meeting><address><addrLine>Verona, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Occupancy Networks: Learning 3d Reconstruction in Function Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on International Conference on Computer Vision (IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic exploration of ambiguities for nonrigid shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="463" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Probabilistic simultaneous pose and non-rigid shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Porta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (IEEE Conference on Computer Vision and Pattern Recognition (CVPR))</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (IEEE Conference on Computer Vision and Pattern Recognition (CVPR))</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1289" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Siclope: Silhouette-based clothed people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tetratsdf: 3d human reconstruction from a single image with a tetrahedral outer shell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Onizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayirci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6011" to="6020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to Estimate 3d Human Pose and Shape from a Single Color Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Geometry-aware network for non-rigid shape prediction from a single view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">C-flow: Conditional generative flow models for images and 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3dpeople: Modeling the geometry of dressed humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez-Riera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<title level="m">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Frankmocap: Fast monocular 3d hand and body motion capture by regression and integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simultaneous pose, correspondence and nonrigid shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>?stlund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (IEEE Conference on Computer Vision and Pattern Recognition (CVPR))</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (IEEE Conference on Computer Vision and Pattern Recognition (CVPR))</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1189" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conferenceon Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Laplacian surface editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?ssl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, SGP &apos;04</title>
		<meeting>the 2004 Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, SGP &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bodynet: Volumetric inference of 3d human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3d human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d Mesh Models from Single RGB Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Marrnet: 3d shape reconstruction via 2.5 d sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="540" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">DISN: Deep Implicit Surface Network for High-quality Single-view 3d Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conferenceon Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Detailed, accurate, human shape estimation from clothed 3D scan sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deephuman: 3d human reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Detailed human shape estimation from a single image by hierarchical mesh deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
