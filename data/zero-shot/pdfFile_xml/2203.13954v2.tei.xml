<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of Human-Object Interaction (HOI) detection could be divided into two core problems, i.e., human-object association and interaction understanding. In this paper, we reveal and address the disadvantages of the conventional query-driven HOI detectors from the two aspects. For the association, previous two-branch methods suffer from complex and costly post-matching, while single-branch methods ignore the features distinction in different tasks. We propose Guided-Embedding Network (GEN) to attain a twobranch pipeline without post-matching. In GEN, we design an instance decoder to detect humans and objects with two independent query sets and a position Guided Embedding (p-GE) to mark the human and object in the same position as a pair. Besides, we design an interaction decoder to classify interactions, where the interaction queries are made of instance Guided Embeddings (i-GE) generated from the outputs of each instance decoder layer. For the interaction understanding, previous methods suffer from long-tailed distribution and zero-shot discovery. This paper proposes Visual-Linguistic Knowledge Transfer (VLKT) training strategy to enhance interaction understanding by transferring knowledge from a visual-linguistic pre-trained model CLIP. In specific, we extract text embeddings for all labels with CLIP to initialize the classifier and adopt a mimic loss to minimize the visual feature distance between GEN and CLIP. As a result, GEN-VLKT outperforms the state of the art by large margins on multiple datasets, e.g., +5.05 mAP on HICO-Det. The source codes are available at https://github.com/YueLiao/gen-vlkt.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-Object Interaction (HOI) detection is a significant task to make a machine understand human activities in a static image at a fine-grained level. In this task, human activities are represented as a series of HOI triplets &lt;Human, Object, Verb&gt;, so an HOI detector is required to localize human and object pairs and recognize their interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Init</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HOI Text Embeddings</head><p>Object Text Embeddings Init (a) Two-branch Decoders of GEN (b) VLKT <ref type="figure">Figure 1</ref>. Our GEN-VLKT pipeline. We propose GEN, a querybased HOI detector with two-branch decoders, where we design a guided embedding association mechanism to replace the traditional post-matching process for simplifying the association. Moreover, we devise a training strategy VLKT, where we transfer knowledge from the large-scale visual-linguistic pre-trained model CLIP to enhance interaction understanding.</p><p>The core problems of HOI detection are to explore how to associate the interactive human and object pairs and understand their interactions. Thus, we consider improving the HOI detector from the two aspects and design a unified and superior HOI detection framework. We first revisit the efforts conducted by traditional methods in such two aspects.</p><p>For the association problem, it can be mainly divided into two paradigms, i.e., bottom-up and top-down. Bottomup methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref> detect humans and objects first and then associate humans and objects through a classifier or a graph model. Top-down methods usually design an anchor to denote the interaction, e.g., interaction point <ref type="bibr" target="#b24">[25]</ref> and queries <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>, and then find the corresponding human and object through pre-defined associative rules. Benefiting from the development of visual transformer, query-based methods are leading the performance of HOI detection, which are mainly two streams, i.e., twobranch prediction-then-matching manner <ref type="bibr" target="#b3">[4]</ref> and singlebranch directly-detection manner <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48]</ref>. The two-branch manner predicts interaction then matches with human and object, struggling with designing effective matching rules and complicated post-processing. The single-branch manner proposes to detect the human, object and the corresponding interaction based on a single query with multiple heads in an end-to-end manner. However, we argue that the three tasks, i.e., human detection, object detection and interaction understanding, exist significant differences in feature representation, where human and object detection mainly focus on the features in their corresponding regions, while interaction understanding attends human posture or context.</p><p>To improve this, as shown in <ref type="figure">Figure 1a</ref>, we propose to keep the two-branch architecture while removing the complicated post-matching. To this end, we propose Guided Embedding Network (GEN), where we adopt an architecture of a visual encoder followed by two-branch decoders, i.e., instance decoder and interaction decoder, and design a guided embedding mechanism to guide the association beforehand. The two branches are both with a query-based transformer decoder architecture. For the instance decoder, we design two independent query sets for human and object detection. Further, we develop a position Guided Embedding (p-GE) to distinguish different human-object pairs by assigning the human query and object query at the same position as a pair. For the interaction decoder, we devise an instance Guided Embedding (i-GE), where we generate each interaction query guided by specific human and object queries to predict its HOIs. Hence, GEN can allow different features for different tasks and guide the association during network forward while without post-matching.</p><p>For the interaction understanding problem, most conventional methods directly apply a multi-label classifier fitted from the dataset to recognize the HOIs. However, such paradigms suffer from the long-tailed distribution and zeroshot discovery due to the complicated human activities with various interactive objects in realistic scenes. Though recent methods propose to alleviate such problems with dataaugmentation <ref type="bibr" target="#b14">[15]</ref> or carefully designed loss <ref type="bibr" target="#b45">[46]</ref>, the performance gain and extension ability are restricted to the limited training scale due to the expensive HOI annotation. We might as well set our sights on image-text data, which can be easily obtained from the internet, while we can naturally convert HOI triplets into text descriptions. Thanks to the development of visual-linguistic pre-trained models, this becomes possible. Especially, CLIP <ref type="bibr" target="#b30">[31]</ref> establishes a strong visual-linguistic model trained on about 400 million imagetext pairs and shows its powerful generalization ability on about 30 tasks. Thus, CLIP can cover most HOI scenes in real life and bring a new idea to understand the HOIs.</p><p>To improve this, as shown in <ref type="figure">Figure 1b</ref>, we design a Visual-Linguistic Knowledge Transfer (VLKT) training strategy to transfer the knowledge from CLIP to the HOI detector to enhance interaction understanding without additional computation cost. We consider two main problems in our VLKT. On the one hand, we design a text-driven classi-fier for prior knowledge integration and zero-shot HOI discovery. In detail, we first covert each HOI triplet label into a phrase description, then extract their text embeddings based on the text encoder of CLIP. Finally, we apply the text embeddings of all HOI labels to initialize the weight of the classifier. In this manner, we can easily extend a novel HOI category only by adding its text embedding into the matrix. Meanwhile, we also adopt the CLIP-initialized object classifier for novel object extension. On the other hand, for textdriven classifier and visual feature alignment, we present a knowledge distillation method to guide the visual features of HOI detection to mimic the CLIP features. Therefore, based on VLKT, the model can well capture information from CLIP and easily extend to novel HOI categories without extra cost during inference.</p><p>Finally, we propose a novel unified HOI detection framework GEN-VLKT based on the above two designs. We have verified the effectiveness of our GEN-VLKT on two representative HOI detection benchmarks, i.e., HICO-Det <ref type="bibr" target="#b29">[30]</ref> and V-COCO <ref type="bibr" target="#b9">[10]</ref>. Our GEN-VLKT has significantly improved the existing methods on both two benchmarks and achieved state-of-the-arts on the zero-shot settings of the HICO-Det dataset. Specifically, our GEN-VLKT has achieved a 5.05 mAP gain on HICO-Det and a 5.28 AP promotion on V-COCO compared with the previous stateof-the-art method QPIC <ref type="bibr" target="#b33">[34]</ref>. It also promotes performance impressively by a 108.12% relative mAP gain for unseen object zero-shot setting compared to the previous state-ofthe-art method ATL <ref type="bibr" target="#b14">[15]</ref>.  <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref> first detect all humans and objects and then associate the human-object pairs and infer their HOI types through an additional classifier. These methods are usually organized as a two-stage paradigm and worked on improving the second stage. Recently, some graph-based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref> have achieved satisfactory performance. However, bottom-up methods suffer from expensive computation consume due to its serial architecture for handling a large number of humanobject pairs. To alleviate this issue, top-down methods become popular in recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref>. Topdown methods mainly design an additional anchor to associate humans and objects, and predict their interactions. The interaction anchor is from the early interaction point <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref> and union box <ref type="bibr" target="#b17">[18]</ref> to recent interaction query <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref> with the development of visual transformers. Recently, CDN <ref type="bibr" target="#b42">[43]</ref> proposed a one-stage method with a cascade decoder to mine the benefits of the two-stage and one-stage HOI detectors. Our GEN is different from CDN in the three aspects. 1) Organization of the decoder: GEN is with a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><formula xml:id="formula_0"># ' ( # ' % # ) ( # ) % $ ' * $ ) * $ + * ! ! " ! # " ? ! $! "</formula><p>Human boxes</p><formula xml:id="formula_1">! ! % ! # % ? ! $! % Object boxes " ! " " # " ? " $! " Object scores # + ( # + % + " ! &amp; " # &amp; ? " $! &amp;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction scores</head><p>Object Queries:  <ref type="figure">Figure 2</ref>. The framework of our GEN. The GEN is organized as a visual encoder equipped with two-branch decoders architecture. Given an image, the visual encoder is first applied to extract the visual features. Then, two branches, i.e., instance decoder and interaction decoder, are used to localize human-object pairs and classify HOI triplets based on learnable queries, respectively. Besides, we design a position Guided Embedding (p-GE) to associate the interactive human and object and an instance Guided Embedding (i-GE) to make the interaction query predict the corresponding HOI categories under the guidance of specific human and object queries. two-branch pipeline and the instance and interaction decoder forward together, while CDN disentangles the HOI detection into two decoders in a serial manner. 2) Instance query design: GEN adopts two isolated human and object queries with positional embedding, while CDN entangles human and object into a unified instance query. 3) motivation: GEN aims to replace the complex post-process with a guided learning manner, while CDN aims to mine the benefits of one-and two-stage detectors. Zero-shot HOI detection. Zero-shot HOI detection <ref type="bibr" target="#b31">[32]</ref> tends to detect unseen HOI triplet categories in the training data. Many methods <ref type="bibr">[1, 12, 14-16, 27, 29, 32, 35, 41]</ref> are investigated to handle zero-shot HOI detection. In detail, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref> factorized the human and object features by disentangled reasoning on verbs and objects and then produced novel HOI triplets during inference. VCL <ref type="bibr" target="#b13">[14]</ref> composed novel HOI samples by combining decomposed object and verb features with pair-wise images and within images. FCL <ref type="bibr" target="#b15">[16]</ref> presented an object fabricator to generate fake object representations for rare and unseen HOIs. ATL <ref type="bibr" target="#b14">[15]</ref> explored object affordances from additional object images to discover novel HOI categories. ConsNet <ref type="bibr" target="#b26">[27]</ref> explicitly encoded the relations among objects, actions and interactions into an undirected graph to propagate knowledge among HOI categories as well as their constituents. The visual-linguistic models <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref> transferred the seen visual phrase embeddings with prior language knowledge to unseen HOIs. HOI Detection with Visual-linguistic Model. An HOI triplet can be regarded as a structural linguistic description, so integrating linguistic prior knowledge into HOI detection is a natural idea. Most conventional methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45]</ref> focus on mining the inductive bias as a pair-wise frequency prior manner or just utilizing the word embeddings as additional features. Recently, large-scale pre-trained visuallinguistic models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref> have shown their powerful performances and generalization abilities in many visual or cross modality tasks. Recently, DEFR <ref type="bibr" target="#b16">[17]</ref> proposed to adopt a visual-linguistic model, CLIP, to initialize the classier for the HOI recognition task. We aim to explore applying visual-linguistic pre-trained model for interaction understanding.</p><formula xml:id="formula_2">! ! $ ,"&amp;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we aim to explore the solutions for the two problems of HOI detection, i.e., association and interaction understanding. We first present a detailed introduction of our one-stage two-branch HOI detector with a simple association mechanism, namely Guided Embedding Network (GEN), in Sec 3.1. We then introduce a Visual-Linguistic Knowledge Transfer (VLKT) training strategy with the large-scale visual-linguistic pre-trained model CLIP to enhance interaction understanding in Sec 3.2. Finally, we show the training and inference pipelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Guided Embedding Network</head><p>In this subsection, we introduce the architecture of our Guided Embedding Network (GEN). As shown in <ref type="figure">Figure 2</ref>, the GEN is organized as an encoder followed by two-branch decoders architecture. We first adopt a CNN equipped with a transformer encoder architecture as the visual encoder to extract sequenced visual features V e . Then, we apply twobranch decoders, i.e., instance decoder and interaction decoder, to detect HOI triplets. In the instance decoder, based on V e , we detect humans and objects through the human query set Q h and the object query set Q o individually. Additionally, we design a position Guided Embedding (p-GE) P q to assign the human and object queries at the same position as a pair. In the interaction decoder, we first dynamically generate the interaction queries Q a i for each interaction decoder layer by computing the mean of the outputs of human and object queries in the corresponding instance decoder layer. Therefore, the interaction decoder can predict the corresponding HOI categories under the guidance of human and object queries. Finally, the HOI prediction results are generated by the output of decoders. Visual Encoder. We follow the query-based transformer detectors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref> to adopt a CNN-transformer combined architecture for the visual encoder. Taking an image I as input, a CNN is first utilized to extract low-resolution visual features V cnn ? R H ?W ?C . Then, we reduce the channels of visual features to C e and flatten the size of the features to (H ? W ) ? C e . Finally, we feed the reduced features adding a cosine positional embedding into a transformer encoder, and extract sequenced visual features V e ? R (H ?W )?Ce for the following tasks. Two-branch Decoders.</p><p>The decoders in the two branches share the same architecture, where we follow the transformer-based detectors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> to adopt the query-based transformer decoder framework. First, we feed a set of learnable queries Q ? R Nq?Cq , the output of last layer, the visual features V e and the positional embedding to N transformer decoder layers, and output the updated queries after the self-attention and co-attention operations. Then with separate FFN heads, the queries are transformed to embeddings for its dedicated task, i.e., instance and interaction representations by the first and second decoder branch, respectively.</p><p>For the instance decoder, we first initialize two sets of queries to detect human and object <ref type="bibr" target="#b4">[5]</ref> where we denote human and object query sets as Q h ? R Nq?Cq and Q o ? R Nq?Cq separately. Then, we design an additional learnable position Guided Embedding (p-GE) P q ? R Nq?Cq for the two query sets to assign the human query and object query at the same position as a pair, where we add the P q to Q h and Q o , respectively. Finally, we generate the query set for the instance decoder by concatenating the added queries:</p><formula xml:id="formula_3">Q ins = [Q h + P q , Q o + P q ],<label>(1)</label></formula><p>where Q ins ? R 2Nq?Cq . We feed Q ins forward the instance decoder to predict the human-object bounding-box </p><formula xml:id="formula_4">pairs (b h i , b o i , s o i ), where b h i ? B h , b o i ? B o and s o i ? S o denote</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLIP Text Encoder</head><formula xml:id="formula_5">? ? w $ % w &amp; % w ' % w ( ( % Interaction Decoder ? ? v $ % v &amp; % v ' % v ( ) % Interaction Classifier ? + Interaction Queries</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Features</head><p>Weight Initialization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLIP Image Encoder</head><p>Pooling <ref type="figure">Figure 3</ref>. VLKT for the interaction decoder. We first covert each HOI label into a phrase description and extract its text embedding based on CLIP text encoder, then apply the text embeddings of all HOI labels to initialize the classifier. Finally, we adopt the CLIP image encoder to extract visual features to guide the interaction visual feature learning. The dotted arrow denotes no gradient.</p><formula xml:id="formula_6">Mimic ? % $ % % &amp; % % ' % % ( ) % Interaction Scores &amp; v )*+,</formula><p>fore, this branch is required to associate interaction query with the human-object query pairs and classify interaction.</p><p>Here, we introduce an instance Guided Embedding (i-GE) method for the association, and the new interaction classification manner will be introduced in the next subsection. Instead of conventional learnable embedding with random initialization, we dynamically generate i-GE as the interaction queries to guide the interaction query to match the human and object queries. In this manner, we generate i-GE under the guidance of the middle visual features</p><formula xml:id="formula_7">[V h , V o ].</formula><p>Specifically, for the input of k-th layer interaction decoder, the interaction queries Q a k is computed by the outputs of k-th layer instances decoder:</p><formula xml:id="formula_8">Q a k = (V h k + V o k )/2.<label>(2)</label></formula><p>In this way, the k-th layer interaction decoder takes the interaction queries Q a k as input, and return the immediate decoded features V a k and HOI categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visual-Linguistic Knowledge Transfer</head><p>In this subsection, we detailedly introduce the training pipeline of the instance decoder and the interaction decoder transferring knowledge from the large-scale visuallinguistic pre-trained model CLIP <ref type="bibr" target="#b30">[31]</ref>, namely Visual-Linguistic Knowledge Transfer (VLKT). In VLKT, inspired by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17]</ref>, we first adopt the CLIP text embeddings to classify interactions and objects. We then introduce how to transfer the visual knowledge from CLIP image embedding to the interaction decoder. We present the pipeline of interaction decoder training with VLKT in <ref type="figure">Figure 3</ref>. Text Embedding for Classifier Initialization. To generate the CLIP text embedding, we first convert HOI triplet labels and object labels into text descriptions. For example, given an HOI triplet &lt;Human, Object, Verb&gt;, we generate the corresponding description following such format 'A photo of a person [Verb-ing] a/an [Object]'. In addition, the 'no-interaction' type is represented as 'A photo of a person and a/an [Object]'. As for an object label, we transform it into the phrase 'A photo of a/an [Object]'. Then, we generate the text embedding for each HOI and object text label through the pre-trained CLIP text-encoder offline. Finally, the text embedding set of HOI labels E a ? R ct?Na and object labels E o ? R ct?No are obtained, where N a and N o denote the number of HOI triplet categories and object categories, respectively, and c t represents the dimension of text embedding from CLIP text encoder.</p><p>After obtaining the text embeddings, we aim to classify the interaction and object under the guidance of the prior knowledge from such text embeddings. The intuitive idea is to adopt such embeddings to initialize the weight of the learnable classifier and fine-tune the classifier with a small learning rate to fit a specific dataset. In this way, each output query feature is computed cosine similarity with all finetuned text embeddings and returned a similarity score vector during the classification process. Specifically, we denote the interaction classifier and object classifier as C a ? E a and C o ? E o , respectively. Taking interaction classifier C a = [w a 1 , w a 2 , ..., w a Na ] as an example, given an output interaction query v a i , we compute the similarity score by:</p><formula xml:id="formula_9">s a i = ? [sim (v a i , w a 1 ) , sim (v a i , w a 2 ) , ? ? ? , sim (v a i , w a Na )] (3)</formula><p>where sim denotes the cosine similarity operation, for example sim (v a i , w a 1 ) = (v a i ? w a 1 )/( v a i w a 1 ), and ? is a logit scale factor following CLIP <ref type="bibr" target="#b30">[31]</ref>. The object classification scores can be got in the same way. Otherwise, we follow <ref type="bibr" target="#b33">[34]</ref> to apply the focal loss and cross-entropy loss to train the interaction and object classifier, respectively. Visual Embedding Mimic. CLIP is trained on image-text pair data, and it aligns visual embedding and text embedding into a unified space. We design a Visual Embedding Mimicking mechanism to pull the interaction feature into such unified space by pulling the distance between the interaction feature and CLIP visual embedding. Here, CLIP serves as the teacher, and the interaction decoder plays the student's role. We design the knowledge distillation strategy from the global image level, because CLIP image encoder is built upon a whole image. We first feed the resized and cropped image into the pre-trained CLIP visual encoder and extract the visual embeddingv clip for the teacher supervision. The global student visual embedding is generated by conducting an average pooling among all output interaction query features. L 1 loss is utilized to pull the distance between the student and the teacher. We formulate the global knowledge distillation as:</p><formula xml:id="formula_10">L glo = |v clip ? 1 Nq Nq i=1 v a i |,<label>(4)</label></formula><p>where N q denotes the number of queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference</head><p>In this subsection, we elaborate the processes of training and inference. Training. During the training stage, we follow the querybased methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref> to assign a bipartite matching prediction with each ground-truth using the Hungarian algorithm. The matching process combines the predictions from the FFN heads of the two-branch decoders since the queries of human, object and interaction are one-to-one corresponding. The matching cost for the matching process and the targeting cost for the training back-propagation share the same strategy, where is composed by the box regression loss L b , the intersection-over-union loss L u and the classification loss L c . The cost is formulated as:</p><formula xml:id="formula_11">L cost = ? b i?(h,o) L i b + ? u j?(h,o) L j u + k?(o,a) ? k c L k c ,<label>(5)</label></formula><p>where ? b , ? u and ? k c are the hyper-parameters for adjusting the weights of each loss. Then, considering the mimic loss, the final training loss is given as:</p><formula xml:id="formula_12">L = L cost + ? mimic L glo ,<label>(6)</label></formula><p>where ? mimic is the hyper-parameter weight for distilling the image embeddings. Additionally, we apply an intermediate supervision for the output of each decoder layer. Inference. The visual embedding mimic only contributes to the training stage, removing it during inference. For each human-object bounding-box pair (b h i , b o i ) with the object score s o i from instance decoder branch, the interaction score is predicted as s a i from the interaction decoder. Then, we extend s o i from N o -dim to N a -dim, where the score for a specific object category will be copy-paste several times for all the corresponding HOI categories. The HOI triplet score is given as</p><formula xml:id="formula_13">s a i + s o i (N a )s o i (N a ) instead of s a i s o i (N o )</formula><p>for balancing the weights of interaction score and object score. The HOI triplets with top K confidence scores are processed by a triplet NMS as the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we demonstrate the effectiveness of our designed GEN-VLKT with comprehensive experiments. In Sec 4.1, we first introduce our experimental settings. Then in Sec 4.2, we compare our GEN-VLKT with the previous state-of-the-art approaches. Next, we present the superiority of our GEN-VLKT on zero-shot HOI detection in Sec 4.3. Finally, we conduct the ablation studies in Sec 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Datasets. We evaluate our model on two public benchmarks, HICO-Det <ref type="bibr" target="#b2">[3]</ref> and V-COCO <ref type="bibr" target="#b9">[10]</ref>. HICO-Det has 47, 776 images (38, 118 for training and 9, 658 for testing). It contains 600 classes of HOI triplets constructed by 80 object categories and 117 action categories. V-COCO is a subset of COCO dataset and has 10, 396 images (5, 400 for training and 4, 964 for testing). It has 29 action categories which includes 4 body motions without interaction to any objects. It has the same 80 object categories. Its actions and objects form 263 classes of HOI triplets.</p><p>Data Structure for Zero-Shot. For zero-shot HOI detection, we conduct experiments on HICO-Det following the setting in [1]: 1) Unseen Composition (UC) and 2) Unseen Object (UO). Specifically, the UC setting indicates the training data contains all categories of object and verb but misses some HOI triplet categories, while the UO setting means the objects in the unseen triplets also do not appear in the training data. We evaluate the 120 unseen, 480 seen, and 600 full categories for the UC setting. Similar to <ref type="bibr" target="#b13">[14]</ref>, the Rare First UC (RF-UC) selects unseen categories from tail HOIs preferentially, while the Non-rare First UC (NF-UC) prefers the head categories. For the UO setting, we use the unseen HOIs with 12 objects unseen among the total 80 objects and form 100 unseen and 500 seen HOIs. Besides, for a more comprehensive demonstration of our method to investigate the novel HOIs, we propose an Unseen Verb (UV) setting, where we randomly select 20 verbs from all total 117 verbs to form 84 unseen and 516 seen HOIs.</p><p>Evaluation Metric. We follow the settings in <ref type="bibr" target="#b2">[3]</ref> to use the mean Average Precision (mAP) for evaluation. We define a HOI triplet prediction as a true positive if 1) both predicted human and object bounding-boxes have IoU larger than 0.5 w.r.t. the GT boxes; and 2) both predicted HOI categories are accurate. For HICO-Det, we evaluate the three different category sets: all 600 HOI categories (Full), 138 HOI categories with less than 10 training instances (Rare) and the other 462 HOI categories (Non-Rare). For V-COCO, we report the role mAPs for two scenarios: S1 for the 29 action categories including the 4 body motions and S2 for the 25 action categories without the no-object HOI categories. Implementation Details. We implement three versions of GEN-VLKT: the small version GEN-VLKT s , the middle version GEN-VLKT m and the large version GEN-VLKT l . The backbone is ResNet-50 for GEN-VLKT s and ResNet-101 for GEN-VLKT m and GEN-VLKT l . N for each decoder of the two branches is 3 for GEN-VLKT s and GEN-VLKT m and 6 for GEN-VLKT l . The number of HOI categories N a is 600 for HICO-Det and 263 for V-COCO. We set the number of queries N q to 64 and the number of channels C e and C q to 256. We optimize our network with AdamW with a weight decay of 10 ?4 . We train the model for 90 epochs with an initial learning rate of 10 ?4 decreased by 10 times at the 60th epoch. The training is initialized with the parameters of MS-COCO trained DETR <ref type="bibr" target="#b1">[2]</ref>. For regular HOI training, we fine-tune the CLIP text embeddings initialized interaction classifier and object classifier with a small learning rate of 10 ?5 . We implement the zeroshot HOI experiments on HICO-Det. For better novel HOI categories extension, we freeze the CLIP initialized weights for both interaction and object classifiers. We set the output dimension of the interaction classifier to the number of 'seen' categories during training, while we update this output dimension to the 'full' 600 categories during inference. We set the cost weights ? b , ? u , ? o c and ? a c to 2.5, 1, 1 and 1, respectively, following QPIC <ref type="bibr" target="#b33">[34]</ref>. We follow the official CLIP data pre-processing for visual embedding mimic to resize and center-crop the real-timely augmented image to 224 and feed the processed image to the CLIP visual encoder. We set the loss weight ? mimic to 20. We conduct all the experiments with a batch size of 16 on 8 Tesla V100 GPUs and CUDA10.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effectiveness for Regular HOI Detection</head><p>We use the official evaluation code to compute the mAPs for both HICO-Det and V-COCO. <ref type="table" target="#tab_3">Table 1</ref> and <ref type="table" target="#tab_4">Table 2</ref> show the performance comparisons of GEN-VLKT with the recent bottom-up and top-down HOI detection methods.</p><p>For HICO-Det as shown in <ref type="table" target="#tab_3">Table 1</ref>, GEN-VLKT s outperforms the all existing bottom-up and top-down methods with a large margin. In specific, compared to the state-ofthe-art top-down method QPIC <ref type="bibr" target="#b33">[34]</ref>, GEN-VLKT s achieves a relative 16.10% mAP gain with a margin of mAP 4.68. Especially for the rare categories, GEN-VLKT s achieves mAP 29.25, which significantly outperforms AS-Net <ref type="bibr" target="#b3">[4]</ref> by a margin of mAP 5.00, even outperforming the 'Full' setting of all existing methods. This is ascribable to the guided embedding design of the GEN architecture and the powerful VLKT for the long-tail categories. From an efficiency perspective, GEN-VLKT s contains in total 6 decoder layers considering the two branches. Thus it has almost the same number of parameters and flops compared to QPIC, and fewer parameters and flops compared to AS-Net <ref type="bibr" target="#b3">[4]</ref> with 12 decoder layers in total. And GEN-VLKT l achieves a new state-of-the-art performance of mAP 34.95.</p><p>For V-COCO, as shown in <ref type="table" target="#tab_4">Table 2</ref>, GEN-VLKT s achieves role AP 62.41 on Scenario 1 and role AP 64.46 on Scenario 2, which also outperform the previous state-ofthe-art method QPIC-R50 <ref type="bibr" target="#b33">[34]</ref> with margins of mAP 3.61 and 3.46, respectively. The promotion is not as significant as that on HICO-Det, since the training samples of V-COCO is insufficient compared to HICO-Det to train such a large number of 263 categories classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effectiveness for Zero-Shot HOI Detection</head><p>We conduct all the experiments with the 's' model. We train the model without VLKT strategy as the baseline. As shown in <ref type="table">Table 3</ref>, GEN-VLKT s outperforms the baseline and previous methods for all the Unseen Composition (UC), Unseen Object (UO) and Unseen Verb (UV) settings.</p><p>Unseen Composition. For UC, compared with FCL <ref type="bibr" target="#b15">[16]</ref>, GEN-VLKT s achieves 38.85% and 22.41% relative mAP gains on full categories for rare first and non-rare first selections, respectively. Specifically, benefiting from the VLKT mechanism, GEN-VLKT s still significantly promotes the performance for the unseen categories without the feature factorization and composition among images like VCL <ref type="bibr" target="#b13">[14]</ref>, FCL <ref type="bibr" target="#b15">[16]</ref> and ATL <ref type="bibr" target="#b14">[15]</ref>. The improvements mainly come from the utilization of CLIP, as indicated by comparing GEN-VLKT s to the baseline. For example, for the rare first UC setting, GEN-VLKT s promotes mAP from 13.16 to 21.36 compared to FCL and promotes mAP by a significant margin of mAP 8.84 compared to the baseline.</p><p>Unseen Object. We further evaluate GEN-VLKT s with unseen object, which reflects the ability to investigate human interactions with novel objects. For full and unseen categories, GEN-VLKT s outperforms ATL <ref type="bibr" target="#b14">[15]</ref> by relative 95.95% and 108.12% mAP gains, respectively. Again, the comparison to the baseline indicates VLKT promotes the performance for the unseen categories significantly.</p><p>Unseen Verb. Finally we propose the UV setting to discovery novel categories of actions, and we argue this reflects the specific characteristic of zero-shot HOI detection. We compare GEN-VLKT s with the baseline and obtain a significant 55.03% relative promotion for unseen categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Anchor AP S1 role AP S2 role Bottom-up Methods:</p><p>InteractNet <ref type="bibr" target="#b7">[8]</ref> 40.0 -GPNN <ref type="bibr" target="#b29">[30]</ref> 44.0 -iCAN <ref type="bibr" target="#b6">[7]</ref> 45.3 52.4 TIN <ref type="bibr" target="#b22">[23]</ref> 47.8 54.2 VCL <ref type="bibr" target="#b13">[14]</ref> 48.3 -DRG <ref type="bibr" target="#b5">[6]</ref> 51.0 -IP-Net <ref type="bibr" target="#b38">[39]</ref> 51.0 -VSGNet <ref type="bibr" target="#b35">[36]</ref> 51.8 57.0 PMFNet <ref type="bibr" target="#b36">[37]</ref> 52.0 -PD-Net <ref type="bibr" target="#b44">[45]</ref> 52.6 -FCMNet <ref type="bibr" target="#b25">[26]</ref> 53.1 -ACP <ref type="bibr" target="#b19">[20]</ref> 53.23 -IDN <ref type="bibr" target="#b21">[22]</ref> 53. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this subsection, we conduct a series of experiments to analyse the effectiveness of our proposed modules and strategies. All experiments are conducted in HICO-Det dataset based on the 's' model. Network Architecture Setting. In this part, we aim to prove the superiority of our designed framework. Thus, we implement two base models with regular training strategies without VLKT. Firstly, we follow the previous query-based methods to adopt a verb classifier with 117 categories for GEN, namely 'Base-verb' in <ref type="table" target="#tab_7">Table 4a</ref>. It shows that our 'Base-verb' has achieved 31.88 mAP outperforming all existing HOI detectors. Secondly, we replace the verb classifier with an HOI triplet classifier with 600 categories, namely 'Base-triplet'. Due to the serious long-tailed distribution, it drops a bit mAP compared with 'Base-Verb', especially for 'Rare' HOIs. Additionally, we explore the importance of components in our GEN. On the one hand, we remove p-GE and replace the independent human and object queries with a unified query for two tasks in 'Base-verb', and the result has dropped a 0.65 mAP. On the other hand, we remove i-GE and use a learnable embedding with random initialization added by the p-GE for interaction queries in 'Base-verb'. It has lost 1.05 mAP compared with 'Baseverb', but still superior to previous single-branch methods.  <ref type="table">Table 3</ref>. Performance comparison for Zero-Shot HOI detection. RF is short for rare first, NF is short for non-rare first, and UC, UO, UV indicate unseen composition, unseen object and unseen verb settings, respectively. The baseline is the model of 's' architecture without VLKT. * means only the detected boxes are used without object identity information from the detector.</p><p>Training Strategies of VLKT. Here, we verify the effectiveness of the proposed components in VLKT and the results are presented in <ref type="table" target="#tab_7">Table 4b</ref>. We take the 'Base-triplet' as the baseline model. We first replace the interaction classifier with the text embedding initialization, and the results are reported as '+interaction text', which shows the mAP of 'Rare' HOIs has improved a lot. Thus, the prior knowledge from linguistic prior can alleviate the long-tail distribution. We then further equip this model with the text embedding driven object classifier, causing 0.38 mAP improvement. Finally, we add the mimic loss to transfer visual knowledge from CLIP. All performances have boosted a lot, which proves aligning features is essential. Mimic Loss Setting. We discuss the choice of the mimic loss with two loss types, i.e., L 1 and L 2 losses. As shown in <ref type="table" target="#tab_7">Table 4c</ref>, if only equipped with L 1 loss, our model has achieved the best performance and a 1.66 mAP gain, and the performance is much better than only equipped with L 2 loss. If we apply L 1 and L 2 losses at the same time by summation, the performance is also worse than only with L 1 . Thus, L 1 is more suitable for the mimic loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a novel framework GEN-VLKT to improve the query-based HOI detectors from two aspects, association and interaction understanding. For association, we design a two-branch framework while removing post-  matching by a guided embedding mechanism. For interaction understanding, we design a training strategy VLKT, adopting CLIP to enhance interaction understanding. GEN-VLKT has achieved leading performances on regular and zero-shot settings on HICO-Det and V-COCO datasets. We conduct a simple attempt to CLIP, where only adopting a global knowledge distillation for visual features mimic may not be sufficient for a dense understanding task. In future, we aim further to mine the benefits of CLIP for HOI detection and improve the mimic strategy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>HOI detection. Conventional HOI detectors are mainly divided into two folds, bottom-up and top-down. The bottomup pipelines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Transformer Encoder CNN Decoder Layer Decoder Layer Decoder Layer c ? ? Interaction Decoder Instance Decoder</head><label></label><figDesc></figDesc><table><row><cell>Positional Encoding</cell><cell></cell></row><row><cell># !""</cell><cell># #</cell></row><row><cell>Visual Encoder</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>human bounding-box, object bounding-box and object category scores. And we extract the middle features decoded by each decoder layer asV ins = [V h , V o ] for the following interaction decoder, where V ins ? R N ?2Nq?Cq .The goal of the interaction decoder is to predict the HOI categories for the corresponding human-object pair. There-</figDesc><table><row><cell>&lt;ride, horse&gt;</cell><cell></cell></row><row><cell>&lt;drive, bus&gt;</cell><cell></cell></row><row><cell></cell><cell>A photo of a</cell></row><row><cell>&lt;fly, airplane&gt;</cell><cell>person [Verb-ing]</cell></row><row><cell></cell><cell>a/an [Object].</cell></row><row><cell>&lt;row, boat&gt;</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on the HICO-Det test set. We present an additional tag 'Anchor' to disgust the interaction anchor types for top-down methods, where the 'B', 'P' and 'Q' denote bounding-box, point and query, respectively.</figDesc><table><row><cell>Default</cell><cell>Know Object</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison on the V-COCO. The 'B', 'P' and 'Q' denote bounding-box, point and query, respectively.</figDesc><table><row><cell></cell><cell></cell><cell>3</cell><cell>60.3</cell></row><row><cell>Top-down Methods:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UnionDet [18]</cell><cell>B</cell><cell>47.5</cell><cell>56.2</cell></row><row><cell>HOI-Trans [48]</cell><cell>Q</cell><cell>52.9</cell><cell>-</cell></row><row><cell>AS-Net [4]</cell><cell>Q</cell><cell>53.9</cell><cell>-</cell></row><row><cell>GG-Net [46]</cell><cell>P</cell><cell>54.7</cell><cell>-</cell></row><row><cell>HOTR [19]</cell><cell>Q</cell><cell>55.2</cell><cell>64.4</cell></row><row><cell>QPIC-R50 [34]</cell><cell>Q</cell><cell>58.8</cell><cell>61.0</cell></row><row><cell>QPIC-R101 [34]</cell><cell>Q</cell><cell>58.3</cell><cell>60.7</cell></row><row><cell>GEN-VLKTs</cell><cell>Q</cell><cell>62.41</cell><cell>64.46</cell></row><row><cell>GEN-VLKTm</cell><cell>Q</cell><cell>63.28</cell><cell>65.58</cell></row><row><cell>GEN-VLKT l</cell><cell>Q</cell><cell>63.58</cell><cell>65.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Training Strategies of VLKT. : Ablations for the training strategies of VLKT. Mimic Loss Setting: The choice of losses for mimic.</figDesc><table><row><cell>Setting Base-triplet 30.96 22.28 Full Rare Non-Rare 33.55 Base-verb 31.88 26.24 33.57 -p-GE 31.23 25.38 32.98 -i-GE 30.83 23.86 32.91 (a) Network Architecture Setting: Train-</cell><cell>Strategy Base-triplet + interaction text 31.71 26.08 Full Rare Non-Rare 30.96 22.28 33.55 33.39 + object text 32.09 26.68 33.71 + mimic 33.75 29.25 35.10 (b) L 1 L 2 ----(c)</cell><cell>Full 32.09 26.68 Rare Non-Rare 33.71 33.75 29.25 35.10 32.41 26.66 34.14 33.10 29.24 34.25</cell></row><row><cell>ing 's' model without VLKT.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Ablations. We conduct experiments on HICO-Det dataset based on 's' model. The mAP in default setting is reported.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sai Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reformulating hoi detection as adaptive set prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual relationship detection using part-and-sum transformers with composite queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ican: Instancecentric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Open-vocabulary object detection via vision and language knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13921</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting scene graphs for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Affordance transfer learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting human-object interaction via fabricated compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Is object detection necessary for human-object interaction recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13083</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Uniondet: Union-level detector towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hotr: End-to-end human-object interaction detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eun-Sol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions with action co-occurrence priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hoi analysis: Integrating and decomposing human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transferable interactiveness prior for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving human-object interaction detection via phrase learning and label composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07383</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Amplifying key cues for human-object-interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Consnet: Learning consistency graph for zero-shot human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vilbert: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Qpic: Query-based pairwise human-object interaction detection with image-wide contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoaki</forename><surname>Yoshinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangalore S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oytun</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contextual heterogeneous graph network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yingbiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A graph-based interactive reasoning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mining the benefits of two-stage and one-stage hoi detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatially conditioned graphs for detecting human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Frederic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Glance and gaze: Inferring action-aware points for one-stage human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>Changxing Ding, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
