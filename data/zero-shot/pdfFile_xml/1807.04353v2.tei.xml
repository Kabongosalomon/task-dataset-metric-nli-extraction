<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient keyword spotting using time delay neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Myer</surname></persName>
							<email>sam.myer@fluent.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">Fluent.ai Inc</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikrant</forename><surname>Singh Tomar</surname></persName>
							<email>vikrant@fluent.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">Fluent.ai Inc</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient keyword spotting using time delay neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: keyword spotting</term>
					<term>wake word</term>
					<term>time-delay neural network</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes a novel method of live keyword spotting using a two-stage time delay neural network. The model is trained using transfer learning: initial training with phone targets from a large speech corpus is followed by training with keyword targets from a smaller data set. The accuracy of the system is evaluated on two separate tasks. The first is the freely available Google Speech Commands dataset. The second is an in-house task specifically developed for keyword spotting. The results show significant improvements in false accept and false reject rates in both clean and noisy environments when compared with previously known techniques. Furthermore, we investigate various techniques to reduce computation in terms of multiplications per second of audio. Compared to recently published work, the proposed system provides up to 89% savings on computational complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Keyword spotting is an essential feature in modern hands-free voice control devices, where the user speaks a predefined keyword to "wake-up" the device before speaking a complete command or query to the device. This keyword is also referred to as a "wake-word". Unlike large vocabulary speech recognition systems, keyword spotting algorithms use a simpler model that only detects whether a phrase or small set of phrases are spoken. Once a wake-word has been detected, then a large vocabulary model could be used to decode the user query that might follow.</p><p>Much research has been done in recent years on improving keyword spotting methods. Recent works have suggested use of fully connected neural networks <ref type="bibr" target="#b0">[1]</ref>, convolution neural networks (CNNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, and recurrent neural networks (RNNs) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. RNNs have also been combined with convolutional layers <ref type="bibr" target="#b5">[6]</ref>. Recently, two dimensional Grid-LSTM RNNs capable of learning sequences in both the time and frequency dimensions have also been shown to produce good results albeit with higher computational complexity <ref type="bibr" target="#b6">[7]</ref>.</p><p>In this work, we present a novel two-stage time-delay neural network architecture (TDNNs) for keyword spotting. We also discuss various optimizations for efficient implementation of the presented system. In a TDNN, different layers or sets of layers can act on different time scales <ref type="bibr" target="#b7">[8]</ref>. As such, it can be seen as a type of CNN <ref type="bibr" target="#b8">[9]</ref> operating over the time dimension. In a TDNN model, the first few layers look at smaller time scales and produce more abstract higher level features. The later layers take larger time windows over these abstract features as input. During training and inference, the sequence of input features are repeatedly shifted in time and fed to the model, producing another sequence as output. This architecture reduces the amount of computation required, as compared with a fully connected network.</p><p>This paper explores several improvements over existing systems. There exists some recent work where the authors have used TDNNs for keyword spotting in combination with a hidden Markov model (HMM) <ref type="bibr" target="#b9">[10]</ref>. However, the presented method uses an end-to-end TDNN architecture and avoids the need for a separate HMM model. The system consists of two sets of layers. The first set is trained with phone label targets as an intermediate representation and the second set learns to predict the keyword targets. The first set of layers is initialized independently of the second set with weights trned on a large vocabulary task. Transfer learning is then used to train the second sets of layers on top of the first set. Furthermore, we investigate using frame skipping and caching to reduce computation in a real-time scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System Description</head><p>An effective keyword detection system must minimize both false positives and false negatives to provide an acceptable user experience. The number of operations and memory usage should also be kept low to reduce power drain and conform to hardware limitations. This section presents variations of our TDNN architecture and measures them on these performance objectives to arrive at an optimal configuration. The model used in this paper is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The number of parameters per layer are provided in  <ref type="table" target="#tab_0">Table 1</ref>: TDNN parameters per layer dimensional log-Mel filterbank (FBANK) features. These features are extracted from the input speech using a frame size of 25 ms and frame shift of 10 ms. The FBANK features are normalized so that they have approximately zero mean and unit variance. The first set of layers takes FBANK features as input and are trained with phone targets as an intermediate representation. For easy reference, we call this set the phone-NN. This set of layers take a context of 5 frames in the past and 5 frames in the future for a total context of 11 frames, or 125 ms. The phone-NN is shown in red in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Architecture</head><p>The phone-NN is trained on monophone targets instead of triphones or senones. However, state-dependent variants of phones are used. There are 3 variants for each non-silence phone, and 5 variants for each silence type (3 types of silence were used). This configuration is motivated by Kaldi <ref type="bibr" target="#b10">[11]</ref> and results in a model with higher discriminative capacity without the overhead of a larger output layer, as would occur when using triphones.</p><p>The output of the phone-NN layers, without the softmax layer, is used as input for the second set of layers. These layers are trained with the keyword labels as the training targets, therefore we refer to this set as the word-NN. The word-NN is shown in blue in <ref type="figure" target="#fig_0">Figure 1</ref>. To reduce the input dimensionality, the phone posteriors are first max-pooled along the time axis with a pooling size of 5 frames and a stride of 4. The pooled phone posteriors are passed to two fully connected followed by a softmax layer.</p><p>Altogether, the whole network looks at a input context of 80 frames covering 815 ms of speech, consisting of 69 frames in the past and 10 frames in the future. The output layer has one unit for each target keyword and one unit for background/filler speech. The input window is shifted in time across the FBANK features producing a sequence of keyword probabilities.</p><p>During decoding, the keyword probabilities are smoothed using a moving average filter with a width of 9 frames. We applied smoothing in a similar manner to the method described in the paper <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. A threshold is applied to the smoothed probabilities, and keyword detection is triggered when one the keyword probabilities goes above the threshold. <ref type="table" target="#tab_2">Table 2</ref> presents the computational complexity of the models discussed in this paper, measured as the number of multiplications necessary per second of input audio. In a real-time system, this metric is roughly proportional to power consumption, which is relevant in environments with limited hardware capabilities. Two methods are described here to reduce multiplications per second: caching intermediate results and frame skipping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Computational complexity</head><p>The phone-NN only looks at small patches of the input data.</p><p>Recalculating all of these patches whenever the full TDNN is shifted a time step results in a lot of redundant computation. The amount of computation can be greatly reduced using dynamic programming. The output phone posteriors from the first set of layers is cached in a buffer. Then, only the rightmost patch at each level of the TDNN needs to be calculated at each time step. Another way of reducing computation is by skipping frames during inference. Since the region of interest in the input, where the keyword is spoken, might span several frames, it is reasonable to argue that the network might still be able to capture sufficient relevant information even if some frames are skipped during decoding. This is discussed in more details in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Setup</head><p>The proposed model is evaluated on two separate speech tasks: one is an in-house dataset specifically developed for keyword spotting tasks and the other is a freely available dataset called Speech Commands <ref type="bibr" target="#b11">[12]</ref>.</p><p>The primary motivation for using the in-house dataset is to test the system in a wide variety of background speech and acoustic conditions. The dataset consists of long speech recordings created by concatenating the keyword ("Fluent"), short pauses, non-keyword filler speech, and noise in random order.</p><p>The amplitude of the keyword speech and filler speech is randomly varied to simulate speakers with varying loudness. For noisy conditions, simulated speech-in-noise data is created by mixing three different noise types, namely babble, music and street, with clean speech at an average of 10 dB SNR. The test set is created in a similar manner, but using different noise files than those used to create the training set. Furthermore, long conversational speech that does not contain any examples of the keyword is used to train the models for background or out-ofvocabulary speech. The resulting dataset consists of 50 hours of training data with 5913 repetitions of the keyword, and 22 hours of testing data with 1563 repetitions of the keyword. The results are evaluated on false alarms per hour and false reject rate.</p><p>Most of the previous work in keyword spotting research has used internal datasets that make it difficult for general scientific community to compare performance of different models. We realize this is also the case with our in-house dataset. To avoid unreproducible research, we perform a second set of experiments on the freely available Speech Commands dataset <ref type="bibr" target="#b11">[12]</ref>. For these experiments, we compare our work with that reported in <ref type="bibr" target="#b2">[3]</ref>, where the authors have used the same dataset. This dataset contains 64752 clean recordings of 30 commands. While the in-house task described above is designed to recognize only one keyword, in this task we are recognizing 10 keywords concurrently. This is done to create experiments in line with the work reported in <ref type="bibr" target="#b2">[3]</ref>. The 10 commands that are used as target keywords are "Yes", "No", "Up", "Down", "Left", "Right", "On", "Off", "Stop" and "Go". The remaining 20 commands are used as filler words: "Bed", "Bird", "Cat", "Dog", "Happy", "House", "Marvin", "Sheila", "Tree", "Wow", and numbers zero through nine.</p><p>The original Speech Commands dataset consists of individual commands. However, a keyword spotting system used in realistic scenario is required to identify the target keywords from a continuous stream of incoming speech. In order to simulate this behavior, we also created a derivative dataset from the Speech Commands dataset by concatenating multiple commands together at varying amplitude level. The order of the commands is randomized before concatenation to further sim-ulate real world behavior. Experiments on the Speech Commands dataset are performed on both the original as well as the derivative dataset. For this experiment, accuracy is measured as the percentage of utterances that are correctly classified as either the appropriate keyword or filler. This metric is chosen to match baseline results found in the literature <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transfer learning</head><p>Transfer learning is a method for initializing weights by first training the network on a larger corpus for a related task <ref type="bibr" target="#b12">[13]</ref> and then using some of the layers of this network to train on the main task. This allows the network to build upon the learning from the larger amount of data of the related task and is particularly useful for scenarios where only a limited amount of data may be available for the main task. Transfer learning and multitask learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10]</ref> are common practices in keyword spotting tasks, where the amount of training data available is often limited. Transfer learning also helps reduce overfitting <ref type="bibr" target="#b13">[14]</ref>.</p><p>In the network architecture used in this work, a softmax layer is added after the phone-NN. The phone weights are trained on a large vocabulary continuous speech recognition (LVCSR) corpus using 132 phone targets. Next, the softmax layer is removed and word-NN layers are added on top of the phone-NN. The entire network is then trained using the keyword dataset.</p><p>To ensure a fair comparison, transfer learning is applied to all the experiment presented in Section 4. This include the baseline models as well as our proposed model. All the models are initialized using transfer learning on the same LVCSR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>This section gives a brief description and results of three experiments: (i) comparison against Speech Commands dataset baseline, (ii) performance on in-house dataset, and (iii) computational savings achieved by using frame skipping without compromising accuracy. The cnn-one-fstride4 keyword spotting system described in <ref type="bibr" target="#b1">[2]</ref> is used as the baseline architecture in this work for both the Speech Command dataset as well as the in-house dataset. The cnn-one-fstride4 model consists of a convolutional layer strided in the frequency dimension, followed by three fully connected layers. <ref type="table" target="#tab_2">Table 2</ref> provides a summary of each of the models discussed. The second and third columns of the table list the number of parameters and multiplications per second performed during inference for each model respectively. The fourth and fifth columns present the experimentally determined false rejection rates (FRR) for each model on clean and noisy data respectively. All false rejection rates in this section are reported for a fixed false alarm rate of 0.5 per hour. The row labeled "CNN" provides the numbers corresponding to the baseline cnn-one-fstride4 model. The row labeled "TDNN" provides numbers corresponding to the proposed model. The other two rows in the table are described in Section 4.3. It can be seen from the table that the proposed TDNN network results in a lower false reject rate -87% lower on clean data and 71% lower on noisy data -relative to the baseline CNN model. Furthermore, the receiver operator characteristic (ROC) curves in <ref type="figure" target="#fig_2">Figure 2</ref> shows that the TDNN consistently performs better when the activation threshold is varied. This is true for both clean and noisy data with 10dB SNR.   <ref type="table">Table 3</ref>: Results on Speech Commands dataset. Percentage of commands correctly labeled on 10-keyword Speech Commands dataset. Our TDNN is compared against the cnn-one-fstride4 model from <ref type="bibr" target="#b1">[2]</ref>. Two numbers are given for the CNN model: the first comes from the literature <ref type="bibr" target="#b2">[3]</ref> and the second was measured by us. Accuracy is measured on the original data, as well as derivative data, as described in 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on the in-house dataset</head><p>The large gap in accuracy between the TDNN and cnn-one-fstride4 model can be partially explained by the difference in input window size -815 ms for the TDNN versus 335 ms for the cnn-one-fstride4 model. To test the effect of input window size on accuracy, a larger CNN with input window size of 815 ms (80 input frames) was trained. Aside from the input size, the rest of the CNN had the same architecture as cnn-one-fstride4, convolving in the frequency dimension only. The CNN with increased frame size had improved accuracy (7.6% FRR in clean and 19.9% in noisy environment), but still was less accurate than the TDNN model. To ensure that the difference in accuracy is indeed due to input window size and not number of parameters, another CNN was trained with the same number of parameters as the TDNN. However, increasing parameters without increasing input size did not improve accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Speech Command dataset</head><p>The results for the speech commands dataset are presented in <ref type="table">Table 3</ref>. The accuracy is presented along with that reported in <ref type="bibr" target="#b2">[3]</ref> on the Speech Command dataset and the proposed model as described in Section 2. The table contains results reported in <ref type="bibr" target="#b2">[3]</ref> for the baseline CNN, and the results we obtained for the baseline CNN and the TDNN proposed in this work. The results are presented for the original Speech Command dataset as well as our derivate dataset. It should be noted that the baseline results that we obtained in this work are higher than those reported in <ref type="bibr" target="#b2">[3]</ref>. This is perhaps due to difference in optimizations such as posterior smoothing <ref type="bibr" target="#b1">[2]</ref>, which improves accuracy on the baseline model. When comparing the presented model to the baseline in <ref type="table">Table 3</ref>, the presented model shows a relative  error reduction of 12 % on the original data. On the derivative data, where the audios were stitched together with varying amplitude to better simulate real-world conditions, error decreased by 39 %. As mentioned in Section 2.2, this work also uses frame-skipping to further reduce computation without significantly diminishing accuracy. Experiments were done with strides of 2 and 4 for both stages of the TDNN, as illustrated in <ref type="figure" target="#fig_3">figure 4</ref>. ROC curves for these experiments are given in <ref type="figure" target="#fig_4">Figure 3</ref>. It can be seen from the ROC curves that the impact of frame-skipping on accuracy of keyword spotting is very minimal. Resulting FRRs are 3.1% without frame skipping, 4.8% with a stride of 2, and 3.9% using a stride of 4. This indicates that frame skipping is a good way to reduce computation without greatly impacting accuracy. The frame skipping methods here could be combined with other optimization methods suggested in the literature such as quantization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3]</ref> or binarization <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> to further improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Frame skipping</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper has presented a novel method of using time delay neural network for keyword spotting. The network weights were initialized using transfer learning on a related LVCSR task. The proposed TDNN architecture was compared against the cnn-one-fstride4 model described in <ref type="bibr" target="#b1">[2]</ref>. It has been shown that with a false alarm rate of 0.5 FA/hr, the presented TDNN model significantly reduces the false reject rate on realistic data compared to the baseline CNN model, while reducing the number of multiplications by 50%. However, on the Speech Commands dataset without any modifications, accuracy improvement was more limited. Furthermore, it has been shown that the number of multiplications can be drastically reduced by applying frame skipping without significantly affecting accuracy. In a continuously listening system, this allows for acceptable accuracy while running on low-power devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of the proposed model. The left side shows a zoom-in of the phone-NN layers. The right side shows the complete architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>ROC curves for the presented TDNN method vs the CNN baseline on the in-house dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of frame skipping mechanism. Blue lines represent the first output frame, and orange lines represent the second output frame. Left, without frame skipping: a) Phone features are calculated at every time step b) Phone features are pooled to reduce dimensionality c) Keyword features are calculated at every time step. Right, with frame skipping: a) Phone features are calculated with a stride of 4 time steps, without pooling b) Keyword features are calculated at every fourth time step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>ROC curves with frame skipping</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>. The</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for in-house dataset. For each model, the table shows the number of parameters, multiplications per second, and false reject rate in percent on clean data (FRR clean) and 10 dB SNR noisy data (FRR noisy). FRR values are for a false alarm rate of 0.5 FA/hr.</figDesc><table><row><cell>Model</cell><cell>Error %</cell><cell>Error %</cell></row><row><cell></cell><cell cols="2">(original) (derivative)</cell></row><row><cell>CNN (from literature)</cell><cell>15.4 [3]</cell><cell>n/a</cell></row><row><cell>CNN</cell><cell>6.5</cell><cell>24.8</cell></row><row><cell>TDNN (ours)</cell><cell>5.7</cell><cell>15.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Smallfootprint keyword spotting using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4087" to="4091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hello edge: Keyword spotting on microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yundong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An end-to-end architecture for keyword spotting and voice activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lengerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09405</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end ASR-free keyword search from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional recurrent neural networks for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1606" to="1610" />
		</imprint>
	</monogr>
	<note type="report_type">Interspeech</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Acoustic modeling for google home</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="1989-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Compressed time delay neural network for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3607" to="3611" />
		</imprint>
	</monogr>
	<note type="report_type">Interspeech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Launching the speech commands dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<ptr target="https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminability-based transfer between neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="204" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-task learning and weighted cross-entropy for dnn-based keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankaran</forename><surname>Panchapagesan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="760" to="764" />
		</imprint>
	</monogr>
	<note type="report_type">Interspeech</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Accurate and compact large vocabulary speech recognition on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gruenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="662" to="665" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Binary Deep Neural Networks for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="533" to="537" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Binarized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4107" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
