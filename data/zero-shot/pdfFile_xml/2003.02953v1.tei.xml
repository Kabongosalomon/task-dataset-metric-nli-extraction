<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Metric-Scale Truncation-Robust Heatmaps for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>S?r?ndi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Robert Bosch GmbH, Corporate Research</orgName>
								<address>
									<settlement>Renningen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Robert Bosch GmbH, Corporate Research</orgName>
								<address>
									<settlement>Renningen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
							<email>leibe@vision.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution" key="instit1">Visual Computing Institute</orgName>
								<orgName type="institution" key="instit2">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Metric-Scale Truncation-Robust Heatmaps for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Heatmap representations have formed the basis of 2D human pose estimation systems for many years, but their generalizations for 3D pose have only recently been considered. This includes 2.5D volumetric heatmaps, whose X and Y axes correspond to image space and the Z axis to metric depth around the subject. To obtain metric-scale predictions, these methods must include a separate, explicit post-processing step to resolve scale ambiguity. Further, they cannot encode body joint positions outside of the image boundaries, leading to incomplete pose estimates in case of image truncation. We address these limitations by proposing metric-scale truncationrobust (MeTRo) volumetric heatmaps, whose dimensions are defined in metric 3D space near the subject, instead of being aligned with image space. We train a fully-convolutional network to estimate such heatmaps from monocular RGB in an end-to-end manner. This reinterpretation of the heatmap dimensions allows us to estimate complete metric-scale poses without test-time knowledge of the focal length or person distance and without relying on anthropometric heuristics in post-processing. Furthermore, as the image space is decoupled from the heatmap space, the network can learn to reason about joints beyond the image boundary. Using ResNet-50 without any additional learned layers, we obtain state-of-the-art results on the Human3.6M and MPI-INF-3DHP benchmarks. As our method is simple and fast, it can become a useful component for real-time top-down multi-person pose estimation systems. We make our code publicly available to facilitate further research. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human pose estimation from camera input is a longstanding problem in computer vision with a wide range of applications including human-robot interaction <ref type="bibr" target="#b55">[56]</ref>, virtual reality <ref type="bibr" target="#b0">[1]</ref>, medicine <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b42">[43]</ref> and commerce <ref type="bibr" target="#b28">[29]</ref>. Since the adoption of deep convolutional neural networks (CNN), and especially heatmap representations, we have witnessed rapid progress in pose estimation research <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Recently, deep CNNs have been successfully applied to the monocular 3D human pose estimation task as well <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Here a person's anatomical landmarks are sought in 3D space, i.e., in millimeters, instead of pixels. These advances tie into one of the major themes of computer vision research, reconstructing 3D structure from images. Such tasks are especially challenging due to inherent geometric ambiguities. One class of ambiguities arise because different 3D articulations may share the same 2D projection. Another ambiguity is between the size of an object and its distance, since small objects near the camera look the same as large ones far away. we can directly predict scale-correct and complete poses. This is in contrast to prior work (top row) that defines the X and Y heatmap axes in image space and requires further post-processing to obtain a metricscale skeleton. The three columns show how zooming affects the heatmap representation (a knee heatmap is shown along with the soft-argmax decoded skeleton). Notice that our heatmap-space representation is largely invariant to image scaling and estimates a complete pose even under body-truncation at the image boundaries.</p><p>There is no clear consensus yet about the most effective way to represent and tackle these problems. One promising line of approaches extend 2D joint heatmaps with a depth axis, resulting in a 2.5D volumetric representation <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Finding heatmap maxima gives the estimated pixel coordinates and root-relative depths per joint (a 2.5D pose). While these estimates can be highly accurate, the 2.5D representation does not address the challenging ambiguity between scale (person size) and distance. Indeed, to bridge the gap between a 2.5D and a 3D pose, one needs to perform scale recovery as a separate post-processing step. Multiple explicit anthropometric heuristics have been proposed as scale cues, e.g. bone length priors <ref type="bibr" target="#b34">[35]</ref> or a skeleton length prior <ref type="bibr" target="#b43">[44]</ref>, computed by averaging over the training poses. However, these simple heuristics have difficulties when the experimental subjects have diverse heights. A further limitation is that 2.5D formulations are constrained to the estimation of joints that lie within the image boundaries. This can be problematic in practical applications with noisy bounding box detectors. While one could use an additional module to estimate missing joints, it is preferable to learn the complete skeleton estimation in a single unified stage.</p><p>Our goal in this paper is to tackle the above limitations in a simple and efficient manner, while keeping the structural advantages of fully-convolutional heatmap estimation, as opposed to numerical coordinate regression. To this end,  <ref type="figure">Fig. 2</ref>. Overview of the method. We predict volumetric heatmaps using an off-the-shelf fully-convolutional backbone. Applying soft-argmax on these heatmaps and scaling by an image-independent constant factor yields joint coordinates in metric space up to translation. We minimize the root-relative L 1 loss. Focusing on simplicity, no learnable parameters are introduced outside the standard backbone. Note that reasoning about truncated body parts, scale-recovery and back-projection also happen implicitly within the backbone. Weak supervision from in-the-wild 2D-labeled data is incorporated by aligning the metric prediction to the 2D ground truth by scaling and translation and computing the L 1 loss (dashed arrows and boxes).</p><p>we propose training a fully-convolutional network to output metric-scale truncation-robust (MeTRo) heatmaps as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. All dimensions of these heatmaps are defined to have a fixed metric extent in meters. This is an unconventional task definition for fully-convolutional networks (FCN). FCNs are predominantly applied for pixelwise prediction tasks, such as semantic segmentation, where the input and output are pixel-to-pixel aligned, or at least are in the same coordinate frame. In our proposed approach, the input pixel positions and the output metric positions only satisfy a looser form of spatial correspondence. Nevertheless, we show that somewhat surprisingly, such a mapping can still be learned effectively by a standard modern FCN backbone.</p><p>While explicit prior knowledge of problem structure is known to be beneficial, it is still an open question how much geometric computation needs to be performed explicitly and how much can be learned by deep networks from data. By skipping the 2.5D stage, we train the backbone FCN to implicitly reason about out-of-image joints, discover scale cues and learn the geometric perspective back-projection in an end-to-end manner. Our MeTRo heatmap representation can naturally encode body parts lying outside the image, since the prediction volume's bounds do not correspond to the image bounds. As there is no need to design an explicit scale recovery step, the pipeline becomes simpler and requires neither the focal length nor the root joint distance to be known at test time.</p><p>Recent approaches have achieved good generalization performance to in-the-wild images by using abundant and diverse images with 2D pose labels in the training procedure besides 3D data <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Applying such weak supervision is challenging in our representation, since the network does not make any pixel-based predictions, its outputs are directly on a metric scale. We tackle this by proposing a scale and translation invariant loss computation method for 2D-annotated examples using an alignment layer. Combined with the recently introduced differentiable soft-argmax <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b30">[31]</ref> layer, our method becomes end-to-end learned all the way from image to final 3D metric-scale prediction as shown in <ref type="figure">Fig. 2</ref>. Soft-argmax also allows rapid training with low-resolution heatmaps and using dense prediction with smaller strides at test time for higher quality results, without the need for a decoder module. Here we find that the details of the striding mechanism are crucial and propose a "centered striding" method that distributes the output neuron receptive fields evenly over the image. Experimentally, our MeTRo heatmap estimation achieves state-of-the-art results on the two largest 3D pose benchmarks, Human3.6M and MPI-INF-3DHP. To isolate the effect of the representation, we perform direct comparisons with 2.5D heatmap learning using bone-length-based scale recovery <ref type="bibr" target="#b34">[35]</ref>, under otherwise equal training conditions. We find that scale cues can indeed be learned implicitly in this fashion and MeTRo outperforms the baseline on most test sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>3D human pose estimation has had a long research history starting with hand-crafted features and part-based models <ref type="bibr" target="#b39">[40]</ref>. Similar to other computer vision problems, the transition to deep convolutional networks has led to a dramatic performance increase in this task as well <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep 3D Human Pose Estimation</head><p>Much of the inspiration in recent 3D pose estimator design has come from lessons learned in 2D pose research. DeepPose, the first neural method for 2D pose estimation <ref type="bibr" target="#b48">[49]</ref> directly regressed 2D body joint coordinates on the RGB input via convolutional and fully-connected layers. Later topperforming methods have transitioned to predicting body joint heatmaps by fully-convolutional networks (e.g., <ref type="bibr" target="#b29">[30]</ref>) as an intermediate representation. These heatmaps are spatially discretized arrays (one for each joint), in which higher values indicate higher confidence that the particular joint is located at the corresponding position.</p><p>One line of 3D pose research builds on top of 2D heatmaps and infers the 3D pose from them by exemplar-matching <ref type="bibr" target="#b3">[4]</ref>, regression <ref type="bibr" target="#b24">[25]</ref> or probabilistic inference <ref type="bibr" target="#b47">[48]</ref>. One downside of such approaches is that the image content only indirectly influences the 3D estimation, as it acts on the result of the 2D estimation stage. Furthermore, 2D-to-3D lifting is performed in a numerical coordinate representation, which does not benefit from the built-in convolutional structure of CNNs.</p><p>Nibali et al. <ref type="bibr" target="#b31">[32]</ref> predict three marginal heatmaps per body joint, for the XY, XZ and YZ planes, respectively. Pavlakos et al. have proposed extending 2D heatmaps with a root-relative metric depth axis <ref type="bibr" target="#b34">[35]</ref>. One can obtain the 2D pixel positions and root-relative depths of the joints by finding maxima in the heatmaps.</p><p>One downside of heatmap representations has been the requirement of a dense output, which can become especially costly in 3D. The recently proposed soft-argmax <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref> a.k.a. integral regression <ref type="bibr" target="#b45">[46]</ref> method greatly alleviates this problem. As opposed to hard-argmax, which simply finds the location of the highest heatmap activation, softargmax is computed as the weighted average of all voxel grid coordinates, using softmaxed heatmap activations as the weights. For example, a low resolution heatmap can encode a joint position lying halfway between two bin centers by outputting 0.5 for both bins. By virtue of being differentiable unlike hard-argmax, it also obviates the need for explicit heatmap-level supervision (e.g., voxel-wise cross-entropy). Instead, the loss can be computed (and its gradients backpropagated) from the coordinates yielded by soft-argmax.</p><p>Besides 2D heatmaps, Mehta et al. estimate three further output channels per joint, the so-called location maps <ref type="bibr" target="#b25">[26]</ref>. These are read out at the position of the corresponding heatmap's peak to obtain the X, Y and Z coordinates on a metric scale. Note how in this approach the final 3D joint coordinates are generated in the form of activation values (of the location maps at the heatmap peaks), as opposed to high-activation locations. We can thus think of it a conceptual hybrid of direct numerical coordinate regression and heatmap estimation. A downside of this method is that it requires high-resolution location maps and cannot benefit from the soft-argmax approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scale Ambiguity</head><p>It is well-known that projecting a 3D world onto a 2D image plane results in ambiguity between size and distance (depth). However, the end goal for 3D scene understanding and 3D human pose estimation in particular is a metricspace output at the true scale. The ambiguity can only be resolved using semantic scale cues, i.e. prior knowledge of the usual size of humans and other objects appearing in the scene. Unfortunately, not all papers describe how this step is performed. Some authors report their results assuming a known focal length and known ground-truth root joint distance <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b5">[6]</ref> and leave their estimation as a separate task. A simple anthropometric approach is used by Pavlakos et al. Given 2D pixel positions and root relative depth estimates from volumetric heatmaps, they optimize the absolute person distance such that the back-projected skeleton's bone lengths match the average over the training set in a least squares sense <ref type="bibr" target="#b34">[35]</ref>. A detailed description of this convex optimization problem is given in <ref type="bibr" target="#b35">[36]</ref>. We use this scale recovery approach as our main baseline comparison throughout the paper. Sun et al. employ a similar idea, but use the overall skeleton length and a weak perspective model instead <ref type="bibr" target="#b43">[44]</ref>. Some recent works have shown that direct regression of person height from an image is a challenging task <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b6">[7]</ref>. V?ges et al. make use of a monocular depth prediction network pretrained on various indoor and outdoor datasets to help with absolute person distance estimation <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Truncated Pose Estimation</head><p>Single-person 3D human pose estimation benchmarks, such as Human3.6M <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, assume that the input is a tight crop around a whole person. In practical applications, however, we need to obtain the bounding box using imperfect person detectors, which may result in body truncation. Performance under truncation has not been studied extensively in the literature. Vosoughi et al. created randomly truncated crops from Human3.6M images, and showed that current methods perform poorly on truncated person images, even when only considering the present (within-boundary) joints <ref type="bibr" target="#b50">[51]</ref>. They tackled the problem using direct numerical coordinate regression, similar to early 2D pose estimation methods <ref type="bibr" target="#b48">[49]</ref>. In this paper, we show that our approach performs significantly better on the truncated task. Other methods, such as LCR-Net <ref type="bibr" target="#b38">[39]</ref>, can also produce out-of-image predictions, but this aspect has not been explicitly evaluated by its authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>Given an input RGB image crop I ? R w?h?3 depicting a person, we aim to predict a (root-relative) 3D skeleton, consisting of J joint coordinates (X j , Y j , Z j ) T J j=1 at metric scale (i.e. in millimeters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Metric-Space Volumetric Heatmap Representation</head><p>First, we apply an off-the-shelf fully-convolutional backbone with effective stride s to produce d ? J spatial output channels, where d is the number of discretization bins along the depth axis of the prediction volume.</p><p>We then split the resulting array along the channel axis into J volumes, each of shape (w/s) ? (h/s) ? d. 3D spatial softmax is applied over each of them, resulting in volumetric heatmap activations V (j) ? R (w/s)?(h/s)?d . The 3D joint coordinates are then decoded using the soft-argmax technique with fixed scaling factors:</p><formula xml:id="formula_0">? ? X j Y j Z j ? ? = p,q,r V (j) p,q,r ? ? p ? s/w ? W q ? s/h ? H r/d ? D ? ? ,<label>(1)</label></formula><p>where the p, q, r are 0-based integer indices into the volumetric heatmap array and W, H, D are the fixed metric width, height and depth extents of the full prediction volume. We set these extents as 2.2 meters in our work, which allows capturing people of usual height even in a stretched out pose. The final root-relative prediction is obtained by subtracting the predicted root coordinates from all joint positions. Supervision is applied on these root-relative coordinates. Crucially, the position of the root joint prediction within the volume is not explicitly prescribed for the network, the gradients are backpropagated through the root-joint-subtraction operation.</p><p>No camera calibration-based back-projection, nor bone or skeleton size-based rescaling is needed. The network is trained to perform these operations within the backbone.</p><p>normal striding centered striding </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture</head><p>In contrast to prior work that employs decoders with upsampling layers and multiple refinement stages with intermediate supervision, we show that the task can be tackled in a significantly simpler fashion. Indeed, we apply the widely used ResNet-50 <ref type="bibr" target="#b11">[12]</ref> architecture to predict spatial heatmaps, without any additional learnable layers, such as transposed convolutions. ResNet-50 has an effective stride of 32, resulting in heatmaps of spatial size 8 ? 8 from the input image of size 256 ? 256 during training. The depth of the volume is set to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Centered Striding for Dense Prediction</head><p>At test time we apply the trained network with an effective stride of 4, to obtain heatmaps with spatial size 64, which is the same size as in <ref type="bibr" target="#b45">[46]</ref> and <ref type="bibr" target="#b34">[35]</ref>. This is called dense prediction and is commonly used in image segmentation <ref type="bibr" target="#b4">[5]</ref>. In this technique, striding is removed from a given number of convolutional layers and the dilation rate of subsequent convolutions is increased correspondingly. To avoid a mismatch between the distribution of the heatmap neuron receptive field centers between training and test time, we apply a slight modification to the striding logic. The first column of <ref type="figure" target="#fig_1">Fig. 3</ref> shows the usual case of a 256x256 input image processed with a training stride 32 (+) and test stride 16 (?). Clearly, the coverage changes significantly between training and test and is not symmetric over the image. This is because each convolutional layer with stride 2 returns the top left output for each 2x2 block. To tackle the issue, we propose centered striding (second column in <ref type="figure" target="#fig_1">Fig. 3)</ref>, where the last strided convolutional layer of the backbone is "reversed", such that it outputs the bottom right result per each 2x2 block. The result is a more evenly distributed coverage over the image, without changing the resolution of either the input or the output. This benefit is evaluated in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Scale and Translation Invariant Loss for 2D Supervision</head><p>Similar to recent approaches <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b22">[23]</ref>, we train simultaneously on 3D-labeled data from motion capture studios and 2D-labeled, in-the-wild data from the MPII dataset <ref type="bibr" target="#b1">[2]</ref>, to incorporate more appearance variability in the training process. Only the arm and leg joints are used from MPII, since we found these to be the most consistently labeled across datasets. Half of each mini-batch is filled with examples of either kind. Supervision via 2D labels is straightforward when using 2.5D heatmaps, as the X and Y heatmap axes correspond to the space in which the 2D labels are defined. However, since our prediction volume is defined on a metric scale and is not aligned with image space, we propose a 2D loss computation method that is invariant to prediction scale and translation. To this end, we first orthographically project the predicted skeleton onto the image plane by discarding the Z coordinate. Then we align the projected prediction to the 2D pixel-scale ground truth by translation and uniform scaling to the least-squares optimal fit before computing the loss. This alignment layer is differentiable and gradients can be backpropagated through it, in a similar manner to batch normalization layers. We note that a similar scale-invariant loss has been used by Rhodin et al. to enforce multi-view consistency of 3D poses <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Estimation of Truncated Poses</head><p>Our metric-space heatmap representation decouples the image boundary from the heatmap boundary. This enables the prediction of joint locations outside the image frame without additional design effort, the network is simply trained to output complete poses at a metric scale, regardless of how the input image is scaled or cropped. To evaluate this aspect, we follow Vosoughi et al. <ref type="bibr" target="#b50">[51]</ref> by randomly cropping H36M inputs, keeping at least 1/4 of the area of the person bounding square. Examples of such crops are in the second row of <ref type="figure" target="#fig_2">Fig. 4</ref>. We consider two scenarios. In the first one, the above described sampling of truncated crops is only performed at test time. In the second case, such crops are used for training as well.</p><p>F. Training Details 1) Loss: Prior work has shown that the L 1 loss is preferable in soft-argmax-based pose estimation <ref type="bibr" target="#b45">[46]</ref>. To balance the losses computed on 3D and 2D examples, we use a fixed weighting factor tuned on a separate validation set of Human3.6M, yielding the overall loss as L = L 1 3D + ?L 1 2D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Training Schedule:</head><p>We initialize the network with ImageNet-pretrained weights and use the Adam optimizer with weight decay <ref type="bibr" target="#b19">[20]</ref> and a batch size of 64. We decay the learning rate exponentially by an overall factor of 100, in two parts: from 10 ?4 to 3.33 ? 10 ?5 over 25 epochs and from 3.33 ? 10 ?6 to 10 ?6 in 2 final cooldown epochs.</p><p>3) Randomness: As usual in deep learning, several sources of randomness influence the exact results of an experiment: random weight initialization, data shuffling, data augmentation and hardware-level non-determinism of execution order. We control these (except the last) by consistently seeding the random number generators. To distinguish random fluctuations from algorithmic differences, we repeat our main experiments with 5 different seeds and report the mean and standard deviation of the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASETS AND PREPROCESSING</head><p>We conduct experiments on the largest 3D pose estimation benchmarks: Human3.6M (shortened as H36M) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and MPI-INF-3DHP (3DHP) <ref type="bibr" target="#b26">[27]</ref>.</p><p>H36M <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> was captured with 4 cameras in a motion capture studio. Two evaluation protocols have been established over the years. In Protocol 1, the training subjects are 1, 5, 6, 7, 8, while 9 and 11 are used for testing. Prediction and ground-truth are aligned at the root joint, but no Procrustes alignment is performed. In Protocol 2, subjects 1, 5, 6, 7, 8, 9 are used in training and 11 in evaluation, with Procrustes alignment between prediction and ground truth. Every 64 th frame is evaluated, as in prior work.</p><p>3DHP <ref type="bibr" target="#b26">[27]</ref> shows 8 training subjects in a green-screen studio. Test frames come from 3 scenes, each with 2 subjects: green-screen studio, studio without green screen, and outdoor. The latter two make this benchmark more challenging than H36M. In this dataset, the hip and pelvis joints are labeled closer to the legs than in MPII. We follow <ref type="bibr" target="#b54">[55]</ref> and move these joints towards the neck by a fifth of the pelvis-neck vector before comparing with MPII-annotated skeletons for 2D loss computation. 3DHP provides two ground truth variants: usual metric-space poses and "universal" (height-normalized) ones. To analyze scale recovery performance, we use metric-scale evaluation, but to be comparable with prior work we also provide results with universal skeletons.</p><p>We downsample the videos from 50 to 10 fps. To further reduce redundancy, frames are only kept for training if at least one body joint moves at least 100 mm since the previous kept frame. For 3DHP, we train on images from chest-height cameras as <ref type="bibr" target="#b26">[27]</ref>, and only on examples where all joints are within the image.</p><p>For H36M examples we use the provided bounding boxes. The 3DHP dataset provides no boxes, we therefore generate them ourselves by combining the bounding box of the labeled joint positions and the most confident person detection from YOLOv3 <ref type="bibr" target="#b36">[37]</ref>. For the 2D examples of MPII, we use the provided rough center positions and person sizes as the center and side length of the box, respectively.</p><p>We crop the image to the person's bounding square and resize it to 256 ? 256 px. Perspective effects must be taken into account when centering the image on the subject as this induces an implicit rotation of the camera <ref type="bibr" target="#b26">[27]</ref>. We compensate for this effect by transforming image and the target joint positions to match the rotated camera frame. The green-screen 3DHP sequences are gamma-adjusted with an exponent of 0.67.</p><p>We apply geometric augmentations (scaling, rotation, translation, horizontal flip) and color distortions (brightness, contrast, hue, saturation). Synthetic occlusion is added with 70% probability, half of which are rectangles with uniform white noise as in <ref type="bibr" target="#b53">[54]</ref>, half are segmented non-person objects from the Pascal VOC dataset <ref type="bibr" target="#b7">[8]</ref> as in <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. On the 3DHP dataset we also apply background augmentation with 70% probability following <ref type="bibr" target="#b26">[27]</ref>, but no compositing for clothes and chair. The backgrounds are taken from the INRIA Holidays dataset <ref type="bibr" target="#b15">[16]</ref> excluding person images. We do not use ensembling or test-time augmentation, all evaluation is done on a single crop. We use the standard metrics from the literature. The main metric on 3DHP is the percentage of correct keypoints (PCK), i.e. the fraction of joints predicted within a certain distance of the ground truth (150 mm by convention). The AUC metric is the area under the PCK curve as the threshold ranges from 0 to 150 mm. The metric on H36M is mean per joint position error (MPJPE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>We achieve state-of-the-art performance on H36M with 49.3 mm MPJPE in the scenario where no ground truth information (focal length, root joint distance) is allowed to be accessed at test-time (see <ref type="table" target="#tab_1">Table I</ref>). This is only surpassed by Chen et al.'s <ref type="bibr" target="#b5">[6]</ref> method <ref type="bibr">(48.4)</ref>, however they do use the ground truth root joint depth for back-projection at testtime and do not perform scale recovery. Similarly, Sun et al. <ref type="bibr" target="#b45">[46]</ref> obtain comparable results <ref type="bibr">(49.6)</ref>, however they also access the ground-truth root joint depth at test time, for image cropping <ref type="bibr" target="#b46">[47]</ref>. Besides simplifying the prediction pipeline and allowing for truncation-robust prediction (see below), our metric heatmap representation also performs better than the 2.5D baseline with bone-length-based scale recovery under the same conditions. On Protocol 2 <ref type="table" target="#tab_1">(Table II)</ref>, the benefit of our method is masked by the use of Procrustes alignment, which explicitly ignores the quality of scale recovery. It is therefore unsurprising that our method performs about equally well as the 2.5D variant (within the standard deviation of repeated experiments).</p><p>On 3DHP, our method outperforms prior work by a large margin, including ones trained on more datasets as well <ref type="table" target="#tab_1">(Table  III)</ref>. Both with universal (height-normalized) skeletons and true metric-scale ones, the MeTRo representation outperforms the baseline due to its better performance on indoor images, where scale cues such as the size of chairs and other objects in the motion capture room can be relied on. The outdoor scenes were recorded on an empty field with no useful scale cues and the explicit bone-length-based scale recovery performs better in that scenario. Qualitative results are in <ref type="figure" target="#fig_2">Fig. 4</ref>.</p><p>We analyze scale recovery in more detail in <ref type="table" target="#tab_1">Table IV</ref>. The 2.5D baseline using mean training bone lengths performs worse on H36M and equivalently on 3DHP than the proposed approach. Interestingly, our MeTRo approach outperforms the 2.5D baseline on H36M even when the latter uses ground truth bone lengths for each test frame <ref type="bibr">(51.9)</ref>. <ref type="table" target="#tab_1">Table VII</ref> shows that training data augmentations improve performance by a large margin.</p><p>When tested on truncated crops, our method by far outperforms prior approaches <ref type="table">(Table V)</ref>. This is true even for our default training configuration, but performance improves substantially when training on truncated images as well. Qualitative examples are in the second row of <ref type="figure" target="#fig_2">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Speed-Accuracy Tradeoff</head><p>Given a bounding box crop, inference only requires a single forward pass of a standard backbone. <ref type="table" target="#tab_1">Table VI</ref> shows that 511 crops can be processed per second on an RTX 2080 Ti desktop GPU when operating on batches of 8 crops at stride 32 (the time cost of performing the detection stage is not considered). Varying the heatmap resolution using dense prediction provides diminishing returns <ref type="table" target="#tab_1">(Table VI)</ref>, showing that soft-argmax can cope with heatmaps of very coarse resolution. This means our method is attractive for use in top-down multi-person pose estimation systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We proposed metric-scale truncation-robust (MeTRo) volumetric heatmaps in the context of 3D human pose estimation. These heatmaps directly represent the metric space around the person instead of being tied to the image space and can be predicted with any standard fully-convolutional network. With a modified weak supervision scheme for 2D labels, careful stride alignment considerations and strong data augmentation, we achieved state-of-the-art results on two important benchmarks: Human3.6M and MPI-INF-3DHP. In carefully controlled experiments, we showed that our approach can implicitly discover scale cues from the data and outperforms a previously proposed explicit bone length based heuristic on all test scenarios except the two outdoor sequences of MPI-INF-3DHP. Future research should consider possibilities for learning similar scale cues from large-scale outdoor data as well. Beyond scale recovery, we demonstrated the second benefit of the MeTRo representation, the prediction ("hallucination") of complete skeletons even when only a part of the body is contained in the image. Given its speed and</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Fig. 1 .</head><label>1</label><figDesc>https://vision.rwth-aachen.de/metro-pose3d By defining heatmaps in the 3D metric space around the person (bottom row)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Receptive field centers of the output neurons in a strided FCN on a 256x256 px input image (+: stride 32, ?: stride 16). Left: Normal striding logic, where the top left result is kept per 2x2 block. Consequently, the receptive field centers are not symmetrically distributed and dense prediction introduces bias. Right: We use centered striding by reversing the stride logic in the last strided layer (i.e., bottom right result taken, instead of top left). This way the receptive fields are symmetrically distributed over the image and dense prediction at test-time introduces new bins in a proportional manner around each training-time bin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>H36MFig. 4 .</head><label>4</label><figDesc>Qualitative results of our method on different datasets. Predictions are shown in color, ground truth in gray (except for MPII, where it is unavailable). Green spheres mark predictions within 150 mm of the ground truth, red cubes beyond that threshold. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>ON H36M PROTOCOL 1, USING MEAN PER JOINT POSITION ERROR (MPJPE) WITHOUT PROCRUSTES ALIGNMENT. WE GIVE MEAN AND STANDARD DEVIATION OF THE OVERALL METRIC FOR 5 DIFFERENT RANDOM SEEDS. ALL METHODS USE EXTRA 2D POSE DATA IN TRAINING. Methods using ground-truth scale or depth information at test time Sun et al. [45] 52.8 54.8 54.2 54.3 61.8 53.1 53.6 71.7 86.7 61.5 67.2 53.4 47.1 61.6 53.4 59.1 Nibali et al.</figDesc><table><row><cell></cell><cell>Dir.</cell><cell>Dis.</cell><cell>Eat</cell><cell cols="4">Gre. Pho. Pose Pur.</cell><cell>Sit</cell><cell>SitD</cell><cell cols="6">Sm. Pho. Wait Walk WD WT</cell><cell>Avg ?</cell></row><row><cell>[32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.0</cell></row><row><cell>Luvizon et al. [23]</cell><cell cols="8">51.5 53.4 49.0 52.5 53.9 50.3 54.4 63.6</cell><cell cols="4">73.5 55.3 61.9 50.1</cell><cell cols="3">46.0 60.2 51.0</cell><cell>55.1</cell></row><row><cell>Sun et al. [46]</cell><cell cols="8">47.5 47.7 49.5 50.2 51.4 43.8 46.4 58.9</cell><cell cols="4">65.7 49.4 55.8 47.8</cell><cell cols="3">38.9 49.0 43.8</cell><cell>49.6</cell></row><row><cell>Chen et al. [6]</cell><cell cols="8">45.3 49.8 46.1 49.6 48.2 41.7 47.4 53.1</cell><cell cols="4">55.2 48.0 57.7 45.6</cell><cell cols="3">40.8 52.4 45.2</cell><cell>48.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">Methods using no ground truth scale or depth information at test time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pavlakos et al. [35]</cell><cell cols="8">67.4 72.0 66.7 69.1 72.0 77.0 65.0 68.3</cell><cell cols="4">83.7 96.5 71.7 65.8</cell><cell cols="3">74.9 59.1 63.2</cell><cell>71.9</cell></row><row><cell>Zhou et al. [55]</cell><cell cols="12">54.8 60.7 58.2 71.4 62.0 53.8 55.6 75.2 111.6 64.2 65.5 66.0</cell><cell cols="3">51.4 63.2 55.3</cell><cell>64.9</cell></row><row><cell>Martinez et al. [25]</cell><cell cols="8">51.8 56.2 58.1 59.0 69.5 55.2 58.1 74.0</cell><cell cols="4">94.6 62.3 78.4 59.1</cell><cell cols="3">49.5 65.1 52.4</cell><cell>62.9</cell></row><row><cell>Fang et al. [9]</cell><cell cols="8">50.1 54.3 57.0 57.1 66.6 53.4 55.7 72.8</cell><cell cols="4">88.6 60.3 73.3 57.7</cell><cell cols="3">47.5 62.7 50.6</cell><cell>60.4</cell></row><row><cell>Yang et al. [53]</cell><cell cols="8">51.5 58.9 50.4 57.0 62.1 49.8 52.7 69.2</cell><cell cols="4">85.2 57.4 65.4 58.4</cell><cell cols="3">43.6 60.1 47.7</cell><cell>58.6</cell></row><row><cell>Pavlakos et al. [34]</cell><cell cols="8">48.5 54.4 54.4 52.0 59.4 49.9 52.9 65.8</cell><cell cols="4">71.1 56.6 65.3 52.9</cell><cell cols="3">44.7 60.9 47.8</cell><cell>56.2</cell></row><row><cell>Liu et al. [19]</cell><cell cols="8">47.0 53.1 50.3 48.8 56.0 48.1 47.6 65.9</cell><cell cols="4">72.6 52.3 61.4 49.1</cell><cell cols="3">39.3 54.2 40.6</cell><cell>52.4</cell></row><row><cell cols="9">2.5D mean bone len. 45.1 50.4 45.4 47.8 50.0 44.6 49.8 59.0</cell><cell cols="4">69.4 49.4 56.5 48.0</cell><cell cols="4">39.6 49.4 45.0 50.2?0.3</cell></row><row><cell>MeTRo (proposed)</cell><cell cols="8">46.3 48.3 43.3 48.2 50.2 45.1 46.1 56.2</cell><cell cols="4">66.8 49.3 54.5 46.7</cell><cell cols="4">40.1 49.6 46.2 49.3?0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISONS</head><label>II</label><figDesc>ON HUMAN3.6M UNDER PROTOCOL 2 WITH PROCRUSTES ALIGNMENT TO THE GROUND TRUTH. Nie [33] Pavlakos [35] Sun [45] Martinez [25] Sun [46] Nibali [32] Habibie [11] Chen [6] 2.5D baseline MeTRo (proposed)TABLE III COMPARISON ON MPI-INF-3DHP WITH PRIOR METHODS. * EVALUATED BEFORE A FEW ANNOTATIONS WERE CHANGED IN THE DATASET. DASHES (-) REFLECT A LACK OF PUBLISHED INFORMATION. SUPERSCRIPTS INDICATE THE TRAINING DATA (FIRST CHARACTERS OF 3DHP, H36M, MPII, LSP AND COCO). WE GIVE THE MEAN AND STANDARD DEVIATION FOR 5 RUNS WITH DIFFERENT RANDOM SEEDS. 89.6?0.5 52.6?0.6 81.1?1.2 TABLE IV COMPARISON WITH BASELINE METHODS OF SCALE RECOVERY, WITH OR WITHOUT ACCESS TO GROUND TRUTH INFORMATION. FOR 3DHP THE METRIC-SCALE (NON-UNIVERSAL) SKELETONS ARE USED HERE.TABLE V MPJPE SCORES ON H36M UNDER TRUNCATION, EVALUATING ALL OR ONLY THE PRESENT JOINTS. * =TRAINING WAS NOT PERFORMED WITH TRUNCATED CROPS. OTHER METHODS' RESULTS ARE FROM<ref type="bibr" target="#b50">[51]</ref>.</figDesc><table><row><cell>P-MPJPE</cell><cell>79.5</cell><cell>51.9</cell><cell cols="2">48.3</cell><cell>47.7</cell><cell>40.6</cell><cell></cell><cell>40.4</cell><cell>49.2</cell><cell></cell><cell>33.7</cell><cell>34.5?0.4</cell><cell></cell><cell>34.7?0.5</cell></row><row><cell></cell><cell></cell><cell cols="4">Stand/ Exer-Sit on Cro./</cell><cell>On</cell><cell></cell><cell></cell><cell>Green</cell><cell>No</cell><cell>Out-</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Walk</cell><cell>cise</cell><cell cols="8">Chair Reach Floor Sport Misc. Screen Gr.Sc. door</cell><cell></cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PCK</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PCK?</cell><cell>AUC?</cell><cell>MPJPE?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">Universal, height-normalized skeletons (simplified scale recovery task)</cell><cell></cell><cell></cell></row><row><cell cols="2">Rogez et al. [39]  *</cell><cell>70.5</cell><cell>56.3</cell><cell>58.5</cell><cell>69.4</cell><cell>39.6</cell><cell>57.7</cell><cell>57.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.7</cell><cell>27.6</cell><cell>158.4</cell></row><row><cell cols="2">Zhou et al. H+M [55]  *</cell><cell>85.4</cell><cell>71.0</cell><cell>60.7</cell><cell>71.4</cell><cell>37.8</cell><cell>70.9</cell><cell>74.4</cell><cell>71.7</cell><cell>64.7</cell><cell>72.7</cell><cell>69.2</cell><cell>32.5</cell><cell>137.1</cell></row><row><cell cols="2">Mehta et al. 3+M+L+H [26]  *</cell><cell>87.7</cell><cell>77.4</cell><cell>74.7</cell><cell>72.9</cell><cell>51.3</cell><cell>83.3</cell><cell>80.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.6</cell><cell>40.4</cell><cell>124.7</cell></row><row><cell cols="2">Mehta et al. 3+M+L+H [27]  *</cell><cell>86.6</cell><cell>75.3</cell><cell>74.8</cell><cell>73.7</cell><cell>52.2</cell><cell>82.1</cell><cell>77.5</cell><cell>84.6</cell><cell>72.4</cell><cell>69.7</cell><cell>75.7</cell><cell>39.3</cell><cell>117.6</cell></row><row><cell cols="2">Mehta et al. 3+M+L+C [28]  *</cell><cell>83.8</cell><cell>75.0</cell><cell>77.8</cell><cell>77.5</cell><cell>55.1</cell><cell>80.4</cell><cell>72.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.2</cell><cell>37.8</cell><cell>122.2</cell></row><row><cell cols="2">Luo et al. 3+M+H [21], [22]</cell><cell>95.5</cell><cell>82.3</cell><cell>89.9</cell><cell>84.6</cell><cell>66.5</cell><cell>92.0</cell><cell>93.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.3</cell><cell>47.5</cell><cell>84.5</cell></row><row><cell cols="2">Nibali et al. 3+M [32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.6</cell><cell>48.8</cell><cell>87.6</cell></row><row><cell cols="2">2.5D mean bone len. 3+M</cell><cell>95.9</cell><cell>91.9</cell><cell>88.6</cell><cell>92.8</cell><cell>77.2</cell><cell>95.1</cell><cell>92.9</cell><cell>93.1</cell><cell>90.5</cell><cell cols="4">89.1 91.2?0.1 57.0?0.3 72.2?0.7</cell></row><row><cell cols="2">MeTRo (proposed) 3+M</cell><cell>95.9</cell><cell>93.2</cell><cell>91.6</cell><cell>92.7</cell><cell>76.4</cell><cell>95.9</cell><cell>93.1</cell><cell>94.4</cell><cell>91.8</cell><cell cols="4">87.9 91.8?0.3 60.3?0.5 67.6?1.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Metric-scale skeletons (full scale recovery task)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">2.5D mean bone len. 3+M</cell><cell>94.1</cell><cell>90.5</cell><cell>84.2</cell><cell>93.3</cell><cell>75.8</cell><cell>93.8</cell><cell>92.2</cell><cell>89.5</cell><cell>89.2</cell><cell cols="4">90.5 89.6?0.7 52.1?1.2 80.6?2.1</cell></row><row><cell cols="2">MeTRo (proposed) 3+M</cell><cell cols="10">95.0 87.0 Uses test 90.6 88.7 90.0 72.0 93.7 91.6 91.3 89.4 H36M 3DHP (non-univ.)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">ground truth?</cell><cell>MPJPE?</cell><cell cols="2">PCK?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">2.5D GT root depth</cell><cell>yes</cell><cell></cell><cell>49.0</cell><cell></cell><cell>90.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">2.5D GT bone len.</cell><cell>yes</cell><cell></cell><cell>51.9</cell><cell></cell><cell>90.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">2.5D mean train bone len.</cell><cell>no</cell><cell></cell><cell>50.2</cell><cell></cell><cell>89.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MeTRo (proposed)</cell><cell>no</cell><cell></cell><cell>49.3</cell><cell></cell><cell>89.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mehta  All joints 396.4</cell><cell>400.5</cell><cell></cell><cell>185.0</cell><cell>124.7</cell><cell>77.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Present joints</cell><cell>338.0</cell><cell>332.5</cell><cell></cell><cell>173.6</cell><cell>76.8</cell><cell>59.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* [26] Zhou* [55] Vosoughi [51] MeTRo* MeTRo</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VI TEST</head><label>VI</label><figDesc>SPEED (CROPS PER SECOND, FPS) AND ACCURACY (MPJPE) TRADEOFF WITH THE TWO STRIDING VARIANTS FROM FIG. 3.</figDesc><table><row><cell></cell><cell>Striding</cell><cell></cell><cell cols="2">Test stride</cell><cell></cell></row><row><cell></cell><cell>variant</cell><cell>32</cell><cell>16</cell><cell>8</cell><cell>4</cell></row><row><cell>MPJPE</cell><cell>normal strides center-aligned</cell><cell cols="4">53.1 52.5 52.7 52.9 50.9 50.2 50.0 49.3</cell></row><row><cell>Speed</cell><cell>no batching</cell><cell>160</cell><cell>150</cell><cell>105</cell><cell>38</cell></row><row><cell>(crop per sec.)</cell><cell>batch size 8</cell><cell>511</cell><cell>475</cell><cell>292</cell><cell>92</cell></row><row><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">AUGMENTATION ABLATION ON H36M.</cell><cell></cell><cell></cell></row><row><cell cols="5">Geometry Color Occlusion MPJPE</cell><cell></cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>58.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>52.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>49.3</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>robustness to detection noise, we expect our approach to be useful in designing top-down multi-person pose estimation systems in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ACKNOWLEDGMENTS</head><p>This work was funded, in parts, by a Bosch Research Foundation grant, by ERC Consolidator Grant project "DeeViSe" (ERC-CoG-2017-773161) and the EU H2020 research and innovation programme under grant agreement No 732737 (ILIAD). We are grateful for compute time granted on the RWTH CLAIX GPU cluster.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3D people models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B B</forename><surname>Shitrit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kranzfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<title level="m">Parsing human skeletons in an operating room. Machine Vision and Applications</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D human pose estimation = 2D pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning depth-aware heatmaps for 3D human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Show me your face and I will tell you your height, weight and body mass index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dantcheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What face and body shapes can tell about height</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2D features and intermediate 3D representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hand pose estimation via latent 2.5D heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1334" to="1373" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving 3D human pose estimation via 3D part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">OriNet: A fully convolutional network for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="https://github.com/chenxuluo/OriNet-demo" />
	</analytic>
	<monogr>
		<title level="j">OriNet-demo</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">2D/3D pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3D human pose estimation with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation in the wild using improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3D pose estimation from monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dense pose transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07372</idno>
		<title level="m">Numerical coordinate regression with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3D human pose estimation with 2D marginal heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coarse-tofine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose: Supplementary material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning monocular 3D human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">LCR-Net: Localizationclassification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D human pose estimation: A review of the literature and analysis of covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How robust is 3D human pose estimation to occlusion?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Robotic Co-workers 4.0</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Synthetic occlusion augmentation with volumetric heatmaps for the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04987</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV PoseTrack challenge on 3D human pose estimation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MVOR: A multi-view RGB-D operating room dataset for 2D and 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Issenhuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI LABELS Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.06079</idno>
		<title level="m">An integral pose regression system for the ECCV2018 PoseTrack Challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Integral Human Pose Regression (code repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://github.com/JimmySuen/integral-human-pose" />
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
	<note>Online; accessed 28</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Absolute human pose estimation with depth prediction network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>V?ges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?rincz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep 3D human pose estimation under partial body presence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICIP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3D human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">3D human pose estimation in RGBD images for robotic task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Welschehold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dornhege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
