<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ECLARE: Extreme Classification with Label Graph Correlations Extreme multi-label classification; product to product recommen- dation; label features; label metadata; large-scale learning LF-AmazonTitles-131K LF-WikiSeeAlsoTitles-320K</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>April 19-23, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Mittal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
							<email>nosachde@ucsd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">San</forename><surname>Uc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diego</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheshansh</forename><surname>Agrawal</surname></persName>
							<email>sheshansh.agrawal@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IIT Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country>India Sumeet Agarwal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">IIT Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">IIT Kanpur Microsoft Research</orgName>
								<address>
									<country>India Manik Varma</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Microsoft Research IIT Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ECLARE: Extreme Classification with Label Graph Correlations Extreme multi-label classification; product to product recommen- dation; label features; label metadata; large-scale learning LF-AmazonTitles-131K LF-WikiSeeAlsoTitles-320K</title>
					</analytic>
					<monogr>
						<title level="m">Slovenia ? 2021 IW3C2 (International World Wide Web Conference Committee)</title>
						<meeting> <address><addrLine>Ljubljana</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">April 19-23, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449815</idno>
					<note>KEYWORDS ACM Reference Format: Anshul Mittal, Noveen Sachdeva, Sheshansh Agrawal, Sumeet Agarwal, Purushottam Kar, and Manik Varma. 2021. ECLARE: Extreme Classification with Label Graph Correlations. In Proceedings of the Web Conference 2021 * Work done during an internship at Microsoft Research India This paper is published under the Creative Commons Attribution 4.0 International (CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution. Number of training points (Log. Scale) Number of training points (Log. Scale) (WWW &apos;21), April 19-23, 2021, Ljubljana, Slovenia. ACM, New York, NY, USA, 12 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep extreme classification (XC) seeks to train deep architectures that can tag a data point with its most relevant subset of labels from an extremely large label set. The core utility of XC comes from predicting labels that are rarely seen during training. Such rare labels hold the key to personalized recommendations that can delight and surprise a user. However, the large number of rare labels and small amount of training data per rare label offer significant statistical and computational challenges. State-of-the-art deep XC methods attempt to remedy this by incorporating textual descriptions of labels but do not adequately address the problem. This paper presents ECLARE, a scalable deep learning architecture that incorporates not only label text, but also label correlations, to offer accurate real-time predictions within a few milliseconds. Core contributions of ECLARE include a frugal architecture and scalable techniques to train deep models along with label correlation graphs at the scale of millions of labels. In particular, ECLARE offers predictions that are 2-14% more accurate on both publicly available benchmark datasets as well as proprietary datasets for a related products recommendation task sourced from the Bing search engine. Code for ECLARE is available at https://github.com/Extreme-classification/ECLARE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Machine learning; Supervised learning by classification.</p></div>
			</abstract>
		</profileDesc>
		<revisionDesc>
				<date type="submission" when="-1" />
		</revisionDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Overview. Extreme multi-label classification (XC) involves tagging a data point with the subset of labels most relevant to it, from an extremely large set of labels. XC finds applications in several domains including product recommendation <ref type="bibr" target="#b27">[28]</ref>, related searches <ref type="bibr" target="#b14">[15]</ref>, related products <ref type="bibr" target="#b30">[31]</ref>, etc. This paper demonstrates that XC methods stand to benefit significantly from utilizing label correlation data, by presenting ECLARE, an XC method that utilizes textual label descriptions and label correlation graphs over millions of labels to offer predictions that can be 2-14% more accurate than those offered by state-of-the-art XC methods, including those that utilize label metadata such as label text.</p><p>Rare Labels. XC applications with millions of labels typically find that most labels are rare, with very few training data points tagged with those labels. <ref type="figure" target="#fig_0">Fig 1 exemplifies</ref> this on two benchmark datasets where 60-80% labels have &lt; 5 training points. The reasons behind rare labels are manifold. In several XC applications, there may exist an inherent skew in the popularity of labels, e.g, it is natural for certain products to be more popular among users on an e-commerce platform. XC applications also face missing labels <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b50">50]</ref> where training points are not tagged with all the labels relevant to them. Reasons for this include the inability of human users to exhaustively mark all products of interest to them, and biases in the recommendation platform (e.g. website, app) itself which may present or impress upon its users, certain products more often than others.</p><p>arXiv:2108.00261v1 [cs.CL] 31 Jul 2021</p><p>Need for Label Metadata. Rare labels are of critical importance in XC applications. They allow highly personalized yet relevant recommendations that may delight and surprise a user, or else allow precise and descriptive tags to be assigned to a document, etc. However, the paucity of training data for rare labels makes it challenging to predict them accurately. Incorporating label metadata such as textual label descriptions <ref type="bibr" target="#b30">[31]</ref>, label taxonomies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref> and label co-occurrence into the classification process are possible ways to augment the available information for rare labels. Key Contributions of ECLARE. This paper presents ECLARE, an XC method that performs collaborative learning that benefits rare labels. This is done by incorporating multiple forms of label metadata such as label text as well as dynamically inferred label correlation graphs. Critical to ECLARE are augmentations to the architecture and learning pipeline that scale to millions of labels:</p><p>(1) Introduce a framework that allows collaborative extreme learning using label-label correlation graphs that are dynamically generated using asymmetric random walks. This is in contrast to existing approaches that often perform collaborative learning on static user-user or document-document graphs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">48]</ref>. (2) Introduce the use of multiple representations for each label: one learnt from label text alone (LTE), one learnt collaboratively from label correlation graphs (GALE), and a label-specific refinement vector. ECLARE proposes a robust yet inexpensive attention mechanism to fuse these multiple representations to generate a single one-vs-all classifier per label. (3) Propose critical augmentations to well-established XC training steps, such as label clustering, negative sampling, classifier initialization, shortlist creation (GAME), etc, in order to incorporate label correlations in a systematic and scalable manner. (4) Offer an end-to-end training pipeline incorporating the above components in an efficient manner which can be scaled to tasks with millions of labels and offer up to 14% performance boost on standard XC prediction metrics. Comparison to State-of-the-art. Experiments indicate that apart from significant boosts on standard XC metrics (see <ref type="bibr">Tab 2)</ref>, ECLARE offers two distinct advantages over existing XC algorithms, including those that do use label metadata such as label text (see <ref type="bibr">Tab 6)</ref> (1) Superior Rare Label Prediction: In the first example in <ref type="bibr">Tab 6,</ref><ref type="bibr"></ref> for the document "Tibetan Terrier", ECLARE correctly predicts the rare label "Dog of Osu" that appeared just twice in the training set. All other methods failed to predict this rare label. It is notable that this label has no common tokens (words) with the document text or other labels which indicates that relying solely on label text is insufficient. ECLARE offers far superior performance on propensity scored XC metrics which place more emphasis on predicting rare labels correctly (see Tabs 2 and 3). (2) Superior Intent Disambiguation: The second and third examples in Tab 6 further illustrate pitfalls of relying on label text alone as metadata. For the document "85th Academy Awards", all other methods are incapable of predicting other award ceremonies held in the same year and make poor predictions. On the other hand, ECLARE was better than other methods at picking up subtle cues and associations present in the training data to correctly identify associated articles. ECLARE offers higher precision@1 and recall@10 (see Tabs 2 and 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Summary. XC algorithms proposed in literature employ a variety of label prediction approaches like tree, embedding, hashing and one-vs-all-based approaches <ref type="bibr">[1, 2, 4, 6, 8, 10, 11, 15-19, 22, 26, 31, 35-37, 40, 41, 44-47, 49]</ref>. Earlier works learnt label classifiers using fixed representations for documents (typically bag-of-words) whereas contemporary approaches learn a document embedding architecture (typically using deep networks) jointly with the label classifiers. In order to operate with millions of labels, XC methods frequently have to rely on sub-linear time data structures for operations such as shortlisting labels, sampling hard negatives, etc. Choices include hashing <ref type="bibr" target="#b27">[28]</ref>, clustering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">49]</ref>, negative sampling <ref type="bibr" target="#b29">[30]</ref>, etc. Notably, most XC methods except DECAF <ref type="bibr" target="#b30">[31]</ref>, GLaS <ref type="bibr" target="#b9">[10]</ref>, and X-Transformer <ref type="bibr" target="#b5">[6]</ref> do not incorporate any form of label metadata, instead treating labels as black-box identifiers.</p><p>Fixed Representation. Much of the early work in XC used fixed bag-of-words (BoW) features to represent documents. One-vs-all methods such as DiSMEC <ref type="bibr" target="#b0">[1]</ref>, PPDSparse <ref type="bibr" target="#b45">[45]</ref>, ProXML <ref type="bibr" target="#b1">[2]</ref> decompose the XC problem into several binary classification problems, one per label. Although these offered state-of-the-art performance until recently, they could not scale beyond a few million labels.</p><p>To address this, several approaches were suggested to speed up training <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">47]</ref>, and prediction <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref> using tree-based classifiers and negative sampling. These offered high performance as well as could scale to several millions of labels. However, these architectures were suited for fixed features and did not support jointly learning document representations. Attempts, such as <ref type="bibr" target="#b14">[15]</ref>, to use pre-trained features such as FastText <ref type="bibr" target="#b19">[20]</ref> were also not very successful if the features were trained on an entirely unrelated task.</p><p>Representation Learning. Recent works such as X-Transformer <ref type="bibr" target="#b5">[6]</ref>, ASTEC <ref type="bibr" target="#b7">[8]</ref>, XML-CNN <ref type="bibr" target="#b25">[26]</ref>, DECAF <ref type="bibr" target="#b30">[31]</ref> and AttentionXML <ref type="bibr" target="#b49">[49]</ref> propose architectures that jointly learn representations for the documents as well as label classifiers. For the most part, these methods outperform their counterparts that operate on fixed document representations which illustrates the superiority of task-specific document representations over generic pre-trained features. However, some of these methods utilize involved architectures such as attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b49">49]</ref> or convolutions <ref type="bibr" target="#b25">[26]</ref>. It has been observed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref> that in addition to being more expensive to train, these architectures also suffer on XC tasks where the documents are short texts, such as user queries, or product titles.</p><p>XC with Label Metadata. Utilizing label metadata such as label text, label correlations, etc. can be critical for accurate prediction of rare labels, especially on short-text applications where documents have textual descriptions containing only 5-10 tokens which are not very descriptive. Among existing works, GLaS <ref type="bibr" target="#b9">[10]</ref> uses label correlations to design a regularizer that improved performance over rare labels, while X-Transformer <ref type="bibr" target="#b5">[6]</ref> and DECAF <ref type="bibr" target="#b30">[31]</ref> use label text as label metadata instead. X-Transformer utilizes label text to perform semantic label indexing (essentially a shortlisting step) along with a pre-trained-then-fine-tuned RoBERTa <ref type="bibr" target="#b26">[27]</ref> architecture. On the other hand, DECAF uses a simpler architecture to learn both label and document representations in an end-to-end manner. Collaborative Learning for XC. Given the paucity of data for rare labels, the use of label text alone can be insufficient to ensure accurate prediction, especially in short-text applications such as related products and related queries search, where the amount of label text is also quite limited. This suggests using label correlations to perform collaborative learning on the label side. User-user or document-document graphs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b51">51]</ref> have become popular, with numerous methods such as GCN <ref type="bibr" target="#b23">[24]</ref>, Light-GCN <ref type="bibr" target="#b13">[14]</ref>, GraphSAGE <ref type="bibr" target="#b11">[12]</ref>, PinSage <ref type="bibr" target="#b48">[48]</ref>, etc. utilizing graph neural networks to augment user/document representations. However, XC techniques that directly enable label collaboration with millions of labels have not been explored. One of the major barriers for this seems to be that label correlation graphs in XC applications turn out to be extremely sparse, e.g, for the label correlation graph ECLARE constructed for the LF-WikiSeeAlsoTitles-320K dataset, nearly 18% of labels had no edges to any other label. This precludes the use of techniques such as Personalised Page Rank (PPR) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b48">48]</ref> over the ground-truth to generate a set of shortlisted labels for negative sampling. ECLARE solves this problem by first mining hard-negatives for each label using a separate technique, and subsequently augmenting this list by adding highly correlated labels.</p><p>3 ECLARE: EXTREME CLASSIFICATION WITH LABEL GRAPH CORRELATIONS Summary. ECLARE consists of four components 1) a text embedding architecture adapted to short-text applications, 2) one-vs-all classifiers, one per label that incorporate label text as well as label correlations, 3) a shortlister that offers high-recall label shortlists for data points, allowing ECLARE to offer sub-millisecond prediction times even with millions of labels, and 4) a label correlation graph that is used to train both the one-vs-all classifiers as well as the shortlister. This section details these components as well as a technique to infer label correlation graphs from training data itself.</p><p>Notation. Let denote the number of labels and the dictionary size. All training points are presented as (x , y ). x ? R is a bag-of-tokens representation for the th document i.e. is the TF-IDF weight of token ? [ ] in the th document. y ? {?1, +1} is the ground truth label vector with <ref type="figure">Figure 3</ref>: (Left) Document Embedding: ECLARE uses the light-weight embedding block E (see <ref type="figure" target="#fig_1">Fig 2)</ref> to embed documents, ensuring rapid processing at test time (see <ref type="figure">Fig 6)</ref>. Stop words such as for, of ) are discarded. (Right) Label classifiers: ECLARE incorporates multiple forms of label metadata including label text (LTE) and label correlation graphs (GALE), fusing them with a per-label refinement vector? 3 using the attention block (see <ref type="figure" target="#fig_1">Fig 2)</ref> to create a one-vs-all classifier w for each label ? [ ]. Connections to and from the attention block are shown in light gray to avoid clutter. A separate instance of the embedding block is used to obtain document embeddings (E ), LTE (E ) and GALE (E ) embeddings.</p><formula xml:id="formula_0">= +1 if label ? [ ] is ? = RELU ? ? 0 Document Text Embedding ? Chevron Faux Fur Women for STOP LTE = 3 ? ? 3 ? 2 ? ? 2 ? 1 ? ? 1 ? 1 ? 3 Classifier ? 1 = ? ? 0 , ? 0 = ? ? 2 ? Dog STOP of Osu GALE ? = ? ? ? ? ? 0 1 3 2 ? 0 ReLU ? ? 0 ? 0 = = ? ? ?</formula><p>relevant to the th document and = ?1 otherwise. For each label ? [ ], its label text is similarly represented as z ? R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Embedding Architecture</head><p>ECLARE learns -dimensional embeddings for each vocabulary token E = [e 1 , . . . , e ] ? R ? and uses a light-weight embedding block (see <ref type="figure" target="#fig_1">Fig 2)</ref> implementing a residual layer. The embedding block E contains two trainable parameters, a weight matrix R and a scalar weight (see <ref type="figure" target="#fig_1">Fig 2)</ref>. Given a document x ? R as a sparse bagof-words vector, ECLARE performs a rapid embedding (see <ref type="figure">Fig 3)</ref> by first using the token embeddings to obtain an initial representation x 0 = Ex ? R , and then passing this through an instantiation E of the text embedding block, and a ReLU non-linearity, to obtain the final representationx. All documents (train/test) share the same embedding block E . Similar architectures have been shown to be well-suited to short-text applications <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Label Correlation Graph</head><p>XC applications often fail to provide label correlation graphs directly as an input. Moreover, since these applications also face extreme label sparsity, using label co-occurrence alone yields fractured correlations as discussed in Sec 2. For example, label correlations gleaned from products purchased together in the same session, or else queries on which advertisers bid together, may be very sparse. To remedy this, ECLARE infers a label correlation graph using the ground-truth label vectors i.e. y , ? [ ] themselves. This ensures that ECLARE is able to operate even in situations where the application is unable to provide a correlation graph itself. ECLARE adopts a scalable strategy based on random walks with restarts (see Algorithm 1) to obtain a label correlation graph G ? R ? that Algorithm 1 Label Correlation Graph Genration. , denote the number of training points and labels. and denote the walk length and restart probability. Y = [y 1 , . . . , y ] ? {?1, +1} ? is a matrix giving the relevant training labels for each document. For any set , Unif( ) returns a uniformly random sample from . The subroutine WalkFrom performs a walk starting at the label . ? Unif( : = +1 ) ? Sample a relevant label <ref type="bibr">10:</ref> v[ ]++ ? Update the visit counts <ref type="bibr">11:</ref> return v</p><formula xml:id="formula_1">1: procedure WalkFrom( , Y, , ) 2: v ? 0 ? R ? Initialize</formula><formula xml:id="formula_2">12: procedure RandomWalk( , Y, ,<label>) 13:</label></formula><p>G ? 00 ? ? R ? ? Initialize visit counts <ref type="bibr">14:</ref> for = 1; ? ; ++ do 15:</p><formula xml:id="formula_3">G ? WalkFrom( , Y, , ) ? Update row of G 16:</formula><p>return G augments the often meager label co-occurrence links (see <ref type="figure" target="#fig_2">Fig 4)</ref> present in the ground truth. Non-rare labels (the so-called head and torso labels) pose a challenge to this step since they are often correlated with several labels and can overwhelm the rare labels. ECLARE takes two precautions to avoid this:</p><p>(1) Partition: Head labels (those with &gt; 500 training points) are disconnected from the graph by setting G ?? = 1 and G ? = 0 = G ? for all all head labels ? ? [ ] and ? ? (see <ref type="figure">Fig 5)</ref>. (2) Normalization: G is normalized to favor edges to/from rare labels as G = A ?1/2 ?G ?B ?1/2 , where A, B ? R ? are diagonal matrices with the row and column-sums of G respectively.</p><p>Algorithm 1 is used with a restart probability of 80% and a random walk length of 400. Thus, it is overwhelmingly likely that several dozens of restarts would occur for each label. A high restart probability does not let the random walk wander too far thus preventing tenuous correlations among labels from getting captured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Label Representation and Classifiers</head><p>As examples in Tab 6 discussed in Sec 4 show, label text alone may not sufficiently inform classifiers for rare labels. ECLARE remedies this by learning high-capacity one-vs-all classifiers W = [w 1 , . . . , w ] ? R ? with 3 distinct components described below.</p><p>Label Text Embedding (LTE). The first component incorporates label text metadata. A separate instance E of the embedding block is used to embed label text. Given a bag-of-words representation z ? R of a label, the LTE representation is obtained as? <ref type="bibr" target="#b0">1</ref> </p><formula xml:id="formula_4">= E (? 0 )</formula><p>where as before, we have the "initial" representation? 0 = Ez . The embedding block E is shared by all labels. We note that the DECAF method <ref type="bibr" target="#b30">[31]</ref> also uses a similar architecture to embed label text.   <ref type="figure">Figure 5</ref>: To avoid head labels from distorting label correlation patterns, labels with &gt; 500 training points are forcibly disconnected from the label correlation graph and also clustered into head meta-labels separately. Consequently, the GALE and GAME steps have a trivial action on head labels.</p><p>Graph Augmented Label Embedding (GALE). ECLARE augments the LTE representation using the label correlation graph G constructed earlier and a graph convolution network (GCN) <ref type="bibr" target="#b23">[24]</ref>. This presents a departure from previous XC approaches. A typical graph convolution operation consists of two steps which ECLARE effectively implements at extreme scales as shown below (1) Convolution: initial label representations are convolved in a scalable manner using G asz 2 = </p><formula xml:id="formula_5">?? 0 ) where G = =1</formula><p>G encodes the ?hop neighborhood, and E is a separate embedding block for each order . Thus, ECLARE's architecture allows parallelizing high-order convolutions. Whereas ECLARE could be used with larger orders &gt; 1, using = 1 was found to already outperform all competing methods, as well be scalable.</p><p>Refinement Vector and Final Classifier. ECLARE combines the LTE and GALE representations for a label with a high-capacity per-label refinement vector? 3 ? R (for a general value of , z +2 is used) to obtain a one-vs-all classifier w (see <ref type="figure">Fig 3)</ref>. To combine? 1 ,? 2 ,? 3 , ECLARE uses a parameterized attention block A (see <ref type="figure" target="#fig_1">Fig 2)</ref> to learn label-specific attention weights for the three components. This is distinct from previous works such as DECAF which use weights that are shared across labels. <ref type="figure">Fig 9 shows</ref> that ECLARE benefits from this flexibility, with the refinement vector? 3 being more dominant for popular labels that have lots of training data whereas the label metadata based vectors? 2 ,? 1 being more important for rare labels with less training data. The attention block is explained below (see also <ref type="bibr">Fig 2)</ref>. Recall that ECLARE uses = 1.</p><formula xml:id="formula_6">(x) = (T ? ReLU(x)) q = (? 1 ); . . . ; (? +2 ) 1 , . . . , +2 = exp(A ? q ) ?exp(A ? q )? 1 w = 1 ?? 1 + . . . + +2 ?? +2</formula><p>The attention block is parameterized by the matrices T ? R ? and A ? R ( +2)?( +2) . q ? R ( +2) concatenates the transformed components before applying the attention layer A. The above attention mechanism can be seen as a scalable paramaterized option (requiring only O 2 + 2 additional parameters) instead of a more expensive label-specific attention scheme which would have required learning ? ( + 2) parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Meta-labels and the Shortlister</head><p>Despite being accurate, if used naively, one-vs-all classifiers require ? ( ) time at prediction and ? ( ) time to train. This is infeasible with millions of labels. As discussed in Sec 2, sub-linear structures are a common remedy in XC methods to perform label shortlisting during prediction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">45]</ref>. These shortlisting techniques take a data point and return a shortlist of O (log ) labels that is expected to contain most of the positive labels for that data point. However, such shortlists also help during training since the negative labels that get shortlisted for a data point are arguably <ref type="bibr" target="#b13">14</ref> , GAME ReLU ? <ref type="figure">Figure 6</ref>: Prediction Pipeline: ECLARE uses a low-cost prediction pipeline that can make millisecond-level predictions with millions of labels. Given a document/query x, its text embeddingx (see <ref type="figure">Fig 3)</ref> is used by the shortlister S to obtain the O (log ) most probable labels while maintaining high recall using label correlation graphs via GAME. Label classifiers (see <ref type="figure">Fig 3)</ref> for only the shortlisted labels are then used by the ranker R to produce the final ranking of labels.</p><formula xml:id="formula_7">Shortlisting + GAME (red ? relevant) Ranking + GAME , 7 , , , , 1 , ? Tibetan Terrier ? Doc Emb. L18 L5 L13 L9 L14 L1 L12 L8 L4 L6 L17 L16 L15 L2 L10 L3 L11 L7 L18 L5 L13 L9 L14 L1 L12 L8 L4 L6 L17 L16 L15 L2 L10 L3 L11 L7 Short- listing failure 11 ,</formula><p>the most challenging and likely to get confused as being positive for that data point. Thus, one-vs-all classifiers are trained only on positive and shortlisted negative labels, bringing training time down to O ( log ). Similar to previous works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35]</ref>, ECLARE uses a clustering-based shortlister S = {C, H} where C = { 1 , . . . , } is a balanced partitioning of the labels into clusters. We refer to each cluster as a meta label. H = [h 1 , . . . , h ] ? R ? is a set of one-vs-all classifiers, learnt one per meta label.</p><p>Graph Assisted Multi-label Expansion (GAME). ECLARE incorporates graph correlations to further to improve its shortlister. Let M ? {0, 1} ? denote the cluster assignment matrix i.e. = 1 if label is in cluster . We normalize M so that each column sums to unity. Given a data point x and a beam-size , its embeddingx (see <ref type="figure">Fig 3)</ref> is used to shortlist labels as follows Note that this cluster re-ranking step uses an induced cluster correlation graph and can bring in clusters with rare labels missed by the one-vs-all models h . This is distinct from previous works which do not use label correlations for re-ranking. Since the clusters are balanced, the shortlisted clusters always contain a total of |S(x)| = / labels. ECLARE uses = 2 17 clusters and a beam size of ? 30 ? 50 (see Sec 4 for a discussion on hyperparameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Prediction with GAME</head><p>The prediction pipeline for ECLARE is depicted in <ref type="figure">Fig 6 and</ref> involves 3 steps that repeatedly utilize label correlations via GAME.</p><p>(1) Given a document x, use the shortlister S to get a set of meta labels S(x) and their corresponding scores p ? R . (2) For shortlisted labels, apply the one-vs-all classifiers w to calculate the ranking score vectorr ? R with?= (?w ,x?) if ? for some ? S(x) else?= 0. (3) GAME-ify the one-vs-all scores to get r = G ?r (note that G is used this time and not G ). Make final predictions using the joint scores := ? if ? , ? S(x) else = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Efficient Training: the DeepXML Pipeline</head><p>Summary. ECLARE adopts the scalable DeepXML pipeline <ref type="bibr" target="#b7">[8]</ref> that splits training into 4 modules. In summary, Module I jointly learns the token embeddings E, the embedding block E and the shortlister S. E remains frozen hereon. Module II refines P and uses it to retrieve label shortlists for all data points. After performing initialization in Module III, Module IV uses the shortlists generated in Module II to jointly fine-tune E and learn the one-vs-all classifiers W, implicitly learning the embedding blocks E , E and the attention block A in the process.</p><p>Module I. Token embeddings E ? R ? are randomly initialized using <ref type="bibr" target="#b12">[13]</ref>, the residual block within E is initialized to identity. After creating label clusters (see below), each cluster is treated as a meta-label yielding a meta-XC problem on the same training points, but with meta-labels instead of the original labels. Meta-label text is created for each ? [ ] as u = ? z . Meta labels are also endowed with the meta-label correlation graph G = M ? GM where M ? {0, 1} ? is the cluster assignment matrix. One-vs-all meta-classifiers H = [h 1 , . . . , h ] ? R ? are now learnt to solve this meta XC problem. These classifiers have the same form as those for the original problem with 3 components, LTE, GALE, (with corresponding blocks? ,? ) and refinement vector, with an attention block? supervising their combination (parameters within? are initialized randomly). However, in Module-I, refinement vectors are turned off for h to force good token embeddings E to be learnt without support from refinement vectors. Module I solves the meta XC problem while training E ,? ,? ,?, E, implicitly learning H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-label Creation with Graph Augmented Label Centroids.</head><p>Existing approaches such as Parabel or DECAF cluster labels by creating a label centroid for each label by aggregating features for training documents associated with that label as c = : =+1 x . However, ECLARE recognizes that missing labels often lead to an incomplete ground truth, and thus, poor label centroids for rare labels in XC settings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>. <ref type="figure">Fig 8 confirms</ref> this suspicion. ECLARE addresses this by augmenting the label centroids using the label cooccurrence graph G to redefine the centroids as? = =1 ? c .</p><p>Balanced hierarchical binary clustering <ref type="bibr" target="#b35">[36]</ref> is now done on these label centroids for 17 levels to generate = 2 17 label clusters. Note that since token embeddings have not been learnt yet, the raw TF-IDF documents vectors x ? R are used instead ofx ? R .</p><p>Module II. The shortlister is fine-tuned in this module. Label centroids are recomputed as? = =1 ? c where this time, c = :y =+1 Ex using E learnt in Module I. The meta XC problem is recreated and solved again. However, this time, ECLARE allows the meta-classifiers h to also include refinement vectors to better solve the meta-XC problem. In the process of re-learning H, the model parameters E ,? ,? ,? are fine-tuned (E is frozen after Module I). The shortlister S thus obtained is thereafter used to retrieve shortlists S(x ) for each data point ? [ ]. However, distinct from previous work such as Slice <ref type="bibr" target="#b14">[15]</ref>, X-Transformer <ref type="bibr" target="#b5">[6]</ref>, DECAF <ref type="bibr" target="#b30">[31]</ref>, ECLARE uses the GAME strategy to obtain negative samples that take label correlations into account.</p><p>Module III. Residual blocks within E , E , E are initialized to identity, parameters within A are initialized randomly, and the shortlister S and token embeddings E are kept frozen. Refinement vectors for all labels are initialized to? <ref type="bibr" target="#b2">3</ref> </p><formula xml:id="formula_8">= ? [ ] ? Ez .</formula><p>We find this initialization to be both crucial (see Sec 4) as well as distinct from previous works such as DECAF which initialized its counterpart of refinement vectors using simply Ez . Such correlation-agnostic initialization was found to offer worse results than ECLARE's graph augmented initialization.</p><p>Module IV. In this module, the embedding blocks E , E , E are learnt jointly with the per-label refinement vectors? 3 and attention block A, thus learning the one-vs-all classifiers w in the process, However, training is done in O ( log ) time by restricting training to positives and shortlisted negatives for each data point.</p><p>Loss Function and Regularization. ECLARE uses the binary cross entropy loss function for training in Modules I, II and IV using the Adam <ref type="bibr" target="#b22">[23]</ref> optimizer. The residual weights R in the various embedding blocks E , E , E ,? ,? as well as the weights T in the attention block were all subjected to spectral regularization <ref type="bibr" target="#b31">[32]</ref>. All ReLU layers in the architecture also included a dropout layer with 20% rate.</p><p>Key Contributions in Training. ECLARE markedly departs from existing techniques by incorporating label correlation information in a scalable manner at every step of the learning process. Right from Module I, label correlations are incorporated while creating the label centroids leading to higher quality clusters (see <ref type="figure">Fig 8</ref> and <ref type="table" target="#tab_10">Table 7</ref>). The architecture itself incorporates label correlation information using the GALE representations. It is crucial to initialize the refinement vectors? <ref type="bibr" target="#b2">3</ref> properly for which ECLARE uses a graph-augmented initialization. ECLARE continues infusing label correlation during negative sampling using the GAME step. Finally, GAME is used multiple times in the prediction pipeline as well. As the discussion in Sec 4 will indicate, these augmentations are crucial to the performance boosts offered by ECLARE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Datasets and Features. ECLARE was evaluated on 4 publicly available 1 benchmark datasets, LF-AmazonTitles-131K, LF-WikiSeeAlso-Titles-320K, LF-WikiTitles-500K and LF-AmazonTitles-1.3M. These datasets were derived from existing datasets e.g. Amazon-670K, by taking those labels for which label text was available and performing other sanitization steps such as reciprocal pair removal (see <ref type="bibr" target="#b30">[31]</ref> for details). ECLARE was also evaluated on proprietary <ref type="table">Table 1</ref>: Dataset Statistics. A ? sign denotes information that was redacted for proprietary datasets. The first four rows are public datasets and the last two rows are proprietary datasets. Dataset names with an asterisk * next to them correspond to product-to-category tasks whereas others are product-to-product tasks. datasets P2P-2M and P2P-10M, both mined from click logs of the Bing search engine, where a pair of products were considered similar if the Jaccard index of the set of queries which led to a click on them was found to be more than a certain threshold. ECLARE used the word piece tokenizer <ref type="bibr" target="#b38">[39]</ref> to create a shared vocabulary for documents and labels. Please refer to Tab 1 for dataset statistics.</p><p>Baseline algorithms. ECLARE's performance was compared to state-of-the-art deep extreme classifiers which jointly learn document and label representations such as DECAF <ref type="bibr" target="#b30">[31]</ref>, AttentionXML <ref type="bibr" target="#b49">[49]</ref>, Astec <ref type="bibr" target="#b7">[8]</ref>, X-Transformer <ref type="bibr" target="#b5">[6]</ref>, and MACH <ref type="bibr" target="#b27">[28]</ref>. DECAF and X-Transformer are the only methods that also use label text and are therefore the most relevant for comparison with ECLARE. For the sake of completeness, ECLARE was also compared to classifiers which use fixed document representations like DiSMEC <ref type="bibr" target="#b0">[1]</ref>, Parabel <ref type="bibr" target="#b35">[36]</ref>, Bonsai <ref type="bibr" target="#b21">[22]</ref>, and Slice <ref type="bibr" target="#b14">[15]</ref>. All these fixed-representation methods use BoW features, except Slice which used pre-trained FastText <ref type="bibr" target="#b4">[5]</ref> features. The GLaS <ref type="bibr" target="#b9">[10]</ref> method could not be included in our analysis as its code was not publicly available.</p><p>Evaluation. Methods were compared using standard XC metrics, namely Precision (P@ ) and Propensity-scored precision (PSP@ ) <ref type="bibr" target="#b16">[17]</ref>. Recall (R@ ) was also included since XC methods are typically used in the shortlisting pipeline of recommendation systems. Thus, having high recall is equally important as having high precision.</p><p>For evaluation, guidelines provided on the XML repository <ref type="bibr" target="#b2">[3]</ref> were followed. To be consistent, all models were run on a 6-core Intel Skylake 2.4 GHz machine with one Nvidia V100 GPU.</p><p>Hyper-parameters. A beam size of = 30 was used for the dataset LF-AmazonTitles-131K and = 50 for all other datasets. Embedding dimension was set to 300 for datasets with &lt; 400K labels and 512 otherwise. The number of meta-labels = |C| was fixed to |C| = 2 17 for all other datasets except for LF-AmazonTitles-131K where |C| = 2 15 was chosen since the dataset itself has around 2 17 labels. The default PyTorch implementation of the Adam optimizer was used. Dropout with probability 0.2 was used for all datasets. Learning rate was decayed by a decay factor of 0.5 after an interval of 0.5? epoch length. Batch size was taken to be 255 for all datasets. ECLARE Module-I used 20 epochs with an initial learning rate of 0.01. In Modules-II and IV, 10 epochs were used for all datasets with an initial learning rate of 0.008. While constructing the label correlation graph using random walks (see Algorithm 1), a walk length of 400 and restart probability of 80% were used for all datasets.</p><p>Results on benchmark datasets. Tab 2 demonstrates that ECLARE can be significantly more accurate than existing XC methods. In particular, ECLARE could be upto 3% and 10% more accurate as compared to DECAF and X-Transformer respectively in terms of P@1. It should be noted that DECAF and X-Transformer are the only XC methods which use label meta-data. Furthermore, ECLARE could outperform Astec which is specifically designed for rare labels, by up to 7% in terms of PSP@1, indicating that ECLARE offers state-of-the-art accuracies without compromising on rare labels. To further understand the gains of ECLARE, the labels were divided into five bins such that each bin contained an equal number of positive training points <ref type="figure">(Fig 7)</ref>. This ensured that each bin had an equal opportunity to contribute to the overall accuracy. <ref type="figure">Fig 7 indicates</ref> that the major gains of ECLARE come from predicting rare labels correctly. ECLARE could outperform all other deep-learning-based XC methods, as well as fixed-feature-based XC methods by a significant margin of at least 7%. Moreover, ECLARE's recall could be up to 2% higher as compared to all other XC methods.</p><p>Results on Propreitary Bing datasets. ECLARE's performance was also compared on the proprietary datasets LF-P2PTitles-2M and LF-P2PTitles-10M. Note that ECLARE was only compared with those XC methods that were able to scale to 10 million labels on a single GPU within a timeout of one week. ECLARE could be up to 14%, 15%, and 15% more accurate as compared to state-of-the-art methods in terms of P@1, PSP@1 and R@10 respectively. Please refer to Tab 3 for more details.</p><p>Ablation experiments. To scale accurately to millions of labels, ECLARE makes several meticulous design choices. To validate their importance, Tab 5 compares different variants of ECLARE:</p><p>(1) Label-graph augmentations: The label correlation graph G is used by ECLARE in several steps, such as in creating meaningful meta labels, negative sampling, shortlisting of meta-labels, etc.</p><p>To evaluate the importance of these augmentations, in ECLARE-NoGraph, the graph convolution component G was removed from all training steps such as GAME, except while generating the label classifiers (GALE). ECLARE-NoGraph was found  to be up to 3% less accurate than ECLARE. Furthermore, in a variant ECLARE-PPR, inspired by state-of-the-art graph algorithms <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b48">48]</ref>, negative sampling was performed via Personalized Page Rank. ECLARE could be up to 25% more accurate than ECLARE-PPR. This could be attributed to the highly sparse label correlation graph and justifies the importance of ECLARE's label correlation graph as well as it's careful negative sampling. (2) Label components: Much work has been done to augment (document) text representations using graph convolution networks (GCNs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b51">51]</ref>. These methods could also be adapted to convolve label text for ECLARE's GALE component. ECLARE's GALE component was replaced by a LightGCN <ref type="bibr" target="#b13">[14]</ref> and GCN <ref type="bibr" target="#b23">[24]</ref> (with the refinement vectors in place  <ref type="figure">Figure 7</ref>: Analyzing the performance of ECLARE and other methods on popular vs rare labels. Labels were divided into 5 bins in increasing order of popularity. The plots show the overall P@5 for each method (histogram group "complete") and how much each bin contributed to this value. ECLARE clearly draws much of its P@5 performance from the bin with the rarest labels (histogram group 5), i.e ECLARE's superiority over other methods in terms of P@5 comes from predicting challenging rare labels and not easy-to-predict popular labels. ECLARE's lead over other methods is also more prominent for rare bins (histogram groups 4, 5).</p><p>we consider the label co-occurrence graph generated by Y ? Y instead of the random-walk based graph G used by ECLARE. ECLARE-Cooc could be up to 2% less accurate in terms of PSP@1 than ECLARE. This shows that ECLARE's random-walk graph indeed captures long-term dependencies thereby resulting in improved performance on rare labels. Normalizing a directed graph, such as the graph G obtained by Algorithm 1 is a nontrivial problem. In ECLARE-PN, we apply the popular Perron normalization <ref type="bibr" target="#b6">[7]</ref> to the random-walk graph G . Unfortunately, ECLARE-PN leads to a significant loss in propensity-scored metrics for rare labels. This validates the choice of ECLARE's normalization strategy. (4) Combining label representations: The LTE and GALE components of ECLARE could potentially be combined using strategies different from the attention mechanism used by ECLARE. A simple average/sum of the components, (ECLARE-SUM) could be up to 2% worse, which corroborates the need for the attention mechanism for combining heterogeneous components. (5) Higher-order Convolutions: Since the ECLARE framework could handle higher order convolutions &gt; 1 efficiently, we validated the effect of increasing the order. Using = 2 was found to hurt precision by upto 2%. Higher orders e.g. = 3, 4 etc. were intractable at XC scales as the graphs got too dense. The drop in performance when using = 2 could be due to two reasons: (a) at XC scales, exploring higher order neighborhoods would add more noise than information unless proper graph pruning is done afterward and (b) ECLARE's random-walk graph creation procedure already encapsulates some form of higher order proximity which negates the potential for massive benefits when using higher order convolutions. (6) Meta-labels and GAME: ECLARE uses a massive fanout of |C| = 2 17 meta-labels in its shortlister. Ablations with |C| = 2 13 (ECLARE-8K) show that using a smaller fanout can lead to upto 8% loss in precision. Additionally Tab 4 shows that incorporating GAME with other XC methods can also improve their accuracies (although ECLARE continues to lead by a wide margin). In particular incorporating GAME in Parabel and At-tentionXML led to up to 1% increase in accuracy. This validates the utility of GAME as a general XC tool.</p><p>Analysis. This section critically analyzes ECLARE's performance gains by scrutinizing the following components:  <ref type="figure">Figure 8</ref>: A comparison of meta-label clusters created by ECLARE compared to those created by DECAF. Note that the clusters for DECAF are rather amorphous, combing labels with diverse intents whereas those for ECLARE are much more focused. We note that other methods such ASTEC and AttentionXML offered similarly noisy clusters. It is clear that clustering based on label centroids that are augmented using label correlation information creates far superior clusters that do not contain noisy and irrelevant labels. Norm. x Attenton wts.</p><p>LF-AmazonTitles-1.3M || 3 z 3 || || 1 z 1 + 2 z 2 || <ref type="figure">Figure 9</ref>: Analysing the dominance of various components in the label classifier w for rare vs popular labels. For rare labels (on the left), components that focus on label metadata i.e.? 1 ,? 2 gain significance whereas for popular labels (on the right), the unrestricted refinement vector? 3 becomes dominant. This illustrates the importance of graph augmentation for data-scarce labels for which the refinement vectors cannot be trained adequately owing to lack of training data.</p><p>variations in the semantics of each label. To investigate the contribution of the label text and refinement classifiers, <ref type="figure">Fig. 9</ref> plots the average product of the attention weight and norm of each component. It was observed that the label text components LTE and GALE are crucial for rare labels whereas the (extreme) refinement vector? 3 is more important for data-abundant popular labels. (3) Label Text Augmentation: For the document "Tibetan Terrier", ECLARE could make correct rare label predictions like "Dog of Osu" even when the label text exhibits no token similarity with the document text or other co-occurring labels. Other methods such as DECAF failed to understand the semantics of the label and mis-predicted the label "Fox Terrier" wrongly relying on the token "Terrier". We attribute this gain to ECLARE's label correlation graph as "Dog of Osu" correlated well with the labels "Tibetan Spaniel", "Tibetan kyi apso" and "Tibetan Mas </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper presents the architecture and accompanying training and prediction techniques for the ECLARE method to perform extreme multi-label classification at the scale of millions of labels. The specific contributions of ECLARE include a framework for incorporating label graph information at massive scales, as well as critical design and algorithmic choices that enable collaborative learning using label correlation graphs with millions of labels. This includes systematic augmentations to standard XC algorithmic operations such as label-clustering, negative sampling, shortlisting, and reranking, to incorporate label correlations in a manner that scales to tasks with millions of labels, all of which were found to be essential to the performance benefits offered by ECLARE. The creation of label correlation graphs from ground truth data alone and its use in a GCN-style architecture to obtain multiple label representations is critical to ECLARE's performance benefits. The proposed approach greatly outperforms state-of-the-art XC methods on multiple datasets while still offering millisecond level prediction times even on the largest datasets. Thus, ECLARE establishes a standard for incorporating label metadata into XC techniques. These findings suggest promising directions for further study including effective graph pruning for heavy tailed datasets, using higher order convolutions ( &gt; 1) in a scalable manner, and performing collaborative learning with heterogeneous and even multi-modal label sets. This has the potential to enable generalisation to settings where labels include textual objects such as (related) webpages and documents, but also videos, songs, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Number of training points per label for two datasets. Labels are ordered from left to right in increasing order of popularity. 60-80% labels have &lt; 5 training points (the bold horizontal line indicates the 5 training point level).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The building blocks of ECLARE. (Left) The embedding block is used in document and label embeddings. (Right) The attention block is used to fuse multiple label representations into a single label classifier (seeFig 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>An execution of Algorithm 1 on a subset of the LF-WikiSeeAlsoTitles-320K dataset starting from the label "Kangal Dog". (Left) ECLARE uses document-label associations taken from ground-truth label vectors (black color) to infer indirect correlations among labels. (Middle) Random walks (distinct restarts colored differently) infer diverse correlations. (Right) These inferred correlations (marked with the color of the restart that discovered them) augment the meager label co-occurrences present in the ground truth (marked with dotted lines). Head label Non-head label Meta label cluster Label correlation edge Metalabel correlation edge Legend</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 )</head><label>1</label><figDesc>Find the top clusters, say = {?1, . . . ,?} ? [ ] according to the meta-label scores h 1 ,x ? h 2 ,x ? . . .. Letp ? R be a vector containing scores for the top clusters passed through a sigmoid i.e.?= (?h ,x?) if ? else?= 0.(2) Use the induced cluster-cluster correlation matrix G = M ? GM to calculate the "GAME-ified" scores p = G ?p.(3) Find the top clusters according to p, say 1 , . . . , , retain their scores and set scores of other clusters in p to 0. Return the shortlist S(x) = { 1 , . . . , } and the modified score vector p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>tiff" in G. Several such examples exist in the datasets. In another example from the P2P-2M dataset, for the product "Draper's &amp; Damon's Women's Chevron Fauz Fur Coat Tan L", ECLARE could deduce the intent of purchasing "Fur Coat" while other XC methods incorrectly fixated on the brand "Draper's &amp; Damon's". Please refer to Tab 6 for detailed examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Transformation: Whereas traditional GCNs often use a simple non-linearity as the transformation, ECLARE instead uses a separate instance E of the embedding block to obtain the GALE representation of the label as? 2 = E (z 2 ).ECLARE uses a single convolution and transformation operation which allowed it to scale to applications with millions of nodes. Recent works such as LightGCN<ref type="bibr" target="#b13">[14]</ref> propose to accelerate GCNs by removing all non-linearities. Despite being scalable, using Light-GCN itself was found to offer imprecise results in experiments. This may be because ECLARE also uses LTE representations. ECLARE can be seen as improving upon existing GCN architectures such as LightGCN by performing higher order label-text augmentations for = 0, . . . , as? +1 = E (</figDesc><table><row><cell>? [ ]</cell><cell cols="2">?? 0 . Note that</cell></row><row><cell cols="2">due to random restarts used by Algorithm 1, we have</cell><cell>&gt; 0</cell></row><row><cell cols="3">for all ? [ ] and thusz 2 contains a component from? 0 itself.</cell></row><row><cell>(2)</cell><cell></cell></row></table><note>? [ ]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on public benchmark datasets. ECLARE could offer 2-3.5% higher P@1 as well as upto 5% higher PSP@1 which focuses on rare labels. Additionally, ECLARE offered up to 3% better recall than leading XC methods.</figDesc><table><row><cell>Method</cell><cell cols="5">PSP@1 PSP@5 P@1 P@5 R@10</cell><cell>Prediction Time (ms)</cell></row><row><cell></cell><cell></cell><cell cols="3">LF-AmazonTitles-131K</cell><cell></cell><cell></cell></row><row><cell>ECLARE</cell><cell>33.51</cell><cell>44.7</cell><cell cols="3">40.74 19.88 54.11</cell><cell>0.1</cell></row><row><cell>DECAF</cell><cell>30.85</cell><cell>41.42</cell><cell>38.4</cell><cell>18.65</cell><cell>51.2</cell><cell>0.1</cell></row><row><cell>Astec</cell><cell>29.22</cell><cell>39.49</cell><cell cols="2">37.12 18.24</cell><cell>49.87</cell><cell>2.34</cell></row><row><cell>AttentionXML</cell><cell>23.97</cell><cell>32.57</cell><cell cols="2">32.25 15.61</cell><cell>42.3</cell><cell>5.19</cell></row><row><cell>Slice</cell><cell>23.08</cell><cell>31.89</cell><cell cols="2">30.43 14.84</cell><cell>41.16</cell><cell>1.58</cell></row><row><cell>MACH</cell><cell>24.97</cell><cell>34.72</cell><cell cols="2">33.49 16.45</cell><cell>44.75</cell><cell>0.23</cell></row><row><cell>X-Transformer</cell><cell>21.72</cell><cell>27.09</cell><cell cols="2">29.95 13.07</cell><cell>35.59</cell><cell>15.38</cell></row><row><cell>Siamese</cell><cell>13.3</cell><cell>13.36</cell><cell>13.81</cell><cell>5.81</cell><cell>14.69</cell><cell>0.2</cell></row><row><cell>Bonsai</cell><cell>24.75</cell><cell>34.86</cell><cell cols="2">34.11 16.63</cell><cell>45.17</cell><cell>7.49</cell></row><row><cell>Parabel</cell><cell>23.27</cell><cell>32.14</cell><cell>32.6</cell><cell>15.61</cell><cell>41.63</cell><cell>0.69</cell></row><row><cell>DiSMEC</cell><cell>25.86</cell><cell>36.97</cell><cell cols="2">35.14 17.24</cell><cell>46.84</cell><cell>5.53</cell></row><row><cell>XT</cell><cell>22.37</cell><cell>31.64</cell><cell cols="2">31.41 15.48</cell><cell>42.11</cell><cell>9.12</cell></row><row><cell>AnneXML</cell><cell>19.23</cell><cell>32.26</cell><cell cols="2">30.05 16.02</cell><cell>45.57</cell><cell>0.11</cell></row><row><cell></cell><cell></cell><cell cols="3">LF-WikiSeeAlsoTitles-320K</cell><cell></cell><cell></cell></row><row><cell>ECLARE</cell><cell>22.01</cell><cell>26.27</cell><cell cols="3">29.35 15.05 36.46</cell><cell>0.12</cell></row><row><cell>DECAF</cell><cell>16.73</cell><cell>21.01</cell><cell cols="2">25.14 12.86</cell><cell>32.51</cell><cell>0.09</cell></row><row><cell>Astec</cell><cell>13.69</cell><cell>17.5</cell><cell cols="2">22.72 11.43</cell><cell>28.18</cell><cell>2.67</cell></row><row><cell>AttentionXML</cell><cell>9.45</cell><cell>11.73</cell><cell>17.56</cell><cell>8.52</cell><cell>20.56</cell><cell>7.08</cell></row><row><cell>Slice</cell><cell>11.24</cell><cell>15.2</cell><cell>18.55</cell><cell>9.68</cell><cell>24.45</cell><cell>1.85</cell></row><row><cell>MACH</cell><cell>9.68</cell><cell>12.53</cell><cell>18.06</cell><cell>8.99</cell><cell>22.69</cell><cell>0.52</cell></row><row><cell>Siamese</cell><cell>10.1</cell><cell>9.59</cell><cell>10.69</cell><cell>4.51</cell><cell>10.34</cell><cell>0.17</cell></row><row><cell>Bonsai</cell><cell>10.69</cell><cell>13.79</cell><cell>19.31</cell><cell>9.55</cell><cell>23.61</cell><cell>14.82</cell></row><row><cell>Parabel</cell><cell>9.24</cell><cell>11.8</cell><cell>17.68</cell><cell>8.59</cell><cell>20.95</cell><cell>0.8</cell></row><row><cell>DiSMEC</cell><cell>10.56</cell><cell>14.82</cell><cell>19.12</cell><cell>9.87</cell><cell>24.81</cell><cell>11.02</cell></row><row><cell>XT</cell><cell>8.99</cell><cell>11.82</cell><cell>17.04</cell><cell>8.6</cell><cell>21.73</cell><cell>12.86</cell></row><row><cell>AnneXML</cell><cell>7.24</cell><cell>11.75</cell><cell>16.3</cell><cell>8.84</cell><cell>23.06</cell><cell>0.13</cell></row><row><cell></cell><cell></cell><cell cols="3">LF-AmazonTitles-1.3M</cell><cell></cell><cell></cell></row><row><cell>ECLARE</cell><cell>23.43</cell><cell>30.56</cell><cell>50.14</cell><cell>40</cell><cell>32.02</cell><cell>0.32</cell></row><row><cell>DECAF</cell><cell>22.07</cell><cell>29.3</cell><cell cols="3">50.67 40.35 31.29</cell><cell>0.16</cell></row><row><cell>Astec</cell><cell>21.47</cell><cell>27.86</cell><cell cols="2">48.82 38.44</cell><cell>29.7</cell><cell>2.61</cell></row><row><cell>AttentionXML</cell><cell>15.97</cell><cell>22.54</cell><cell cols="2">45.04 36.25</cell><cell>26.26</cell><cell>29.53</cell></row><row><cell>Slice</cell><cell>13.96</cell><cell>19.14</cell><cell>34.8</cell><cell>27.71</cell><cell>20.21</cell><cell>1.45</cell></row><row><cell>MACH</cell><cell>9.32</cell><cell>13.26</cell><cell cols="2">35.68 28.35</cell><cell>19.08</cell><cell>2.09</cell></row><row><cell>Bonsai</cell><cell>18.48</cell><cell>25.95</cell><cell cols="2">47.87 38.34</cell><cell>29.66</cell><cell>39.03</cell></row><row><cell>Parabel</cell><cell>16.94</cell><cell>24.13</cell><cell cols="2">46.79 37.65</cell><cell>28.43</cell><cell>0.89</cell></row><row><cell>DiSMEC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>XT</cell><cell>13.67</cell><cell>19.06</cell><cell>40.6</cell><cell>32.01</cell><cell>22.51</cell><cell>5.94</cell></row><row><cell>AnneXML</cell><cell>15.42</cell><cell>21.91</cell><cell cols="2">47.79 36.91</cell><cell>26.79</cell><cell>0.12</cell></row><row><cell></cell><cell></cell><cell cols="3">LF-WikiTitles-500K</cell><cell></cell><cell></cell></row><row><cell>ECLARE</cell><cell>21.58</cell><cell>19.84</cell><cell cols="2">44.36 16.91</cell><cell>30.59</cell><cell>0.14</cell></row><row><cell>DECAF</cell><cell>19.29</cell><cell>19.96</cell><cell cols="3">44.21 17.36 32.02</cell><cell>0.09</cell></row><row><cell>Astec</cell><cell>18.31</cell><cell>18.56</cell><cell cols="3">44.4 17.49 31.58</cell><cell>2.7</cell></row><row><cell>AttentionXML</cell><cell>14.8</cell><cell>13.88</cell><cell>40.9</cell><cell>15.05</cell><cell>25.8</cell><cell>9</cell></row><row><cell>Slice</cell><cell>13.9</cell><cell>13.82</cell><cell cols="2">25.48 10.98</cell><cell>22.65</cell><cell>1.76</cell></row><row><cell>MACH</cell><cell>13.71</cell><cell>12</cell><cell cols="2">37.74 13.26</cell><cell>23.81</cell><cell>0.8</cell></row><row><cell>Bonsai</cell><cell>16.58</cell><cell>16.4</cell><cell cols="2">40.97 15.66</cell><cell>28.04</cell><cell>17.38</cell></row><row><cell>Parabel</cell><cell>15.55</cell><cell>15.35</cell><cell cols="2">40.41 15.42</cell><cell>27.34</cell><cell>0.81</cell></row><row><cell>DiSMEC</cell><cell>15.88</cell><cell>15.89</cell><cell cols="2">39.42 14.85</cell><cell>26.73</cell><cell>11.71</cell></row><row><cell>XT</cell><cell>14.1</cell><cell>14.38</cell><cell cols="2">38.13 14.66</cell><cell>26.48</cell><cell>7.56</cell></row><row><cell>AnneXML</cell><cell>13.91</cell><cell>13.75</cell><cell>39</cell><cell>14.55</cell><cell>26.27</cell><cell>0.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on proprietary product-to-product (P2P) recommendation datasets. ECLARE could offer significant gains -upto 14% higher P@1, 15% higher PSP@1 and 7% higher R@10 -than competing classifiers.</figDesc><table><row><cell cols="8">Method PSP@1 PSP@3 PSP@5 P@1 P@3 P@5 R@10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">LF-P2PTitles-2M</cell><cell></cell><cell></cell></row><row><cell>ECLARE</cell><cell>41.97</cell><cell>44.92</cell><cell>49.46</cell><cell cols="4">43.79 39.25 33.15 54.44</cell></row><row><cell>DECAF</cell><cell>36.65</cell><cell>40.14</cell><cell>45.15</cell><cell cols="3">40.27 36.65 31.45</cell><cell>48.46</cell></row><row><cell>Astec</cell><cell>32.75</cell><cell>36.3</cell><cell>41</cell><cell cols="3">36.34 33.33 28.74</cell><cell>46.07</cell></row><row><cell>Parabel</cell><cell>30.21</cell><cell>33.85</cell><cell>38.46</cell><cell cols="3">35.26 32.44 28.06</cell><cell>42.84</cell></row><row><cell></cell><cell></cell><cell cols="3">LF-P2PTitles-10M</cell><cell></cell><cell></cell></row><row><cell>ECLARE</cell><cell>35.52</cell><cell>37.91</cell><cell>39.91</cell><cell cols="3">43.14 39.93 36.9</cell><cell>35.82</cell></row><row><cell>DECAF</cell><cell>20.51</cell><cell>21.38</cell><cell>22.85</cell><cell>28.3</cell><cell cols="2">25.75 23.99</cell><cell>20.9</cell></row><row><cell>Astec</cell><cell>20.31</cell><cell>22.16</cell><cell>24.23</cell><cell cols="3">29.75 27.49 25.85</cell><cell>22.3</cell></row><row><cell>Parabel</cell><cell>19.99</cell><cell>22.05</cell><cell>24.33</cell><cell cols="2">30.22 27.77</cell><cell>26.1</cell><cell>22.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>An ablation study exploring the benefits of the GAME step for other XC methods. Although ECLARE still provides the leading accuracies, existing methods show consistent gains from the use of the GAME step.</figDesc><table><row><cell>Method</cell><cell cols="8">PSP@1 PSP@5 P@1 P@5 PSP@1 PSP@5 P@1 P@5</cell></row><row><cell></cell><cell></cell><cell cols="4">LF-AmazonTitles-131K</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Original</cell><cell></cell><cell>|</cell><cell></cell><cell cols="2">With GAME</cell></row><row><cell>ECLARE</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>33.51</cell><cell>44.7</cell><cell cols="2">40.74 19.88</cell></row><row><cell>Parabel</cell><cell>23.27</cell><cell>32.14</cell><cell cols="2">32.6 15.61</cell><cell>24.81</cell><cell>34.94</cell><cell cols="2">33.24 16.51</cell></row><row><cell>AttentionXML</cell><cell>23.97</cell><cell>32.57</cell><cell cols="2">32.25 15.61</cell><cell>24.63</cell><cell>34.48</cell><cell cols="2">32.59 16.25</cell></row><row><cell></cell><cell></cell><cell cols="4">LF-WikiSeeAlsoTitles-320K</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Original</cell><cell></cell><cell>|</cell><cell></cell><cell cols="2">With GAME</cell></row><row><cell>ECLARE</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>22.01</cell><cell>26.27</cell><cell cols="2">29.35 15.05</cell></row><row><cell>Parabel</cell><cell>9.24</cell><cell>11.8</cell><cell cols="2">17.68 8.59</cell><cell>10.28</cell><cell>13.06</cell><cell>17.99</cell><cell>9</cell></row><row><cell>AttentionXML</cell><cell>9.45</cell><cell>11.73</cell><cell cols="2">17.56 8.52</cell><cell>10.05</cell><cell>12.59</cell><cell>17.49</cell><cell>8.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Graph construction: To evaluate the efficacy of ECLARE's graph construction, we compare it to ECLARE-Cooc where</figDesc><table><row><cell></cell><cell></cell><cell>LF-AmazonTitles-131K</cell><cell></cell></row><row><cell></cell><cell>20</cell><cell>ECLARE</cell><cell></cell></row><row><cell></cell><cell></cell><cell>DECAF</cell><cell></cell></row><row><cell>Precision@5</cell><cell>5 10 15</cell><cell>Astec MACH Parabel AttentionXML Slice</cell><cell></cell></row><row><cell>Precision@5</cell><cell>0 0 15 5 10</cell><cell>(#131K) complete (#312K) (#195K) 5 (#68K) (1.96) (Increasing Freq.) 4 (#28K) (4.72) 3 (#18K) (7.38) (#11K) 2 (12.11) Quantiles 5 (1.49) 4 (#74K) (3.94) 3 (#31K) (9.14) 2 (#9K) (30.98) Quantiles (Increasing Freq.) LF-WikiSeeAlsoTitles-320K (#4K) 1 (31.32) 1 (#1K) (200.78) ECLARE DECAF (3) complete Astec MACH Parabel AttentionXML Slice</cell><cell>). Results indicate that ECLARE could be upto 2% and 10% more accurate as compared to LightGCN and GCN. In another variant of ECLARE, the refinement vectors? 3 were removed (ECLARE-NoRefine). Results indicate that ECLARE could be up to 10% more accurate as compared to ECLARE-NoRefine which indicates that the per-label (extreme) refinement vectors are essential for accuracy.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>An ablation study exploring alternate design decisions. Design choices made by ECLARE for its components were found to be optimal among popular alternatives.</figDesc><table><row><cell>Method</cell><cell cols="6">PSP@1 PSP@3 PSP@5 P@1 P@3 P@5</cell></row><row><cell></cell><cell cols="3">LF-AmazonTitles-131K</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECLARE</cell><cell>33.51</cell><cell>39.55</cell><cell>44.7</cell><cell cols="3">40.74 27.54 19.88</cell></row><row><cell>ECLARE-GCN</cell><cell>24.02</cell><cell>29.32</cell><cell>34.02</cell><cell cols="3">30.94 21.12 15.52</cell></row><row><cell>ECLARE-LightGCN</cell><cell>31.36</cell><cell>36.79</cell><cell>41.58</cell><cell cols="3">38.39 25.59 18.44</cell></row><row><cell>ECLARE-Cooc</cell><cell>32.82</cell><cell>38.67</cell><cell>43.72</cell><cell>39.95</cell><cell>26.9</cell><cell>19.39</cell></row><row><cell>ECLARE-PN</cell><cell>32.49</cell><cell>38.15</cell><cell>43.25</cell><cell cols="3">39.63 26.64 19.24</cell></row><row><cell>ECLARE-PPR</cell><cell>12.51</cell><cell>16.42</cell><cell>20.25</cell><cell cols="2">14.42 11.04</cell><cell>8.75</cell></row><row><cell>ECLARE-NoGraph</cell><cell>30.49</cell><cell>36.09</cell><cell>41.13</cell><cell cols="3">37.45 25.45 18.46</cell></row><row><cell>ECLARE-NoLTE</cell><cell>32.3</cell><cell>37.88</cell><cell>42.87</cell><cell cols="3">39.33 26.41 19.05</cell></row><row><cell>ECLARE-NoRefine</cell><cell>28.18</cell><cell>33.14</cell><cell>38.3</cell><cell>29.99</cell><cell>21.6</cell><cell>16.32</cell></row><row><cell>ECLARE-SUM</cell><cell>31.45</cell><cell>36.73</cell><cell>41.65</cell><cell cols="3">38.02 25.54 18.49</cell></row><row><cell>ECLARE-k=2</cell><cell>32.23</cell><cell>38.06</cell><cell>43.23</cell><cell cols="3">39.38 26.57 19.22</cell></row><row><cell>ECLARE-8K</cell><cell>29.98</cell><cell>35.08</cell><cell>39.71</cell><cell>37</cell><cell cols="2">24.86 17.93</cell></row><row><cell></cell><cell cols="3">LF-WikiSeeAlsoTItles-320K</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECLARE</cell><cell>22.01</cell><cell>24.23</cell><cell>26.27</cell><cell cols="3">29.35 19.83 15.05</cell></row><row><cell>ECLARE-GCN</cell><cell>13.76</cell><cell>15.88</cell><cell>17.67</cell><cell cols="3">21.76 14.61 11.14</cell></row><row><cell>ECLARE-LightGCN</cell><cell>19.05</cell><cell>21.24</cell><cell>23.14</cell><cell cols="3">26.31 17.64 13.35</cell></row><row><cell>ECLARE-Cooc</cell><cell>20.96</cell><cell>23.1</cell><cell>25.07</cell><cell cols="2">28.54 19.06</cell><cell>14.4</cell></row><row><cell>ECLARE-PN</cell><cell>20.42</cell><cell>22.56</cell><cell>24.59</cell><cell cols="2">28.24 18.88</cell><cell>14.3</cell></row><row><cell>ECLARE-PPR</cell><cell>4.83</cell><cell>5.53</cell><cell>7.12</cell><cell>5.21</cell><cell>3.82</cell><cell>3.5</cell></row><row><cell>ECLARE-NoGraph</cell><cell>18.44</cell><cell>20.49</cell><cell>22.42</cell><cell cols="3">26.11 17.59 13.35</cell></row><row><cell>ECLARE-NoLTE</cell><cell>20.16</cell><cell>22.22</cell><cell>24.16</cell><cell cols="3">27.73 18.48 13.99</cell></row><row><cell>ECLARE-NoRefine</cell><cell>20.27</cell><cell>21.26</cell><cell>22.8</cell><cell cols="3">24.83 16.61 12.66</cell></row><row><cell>ECLARE-SUM</cell><cell>20.59</cell><cell>22.48</cell><cell>24.36</cell><cell>27.59</cell><cell>18.5</cell><cell>13.99</cell></row><row><cell>ECLARE-k=2</cell><cell>20.12</cell><cell>22.38</cell><cell>24.43</cell><cell cols="3">27.77 18.64 14.14</cell></row><row><cell>ECLARE-8K</cell><cell>13.42</cell><cell>15.03</cell><cell>16.47</cell><cell cols="3">20.31 13.51 10.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>A subjective comparison of the top 5 label predictions by ECLARE and other algorithms on the WikiSeeAlso-350K and P2PTitles-2M datasets. Predictions typeset in black color were a part of the ground truth whereas those in light gray color were not. ECLARE is able to offer precise recommendations for extremely rare labels missed by other methods. For instance, the label "Dog of Osu" in the first example is so rare that it occurred only twice in the training set. This label does not have any token overlaps with its document or co-occurring labels either. This may have caused techniques such as DECAF that rely solely on label text, to miss such predictions. The examples also establish that incorporating label co-occurrence allows ECLARE to infer the correct intent of a document or a user query. For instance, in the second example, all other methods, including DECAF, either incorrectly focus on the tokens "Academy Awards" in the document title and start predicting labels related to other editions of the Academy Awards, or else amorphous labels about entertainment awards in general. On the other hand, ECLARE is able to correctly predict other labels corresponding to award ceremonies held in the same year as the 85th Academy awards, as well as the rare label "List of . . . Best Foreign Language Film". Similarly, in the third example, ECLARE correctly determines that the user is interested in faux fur coats and not necessarily in the brand Draper's &amp; Damon's itself whereas methods such as DECAF that rely solely on label and document text, focus on the brand name alone and start predicting shirts and jackets of the same brand which are irrelevant to the user query.List of organizations of Tibetans in exile, List of domesticated Scottish breeds, History of Tibet, Languages of Bhutan AttentionXML List of organizations of Tibetans in exile, List of domesticated Scottish breeds, Dog, Bull Terrier, Dog crossbreed Document 85th Academy Awards ECLARE List of submissions to the 85th Academy Awards for Best Foreign Language Film, 33rd Golden Raspberry Awards, 19th Screen Actors Guild Awards, 67th Tony Awards, 70th Golden Globe Awards DECAF List of American films of 1956, 87th Academy Awards, List of American films of 1957, 1963 in film, 13th Primetime Emmy Awards Astec 65th Tony Awards, 29th Primetime Emmy Awards, 32nd Golden Raspberry Awards, 64th Primetime Emmy Awards, 18th Screen Actors Guild Awards Parabel 1928 in film, 1931 in film, 1930 in film, 48th Academy Awards, 26th Primetime Emmy Awards, 31st European Film Awards AttentionXML 29th Primetime Emmy Awards, 62nd British Academy Film Awards, 60th Primetime Emmy Awards, 65th Tony Awards, 29th Golden Clustering: As is evident from Fig 8, ECLARE's offers signifi-cantly better clustering quality. Other methods such as DECAF use label centroids over an incomplete ground truth, resulting in clusters of seemingly unrelated labels. For e.g. the label "Bulldog" was clustered with "Great house at Sonning" by DECAF and the label "Dog of Osu" was clustered with "Ferdinand II of Aragon" which never co-occur in training. However ECLARE clusters much more relevant labels together, possibly since it was able to (partly) complete the ground truth using it's label correlation graph G. This was also verified quantitatively by evaluating the clustering quality using the standard Loss of Mutual Information metric (LMI)<ref type="bibr" target="#b8">[9]</ref>. Tab 7 shows that ECLARE has the least LMI compared to other methods such as DECAF and those such as MACH that use random hashes to cluster labels. (2) Component Contribution: ECLARE chooses to dynamically attend on multiple (and heterogeneous) label representations in its classifier components, which allows it to capture nuanced</figDesc><table><row><cell>Algorithm</cell><cell>Predictions</cell></row><row><cell></cell><cell>LF-WikiSeeAlsoTitles-320K</cell></row><row><cell>Document</cell><cell>Tibetan Terrier</cell></row><row><cell>ECLARE</cell><cell>Tibetan Spaniel, Tibetan kyi apso, Lhasa Apso, Dog of Osu, Tibetan Mastiff</cell></row><row><cell>DECAF</cell><cell>Fox Terrier, Tibetan Spaniel, Terrier, Bull Terrier, Bulldog</cell></row><row><cell>Astec</cell><cell>Standard Tibetan, List of domesticated Scottish breeds, List of organizations of Tibetans in exile, Tibet, Riwoche horse</cell></row><row><cell>Parabel</cell><cell>Tibet, Raspberry Awards</cell></row><row><cell></cell><cell>P2PTitles-2M</cell></row><row><cell>Document</cell><cell>Draper's &amp; Damon's Women's Chevron Faux Fur Coat Tan L</cell></row><row><cell>ECLARE</cell><cell>Grey Wolf Faux Fur Coat XXL / Grey, Big on Dots Faux-Fur Coat by LUXE, Avec Les Filles Bonded Faux-Fur Long Coat Size Large</cell></row><row><cell></cell><cell>Black, Roaman's Women's Short Faux-Fur Coat (Black) 1X, Dennis Basso Faux Fur Jacket with Stand Collar Size XX-Small Cappuccino</cell></row><row><cell>DECAF</cell><cell>Draper's &amp; Damon's Women's Petite Cabana Big Shirt Blue P-L, Draper's &amp; Damon's Women's Petite Top It Off Stripe Jacket Blue</cell></row><row><cell></cell><cell>P-L, Draper's &amp; Damon's Women's Petite Standing Ovation Jacket Black P-L, Draper &amp; Damon Jackets &amp; Coats | Draper &amp; Damons</cell></row><row><cell></cell><cell>Size L Colorful Coat Wpockets | Color: Black/Green | Size: L, Draper's &amp; Damon's Women's Impressionist Textured Jacket Multi L</cell></row><row><cell>Astec</cell><cell>Draper's &amp; Damon's Women's Petite Cabana Big Shirt Blue P-L, Draper's &amp; Damon's Women's Petite Top It Off Stripe Jacket Blue</cell></row><row><cell></cell><cell>P-L, Draper's &amp; Damon's Women's Impressionist Textured Jacket Multi L, Draper's &amp; Damon's Women's Embroidered Tulle Jacket</cell></row><row><cell></cell><cell>Dress Blue 14, Draper's &amp; Damon's Women's Petite Standing Ovation Jacket Black P-L</cell></row><row><cell>Parabel</cell><cell>Draper's &amp; Damon's Women's Impressionist Textured Jacket Multi L, Draper's &amp; Damon's Women's Over The Rainbow Jacket Multi</cell></row><row><cell></cell><cell>P-L, Draper's &amp; Damon's Women's Petite Painted Desert Jacket White P-M, Draper Women's Drapers &amp; Damons Pants Suit -Pant</cell></row><row><cell></cell><cell>Suit | Color: Black | Size: L, Draper's &amp; Damon's Women's Petite Floral &amp; Stripe Knit Mesh Jacket Scarlet Multi P-L</cell></row><row><cell>(1)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>An ablation study showing loss of mutual information (lower is better) using various clustering strategies as well as fanouts. Lowering the number of metalabels = |C| hurts performance. Competing methods that do not use graph-augmented clustering offer poor LMI, especially MACH that uses random hashes to cluster labels.</figDesc><table><row><cell>Dataset</cell><cell cols="4">ECLARE ECLARE DECAF MACH</cell></row><row><cell></cell><cell cols="2">|C| = 2 17 |C| = 2 15</cell><cell></cell><cell></cell></row><row><cell>LF-AmazonTitles-131K</cell><cell>5.44</cell><cell>5.82</cell><cell>7.40</cell><cell>29.68</cell></row><row><cell>LF-WikiSeeAlsoTitles-320K</cell><cell>3.96</cell><cell>11.31</cell><cell>5.47</cell><cell>35.31</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Extreme Classification Repository<ref type="bibr" target="#b2">[3]</ref> </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors thank the reviewers for helpful comments that improved the presentation of the paper. They also thank the IIT Delhi HPC facility for computational resources. AM is supported by a Google PhD Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DiSMEC: Distributed Sparse Machines for Extreme Multi-label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data scarcity, robustness and extreme multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The extreme classification repository: Multi-label datasets and code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<ptr target="http://manikvarma.org/downloads/XC/XMLRepository.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparse Local Embeddings for Extreme Multi-label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Taming Pretrained Transformers for Extreme Multi-label Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Laplacians and the Cheeger inequality for directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Combinatorics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeepXML: A Deep Extreme Multi-Label Learning Framework Applied to Short Text Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Divisive Information-Theoretic Feature Clustering Algorithm for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1265" to="1287" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Holtmann-Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In Neurips</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wadbude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<title level="m">Distributional Semantics Meets Multi-Label Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;20)</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;20)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Slice: Scalable Linear Extreme Classifiers trained on 100 Million Labels for Related Searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chunduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Extreme Multi-label Loss Functions for Recommendation, Tagging, Ranking and Other Missing Label Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable Generative Models for Multi-label Learning with Missing Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Modhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accelerating Extreme Classification via Adaptive Feature Agglomeration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extreme F-measure Maximization using Sparse Probability Estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jasinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dembczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pfannschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klerx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hullermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter</title>
		<meeting>the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Supercharging Recommender Systems Using Taxonomies for Learning User Purchase Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhargav</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Garcia-Pueyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bonsai -Diverse and Shallow Trees for Extreme Multi-label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khandagale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babbar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Learning for Extreme Multi-label Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K R</forename><surname>Medini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>In Neurips</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Response Prediction Using Collaborative Filtering with Hierarchies and Side-Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna-Prasad</forename><surname>Chitrapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Deepak Agarwal, and Nagaraj Kota</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and Their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DECAF: Deep Extreme Classification with Label Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spectral Normalization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Label Filters for Large Scale Multilabel Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Abbasnejad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403280</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403280" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20)</title>
		<meeting><address><addrLine>Virtual Event, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2311" to="2320" />
		</imprint>
	</monogr>
	<note>KDD &apos;20</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Extreme multi-label learning with label features for warm-start tagging, ranking and recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harsola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harsola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FastXML: A Fast, Accurate and Stable Treeclassifier for eXtreme Multi-label Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Attentive Neural Architecture Incorporating Song Features for Music Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Pudi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">CRAFTML, an Efficient Clustering-based Random Forest for Extreme Multi-label Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Siblini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuntz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">AnnexML: Approximate Nearest Neighbor Search for Extreme Multi-label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tagami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-Label Patent Categorization with Non-Local Attention-Based Graph Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Pitera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph Attention Networks. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A no-regret generalization of hierarchical softmax to extreme multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jasinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dembczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">PPDSparse: A Parallel Primal-Dual Sparse Method for Extreme Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H I</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PD-Sparse: A Primal and Dual Sparse Approach to Extreme Multiclass and Multilabel Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H I</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Loss Decomposition for Fast Learning in Large Output Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holtmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">AttentionXML: Extreme Multi-Label Text Classification with Multi-Label Attention Based Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Large-scale Multi-label Learning with Missing Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
