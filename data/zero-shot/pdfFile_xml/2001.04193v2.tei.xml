<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Deep Learning for Person Re-identification: A Survey and Outlook</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Deep Learning for Person Re-identification: A Survey and Outlook</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person Re-Identification</term>
					<term>Pedestrian Retrieval</term>
					<term>Literature Survey</term>
					<term>Evaluation Metric</term>
					<term>Deep Learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained significantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, we categorize it into the closed-world and open-world settings. The widely studied closed-world setting is usually applied under various research-oriented assumptions, and has achieved inspiring success using deep learning techniques on a number of datasets. We first conduct a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under specific scenarios. We summarize the open-world Re-ID in terms of five different aspects. By analyzing the advantages of existing methods, we design a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on twelve datasets for FOUR different Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP) for person Re-ID, indicating the cost for finding all the correct matches, which provides an additional criteria to evaluate the Re-ID system for real applications. Finally, some important yet under-investigated open issues are discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>P ERSON re-identification (Re-ID) has been widely studied as a specific person retrieval problem across non-overlapping cameras <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Given a query person-of-interest, the goal of Re-ID is to determine whether this person has appeared in another place at a distinct time captured by a different camera, or even the same camera at a different time instant <ref type="bibr" target="#b2">[3]</ref>. The query person can be represented by an image <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, a video sequence <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and even a text description <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Due to the urgent demand of public safety and increasing number of surveillance cameras, person Re-ID is imperative in intelligent surveillance systems with significant research impact and practical importance.</p><p>Re-ID is a challenging task due to the presence of different viewpoints <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, varying low-image resolutions <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, illumination changes <ref type="bibr" target="#b14">[15]</ref>, unconstrained poses <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, occlusions <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, heterogeneous modalities <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref>, complex camera environments, background clutter <ref type="bibr" target="#b21">[22]</ref>, unreliable bounding box generations, etc. These result in varying variations and uncertainty. In addition, for practical model deployment, the dynamic updated camera network <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, large scale gallery with efficient retrieval <ref type="bibr" target="#b24">[25]</ref>, group uncertainty <ref type="bibr" target="#b25">[26]</ref>, significant domain shift <ref type="bibr" target="#b26">[27]</ref>, unseen testing scenarios <ref type="bibr" target="#b27">[28]</ref>, incremental model updating <ref type="bibr" target="#b28">[29]</ref> and changing cloths <ref type="bibr" target="#b29">[30]</ref> also greatly increase the difficulties. These challenges lead that Re-ID is still unsolved problem. Early research efforts mainly focus on the handcrafted feature construction with body structures <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> or distance metric learning <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. With the advancement of deep learning, person Re-ID has achieved inspiring performance on the widely used benchmarks <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. However, there is still a large gap between the research-oriented scenarios and practical applications <ref type="bibr" target="#b44">[45]</ref>. This motivates us to conduct a comprehensive survey, develop a powerful baseline for different Re-ID tasks and discuss several future directions.</p><p>Though some surveys have also summarized the deep learning techniques <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, our survey makes three major differences: 1) We provide an in-depth and comprehensive analysis of existing deep learning methods by discussing their advantages and limitations, analyzing the state-of-the-arts. This provides insights for future algorithm design and new topic exploration. 2) We design a new powerful baseline (AGW: Attention Generalized mean pooling with Weighted triplet loss) and a new evaluation metric (mINP: mean Inverse Negative Penalty) for future developments. AGW achieves state-of-the-art performance on twelve datasets for four different Re-ID tasks. mINP provides a supplement metric to existing CMC/mAP, indicating the cost to find all the correct matches. 3) We make an attempt to discuss several important research directions with under-investigated open issues to narrow the gap between the closed-world and open-world applications, taking a step towards real-world Re-ID system design.  <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. However, in practical open-world applications, we might also need to process heterogeneous data, which are infrared images <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b59">[60]</ref>, sketches <ref type="bibr" target="#b60">[61]</ref>, depth images <ref type="bibr" target="#b61">[62]</ref>, or even text descriptions <ref type="bibr" target="#b62">[63]</ref>. This motivates the heterogeneous Re-ID in ? 3.1. 2) Bounding Box Generation vs. Raw Images/Videos : For the bounding box generation in Step 2, the closed-world person Re-ID usually performs the training and testing based on the generated bounding boxes, where the bounding boxes mainly contain the person appearance information. In contrast, some practical open-world applications require end-to-end person search from the raw images or videos <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b63">[64]</ref>. This leads to another open-world topic, i.e., end-to-end person search in ? 3.2. 3) Sufficient Annotated Data vs. Unavailable/Limited Labels:</p><p>For the training data annotation in Step 3, the closedworld person Re-ID usually assumes that we have enough annotated training data for supervised Re-ID model training. However, label annotation for each camera pair in every new environment is time consuming and labor intensive, incurring high costs. In openworld scenarios, we might not have enough annotated data (i.e., limited labels) <ref type="bibr" target="#b64">[65]</ref> or even without any label information <ref type="bibr" target="#b65">[66]</ref>. This inspires the discussion of the unsupervised and semi-supervised Re-ID in ? 3.3. 4) Correct Annotation vs. Noisy Annotation: For Step 4, existing closed-world person Re-ID systems usually assume that all the annotations are correct, with clean labels. However, annotation noise is usually unavoidable due to annotation error (i.e., label noise) or imperfect detection/tracking results (i.e., sample noise, partial Re-ID <ref type="bibr" target="#b66">[67]</ref>). This leads to the analysis of noise-robust person Re-ID under different noise types in ? 3.4. 5) Query Exists in Gallery vs. Open-set: In the pedestrian retrieval stage (Step 5), most existing closed-world person Re-ID works assume that the query must occur in the gallery set by calculating the CMC <ref type="bibr" target="#b67">[68]</ref> and mAP <ref type="bibr" target="#b4">[5]</ref>. However, in many scenarios, the query person may not appear in the gallery set <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, or we need to perform the verification rather than retrieval <ref type="bibr" target="#b25">[26]</ref>. This brings us to the open-set person Re-ID in ? 3.5.</p><p>This survey first introduces the widely studied person Re-ID under closed-world settings in ? 2. A detailed review on the datasets and the state-of-the-arts are conducted in ? 2.4. We then introduce the open-world person Re-ID in ? 3. An outlook for future Re-ID is presented in ? 4, including a new evaluation metric ( ? 4.1), a new powerful AGW baseline <ref type="bibr">( ? 4.2)</ref>. We discuss several under-investigated open issues for future study <ref type="bibr">( ? 4.3)</ref>. Conclusions will be drawn in ? 5. A structure overview is shown in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CLOSED-WORLD PERSON RE-IDENTIFICATION</head><p>This section provides an overview for closed-world person Re-ID. As discussed in ? 1, this setting usually has the following assumptions: 1) person appearances are captured by single-modality visible cameras, either by image or video; 2) The persons are represented by bounding boxes, where most of the bounding box area belongs the same identity; 3) The training has enough annotated training data for supervised discriminative Re-ID model learning; 4) The annotations are generally correct; 5) The query person must appear in the gallery set. Typically, a standard closed-world Re-ID system contains three main components: Feature Representation Learning ( ? 2.1), which focuses on developing the feature construction strategies; Deep Metric Learning ( ? 2.2), which aims at designing the training objectives with different loss functions or sampling strategies; and Ranking Optimization ( ? 2.3), which concentrates on optimizing the retrieved ranking list. An overview of the datasets and state-of-thearts with in-depth analysis is provided in ? 2.4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Representation Learning</head><p>We firstly discuss the feature learning strategies in closedworld person Re-ID. There are four main categories (as shown in <ref type="figure">Fig. 2</ref>): a) Global Feature ( ? 2.1.1), it extracts a global feature representation vector for each person image without additional annotation cues <ref type="bibr" target="#b54">[55]</ref>; b) Local Feature ( ? 2.1.2), it aggregates part-level local features to formulate a combined representation for each person image <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>; c) Auxiliary Feature ( ? 2.1.3), it improves the feature representation learning using auxiliary information, e.g., attributes <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b77">[78]</ref>, GAN generated images <ref type="bibr" target="#b41">[42]</ref>, etc. d) Video Feature ( ? 2.1.4), it learns video representation for video-based Re-ID <ref type="bibr" target="#b6">[7]</ref> using multiple image frames and temporal information <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>. We also review several specific architecture designs for person Re-ID in ? 2.1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Global Feature Representation Learning</head><p>Global feature representation learning extracts a global feature vector for each person image, as shown in <ref type="figure">Fig. 2(a)</ref>. Since deep neural networks are originally applied in image classification <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref>, global feature learning is the primary choice when integrating advanced deep learning techniques into the person Re-ID field in early years.</p><p>To capture the fine-grained cues in global feature learning, A joint learning framework consisting of a singleimage representation (SIR) and cross-image representation (CIR) is developed in <ref type="bibr" target="#b80">[81]</ref>, trained with triplet loss using specific sub-networks. The widely-used ID-discriminative Embedding (IDE) model <ref type="bibr" target="#b54">[55]</ref> constructs the training process as a multi-class classification problem by treating each identity as a distinct class. It is now widely used in Re-ID community <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref>. Qian et al. <ref type="bibr" target="#b83">[84]</ref> develop a multi-scale deep representation learning model to capture discriminative cues at different scales. Attention Information. Attention schemes have been widely studied in literature to enhance representation learning <ref type="bibr" target="#b84">[85]</ref>. 1) Group 1: Attention within the person image. Typical strategies include the pixel level attention <ref type="bibr" target="#b85">[86]</ref> and the channel-wise feature response re-weighting <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>, or background suppressing <ref type="bibr" target="#b21">[22]</ref>. The spatial information is integrated in <ref type="bibr" target="#b89">[90]</ref>. 2) Group 2: attention across multiple person images. A context-aware attentive feature learning method is proposed in <ref type="bibr" target="#b90">[91]</ref>, incorporating both an intra-sequence and inter-sequence attention for pair-wise feature alignment and refinement. The attention consistency property is added in <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b92">[93]</ref>. Group similarity <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b94">[95]</ref> is another popular approach to leverage the cross-image attention, which involves multiple images for local and global similarity modeling. The first group mainly enhances the robustness against misalignment/imperfect detection, and the second improves the feature learning by mining the relations across multiple images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Local Feature Representation Learning</head><p>It learns part/region aggregated features, making it robust against misalignment <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b95">[96]</ref>. The body parts are either automatically generated by human parsing/pose estimation (Group 1) or roughly horizontal division (Group 2).</p><p>With automatic body part detection, the popular solution is to combine the full body representation and local part features <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b97">[98]</ref>. Specifically, the multi-channel aggregation <ref type="bibr" target="#b98">[99]</ref>, multi-scale context-aware convolutions <ref type="bibr" target="#b99">[100]</ref>, multistage feature decomposition <ref type="bibr" target="#b16">[17]</ref> and bilinear-pooling <ref type="bibr" target="#b96">[97]</ref> are designed to improve the local feature learning. Rather than feature level fusion, the part-level similarity combination is also studied in <ref type="bibr" target="#b97">[98]</ref>. Another popular solution is to enhance the robustness against background clutter, using the pose-driven matching <ref type="bibr" target="#b100">[101]</ref>, pose-guided part attention module <ref type="bibr" target="#b101">[102]</ref>, semantically part alignment <ref type="bibr" target="#b102">[103]</ref>, <ref type="bibr" target="#b103">[104]</ref>.</p><p>For horizontal-divided region features, multiple partlevel classifiers are learned in Part-based Convolutional Baseline (PCB) <ref type="bibr" target="#b76">[77]</ref>, which now serves as a strong part feature learning baseline in the current state-of-the-art <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b105">[106]</ref>. To capture the relations across multiple body parts, the Siamese Long Short-Term Memory (LSTM) architecture <ref type="bibr" target="#b95">[96]</ref>, second-order non-local attention <ref type="bibr" target="#b106">[107]</ref>, Interaction-and-Aggregation (IA) <ref type="bibr" target="#b107">[108]</ref> are designed to reinforce the feature learning.</p><p>The first group uses human parsing techniques to obtain semantically meaningful body parts, which provides wellalign part features. However, they require an additional pose detector and are prone to noisy pose detections <ref type="bibr" target="#b76">[77]</ref>. The second group uses a uniform partition to obtain the horizontal stripe parts, which is more flexible, but it is sensitive to heavy occlusions and large background clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Auxiliary Feature Representation Learning</head><p>Auxiliary feature representation learning usually requires additional annotated information (e.g., semantic attributes <ref type="bibr" target="#b70">[71]</ref>) or generated/augmented training samples to reinforce the feature representation <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b41">[42]</ref>.</p><p>Semantic Attributes. A joint identity and attribute learning baseline is introduced in <ref type="bibr" target="#b71">[72]</ref>. Su et al. <ref type="bibr" target="#b70">[71]</ref> propose a deep attribute learning framework by incorporating the predicted semantic attribute information, enhancing the CNN CNN CNN "male", "short hair" CNN (a) Global (b) Local (c) Auxiliary (d) Video <ref type="figure">Fig. 2</ref>: Four different feature learning strategies. a) Global Feature, learning a global representation for each person image in ? 2.1.1; b) Local Feature, learning part-aggregated local features in ? 2.1.2; c) Auxiliary Feature, learning the feature representation using auxiliary information, e.g., attributes <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref> in ? 2.1.3 and d) Video Feature , learning the video representation using multiple image frames and temporal information <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref> in ? 2.1.4. generalizability and robustness of the feature representation in a semi-supervised learning manner. Both the semantic attributes and the attention scheme are incorporated to improve part feature learning <ref type="bibr" target="#b108">[109]</ref>. Semantic attributes are also adopted in <ref type="bibr" target="#b109">[110]</ref> for video Re-ID feature representation learning. They are also leveraged as the auxiliary supervision information in unsupervised learning <ref type="bibr" target="#b110">[111]</ref>.</p><p>Viewpoint Information. The viewpoint information is also leveraged to enhance the feature representation learning <ref type="bibr" target="#b111">[112]</ref>, <ref type="bibr" target="#b112">[113]</ref>. Multi-Level Factorisation Net (MLFN) <ref type="bibr" target="#b111">[112]</ref> also tries to learn the identity-discriminative and viewinvariant feature representations at multiple semantic levels. Liu et al. <ref type="bibr" target="#b112">[113]</ref> extract a combination of view-generic and view-specific learning. An angular regularization is incorporated in <ref type="bibr" target="#b113">[114]</ref> in the viewpoint-aware feature learning.</p><p>Domain Information. A Domain Guided Dropout (DGD) algorithm <ref type="bibr" target="#b53">[54]</ref> is designed to adaptively mine the domain-sharable and domain-specific neurons for multidomain deep feature representation learning. Treating each camera as a distinct domain, Lin et al. <ref type="bibr" target="#b114">[115]</ref> propose a multicamera consistent matching constraint to obtain a globally optimal representation in a deep learning framework. Similarly, the camera view information or the detected camera location is also applied in <ref type="bibr" target="#b17">[18]</ref> to improve the feature representation with camera-specific information modeling. GAN Generation. This section discusses the use of GAN generated images as the auxiliary information. Zheng et al. <ref type="bibr" target="#b41">[42]</ref> start the first attempt to apply the GAN technique for person Re-ID. It improves the supervised feature representation learning with the generated person images. Pose constraints are incorporated in <ref type="bibr" target="#b115">[116]</ref> to improve the quality of the generated person images, generating the person images with new pose variants. A pose-normalized image generation approach is designed in <ref type="bibr" target="#b116">[117]</ref>, which enhances the robustness against pose variations. Camera style information <ref type="bibr" target="#b117">[118]</ref> is also integrated in the image generation process to address the cross camera variations. A joint discriminative and generative learning model <ref type="bibr" target="#b118">[119]</ref> separately learns the appearance and structure codes to improve the image generation quality. Using the GAN generated images is also a widely used approach in unsupervised domain adaptation Re-ID <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b120">[121]</ref>, approximating the target distribution.</p><p>Data Augmentation. For Re-ID, custom operations are random resize, cropping and horizontal flip <ref type="bibr" target="#b121">[122]</ref>. Besides, adversarially occluded samples <ref type="bibr" target="#b18">[19]</ref> are generated to augment the variation of training data. A similar random erasing strategy is proposed in <ref type="bibr" target="#b122">[123]</ref>, adding random noise to the input images. A batch DropBlock <ref type="bibr" target="#b123">[124]</ref> randomly drops a region block in the feature map to reinforce the attentive feature learning. Bak et al. <ref type="bibr" target="#b124">[125]</ref> generate the virtual humans rendered under different illumination conditions. These methods enrich the supervision with the augmented samples, improving the generalizability on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Video Feature Representation Learning</head><p>Video-based Re-ID is another popular topic <ref type="bibr" target="#b125">[126]</ref>, where each person is represented by a video sequence with multiple frames. Due to the rich appearance and temporal information, it has gained increasing interest in the Re-ID community. This also brings in additional challenges in video feature representation learning with multiple images.</p><p>The primary challenge is to accurately capture the temporal information. A recurrent neural network architecture is designed for video-based person Re-ID <ref type="bibr" target="#b126">[127]</ref>, which jointly optimizes the final recurrent layer for temporal information propagation and the temporal pooling layer. A weighted scheme for spatial and temporal streams is developed in <ref type="bibr" target="#b127">[128]</ref>. Yan et al. <ref type="bibr" target="#b128">[129]</ref> present a progressive/sequential fusion framework to aggregate the framelevel human region representations. Semantic attributes are also adopted in <ref type="bibr" target="#b109">[110]</ref> for video Re-ID with feature disentangling and frame re-weighting. Jointly aggregating the framelevel feature and spatio-temporal appearance information is crucial for video representation learning <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b130">[131]</ref>, <ref type="bibr" target="#b131">[132]</ref>.</p><p>Another major challenge is the unavoidable outlier tracking frames within the videos. Informative frames are selected in a joint Spatial and Temporal Attention Pooling Network (ASTPN) <ref type="bibr" target="#b130">[131]</ref>, and the contextual information is integrated in <ref type="bibr" target="#b129">[130]</ref>. A co-segmentation inspired attention model <ref type="bibr" target="#b131">[132]</ref> detects salient features across multiple video frames with mutual consensus estimation. A diversity regularization <ref type="bibr" target="#b132">[133]</ref> is employed to mine multiple discriminative body parts in each video sequence. An affine hull is adopted to handle the outlier frames within the video sequence <ref type="bibr" target="#b82">[83]</ref>. An interesting work <ref type="bibr" target="#b19">[20]</ref> utilizes the multiple video frames to auto-complete occluded regions. These works demonstrate that handling the noisy frames can greatly improve the video representation learning.</p><p>It is also challenging to handle the varying lengths of video sequences, Chen et al. <ref type="bibr" target="#b133">[134]</ref> divide the long video sequences into multiple short snippets, aggregating the topranked snippets to learn a compact embedding. A clip-level learning strategy <ref type="bibr" target="#b134">[135]</ref> exploits both spatial and temporal dimensional attention cues to produce a robust clip-level representation. Both the short-and long-term relations <ref type="bibr" target="#b135">[136]</ref> are integrated in a self-attention scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Architecture Design</head><p>Framing person Re-ID as a specific pedestrian retrieval problem, most existing works adopt the network architectures <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref> designed for image classification as the backbone. Some works have tried to modify the backbone architecture to achieve better Re-ID features. For the widely used ResNet50 backbone <ref type="bibr" target="#b79">[80]</ref>, the important modifications include changing the last convolutional stripe/size to 1 <ref type="bibr" target="#b76">[77]</ref>, employing adaptive average pooling in the last pooling layer <ref type="bibr" target="#b76">[77]</ref>, and adding bottleneck layer with batch normalization after the pooling layer <ref type="bibr" target="#b81">[82]</ref>.</p><p>Accuracy is the major concern for specific Re-ID network architecture design to improve the accuracy, Li et al. <ref type="bibr" target="#b42">[43]</ref> start the first attempt by designing a filter pairing neural network (FPNN), which jointly handles misalignment and occlusions with part discriminative information mining. Wang et al. <ref type="bibr" target="#b88">[89]</ref> propose a BraidNet with a specially designed WConv layer and Channel Scaling layer. The WConv layer extracts the difference information of two images to enhance the robustness against misalignments and Channel Scaling layer optimizes the scaling factor of each input channel. A Multi-Level Factorisation Net (MLFN) <ref type="bibr" target="#b111">[112]</ref> contains multiple stacked blocks to model various latent factors at a specific level, and the factors are dynamically selected to formulate the final representation. An efficient fully convolutional Siamese network <ref type="bibr" target="#b136">[137]</ref> with convolution similarity module is developed to optimize multi-level similarity measurement. The similarity is efficiently captured and optimized by using the depth-wise convolution.</p><p>Efficiency is another important factor for Re-ID architecture design. An efficient small scale network, namely Omni-Scale Network (OSNet) <ref type="bibr" target="#b137">[138]</ref>, is designed by incorporating the point-wise and depth-wise convolutions. To achieve multi-scale feature learning, a residual block composed of multiple convolutional streams is introduced.</p><p>With the increasing interest in auto-machine learning, an Auto-ReID <ref type="bibr" target="#b138">[139]</ref> model is proposed. Auto-ReID provides an efficient and effective automated neural architecture design based on a set of basic architecture components, using a part-aware module to capture the discriminative local Re-ID features. This provides a potential research direction in exploring powerful domain-specific architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Metric Learning</head><p>Metric learning has been extensively studied before the deep learning era by learning a Mahalanobis distance function <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> or projection matrix <ref type="bibr" target="#b39">[40]</ref>. The role of metric learning has been replaced by the loss function designs to guide the feature representation learning. We will first review the widely used loss functions in ? 2.2.1 and then summarize the training strategies with specific sampling designs ? 2.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Loss Function Design</head><p>This survey only focuses on the loss functions designed for deep learning <ref type="bibr" target="#b55">[56]</ref>. An overview of the distance metric learning designed for hand-crafted systems can be found in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b142">[143]</ref>. There are three widely studied loss functions with their variants in the literature for person Re-ID, including the identity loss, verification loss and triplet loss. An illustration of three loss functions is shown in <ref type="figure" target="#fig_0">Fig. 3</ref>.  <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b117">[118]</ref>, <ref type="bibr" target="#b139">[140]</ref> ; (b) Verification Loss <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b140">[141]</ref> and (c) Triplet Loss <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b56">[57]</ref>. Many works employ their combinations <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b140">[141]</ref>, <ref type="bibr" target="#b141">[142]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity Loss. It treats the training process of person</head><p>Re-ID as an image classification problem <ref type="bibr" target="#b54">[55]</ref>, i.e., each identity is a distinct class. In the testing phase, the output of the pooling layer or embedding layer is adopted as the feature extractor. Given an input image x i with label y i , the predicted probability of x i being recognized as class y i is encoded with a softmax function, represented by p(y i |x i ). The identity loss is then computed by the cross-entropy</p><formula xml:id="formula_0">L id = ? 1 n n i=1 log(p(y i |x i )),<label>(1)</label></formula><p>where n represents the number of training samples within each batch. The identity loss has been widely used in existing methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b105">[106]</ref>, <ref type="bibr" target="#b117">[118]</ref>, <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b139">[140]</ref>, <ref type="bibr" target="#b143">[144]</ref>. Generally, it is easy to train and automatically mine the hard samples during the training process, as demonstrated in <ref type="bibr" target="#b144">[145]</ref>. Several works have also investigated the softmax variants <ref type="bibr" target="#b145">[146]</ref>, such as the sphere loss in <ref type="bibr" target="#b146">[147]</ref> and AM softmax in <ref type="bibr" target="#b94">[95]</ref>. Another simple yet effective strategy, i.e., label smoothing <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b121">[122]</ref>, is generally integrated into the standard softmax cross-entropy loss. Its basic idea is to avoid the model fitting to over-confident annotated labels, improving the generalizability <ref type="bibr" target="#b147">[148]</ref>. Verification Loss. It optimizes the pairwise relationship, either with a contrastive loss <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b119">[120]</ref> or binary verification loss <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b140">[141]</ref>. The contrastive loss improves the relative pairwise distance comparison, formulated by</p><formula xml:id="formula_1">L con = (1 ? ? ij ){max(0, ? ? d ij )} 2 + ? ij d 2 ij ,<label>(2)</label></formula><p>where d ij represents the Euclidean distance between the embedding features of two input samples x i and x j . ? ij is a binary label indicator (? ij = 1 when x i and x j belong to the same identity, and ? ij = 0, otherwise). ? is a margin parameter. There are several variants, e.g., the pairwise comparison with ranking SVM in <ref type="bibr" target="#b80">[81]</ref>. Binary verification <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b140">[141]</ref> discriminates the positive and negative of a input image pair. Generally, a differential feature f ij is obtained by f ij = (f j ? f j ) 2 <ref type="bibr" target="#b140">[141]</ref>, where f i and f j are the embedding features of two samples x i and x j . The verification network classifies the differential feature into positive or negative. We use p(? ij |f ij ) to represent the probability of an input pair (x i and x j ) being recognized as ? ij (0 or 1). The verification loss with cross-entropy is</p><formula xml:id="formula_2">L veri (i, j) = ?? ij log(p(? ij |f ij ))?(1?? ij ) log(1?p(? ij |f ij )).</formula><p>(3) The verification is often combined with the identity loss to improve the performance <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b119">[120]</ref>, <ref type="bibr" target="#b140">[141]</ref>.</p><p>Triplet loss. It treats the Re-ID model training process as a retrieval ranking problem. The basic idea is that the distance between the positive pair should be smaller than the negative pair by a pre-defined margin <ref type="bibr" target="#b56">[57]</ref>. Typically, a triplet contains one anchor sample x i , one positive sample x j with the same identity, and one negative sample x k from a different identity. The triplet loss with a margin parameter is represented by</p><formula xml:id="formula_3">L tri (i, j, k) = max(? + d ij ? d ik , 0),<label>(4)</label></formula><p>where d(?) measures the Euclidean distance between two samples. The large proportion of easy triplets will dominate the training process if we directly optimize above loss function, resulting in limited discriminability. To alleviate this issue, various informative triplet mining methods have been designed <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b96">[97]</ref>. The basic idea is to select the informative triplets <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b148">[149]</ref>. Specifically, a moderate positive mining with a weight constraint is introduced in <ref type="bibr" target="#b148">[149]</ref>, which directly optimizes the feature difference. Hermans et al. <ref type="bibr" target="#b56">[57]</ref> demonstrate that the online hardest positive and negative mining within each training batch is beneficial for discriminative Re-ID model learning. Some methods also studied the point to set similarity strategy for informative triplet mining <ref type="bibr" target="#b149">[150]</ref>, <ref type="bibr" target="#b150">[151]</ref>. This enhances robustness against the outlier samples with a soft hard-mining scheme.</p><p>To further enrich the triplet supervision, a quadruplet deep network is developed in <ref type="bibr" target="#b151">[152]</ref>, where each quadruplet contains one anchor sample, one positive sample and two mined negative samples. The quadruplets are formulated with a margin-based online hard negative mining. Optimizing the quadruplet relationship results in smaller intra-class variation and larger inter-class variation.</p><p>The combination of triplet loss and identity loss is one of the most popular solutions for deep Re-ID model learning <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b102">[103]</ref>, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b115">[116]</ref>, <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b141">[142]</ref>, <ref type="bibr" target="#b152">[153]</ref>, <ref type="bibr" target="#b153">[154]</ref>. These two components are mutually beneficial for discriminative feature representation learning.</p><p>OIM loss. In addition to the above three kinds of loss functions, an Online Instance Matching (OIM) loss <ref type="bibr" target="#b63">[64]</ref> is designed with a memory bank scheme. A memory bank {v k , k = 1, 2, ? ? ? , c} contains the stored instance features, where c denotes the class number. The OIM loss is then formulated by</p><formula xml:id="formula_4">L oim = ? 1 n n i=1 log exp(v T i f i /? ) c k=1 exp(v T k f i /? ) ,<label>(5)</label></formula><p>where v i represents the corresponding stored memory feature for class y i , and ? is a temperature parameter that controls the similarity space <ref type="bibr" target="#b144">[145]</ref>. v T i f i measures the online instance matching score. The comparison with a memorized feature set of unlabelled identities is further included to calculate the denominator <ref type="bibr" target="#b63">[64]</ref>, handling the large instance number of non-targeted identities. This memory scheme is also adopted in unsupervised domain adaptive Re-ID <ref type="bibr" target="#b105">[106]</ref>.   <ref type="figure">Fig. 4</ref>: An illustration of re-ranking in person Re-ID. Given a query example, an initial rank list is retrieved, where the hard matches are ranked in the bottom. Using the topranked easy positive match (1) as query to search in the gallery, we can get the hard match <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula">(3)</ref> with similarity propagation in the gallery set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Training strategy</head><p>The batch sampling strategy plays an important role in discriminative Re-ID model learning. It is challenging since the number of annotated training images for each identity varies significantly <ref type="bibr" target="#b4">[5]</ref>. Meanwhile, the severely imbalanced positive and negative sample pairs increases additional difficulty for the training strategy design <ref type="bibr" target="#b39">[40]</ref>. The most commonly used training strategy for handling the imbalanced issue is identity sampling <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b121">[122]</ref>. For each training batch, a certain number of identities are randomly selected, and then several images are sampled from each selected identity. This batch sampling strategy guarantees the informative positive and negative mining.</p><p>To handle the imbalance issue between the positive and negative, adaptive sampling is the popular approach to adjust the contribution of positive and negative samples, such as Sample Rate Learning (SRL) <ref type="bibr" target="#b88">[89]</ref>, curriculum sampling <ref type="bibr" target="#b86">[87]</ref>. Another approach is sample re-weighting, using the sample distribution <ref type="bibr" target="#b86">[87]</ref> or similarity difference <ref type="bibr" target="#b51">[52]</ref> to adjust the sample weight. An efficient reference constraint is designed in <ref type="bibr" target="#b154">[155]</ref> to transform the pairwise/triplet similarity to a sample-to-reference similarity, addressing the imbalance issue and enhancing the discriminability, which is also robust to outliers.</p><p>To adaptively combine multiple loss functions, a multiloss dynamic training strategy <ref type="bibr" target="#b155">[156]</ref> adaptively reweights the identity loss and triplet loss, extracting appropriate component shared between them. This multi-loss training strategy leads to consistent performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Ranking Optimization</head><p>Ranking optimization plays a crucial role in improving the retrieval performance in the testing stage. Given an initial ranking list, it optimizes the ranking order, either by automatic gallery-to-gallery similarity mining <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b156">[157]</ref> or human interaction <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b158">[159]</ref>. Rank/Metric fusion <ref type="bibr" target="#b159">[160]</ref>, <ref type="bibr" target="#b160">[161]</ref> is another popular approach for improving the ranking performance with multiple ranking list inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Re-ranking</head><p>The basic idea of re-ranking is to utilize the gallery-togallery similarity to optimize the initial ranking list, as shown in <ref type="figure">Fig. 4</ref>. The top-ranked similarity pulling and bottom-ranked dissimilarity pushing is proposed in <ref type="bibr" target="#b156">[157]</ref>. The widely-used k-reciprocal reranking <ref type="bibr" target="#b57">[58]</ref> mines the contextual information. Similar idea for contextual information modeling is applied in <ref type="bibr" target="#b24">[25]</ref>. Bai et al. <ref type="bibr" target="#b161">[162]</ref> utilize the geometric structure of the underlying manifold. An expanded cross neighborhood re-ranking method <ref type="bibr" target="#b17">[18]</ref> is introduced by integrating the cross neighborhood distance. A local blurring re-ranking <ref type="bibr" target="#b94">[95]</ref> employs the clustering structure to improve neighborhood similarity measurement.</p><p>Query Adaptive. Considering the query difference, some methods have designed the query adaptive retrieval strategy to replace the uniform searching engine to improve the performance <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b163">[164]</ref>. Andy et al. <ref type="bibr" target="#b162">[163]</ref> propose a query adaptive re-ranking method using locality preserving projections. An efficient online local metric adaptation method is presented in <ref type="bibr" target="#b163">[164]</ref>, which learns a strictly local metric with mined negative samples for each probe.</p><p>Human Interaction. It involves using human feedback to optimize the ranking list <ref type="bibr" target="#b157">[158]</ref>. This provides reliable supervision during the re-ranking process. A hybrid humancomputer incremental learning model is presented in <ref type="bibr" target="#b158">[159]</ref>, which cumulatively learns from human feedback, improving the Re-ID ranking performance on-the-fly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Rank Fusion</head><p>Rank fusion exploits multiple ranking lists obtained with different methods to improve the retrieval performance <ref type="bibr" target="#b58">[59]</ref>. Zheng et al. <ref type="bibr" target="#b164">[165]</ref> propose a query adaptive late fusion method on top of a "L" shaped observation to fuse methods. A rank aggregation method by employing the similarity and dissimilarity is developed in <ref type="bibr" target="#b58">[59]</ref>. The rank fusion process in person Re-ID is formulated as a consensus-based decision problem with graph theory <ref type="bibr" target="#b165">[166]</ref>, mapping the similarity scores obtained by multiple algorithms into a graph with path searching. An Unified Ensemble Diffusion (UED) <ref type="bibr" target="#b160">[161]</ref> is recently designed for metric fusion. UED maintains the advantages of three existing fusion algorithms, optimized by a new objective function and derivation. The metric ensemble learning is also studied in <ref type="bibr" target="#b159">[160]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Datasets and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Datasets and Evaluation Metrics</head><p>Datasets. We first review the widely used datasets for the closed-world setting, including 11 image datasets (VIPeR <ref type="bibr" target="#b30">[31]</ref>, iLIDS <ref type="bibr" target="#b166">[167]</ref>, GRID <ref type="bibr" target="#b167">[168]</ref>, PRID2011 <ref type="bibr" target="#b125">[126]</ref>, CUHK01-03 <ref type="bibr" target="#b42">[43]</ref>, Market-1501 <ref type="bibr" target="#b4">[5]</ref>, DukeMTMC <ref type="bibr" target="#b41">[42]</ref>, Airport <ref type="bibr" target="#b168">[169]</ref> and MSMT17 <ref type="bibr" target="#b43">[44]</ref>) and 7 video datasets (PRID-2011 <ref type="bibr" target="#b125">[126]</ref>, iLIDS-VID <ref type="bibr" target="#b6">[7]</ref>, MARS <ref type="bibr" target="#b7">[8]</ref>, Duke-Video <ref type="bibr" target="#b143">[144]</ref>, Duke-Tracklet <ref type="bibr" target="#b169">[170]</ref>, LPW <ref type="bibr" target="#b170">[171]</ref> and LS-VID <ref type="bibr" target="#b135">[136]</ref>). The statistics of these datasets are shown in <ref type="table" target="#tab_2">Table 2</ref>. This survey only focuses on the general large-scale datsets for deep learning methods. A comprehensive summarization of the Re-ID datasets can be found in <ref type="bibr" target="#b168">[169]</ref> and their website <ref type="bibr" target="#b0">1</ref> . Several observations can be made in terms of the dataset collection over recent years: 1) The dataset scale (both #image and #ID) has increased rapidly. Generally, the deep learning approach can benefit from more training samples. This also increases the annotation difficulty needed in closed-world person Re-ID. 2) The camera number is also greatly increased to approximate the large-scale camera network in practical scenarios. This also introduces additional challenges for model generalizability in a dynamically updated network. 3) The bounding boxes generation is usually performed automatically detected/tracked, rather than mannually cropped. This simulates the real-world scenario with tracking/detection errors.</p><formula xml:id="formula_5">1. https://github.com/NEU-Gou/awesome-reid-dataset</formula><p>Evaluation Metrics. To evaluate a Re-ID system, Cumulative Matching Characteristics (CMC) <ref type="bibr" target="#b67">[68]</ref> and mean Average Precision (mAP) <ref type="bibr" target="#b4">[5]</ref> are two widely used measurements.</p><p>CMC-k (a.k.a, Rank-k matching accuracy) <ref type="bibr" target="#b67">[68]</ref> represents the probability that a correct match appears in the top-k ranked retrieved results. CMC is accurate when only one ground truth exists for each query, since it only considers the first match in evaluation process. However, the gallery set usually contains multiple groundtruths in a large camera network, and CMC cannot completely reflect the discriminability of a model across multiple cameras.</p><p>Another metric, i.e., mean Average Precision (mAP) <ref type="bibr" target="#b4">[5]</ref>, measures the average retrieval performance with multiple grountruths. It is originally widely used in image retrieval. For Re-ID evaluation, it can address the issue of two systems performing equally well in searching the first ground truth (might be easy match as in <ref type="figure">Fig. 4</ref>), but having different retrieval abilities for other hard matches.</p><p>Considering the efficiency and complexity of training a Re-ID model, some recent works <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b138">[139]</ref> also report the FLoating-point Operations Per second (FLOPs) and the network parameter size as the evaluation metrics. These two metrics are crucial when the training/testing device has limited computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">In-depth Analysis on State-of-The-Arts</head><p>We review the state-of-the-arts from both image-based and video-based perspectives. We include methods published in top CV venues over the past three years.   reported. For CUHK03 <ref type="bibr" target="#b42">[43]</ref>, the detected data under the setting <ref type="bibr" target="#b57">[58]</ref> is reported. For Market-1501, the single query setting is used. The best result is highlighted with a red star. All the listed results do not use re-ranking or additional annotated information.</p><p>Image-based Re-ID. There are a large number of published papers for image-based Re-ID 2 . We mainly review the works published in 2019 as well as some representative works in 2018. Specifically, we include PCB <ref type="bibr" target="#b76">[77]</ref>, MGN <ref type="bibr" target="#b171">[172]</ref>, PyrNet <ref type="bibr" target="#b5">[6]</ref>, Auto-ReID <ref type="bibr" target="#b138">[139]</ref>, ABD-Net <ref type="bibr" target="#b172">[173]</ref>, BagTricks <ref type="bibr" target="#b121">[122]</ref>, OSNet <ref type="bibr" target="#b137">[138]</ref>, DGNet <ref type="bibr" target="#b118">[119]</ref>, SCAL <ref type="bibr" target="#b89">[90]</ref>, MHN <ref type="bibr" target="#b173">[174]</ref>, P2Net <ref type="bibr" target="#b103">[104]</ref>, BDB <ref type="bibr" target="#b123">[124]</ref>, SONA <ref type="bibr" target="#b106">[107]</ref>, SFT <ref type="bibr" target="#b94">[95]</ref>, ConsAtt <ref type="bibr" target="#b92">[93]</ref>, DenseS <ref type="bibr" target="#b102">[103]</ref>, Pyramid <ref type="bibr" target="#b155">[156]</ref>, IANet <ref type="bibr" target="#b107">[108]</ref>, VAL <ref type="bibr" target="#b113">[114]</ref>. We summarize the results on four datasets ( <ref type="figure" target="#fig_3">Fig. 5</ref>). This overview motivates five major insights, as discussed below.</p><p>First, with the advancement of deep learning, most of the image-based Re-ID methods have achieved higher rank-1 accuracy than humans (93.5% <ref type="bibr" target="#b174">[175]</ref>) on the widely used Market-1501 dataset. In particular, VAL <ref type="bibr" target="#b113">[114]</ref> obtains the best mAP of 91.6% and Rank-1 accuracy of 96.2% on Market-1501 dataset. The major advantage of VAL is the usage of viewpoint information. The performance can be further improved when using re-ranking or metric fusion. The success of deep learning on these closed-world datasets also motivates the shift focus to more challenging scenarios, i.e., large data size <ref type="bibr" target="#b135">[136]</ref> or unsupervised learning <ref type="bibr" target="#b175">[176]</ref>.</p><p>Second, part-level feature learning is beneficial for discriminative Re-ID model learning. Global feature learning directly learns the representation on the whole image without the part constraints <ref type="bibr" target="#b121">[122]</ref>. It is discriminative when the person detection/ tracking can accurately locate the human body. When the person images suffer from large background clutter or heavy occlusions, part-level feature learning usually achieves better performance by mining discriminative body regions <ref type="bibr" target="#b66">[67]</ref>. Due to its advantage in handling misalignment/occlusions, we observe that most of the state-of-the-art methods developed recently adopt the features aggregation paradigm, combining the part-level and full human body features <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b155">[156]</ref>.</p><p>Third, attention is beneficial for discriminative Re-ID model learning. We observe that all the methods (ConsAtt <ref type="bibr" target="#b92">[93]</ref>, SCAL <ref type="bibr" target="#b89">[90]</ref>, SONA <ref type="bibr" target="#b106">[107]</ref>, ABD-Net <ref type="bibr" target="#b172">[173]</ref>) achieving the best performance on each dataset adopt an attention scheme. The attention captures the relationship between different convolutional channels, multiple feature maps, hierarchical layers, different body parts/regions, and even multiple images. Meanwhile, discriminative <ref type="bibr" target="#b172">[173]</ref>, diverse <ref type="bibr" target="#b132">[133]</ref>, consistent <ref type="bibr" target="#b92">[93]</ref> and high-order <ref type="bibr" target="#b106">[107]</ref> properties are 2. https://paperswithcode.com/task/person-re-identification incorporated to enhance the attentive feature learning. Considering the powerful attention schemes and the specificity of the Re-ID problem, it is highly possible that attentive deeply learned systems will continue dominating the Re-ID community, with more domain specific properties.</p><p>Fourth, multi-loss training can improve the Re-ID model learning. Different loss functions optimize the network from a multi-view perspective. Combining multiple loss functions can improve the performance, evidenced by the multi-loss training strategy in the state-of-the-art methods, including ConsAtt <ref type="bibr" target="#b92">[93]</ref>, ABD-Net <ref type="bibr" target="#b172">[173]</ref> and SONA <ref type="bibr" target="#b106">[107]</ref>. In addition, a dynamic multi-loss training strategy is designed in <ref type="bibr" target="#b155">[156]</ref> to adaptively integrated two loss functions. The combination of identity loss and triplet loss with hard mining is the primary choice. Moreover, due to the imbalanced issue, sample weighting strategy generally improves the performance by mining informative triplets <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b88">[89]</ref>.</p><p>Finally, there is still much room for further improvement due to the increasing size of datasets, complex environment, limited training samples. For example, the Rank-1 accuracy (82.3%) and mAP (60.8%) on the newly released MSMT17 dataset <ref type="bibr" target="#b43">[44]</ref> are much lower than that on Market-1501 (Rank-1: 96.2% and mAP 91.7%) and DukeMTMC (Rank-1: 91.6% and mAP 84.5%). On some other challenging datasets with limited training samples (e.g., GRID <ref type="bibr" target="#b167">[168]</ref> and VIPeR <ref type="bibr" target="#b30">[31]</ref>), the performance is still very low. In addition, Re-ID models usually suffers significantly on cross-dataset evaluation <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b53">[54]</ref>, and the performance drops dramatically under adversarial attack <ref type="bibr" target="#b176">[177]</ref>. We are optimistic that there would be important breakthroughs in person Re-ID, with increasing discriminability, robustness, and generalizability.</p><p>Video-based Re-ID. Video-based Re-ID has received less interest, compared to image-based Re-ID. We review the deeply learned Re-ID models, including CoSeg <ref type="bibr" target="#b131">[132]</ref>, GLTR <ref type="bibr" target="#b135">[136]</ref>, STA <ref type="bibr" target="#b134">[135]</ref>, ADFD <ref type="bibr" target="#b109">[110]</ref>, STC <ref type="bibr" target="#b19">[20]</ref>, DRSA <ref type="bibr" target="#b132">[133]</ref>, Snippet <ref type="bibr" target="#b133">[134]</ref>, ETAP <ref type="bibr" target="#b143">[144]</ref>, DuATM <ref type="bibr" target="#b90">[91]</ref>, SDM <ref type="bibr" target="#b177">[178]</ref>, TwoS <ref type="bibr" target="#b127">[128]</ref>, ASTPN <ref type="bibr" target="#b130">[131]</ref>, RQEN <ref type="bibr" target="#b170">[171]</ref>, Forest <ref type="bibr" target="#b129">[130]</ref>, RNN <ref type="bibr" target="#b126">[127]</ref> and IDEX <ref type="bibr" target="#b7">[8]</ref>. We also summarize the results on four video Re-ID datasets, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. From these results, the following observations can be drawn.</p><p>First, a clear trend of increasing performance can be seen over the years with the development of deep learning techniques. Specifically, the Rank-1 accuracy increases from 70% (RNN <ref type="bibr" target="#b126">[127]</ref> in 2016) to <ref type="bibr" target="#b94">95</ref>  reported. mAP values (%) on MARS <ref type="bibr" target="#b7">[8]</ref> and Duke-Video <ref type="bibr" target="#b143">[144]</ref> are reported. For Duke-Video, we refer to the settings in <ref type="bibr" target="#b143">[144]</ref>. The best result is highlighted with a red star. All the listed results do not use re-ranking or additional annotated information.</p><p>(ADFD <ref type="bibr" target="#b109">[110]</ref>) on iLIDS-VID dataset. On the large-scale MARS dataset, the Rank-1 accuracy/mAP increase from 68.3%/49.3% (IDEX <ref type="bibr" target="#b7">[8]</ref>) to 88.5%/82.3% (STC <ref type="bibr" target="#b19">[20]</ref>). On the Duke-Video dataset <ref type="bibr" target="#b143">[144]</ref>, STA <ref type="bibr" target="#b134">[135]</ref> also achieves a Rank-1 accuracy of 96.2%, and the mAP is 94.9%. Second, spatial and temporal modeling is crucial for discriminative video representation learning. We observe that all the methods (STA <ref type="bibr" target="#b134">[135]</ref>, STC <ref type="bibr" target="#b19">[20]</ref>, GLTR <ref type="bibr" target="#b135">[136]</ref>) design spatial-temporal aggregation strategies to improve the video Re-ID performance. Similar to image-based Re-ID, the attention scheme across multiple frames <ref type="bibr" target="#b109">[110]</ref>, <ref type="bibr" target="#b134">[135]</ref> also greatly enhances the discriminability. Another interesting observation in <ref type="bibr" target="#b19">[20]</ref> demonstrates that utilizing multiple frames within the video sequence can fill in the occluded regions, which provides a possible solution for handling the challenging occlusion problem in the future.</p><p>Finally, the performance on these datases has reached a saturation state, usually about less than 1% accuracy gain on these four video datasets. However, there is still large room for improvements on the challenging cases. For example, on the newly collected video dataset, LS-VID <ref type="bibr" target="#b135">[136]</ref>, the Rank-1 accuracy/mAP of GLTR <ref type="bibr" target="#b135">[136]</ref> are only 63.1%/44.43%, while GLTR <ref type="bibr" target="#b135">[136]</ref> can achieve state-of-the-art or at least comparable performance on the other four daatsets. LS-VID <ref type="bibr" target="#b135">[136]</ref> contains significantly more identities and video sequences. This provides a challenging benchmark for future breakthroughs in video based Re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OPEN-WORLD PERSON RE-IDENTIFICATION</head><p>This section reviews open-world person Re-ID as discussed in ? 1, including heterogeneous Re-ID by matching person images across heterogeneous modalities ( ? 3.1), endto-end Re-ID from the raw images/videos ( ? 3.2), semi-/unsupervised learning with limited/unavailable annotated labels ( ? 3.3), robust Re-ID model learning with noisy annotations ( ? 3.4) and open-set person Re-ID when the correct match does not occur in the gallery ( ? 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Heterogeneous Re-ID</head><p>This subsection summarizes four main kinds of heterogeneous Re-ID, including Re-ID between depth and RGB images ( ? 3.1.1), text-to-image Re-ID ( ? 3.1.2), visible-toinfrared Re-ID ( ? 3.1.3) and cross resolution Re-ID ( ? 3.1.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Depth-based Re-ID</head><p>Depth images capture the body shape and skeleton information. This provides the possibility for Re-ID under illumination/clothes changing environments, which is also important for personalized human interaction applications.</p><p>A recurrent attention-based model is proposed in <ref type="bibr" target="#b178">[179]</ref> to address the depth-based person identification. In a reinforcement learning framework, they combine the convolutional and recurrent neural networks to identify small, discriminative local regions of the human body.</p><p>Karianakis et al. <ref type="bibr" target="#b179">[180]</ref> leverage the large RGB datasets to design a split-rate RGB-to-Depth transfer method, which bridges the gap between the depth images and the RGB images. Their model further incorporates a temporal attention to enhance video representation for depth Re-ID.</p><p>Some methods <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b180">[181]</ref> have also studied the combination of RGB and depth information to improve the Re-ID performance, addressing the clothes-changing challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Text-to-Image Re-ID</head><p>Text-to-image Re-ID addresses the matching between a text description and RGB images <ref type="bibr" target="#b62">[63]</ref>. It is imperative when the visual image of query person cannot be obtained, and only a text description can be alternatively provided.</p><p>A gated neural attention model <ref type="bibr" target="#b62">[63]</ref> with recurrent neural network learns the shared features between the text description and the person images. This enables the endto-end training for text to image pedestrian retrieval. Cheng et al. <ref type="bibr" target="#b181">[182]</ref> propose a global discriminative image-language association learning method, capturing the identity discriminative information and local reconstructive image-language association under a reconstruction process. A cross projection learning method <ref type="bibr" target="#b182">[183]</ref> also learns a shared space with image-to-text matching. A deep adversarial graph attention convolution network is designed in <ref type="bibr" target="#b183">[184]</ref> with graph relation mining. However, the large semantic gap between the text descriptions and the visual images is still challenging. Meanwhile, how to combine the texts and hand-painting sketch image is also worth studying in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Visible-Infrared Re-ID</head><p>Visible-Infrared Re-ID handles the cross-modality matching between the daytime visible and night-time infrared images. It is important in low-lighting conditions, where the images can only be captured by infrared cameras <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b184">[185]</ref>.</p><p>Wu et al. <ref type="bibr" target="#b20">[21]</ref> start the first attempt to address this issue, by proposing a deep zero-padding framework <ref type="bibr" target="#b20">[21]</ref> to adaptively learn the modality sharable features. A two stream network is introduced in <ref type="bibr" target="#b141">[142]</ref>, <ref type="bibr" target="#b185">[186]</ref> to model the modalitysharable and -specific information, addressing the intra-and cross-modality variations simultaneously. Besides the crossmodality shared embedding learning <ref type="bibr" target="#b186">[187]</ref>, the classifierlevel discrepancy is also investigated in <ref type="bibr" target="#b187">[188]</ref>. Recent methods <ref type="bibr" target="#b188">[189]</ref>, <ref type="bibr" target="#b189">[190]</ref> adopt the GAN technique to generate crossmodality person images to reduce the cross-modality discrepancy at both image and feature level. Hierarchical cross-Modality disentanglement factors are modeled in <ref type="bibr" target="#b190">[191]</ref>. A dual-attentive aggregation learning method is presented in <ref type="bibr" target="#b191">[192]</ref> to capture multi-level relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Cross-Resolution Re-ID</head><p>Cross-Resolution Re-ID conducts the matching between low-resolution and high-resolution images, addressing the large resolution variations <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. A cascaded SR-GAN <ref type="bibr" target="#b192">[193]</ref> generates the high-resolution person images in a cascaded manner, incorporating the identity information. Li et al. <ref type="bibr" target="#b193">[194]</ref> adopt the adversarial learning technique to obtain resolution-invariant image representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">End-to-End Re-ID</head><p>End-to-end Re-ID alleviates the reliance on additional step for bounding boxes generation. It involves the person Re-ID from raw images or videos, and multi-camera tracking.</p><p>Re-ID in Raw Images/Videos This task requires that the model jointly performs the person detection and reidentification in a single framework <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b63">[64]</ref>. It is challenging due to the different focuses of two major components.</p><p>Zheng et al. <ref type="bibr" target="#b54">[55]</ref> present a two-stage framework, and systematically evaluate the benefits and limitations of person detection for the later stage person Re-ID. Xiao et al. <ref type="bibr" target="#b63">[64]</ref> design an end-to-end person search system using a single convolutional neural network for joint person detection and re-identification. A Neural Person Search Machine (NPSM) <ref type="bibr" target="#b194">[195]</ref> is developed to recursively refine the searching area and locate the target person by fully exploiting the contextual information between the query and the detected candidate region. Similarly, a contextual instance expansion module <ref type="bibr" target="#b195">[196]</ref> is learned in a graph learning framework to improve the end-to-end person search. A query-guided end-to-end person search system <ref type="bibr" target="#b196">[197]</ref> is developed using the Siamese squeeze-and-excitation network to capture the global context information with query-guided region proposal generation. A localization refinement scheme with discriminative Re-ID feature learning is introduced in <ref type="bibr" target="#b197">[198]</ref> to generate more reliable bounding boxes. An Identity DiscriminativE Attention reinforcement Learning (IDEAL) method <ref type="bibr" target="#b198">[199]</ref> selects informative regions for auto-generated bounding boxes, improving the Re-ID performance.</p><p>Yamaguchi et al. <ref type="bibr" target="#b199">[200]</ref> investigate a more challenging problem, i.e., searching for the person from raw videos with text description. A multi-stage method with spatio-temporal person detection and multi-modal retrieval is proposed. Further exploration along this direction is expected.</p><p>Multi-camera Tracking End-to-end person Re-ID is also closely related to multi-person, multi-camera tracking <ref type="bibr" target="#b51">[52]</ref>.</p><p>A graph-based formulation to link person hypotheses is proposed for multi-person tracking <ref type="bibr" target="#b200">[201]</ref>, where the holistic features of the full human body and body pose layout are combined as the representation for each person. Ristani et al. <ref type="bibr" target="#b51">[52]</ref> learn the correlation between the multi-target multicamera tracking and person Re-ID by hard-identity mining and adaptive weighted triplet learning. Recently, a locality aware appearance metric (LAAM) <ref type="bibr" target="#b201">[202]</ref> with both intra-and inter-camera relation modeling is proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semi-supervised and Unsupervised Re-ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Unsupervised Re-ID</head><p>Early unsupervised Re-ID mainly learns invariant components, i.e., dictionary <ref type="bibr" target="#b202">[203]</ref>, metric <ref type="bibr" target="#b203">[204]</ref> or saliency <ref type="bibr" target="#b65">[66]</ref>, which leads to limited discriminability or scalability.</p><p>For deeply unsupervised methods, cross-camera label estimation is one the popular approaches <ref type="bibr" target="#b175">[176]</ref>, <ref type="bibr" target="#b204">[205]</ref>. Dynamic graph matching (DGM) <ref type="bibr" target="#b205">[206]</ref> formulates the label estimation as a bipartite graph matching problem. To further improve the performance, global camera network constraints <ref type="bibr" target="#b206">[207]</ref> are exploited for consistent matching. Liu et al. progressively mine the labels with step-wise metric promotion <ref type="bibr" target="#b203">[204]</ref>. A robust anchor embedding method <ref type="bibr" target="#b82">[83]</ref> iteratively assigns labels to the unlabelled tracklets to enlarge the anchor video sequences set. With the estimated labels, deep learning can be applied to learn Re-ID models.</p><p>For end-to-end unsupervised Re-ID, an iterative clustering and Re-ID model learning is presented in <ref type="bibr" target="#b204">[205]</ref>. Similarly, the relations among samples are utilized in a hierarchical clustering framework <ref type="bibr" target="#b207">[208]</ref>. Soft multi-label learning <ref type="bibr" target="#b208">[209]</ref> mines the soft label information from a reference set for unsupervised learning. A Tracklet Association Unsupervised Deep Learning (TAUDL) framework <ref type="bibr" target="#b169">[170]</ref> jointly conducts the within-camera tracklet association and model the cross-camera tracklet correlation. Similarly, an unsupervised camera-aware similarity consistency mining method <ref type="bibr" target="#b209">[210]</ref> is also presented in a coarse-to-fine consistency learning scheme. The intra-camera mining and inter-camera association is applied in a graph association framework <ref type="bibr" target="#b210">[211]</ref>. The semantic attributes are also adopted in Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) framework <ref type="bibr" target="#b110">[111]</ref>. However, it is still challenging for model updating with newly arriving unlabelled data.</p><p>Besides, several methods have also tried to learn a partlevel representation based on the observation that it is easier to mine the label information in local parts than that of a whole image. A PatchNet <ref type="bibr" target="#b152">[153]</ref> is designed to learn discriminative patch features by mining patch level similarity. A Self-similarity Grouping (SSG) approach <ref type="bibr" target="#b211">[212]</ref> iteratively conducts grouping (exploits both the global body and local parts similarity for pseudo labeling) and Re-ID model training in a self-paced manner.</p><p>Semi-/Weakly supervised Re-ID. With limited label information, a one-shot metric learning method is proposed in <ref type="bibr" target="#b212">[213]</ref>, which incorporates a deep texture representation and a color metric. A stepwise one-shot learning method (EUG) is proposed in <ref type="bibr" target="#b143">[144]</ref> for video-based Re-ID, gradually selecting a few candidates from unlabeled tracklets to enrich the labeled tracklet set. A multiple instance attention learning framework <ref type="bibr" target="#b213">[214]</ref> uses the video-level labels for representation learning, alleviating the reliance on full annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Unsupervised Domain Adaptation</head><p>Unsupervised domain adaptation (UDA) transfers the knowledge on a labeled source dataset to the unlabeled target dataset <ref type="bibr" target="#b52">[53]</ref>. Due to the large domain shift and powerful supervision in source dataset, it is another popular approach for unsupervised Re-ID without target dataset labels.</p><p>Target Image Generation. Using GAN generation to transfer the source domain images to target-domain style is a popular approach for UDA Re-ID. With the generated images, this enables supervised Re-ID model learning in the unlabeled target domain. Wei et al. <ref type="bibr" target="#b43">[44]</ref> propose a Person Transfer Generative Adversarial Network (PTGAN), transferring the knowledge from one labeled source dataset to the unlabeled target dataset. Preserved self-similarity and domain-dissimilarity <ref type="bibr" target="#b119">[120]</ref> is trained with a similarity preserving generative adversarial network (SPGAN). A Hetero-Homogeneous Learning (HHL) method <ref type="bibr" target="#b214">[215]</ref> simultaneously considers the camera invariance with homogeneous learning and domain connectedness with heterogeneous learning. An adaptive transfer network <ref type="bibr" target="#b215">[216]</ref> decomposes the adaptation process into certain imaging factors, including illumination, resolution, camera view, etc. This strategy improves the cross-dataset performance. Huang et al. <ref type="bibr" target="#b216">[217]</ref> try to suppress the background shift to minimize the domain shift problem. Chen et al. <ref type="bibr" target="#b217">[218]</ref> design an instance-guided context rendering scheme to transfer the person identities from source domain into diverse contexts in the target domain. Besides, a pose disentanglement scheme is added to improve the image generation <ref type="bibr" target="#b120">[121]</ref>. A mutual mean-teacher learning scheme is also developed in <ref type="bibr" target="#b218">[219]</ref>. However, the scalability and stability of the image generation for practical large-scale changing environment are still challenging.</p><p>Bak et al. <ref type="bibr" target="#b124">[125]</ref> generate a synthetic dataset with different illumination conditions to model realistic indoor and outdoor lighting. The synthesized dataset increases generalizability of the learned model and can be easily adapted to a new dataset without additional supervision <ref type="bibr" target="#b219">[220]</ref>.</p><p>Target Domain Supervision Mining. Some methods directly mine the supervision on the unlabeled target dataset with a well trained model from source dataset. An exemplar memory learning scheme <ref type="bibr" target="#b105">[106]</ref> considers three invariant cues as the supervision, including exemplar-invariance, camera invariance and neighborhood-invariance. The Domain-Invariant Mapping Network (DIMN) <ref type="bibr" target="#b27">[28]</ref> formulates a meta-learning pipeline for the domain transfer task, and a subset of source domain is sampled at each training episode to update the memory bank, enhancing the scalability and discriminability. The camera view information is also applied in <ref type="bibr" target="#b220">[221]</ref> as the supervision signal to reduce the domain gap. A self-training method with progressive augmentation <ref type="bibr" target="#b221">[222]</ref> jointly captures the local structure and global data distribution on the target dataset. Recently, a self-paced contrastive learning framework with hybrid memory <ref type="bibr" target="#b222">[223]</ref> is developed with great success, which dynamically generates multi-level supervision signals.</p><p>The spatio-temporal information is also utilized as the supervision in TFusion <ref type="bibr" target="#b223">[224]</ref>. TFusion transfers the spatiotemporal patterns learned in the source domain to the target domain with a Bayesian fusion model. Similarly, Query-Adaptive Convolution (QAConv) <ref type="bibr" target="#b224">[225]</ref> is developed to improve cross-dataset accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">State-of-The-Arts for Unsupervised Re-ID</head><p>Unsupervised Re-ID has achieved increasing attention in recent years, evidenced by the increasing number of publications in top venues. We review the SOTA for unsupervised deeply learned methods on two widely-used image-based Re-ID datasets. The results are summarized in <ref type="table" target="#tab_5">Table 3</ref>. From these results, the following insights can be drawn. First, the unsupervised Re-ID performance has increased significantly over the years. The Rank-1 accuracy/mAP increases from 54.5%/26.3% (CAMEL <ref type="bibr" target="#b225">[226]</ref>) to 90.3%/76.7% (SpCL <ref type="bibr" target="#b222">[223]</ref>) on the Market-1501 dataset within three years. The performance for DukeMTMC dataset increases from 30.0%/16.4% to 82.9%/68.8%. The gap between the supervised upper bound and the unsupervised learning is narrowed significantly. This demonstrates the success of unsupervised Re-ID with deep learning.</p><p>Second, current unsupervised Re-ID is still underdeveloped and it can be further improved in the following aspects: 1) The powerful attention scheme in supervised Re-ID methods has rarely been applied in unsupervised Re-ID. 2) Target domain image generation has been proved effective in some methods, but they are not applied in two best methods (PAST <ref type="bibr" target="#b221">[222]</ref>, SSG <ref type="bibr" target="#b211">[212]</ref>). 3) Using the annotated source data in the training process of the target domain is beneficial for cross-dataset learning, but it is also not included in above two methods. These observations provide the potential basis for further improvements.</p><p>Third, there is still a large gap between the unsupervised and supervised Re-ID. For example, the rank-1 accuracy of supervised ConsAtt <ref type="bibr" target="#b92">[93]</ref> has achieved 96.1% on the Market-1501 dataset, while the highest accuracy of unsupervised SpCL <ref type="bibr" target="#b222">[223]</ref> is about 90.3%. Recently, He et al. <ref type="bibr" target="#b228">[229]</ref> have demonstrated that unsupervised learning with large-scale unlabeled training data has the ability to outperform the supervised learning on various tasks <ref type="bibr" target="#b229">[230]</ref>. We expect that several breakthroughs in future unsupervised Re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Noise-Robust Re-ID</head><p>Re-ID usually suffers from unavoidable noise due to data collection and annotation difficulty. We review noise-robust Re-ID from three aspects: Partial Re-ID with heavy occlusion, Re-ID with sample noise caused by detection or tracking errors, and Re-ID with label noise caused by annotation error.</p><p>Partial Re-ID. This addresses the Re-ID problem with heavy occlusions, i.e., only part of the human body is visible <ref type="bibr" target="#b230">[231]</ref>. A fully convolutional network <ref type="bibr" target="#b231">[232]</ref> is adopted to generate fix-sized spatial feature maps for the incomplete person images. Deep Spatial feature Reconstruction (DSR) is further incorporated to avoid explicit alignment by exploiting the reconstructing error. Sun et al. <ref type="bibr" target="#b66">[67]</ref> design a Visibility-aware Part Model (VPM) to extract sharable region-level features, thus suppressing the spatial misalignment in the incomplete images. A foreground-aware pyramid reconstruction scheme <ref type="bibr" target="#b232">[233]</ref> also tries to learn from the unoccluded regions. The Pose-Guided Feature Alignment (PGFA) <ref type="bibr" target="#b233">[234]</ref> exploits the pose landmarks to mine discriminative part information from occlusion noise. However, it is still challenging due to the severe partial misalignment, unpredictable visible regions and distracting unshared body regions. Meanwhile, how to adaptively adjust the matching model for different queries still needs further investigation.</p><p>Re-ID with Sample Noise. This refers to the problem of the person images or the video sequence containing outlying regions/frames, either caused by poor detection/inaccurate tracking results. To handle the outlying regions or background clutter within the person image, pose estimation cues <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> or attention cues <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b198">[199]</ref> are exploited. The basic idea is to suppress the contribution of the noisy regions in the final holistic representation. For video sequences, set-level feature learning <ref type="bibr" target="#b82">[83]</ref> or frame level re-weighting <ref type="bibr" target="#b133">[134]</ref> are the commonly used approaches to reduce the impact of noisy frames. Hou et al. <ref type="bibr" target="#b19">[20]</ref> also utilize multiple video frames to auto-complete occluded regions. It is expected that more domain-specific sample noise handling designs in the future.</p><p>Re-ID with Label Noise. Label noise is usually unavoidable due to annotation error. Zheng et al. adopt a label smoothing technique to avoid label overfiting issues <ref type="bibr" target="#b41">[42]</ref>. A Distribution Net (DNet) that models the feature uncertainty is proposed in <ref type="bibr" target="#b234">[235]</ref> for robust Re-ID model learning against label noise, reducing the impact of samples with high feature uncertainty. Different from the general classification problem, robust Re-ID model learning suffers from limited training samples for each identity <ref type="bibr" target="#b235">[236]</ref>. In addition, the unknown new identities increase additional difficulty for the robust Re-ID model learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Open-set Re-ID and Beyond</head><p>Open-set Re-ID is usually formulated as a person verification problem, i.e., discriminating whether or not two person images belong to the same identity <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>. The verification usually requires a learned condition ? , i.e., sim(query, gallery) &gt; ? . Early researches design handcrafted systems <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>. For deep learning methods, an Adversarial PersonNet (APN) is proposed in <ref type="bibr" target="#b236">[237]</ref>, which jointly learns a GAN module and the Re-ID feature extractor. The basic idea of this GAN is to generate realistic target- ... ...  <ref type="figure">Fig. 7</ref>: Difference between the widely used CMC, AP and the negative penalty (NP) measurements. True matching and false matching are bounded in green and red boxes, respectively. Assume that only three correct matches exist in the gallery, rank list 1 gets better AP, but gets much worse NP than rank list 2. The main reason is that rank list 1 contains too many false matchings before finding the hardest true matching. For consistency with CMC and mAP, we compute the inverse negative penalty (INP), e.g., INP = 1-NP. Larger INP means better performance.</p><p>like images (imposters) and enforce the feature extractor is robust to the generated image attack. Modeling feature uncertainty is also investigated in <ref type="bibr" target="#b234">[235]</ref>. However, it remains quite challenging to achieve a high true target recognition and maintain low false target recognition rate <ref type="bibr" target="#b237">[238]</ref>. Group Re-ID. It aims at associating the persons in groups rather than individuals <ref type="bibr" target="#b166">[167]</ref>. Early researches mainly focus on group representation extraction with sparse dictionary learning <ref type="bibr" target="#b238">[239]</ref> or covariance descriptor aggregation <ref type="bibr" target="#b239">[240]</ref>. The multi-grain information is integrated in <ref type="bibr" target="#b240">[241]</ref> to fully capture the characteristics of a group. Recently, the graph convoltuional network is applied in <ref type="bibr" target="#b241">[242]</ref>, representing the group as a graph. The group similarity is also applied in the end-to-end person search <ref type="bibr" target="#b195">[196]</ref> and the individual re-identification <ref type="bibr" target="#b196">[197]</ref>, <ref type="bibr" target="#b242">[243]</ref> to improve the accuracy. However, group Re-ID is still challenging since the group variation is more complicated than the individuals.</p><p>Dynamic Multi-Camera Network. Dynamic updated multi-camera network is another challenging issue <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, which needs model adaptation for new cameras or probes. A human in-the-loop incremental learning method is introduced in <ref type="bibr" target="#b23">[24]</ref> to update the Re-ID model, adapting the representation for different probe galleries. Early research also applies the active learning <ref type="bibr" target="#b26">[27]</ref> for continuous Re-ID in multi-camera network. A continuous adaptation method based on sparse non-redundant representative selection is introduced in <ref type="bibr" target="#b22">[23]</ref>. A transitive inference algorithm <ref type="bibr" target="#b243">[244]</ref> is designed to exploit the best source camera model based on a geodesic flow kernel. Multiple environmental constraints (e.g., Camera Topology) in dense crowds and social relationships are integrated for an open-world person Re-ID system <ref type="bibr" target="#b244">[245]</ref>. The model adaptation and environmental factors of cameras are crucial in practical dynamic multicamera network. Moreover, how to apply the deep learning technique for the dynamic multi-camera network is still less investigated.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AN OUTLOOK: RE-ID IN NEXT ERA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">mINP: A New Evaluation Metric for Re-ID</head><p>For a good Re-ID system, the target person should be retrieved as accurately as possible, i.e., all the correct matches should have low rank values. Considering that the target person should not be neglected in the top-ranked retrieved list, especially for multi-camera network, so as to accurately track the target. When the target person appears in the gallery set at multiple time stamps, the rank position of the hardest correct match determines the workload of the inspectors for further investigation. However, the currently widely used CMC and mAP metrics cannot evaluate this property, as shown in <ref type="figure">Fig. 7</ref>. With the same CMC, rank list 1 achieves a better AP than rank list 2, but it requires more efforts to find all the correct matches. To address this issue, we design a computationally efficient metric, namely a negative penalty (NP), which measures the penalty to find the hardest correct match</p><formula xml:id="formula_6">NP i = R hard i ? |G i | R hard i ,<label>(6)</label></formula><p>where R hard i indicates the rank position of the hardest match, and |G i | represents the total number of correct matches for query i. Naturally, a smaller NP represents better performance. For consistency with CMC and mAP, we prefer to use the inverse negative penalty (INP), an inverse operation of NP. Overall, the mean INP of all the queries is represented by</p><formula xml:id="formula_7">mINP = 1 n i (1 ? NP i ) = 1 n i |G i | R hard i .<label>(7)</label></formula><p>The calculation of mINP is quite efficient and can be seamlessly integrated in the CMC/mAP calculating process. mINP avoids the domination of easy matches in the mAP/CMC evaluation. One limitation is that mINP value difference for large gallery size would be much smaller compared to small galleries. But it still can reflect the relative performance of a Re-ID model, providing a supplement to the widely-used CMC and mAP metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A New Baseline for Single-/Cross-Modality Re-ID</head><p>According to the discussion in ? 2.4.2, we design a new AGW 3 baseline for person Re-ID, which achieves competitive performance on both single-modality (image and video) and cross-modality Re-ID tasks. Specifically, our new baseline is designed on top of BagTricks <ref type="bibr" target="#b121">[122]</ref>, and AGW contains the following three major improved components:</p><p>3. Details are in https://github.com/mangye16/ReID-Survey and comprehensive comparison is shown in the supplementary material.  <ref type="figure">Fig. 8</ref>: The framework of the proposed AGW baseline using the widely used ResNet50 <ref type="bibr" target="#b79">[80]</ref> as the backbone network.</p><p>(1) Non-local Attention (Att) Block. As discussed in ? 2.4.2, the attention scheme plays a crucial role in discriminative Re-ID model learning. We adopt the powerful nonlocal attention block <ref type="bibr" target="#b245">[246]</ref> to obtain the weighted sum of the features at all positions, represented by</p><formula xml:id="formula_8">z i = W z * ?(x i ) + x i ,<label>(8)</label></formula><p>where W z is a weight matrix to be learned, ?(?) represents a non-local operation, and +x i formulates a residual learning strategy. Details can be found in <ref type="bibr" target="#b245">[246]</ref>. We adopt the default setting from <ref type="bibr" target="#b245">[246]</ref> to insert the non-local attention block.</p><p>(2) Generalized-mean (GeM) Pooling. As a fine-grained instance retrieval, the widely-used max-pooling or average pooling cannot capture the domain-specific discriminative features. We adopt a learnable pooling layer, named generalized-mean (GeM) pooling <ref type="bibr" target="#b246">[247]</ref>, formulated by</p><formula xml:id="formula_9">f = [f 1 ? ? ? f k ? ? ? f K ] T , f k = ( 1 |X k | xi?X k x p k i ) 1 p k ,<label>(9)</label></formula><p>where f k represents the feature map, and K is number of feature maps in the last layer. X k is the set of W ? H activations for feature map k ? {1, 2, ? ? ? , K}. p k is a pooling hyper-parameter, which is learned in the back-propagation process <ref type="bibr" target="#b246">[247]</ref>. The above operation approximates max pooling when p k ? ? and average pooling when p k = 1.</p><p>(3) Weighted Regularization Triplet (WRT) loss. In addition to the baseline identity loss with softmax crossentropy, we integrate with another weighted regularized triplet loss,</p><formula xml:id="formula_10">L wrt (i) = log(1 + exp( j w p ij d p ij ? k w n ik d n ik )). (10) w p ij = exp (d p ij ) d p ij ?Pi exp(d p ij ) , w n ik = exp (?d n ik ) d n ik ?Ni exp(?d n ik ) ,<label>(11)</label></formula><p>where (i, j, k) represents a hard triplet within each training batch. For anchor i, P i is the corresponding positive set, and N i is the negative set. d p ij /d n ik represents the pairwise distance of a positive/negative sample pair. The above weighted regularization inherits the advantage of relative distance optimization between positive and negative pairs, but it avoids introducing any additional margin parameters. Our weighting strategy is similar to <ref type="bibr" target="#b247">[248]</ref>, but our solution does not introduce additional hyper-parameters.</p><p>The overall framework of AGW is shown in <ref type="figure">Fig 8.</ref> Other components are exactly the same as <ref type="bibr" target="#b121">[122]</ref>. In the testing phase, the output of BN layer is adopted as the feature representation for Re-ID. The implementation details and more experimental results are in the supplementary material.  Results on Single-modality Image Re-ID. We first evaluate each component on two image-based datasets (Market-1501 and DukeMTMC) in <ref type="table" target="#tab_6">Table 4</ref>. We also list two stateof-the-art methods, BagTricks <ref type="bibr" target="#b121">[122]</ref> and ABD-Net <ref type="bibr" target="#b172">[173]</ref>. We report the results on CUHK03 and MSMT17 datasets in <ref type="table" target="#tab_7">Table 5</ref>. We obtain the following two observations: 1) All the components consistently contribute the accuracy gain, and AGW performs much better than the original BagTricks under various metrics. AGW provides a strong baseline for future improvements. We have also tried to incorporate part-level feature learning <ref type="bibr" target="#b76">[77]</ref>, but extensive experiments show that it does not improve the performance. How to aggregate part-level feature learning with AGW needs further study in the future. 2) Compared to the current state-of-the-art, ABD-Net <ref type="bibr" target="#b172">[173]</ref>, AGW performs favorably in most cases. In particular, we achieve much higher mINP on DukeMTMC dataset, 45.7% vs. 42.1%. This demonstrates that AGW requires less effort to find all the correct matches, verifying the ability of mINP.</p><p>Results on Single-modality Video Re-ID. We also evaluate the proposed AGW on four widely used single modality video-based datasets ( MARS <ref type="bibr" target="#b7">[8]</ref>, DukeVideo <ref type="bibr" target="#b143">[144]</ref>, PRID2011 <ref type="bibr" target="#b125">[126]</ref> and iLIDS-VID <ref type="bibr" target="#b6">[7]</ref>, as shown in <ref type="table" target="#tab_8">Table 6</ref>. We also compare two state-of-the-art methods, BagTricks <ref type="bibr" target="#b121">[122]</ref> and Co-Seg <ref type="bibr" target="#b131">[132]</ref>. For video data, we develop a variant (AGW + ) to capture the temporal information with framelevel average pooling for sequence representation. Meanwhile, constraint random sampling strategy <ref type="bibr" target="#b132">[133]</ref> is applied for training. Compared to Co-Seg <ref type="bibr" target="#b131">[132]</ref>, our AGW + obtains better Rank-1, mAP and mINP in most cases.</p><p>Results on Partial Re-ID. We also test the performance of AGW on two partial Re-ID datasets, as shown in <ref type="table" target="#tab_9">Table  7</ref>. The experimental setting are from DSR <ref type="bibr" target="#b231">[232]</ref>. We also achieve comparable performance with the state-of-the-art VPM method <ref type="bibr" target="#b66">[67]</ref>. This experiment further demonstrates the superiority of AGW for the open-world partial Re-ID task.</p><p>Meanwhile, the mINP also shows the applicability for this open-world Re-ID problem.</p><p>Results on Cross-modality Re-ID. We also test the performance of AGW using a two-stream architecture on the cross-modality visible-infrared Re-ID task. The comparison  with the current state-of-the-arts on two datasets is shown in <ref type="table" target="#tab_10">Table 8</ref>. We follow the settings in AlignG <ref type="bibr" target="#b189">[190]</ref> to perform the experiments. Results show that AGW achieves much higher accuracy than existing cross-modality Re-ID models, verifying the effectiveness for the open-world Re-ID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Under-Investigated Open Issues</head><p>We discuss the open-issues from five different aspects according to the five steps in ?1, including uncontrollable data collection, human annotation minimization, domainspecific/generalizable architecture design, dynamic model updating and efficient model deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Uncontrollable Data Collection</head><p>Most existing Re-ID works evaluate their method on a welldefined data collection environment. However, the data collection in real complex environment is uncontrollable. The data might be captured from unpredictable modality, modality combinations, or even cloth changing data <ref type="bibr" target="#b29">[30]</ref>. Multi-Heterogeneous Data. In real applications, the Re-ID data might be captured from multiple heterogeneous modalities, i.e., the resolutions of person images vary a lot <ref type="bibr" target="#b192">[193]</ref>, both the query and gallery sets may contain different modalities (visible, thermal <ref type="bibr" target="#b20">[21]</ref>, depth <ref type="bibr" target="#b61">[62]</ref> or text description <ref type="bibr" target="#b9">[10]</ref>). This results in a challenging multiple heterogeneous person Re-ID. A good person Re-ID system would be able to automatically handle the changing resolutions, different modalities, various environments and multiple domains. Future work with broad generalizability is expected, evaluating their method for different Re-ID tasks.</p><p>Cloth-Changing Data. In practical surveillance system, it is very likely to contain a large number of target persons with changing clothes. A cloth-Clothing Change Aware Network (CCAN) <ref type="bibr" target="#b249">[250]</ref> addresses this issue by separately extracting the face and body context representation, and similar idea is applied in <ref type="bibr" target="#b250">[251]</ref>. Yang et al. <ref type="bibr" target="#b29">[30]</ref> present a spatial polar transformation (SPT) to learn cross-cloth invariant representation. However, they still rely heavily on the face and body appearance, which might be unavailable and unstable in real scenarios. It would be interesting to further explore the possibility of other discriminative cues (e.g., gait, shape) to address the cloth-changing issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Human Annotation Minimization</head><p>Besides the unsupervised learning, active learning or human interaction <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b153">[154]</ref>, <ref type="bibr" target="#b158">[159]</ref> provides another possible solution to alleviate the reliance on human annotation.</p><p>Active Learning. Incorporating human interaction, labels are easily provided for newly arriving data and the model can be subsequently updated <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>. A pairwise subset selection framework <ref type="bibr" target="#b251">[252]</ref> minimizes human labeling effort by firstly constructing an edge-weighted complete kpartite graph and then solving it as a triangle free subgraph maximization problem. Along this line, a deep reinforcement active learning method <ref type="bibr" target="#b153">[154]</ref> iteratively refines the learning policy and trains a Re-ID network with human-inthe-loop supervision. For video data, an interpretable reinforcement learning method with sequential decision making <ref type="bibr" target="#b177">[178]</ref> is designed. The active learning is crucial in practical Re-ID system design, but it has received less attention in the research community. Additionally, the newly arriving identities is extremely challenging, even for human. Efficient human in-the-loop active learning is expected in the future.</p><p>Learning for Virtual Data. This provides an alternative for minimizing the human annotation. A synthetic dataset is collected in <ref type="bibr" target="#b219">[220]</ref> for training, and they achieve competitive performance on real-world datasets when trained on this synthesized dataset. Bak et al. <ref type="bibr" target="#b124">[125]</ref> generate a new synthetic dataset with different illumination conditions to model realistic indoor and outdoor lighting. A large-scale synthetic PersonX dataset is collected in <ref type="bibr" target="#b104">[105]</ref> to systematically study the effect of viewpoint for a person Re-ID system. Recently, the 3D person images are also studied in <ref type="bibr" target="#b252">[253]</ref>, generating the 3D body structure from 2D images. However, how to bridge the gap between synthesized images and real-world datasets remains challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Domain-Specific/Generalizable Architecture Design</head><p>Re-ID Specific Architecture. Existing Re-ID methods usually adopt architectures designed for image classification as the backbone. Some methods modify the architecture to achieve better Re-ID features <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b121">[122]</ref>. Very recently, researchers have started to design domain specific architectures, e.g., OSNet with omni-scale feature learning <ref type="bibr" target="#b137">[138]</ref>. It detects the small-scale discriminative features at a certain scale. OSNet is extremely lightweight and achieves competitive performance. With the advancement of automatic neural architecture search (e.g., Auto-ReID <ref type="bibr" target="#b138">[139]</ref>), more domainspecific powerful architectures are expected to address the task-specific Re-ID challenges. Limited training samples in Re-ID also increase the difficulty in architecture design.</p><p>Domain Generalizable Re-ID. It is well recognized that there is a large domain gap between different datsets <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b224">[225]</ref>. Most existing methods adopt domain adaptation for cross-dataset training. A more practical solution would be learning a domain generalized model with a number of source datasets, such that the learned model can be generalized to new unseen datasets for discriminative Re-ID without additional training <ref type="bibr" target="#b27">[28]</ref>. Hu et al. <ref type="bibr" target="#b253">[254]</ref> studied the cross-dataset person Re-ID by introducing a part-level CNN framework. The Domain-Invariant Mapping Network (DIMN) <ref type="bibr" target="#b27">[28]</ref> designs a meta-learning pipeline for domain generalizable Re-ID, learning a mapping between a person image and its identity classifier. The domain generalizability is crucial to deploy the learned Re-ID model under an unknown scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Dynamic Model Updating</head><p>Fixed model is inappropriate for practical dynamically updated surveillance system. To alleviate this issue, dynamic model updating is imperative, either to a new domain/camera or adaptation with newly collected data.</p><p>Model Adaptation to New Domain/Camera. Model adaptation to a new domain has been widely studied in the literature as a domain adaptation problem <ref type="bibr" target="#b124">[125]</ref>, <ref type="bibr" target="#b215">[216]</ref>. In practical dynamic camera network, a new camera may be temporarily inserted into an existing surveillance system. Model adaptation is crucial for continuous identification in a multi-camera network <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>. To adapt a learned model to a new camera, a transitive inference algorithm <ref type="bibr" target="#b243">[244]</ref> is designed to exploit the best source camera model based on a geodesic flow kernel. However, it is still challenging when the newly collected data by the new camera has totally different distributions. In addition, the privacy and efficiency issue <ref type="bibr" target="#b254">[255]</ref> also need further consideration.</p><p>Model Updating with Newly Arriving Data. With the newly collected data, it is impractical to training the previously learned model from the scratch <ref type="bibr" target="#b23">[24]</ref>. An incremental learning approach together with human interaction is designed in <ref type="bibr" target="#b23">[24]</ref>. For deeply learned model, an addition using covariance loss <ref type="bibr" target="#b255">[256]</ref> is integrated in the overall learning function. However, this problem is not well studied since the deep model training require large amount of training data. Besides, the unknown new identities in the newly arriving data is hard to be identified for the model updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Efficient Model Deployment</head><p>It is important to design efficient and adaptive models to address scalability issue for practical model deployment.</p><p>Fast Re-ID. For fast retrieval, hashing has been extensively studied to boost the searching speed, approximating the nearest neighbor search <ref type="bibr" target="#b256">[257]</ref>. Cross-camera Semantic Binary Transformation (CSBT) <ref type="bibr" target="#b257">[258]</ref> transforms the original high-dimensional feature representations into compact lowdimensional identity-preserving binary codes. A Coarse-to-Fine (CtF) hashing code search strategy is developed in <ref type="bibr" target="#b258">[259]</ref>, complementarily using short and long codes. However, the domain-specific hashing still needs further study.</p><p>Lightweight Model. Another direction for addressing the scalability issue is to design a lightweight Re-ID model. Modifying the network architecture to achieve a lightweight model is investigated in <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b138">[139]</ref>. Model distillation is another approach, e.g., a multi-teacher adaptive similarity distillation framework is proposed in <ref type="bibr" target="#b259">[260]</ref>, which learns a user-specified lightweight student model from multiple teacher models, without access to source domain data.</p><p>Resource Aware Re-ID. Adaptively adjusting the model according to the hardware configurations also provides a solution to handle the scalability issue. Deep Anytime Re-ID (DaRe) <ref type="bibr" target="#b13">[14]</ref> employs a simple distance based routing strategy to adaptively adjust the model, fitting to hardware devices with different computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUDING REMARKS</head><p>This paper presents a comprehensive survey with in-depth analysis from a both closed-world and open-world perspectives. We first introduce the widely studied person Re-ID under the closed-world setting from three aspects: feature representation learning, deep metric learning and ranking optimization. With powerful deep learning, the closedworld person Re-ID has achieved performance saturation on several datasets. Correspondingly, the open-world setting has recently gained increasing attention, with efforts to address various practical challenges. We also design a new AGW baseline, which achieves competitive performance on four Re-ID tasks under various metrics. It provides a strong baseline for future improvements. This survey also introduces a new evaluation metric to measure the cost of finding all the correct matches. We believe this survey will provide important guidance for future Re-ID research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Materials:</head><p>This supplementary material accompanies our main manuscript with the implementation details and more comprehensive experiments. We first present the experiments on two single-modality closed-world Re-ID tasks, including image-based Re-ID on four datasets in Section A and videobased Re-ID on four datasets in Section B. Then we introduce the comprehensive comparison on two open-world Re-ID tasks, including visible-infrared cross-modality Re-ID on two datasets in Section C and partial Re-ID on two datasets in Section D. In addition, a structure overview for our survey is finally summarized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments on Single-modality Image-based Re-ID</head><p>Architecture Design. The overall structure 4 of our proposed AGW baseline for single-modality Re-ID is illustrated in ? 4 ( <ref type="figure" target="#fig_10">Fig. R1</ref>). We adopt ResNet50 pre-trained on ImageNet as our backbone network and change the dimension of the fully connected layer to be consistent with the number of identities in the training dataset. The stride of the last spatial down-sampling operation in the backbone network is changed from 2 to 1. Consequently, the spatial size of the output feature map is changed from 8 ? 4 to 16 ? 8, when feeding an image of resolution 256 ? 128 as input. In our method, we replace the Global Average Pooling in the original ResNet50 with the Generalized-mean (GeM) pooling. The pooling hyper parameter p k for generalized-mean pooling is initialized as 3.0. A BatchNorm layer, named BNNeck is plugged between the GeM pooling layer and the fully connected layer. The output of the GeM pooling layer is adopted for computing center loss and triplet loss in the training stage, while the feature after BNNeck is used for computing distance between pedestrian images during testing inference stage. <ref type="bibr" target="#b3">4</ref>. https://github.com/mangye16/ReID-Survey Non-local Attention. The ResNet contains 4 residual stages, i.e. conv2 x, conv3 x, conv4 x and conv5 x, each containing stacks of bottleneck residual blocks. We inserted five non-local blocks after conv3 3, conv3 4, conv4 4, conv4 5 and conv4 6 respectively. We adopt the Dot Product version of non-local block with a bottleneck of 512 channels in our experiment. For each non-local block, a BatchNorm layer is added right after the last linear layer that represents W z . The affine parameter of this BatchNorm layer is initialized as zeros to ensure that the non-local block can be inserted into any pre-trained networks while maintaining its initial behavior.</p><p>Training Strategy. In the training stage, we randomly sample 16 identities and 4 images for each identity to form a mini-batch of size 64. Each image is resized into 256 ? 128 pixels, padding 10 pixels with zero values, and then randomly cropped into 256 ? 128 pixels. Random horizontally flipping and random erasing with 0.5 probability respectively are also adopted for data augmentation. Specifically, random erasing augmentation <ref type="bibr" target="#b122">[123]</ref> randomly selects a rectangle region with area ratio r e to the whole image, and erase its pixels with the mean value of the image. Besides, the aspect ratio of this region is randomly initialized between r 1 and r 2 . In our method, we set the above hyper-parameter as 0.02 &lt; r e &lt; 0.4, r1 = 0.3 and r2 = 3.33. At last, we normalize the RGB channels of each image with mean 0.485, 0.456, 0.406 and stand deviation 0.229, 0.224, 0.225, respectively, which are the same with settings in <ref type="bibr" target="#b121">[122]</ref>.</p><p>Training Loss. In the training stage, three types of loss are combined for optimization, including identity classification loss (L id ), center loss (L ct ) and our proposed weighted regularization triplet loss (L wrt ).</p><formula xml:id="formula_11">L total = L id + ? 1 L ct + ? 2 L wrt .<label>(R1)</label></formula><p>The balanced weight of the center loss (? 1 ) is set to 0.0005 and the one (? 2 ) of the weighted regularized triplet loss is set to 1.0. Label smoothing is adopted to improve the original identity classification loss, which encourages the model to be less confident during training and prevent overfitting for classification task. Concretely, it changes the one-hot label as follow:</p><formula xml:id="formula_12">q i = 1 ? N ?1 N ? if i = y ?/N otherwise ,<label>(R2)</label></formula><p>where N is the total number of identities, is a small constant to reduce the confidence for the true identity label y and q i is treated as a new classification target for training. In our method, we set to be 0.1. Optimizer Setting. Adam optimizer with a weight decay 0.0005 is adopted to train our model. The initial learning rate is set as 0.00035 and is decreased by 0.1 at the 40th epoch and 70th epoch, respectively. The model is trained for 120 epochs in total. Besides, a warm-up learning rate scheme is also employed to improve the stability of training process and bootstrap the network for better performance. Specifically, in the first 10 epochs, the learning rate is linearly  (R3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on Video-based Re-ID</head><p>Implementation Details. We extend our proposed AGW baseline to a video-based Re-ID model by several minor changes to the backbone structure and training strategy of single-modality image-based Re-ID model. The videobased AGW baseline takes a video sequence as input and extracts the frame-level feature vectors, which are then averaged to be a video-level feature vector before the BNNeck layer. Besides, the video-based AGW baseline is trained for 400 epochs totally to better fit the video person Re-ID datasets. The learning rate is decayed by 10 times every 100 epochs. To form an input video sequence, we adopt the constraint random sampling strategy <ref type="bibr" target="#b132">[133]</ref> to sample 4 frames as a summary for the original pedestrian tracklet. The BagTricks <ref type="bibr" target="#b121">[122]</ref> baseline is extended to a video-based Re-ID model in the same way as AGW baseline for fair comparison. In addition, we also develop a variant of AGW baseline, termed as AGW + , to model more abundant temporal information in a pedestrian tracklet. AGW + baseline adopts the dense sampling strategy to form an input video sequence in the testing stage. Dense sampling strategy takes all the frames in a pedestrian tracklet to form input video sequence, resulting better performance but higher computational cost. To further improve the performance of AGW + baseline on video re-ID datasets, we also remove the warmup learning rate strategy and add dropout operation before the linear classification layer.</p><p>Detailed Comparison. In this section, we conduct the performance comparison between AGW baseline and other state-of-the-art video-based person Re-ID methods, including ETAP <ref type="bibr" target="#b143">[144]</ref>, DRSA <ref type="bibr" target="#b132">[133]</ref>, STA <ref type="bibr" target="#b134">[135]</ref> Snippet <ref type="bibr" target="#b133">[134]</ref>, VRSTC <ref type="bibr" target="#b19">[20]</ref>, ADFD <ref type="bibr" target="#b109">[110]</ref>, GLTR <ref type="bibr" target="#b135">[136]</ref> and CoSeg <ref type="bibr" target="#b131">[132]</ref>. The comparison results on four video person Re-ID datasets (MARS, DukeVideo, PRID2011 and iLIDS-VID) are listed in <ref type="table" target="#tab_11">Table R1</ref>. As we can see, by simply taking video sequence as input and adopting average pooling to aggregate framelevel feature, our AGW baseline achieves competitive results on two large-scale video Re-ID dataset, MARS and DukeVideo. Besides, AGW baseline also performs significantly better than BagTricks <ref type="bibr" target="#b121">[122]</ref> baseline under multiple evaluation metrics. By further modeling more temporal information and adjusting training strategy, AGW + baseline gains huge improvement and also achieves competitive results on both PRID2011 and iLIDS-VID datasets. AGW + baseline outperforms most state-of-the-art methods on MARS, DukeVideo and PRID2011 datasets. Most of these video-based person Re-ID methods achieve state-of-the-art performance by designing complicate temporal attention mechanism to exploit temporal dependency in pedestrian video. We believe that our AGW baseline can help video Re-ID model achieve higher performance with properly designed mechanism to further exploit spatial and temporal dependency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on Cross-modality Re-ID</head><p>Architecture Design. We adopt a two-stream network structure as the backbone for cross-modality visible-infrared Re-ID <ref type="bibr" target="#b4">5</ref> . Compared to the one-stream architecture in singlemodality person Re-ID ( <ref type="figure">Fig. 8</ref>), the major difference is that, i.e., the first block is specific for two modalities in order to capture modality-specific information, while the remaining blocks are shared to learn modality sharable features. Compared to the two-stream structure widely used in <ref type="bibr" target="#b141">[142]</ref>, <ref type="bibr" target="#b260">[261]</ref>, which only has one shared embedding layer, our design captures more sharable components. An illustration for cross-modality visible-infrared Re-ID is shown in <ref type="figure" target="#fig_11">Fig. R2</ref>. Training Strategy. At each training step, we random sample 8 identities from the whole dataset. Then 4 visible and 4 infrared images are randomly selected for each identity. Totally, each training batch contains 32 visible and 32 infrared images. This guarantees the informative hard triplet mining from both modalities, i.e., we directly select the hard positive and negative from both intra-and intermodalities. This approximates the idea of bi-directional center-constrained top-ranking loss, handling the inter-and intra-modality variations simultaneously.</p><p>For fair comparison, we follow the settings in <ref type="bibr" target="#b141">[142]</ref> exactly to conduct the image processing and data aug-5. https://github.com/mangye16/Cross-Modal-Re-ID-baseline mentation. For infrared images, we keep the original three channels, just like the visible RGB images. All the input images from both modalities are first resized to 288 ? 144, and random crop with zero-padding together with random horizontal flipping are adopted for data argumentation. The cropped image sizes are 256 ? 128 for both modality. The image normalization are exactly following the singlemodality setting.</p><p>Training Loss. In the training stage, we combine with the identity classification loss (L id ) and our proposed weighted regularization triplet loss (L wrt ). The weight of combining the identity loss and weighted regularized triplet loss is set to 1, the same as the single-modality setting. The pooling parameter p k is set to 3. For stable training, we adopt the same identity classifier for two heterogeneous modalities, mining sharable information.</p><p>Optimizer Setting. We set the initial learning rate as 0.1 on both datasets, and decay it by 0.1 and 0.01 at 20 and 50 epochs, respectively. The total number of training epoch is 60. We also adopt a warm-up learning rate scheme. We adopt the stochastic gradient descent (SGD) optimizer for optimization, and the momentum parameter is set to 0.9. We have tried the same Adam optimizer (used in single-modality Re-ID) on cross-modality Re-ID task, but the performance is much lower than that of SGD optimizer by using a large learning rate. This is crucial since ImageNet R3: Comparison with the state-of-the-arts on RegDB dataset on both query settings. Rank at r accuracy (%), mAP (%) and mINP (%) are reported. (Both the visible to thermal and thermal to visible query settings are evaluated.) "*" represents methods published after the paper submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head><p>Visible to Thermal Thermal to Visible Method Venue r = 1 r = 10 r = 20 mAP mINP r = 1 r = 10 r = 20 mAP mINP HCML <ref type="bibr" target="#b185">[186]</ref> AAAI18 <ref type="bibr" target="#b23">24</ref> initialization is adopted for the infrared images. Detailed Comparison This section conducts the comparison with the state-of-the-art cross-modality VI-ReID methods, including eBDTR <ref type="bibr" target="#b141">[142]</ref>, HSME <ref type="bibr" target="#b186">[187]</ref>, D 2 RL <ref type="bibr" target="#b188">[189]</ref>, MAC <ref type="bibr" target="#b260">[261]</ref>, MSR <ref type="bibr" target="#b261">[262]</ref> and AlignGAN <ref type="bibr" target="#b189">[190]</ref>. These methods are published in the past two years. AlignGAN <ref type="bibr" target="#b189">[190]</ref>, published in ICCV 2019, achieves the state-of-the-art performance by aligning the cross-modality representation at both the feature level and pixel level with GAN generated images. The results on two datasets are shown in Tables R2 and R3. We observe that the proposed AGW consistently outperforms the current state-of-the-art, without the time-consuming image generation process. For different query settings on RegDB dataset, our proposed baseline generally keeps the same performance. Our proposed baseline has been widely used in many recently developed methods. We believe our new baseline will provide a good guidance to boost the cross-modality Re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments on Partial Re-ID</head><p>Implementation Details. We also evaluate the performance of our proposed AGW baseline on two commonly-used partial Re-ID datasets, Partial-REID and Partial-iLIDS. The overall backbone structure and training strategy for partial Re-ID AGW baseline model are the same as the one for single-modality image-based Re-ID model. Both Partial-REID and Partial-iLIDS datasets offer only query image set and gallery image set. So we train AGW baseline model on the training set of Market-1501 dataset, then evaluate its performance on the testing set of two partial Re-ID datasets. We adopt the same way to evaluate the performance of BagTricks <ref type="bibr" target="#b121">[122]</ref> baseline on these two partial Re-ID datasets for better comparison and analysis.</p><p>Detailed Comparison. We compare the performance of AGW baseline with other state-of-the-art partial Re-ID methods, including DSR <ref type="bibr" target="#b231">[232]</ref>, SFR <ref type="bibr" target="#b248">[249]</ref> and VPM <ref type="bibr" target="#b66">[67]</ref>. All these methods are published in recent years. The comparison results on both Partial-REID and Partial-iLIDS datasets are shown in <ref type="table" target="#tab_6">Table R4</ref>. The VPM <ref type="bibr" target="#b66">[67]</ref> achieves a very high performance by perceiving the visibility of regions through self-supervision and extracting region-level features. Considering only global features, our proposed AGW baseline still achieves competitive results compared to the current state-of-the-arts on both datasets. Besides, AGW baseline brings significant improvement comparing to BagTricks <ref type="bibr" target="#b121">[122]</ref> under multiple evaluation metrics, demonstrating its effectiveness for partial Re-ID problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Overview of This Survey</head><p>The overview figure of this survey is shown in <ref type="figure" target="#fig_0">Fig. R3</ref>. According to the five steps in developing a person Re-ID system, we conduct the survey from both closed-world and open-world settings. The closed-world setting is detailed in three different aspects: feature representation learning, deep metric learning and ranking optimization. We then summarize the datasets and SOTAs from both image-and video-based perspectives. For open-world person Re-ID, we summarize it into five aspects: including heterogeneous data, Re-ID from raw images/videos, unavailable/limited labels, noisy annotation and open-set Re-ID.</p><p>Following the summary, we present an outlook for future person Re-ID. We design a new evaluation metric (mINP) to evaluate the difficulty to find all the correct matches. By analyzing the advantages of existing Re-ID methods, we develop a strong AGW baseline for future developments, which achieves competitive performance on four Re-ID tasks. Finally, some under-investigated open issues are discussed. Our survey provides a comprehensive summarization of existing state-of-the-art in different subtasks. Meanwhile, the analysis of future directions is also presented for further development guidance.</p><p>Acknowledgement. The authors would like to thank the anonymous reviewers for providing valuable feedbacks to improve the quality of this survey. The authors also would like to thank the pioneer researchers in person reidentification and other related fields. This work is sponsored by CAAI-Huawei MindSpore Open Fund.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Closed-World Setting</head><p>Open-World Setting </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Three kinds of widely used loss functions in the literature. (a) Identity Loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>State-of-the-arts (SOTA) on four image-based person Re-ID datasets. Both the Rank-1 accuracy (%) and mAP value (%) are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>State-of-the-arts (SOTA) on four widely used video-based person Re-ID datasets. The Rank-1 accuracies (%) over years are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>This section firstly presents a new evaluation metric in ? 4.1, a strong baseline (in ? 4.2) for person Re-ID. It provides an important guidance for future Re-ID research. Finally, we discuss some under-investigated open issues in ? 4.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. R1 :</head><label>R1</label><figDesc>The framework of the proposed AGW baseline for single-modality image-based Re-ID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. R2 :</head><label>R2</label><figDesc>The framework of the proposed AGW baseline for cross-modality visible-infrared Re-ID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>?</head><label></label><figDesc>Single-modality Data ? Bounding Boxes Generation ? Sufficient Annotated Data ? Correct Annotation ? Query Exists in Gallery Measure the ability to retrieve the hardest match AGW: Achieve state-of-theart/comparable performance on four Re-ID tasks Discuss five open issues from different aspects Person Re-ID Open-set Re-ID Group Re-ID Dynamic Multi-Camera NetworkFig. R3: An overview of this survey. It contains three main components, including Closed-World Setting in Section 2, Open-World Setting in Section 3 and an outlook of Re-ID in Next Era in Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2001.04193v2 [cs.CV] 6 Jan 2021 2. Bounding Box Generation 1. Raw Data Collection 3. Train Data Annotation 1) Single-modality vs. Heterogeneous Data: For the raw data collection in Step 1, all the persons are represented by images/videos captured by single-modality visible cameras in the closed-world setting</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Statistics of some commonly used datasets for closed-world person Re-ID. "both" means that it contains both hand-cropped and detected bounding boxes. "C&amp;M" means both CMC and mAP are evaluated.</figDesc><table><row><cell>Image datasets</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.5% (GLTR<ref type="bibr" target="#b135">[136]</ref> in 2019) on PRID-2011 dataset, and from 58% (RNN<ref type="bibr" target="#b126">[127]</ref>) to 86.3%</figDesc><table><row><cell></cell><cell>'56$ 6QLSSHW 54(1</cell><cell>*/75 $')'</cell><cell></cell><cell>6QLSSHW '56$ 54(1</cell><cell>&amp;R6HJ */75 $')' 67&amp;</cell><cell>*/75 67$ 67&amp; &amp;R6HJ $')' 6QLSSHW 'X$70 '56$</cell><cell>*/7567$ &amp;R6HJ 67&amp;</cell></row><row><cell>5DQN</cell><cell>6'0</cell><cell>5DQN</cell><cell></cell><cell></cell><cell>5DQN</cell><cell>(7$3</cell><cell>5DQN</cell></row><row><cell>,'(;</cell><cell>7ZR6 $6731 )RUHVW</cell><cell>511</cell><cell>$6731 7ZR6</cell><cell>6'0</cell><cell>54(1 )RUHVW</cell></row><row><cell>511</cell><cell></cell><cell>,'(;</cell><cell>)RUHVW</cell><cell></cell><cell>,'(;</cell><cell>(7$3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>&lt;HDU</cell><cell></cell><cell></cell><cell>P$3</cell><cell>P$3</cell></row><row><cell cols="2">(a) SOTA on PRID-2011 [126]</cell><cell cols="3">(b) SOTA on iLIDS-VID [7]</cell><cell cols="2">(c) SOTA on MARS [8]</cell><cell>(d) SOTA on Duke-Video [144]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 :</head><label>3</label><figDesc>Statistics of SOTA unsupervised person Re-ID on two image-based datasets. "Source" represents if it utilizes the source annotated data in training the target Re-ID model. "Gen." indicates if it contains an image generation process. Rank-1 accuracy (%) and mAP (%) are reported.</figDesc><table><row><cell>Market-1501</cell><cell>DukeMTMC</cell></row></table><note>? ? TJ-AIDL [111] requires additional attribute annotation.? ? DAS [125] generates synthesized virtual humans under vairous lightings.? ? PAUL [153], MAR [209] and CASC [210] use MSMT17 as source dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4 :</head><label>4</label><figDesc>Comparison with the state-of-the-arts on singlemodality image-based Re-ID. Rank-1 accuracy (%), mAP (%) and mINP (%) are reported on two public datasets.</figDesc><table><row><cell></cell><cell cols="3">Market-1501 [5]</cell><cell cols="3">DukeMTMC [42]</cell></row><row><cell>Method</cell><cell>R1</cell><cell cols="2">mAP mINP</cell><cell>R1</cell><cell cols="2">mAP mINP</cell></row><row><cell cols="2">BagTricks [122] CVPR19W 94.5</cell><cell>85.9</cell><cell>59.4</cell><cell>86.4</cell><cell>76.4</cell><cell>40.7</cell></row><row><cell>ABD-Net [173] ICCV19</cell><cell>95.6</cell><cell>88.3</cell><cell>66.2</cell><cell>89.0</cell><cell>78.6</cell><cell>42.1</cell></row><row><cell>B (ours)</cell><cell>94.2</cell><cell>85.4</cell><cell>58.3</cell><cell>86.1</cell><cell>76.1</cell><cell>40.3</cell></row><row><cell>B + Att [246]</cell><cell>94.9</cell><cell>86.9</cell><cell>62.2</cell><cell>87.5</cell><cell>77.6</cell><cell>41.9</cell></row><row><cell>B + WRT</cell><cell>94.6</cell><cell>86.8</cell><cell>61.9</cell><cell>87.1</cell><cell>77.0</cell><cell>41.4</cell></row><row><cell>B + GeM [247]</cell><cell>94.4</cell><cell>86.3</cell><cell>60.1</cell><cell>87.3</cell><cell>77.3</cell><cell>41.9</cell></row><row><cell>B + WRT + GeM</cell><cell>94.9</cell><cell>87.1</cell><cell>62.5</cell><cell>88.2</cell><cell>78.1</cell><cell>43.4</cell></row><row><cell>AGW (Full)</cell><cell>95.1</cell><cell>87.8</cell><cell>65.0</cell><cell>89.0</cell><cell>79.6</cell><cell>45.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5 :</head><label>5</label><figDesc>Comparison with the state-of-the-arts on two image Re-ID datasets, including CUHK03 and MSMT17.</figDesc><table><row><cell cols="7">Rank-1 accuracy (%), mAP (%) and mINP (%) are reported.</cell></row><row><cell></cell><cell cols="3">CUHK03 [43]</cell><cell cols="3">MSMT17 [44]</cell></row><row><cell>Method</cell><cell>R1</cell><cell cols="2">mAP mINP</cell><cell>R1</cell><cell cols="2">mAP mINP</cell></row><row><cell cols="2">BagTricks [122] CVPR19W 58.0</cell><cell>56.6</cell><cell>43.8</cell><cell>63.4</cell><cell>45.1</cell><cell>12.4</cell></row><row><cell>AGW (Full)</cell><cell>63.6</cell><cell>62.0</cell><cell>50.3</cell><cell>68.3</cell><cell>49.3</cell><cell>14.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6 :</head><label>6</label><figDesc>Comparison with the state-of-the-arts on four video-based Re-ID datasets, including MARS<ref type="bibr" target="#b7">[8]</ref>, Duke-</figDesc><table><row><cell cols="7">Video [144], PRID2011 [126] and iLIDS-VID [7]. Rank-1</cell></row><row><cell cols="7">accuracy (%), mAP (%) and mINP (%) are reported.</cell></row><row><cell></cell><cell></cell><cell cols="2">MARS [8]</cell><cell cols="3">DukeVideo [144]</cell></row><row><cell>Method</cell><cell>R1</cell><cell cols="2">mAP mINP</cell><cell>R1</cell><cell cols="2">mAP mINP</cell></row><row><cell cols="2">BagTricks [122] CVPR19W 85.8</cell><cell>81.6</cell><cell>62.0</cell><cell>92.6</cell><cell>92.4</cell><cell>88.3</cell></row><row><cell>CoSeg [132] ICCV19</cell><cell>84.9</cell><cell>79.9</cell><cell>57.8</cell><cell>95.4</cell><cell>94.1</cell><cell>89.8</cell></row><row><cell>AGW (Ours)</cell><cell>87.0</cell><cell>82.2</cell><cell>62.8</cell><cell>94.6</cell><cell>93.4</cell><cell>89.2</cell></row><row><cell>AGW+ (Ours)</cell><cell>87.6</cell><cell>83.0</cell><cell>63.9</cell><cell>95.4</cell><cell>94.9</cell><cell>91.9</cell></row><row><cell></cell><cell cols="3">PRID2011 [126]</cell><cell cols="3">iLIDS-VID [7]</cell></row><row><cell>Method</cell><cell>R1</cell><cell>R5</cell><cell>mINP</cell><cell>R1</cell><cell>R5</cell><cell>mINP</cell></row><row><cell cols="2">BagTricks [122] CVPR19W 84.3</cell><cell>93.3</cell><cell>88.5</cell><cell>74.0</cell><cell>93.3</cell><cell>82.2</cell></row><row><cell>AGW (Ours)</cell><cell>87.8</cell><cell>96.6</cell><cell>91.7</cell><cell>78.0</cell><cell>97.0</cell><cell>85.5</cell></row><row><cell>AGW+ (Ours)</cell><cell>94.4</cell><cell>98.4</cell><cell>95.4</cell><cell>83.2</cell><cell>98.3</cell><cell>89.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7 :</head><label>7</label><figDesc>Comparison with the state-of-the-arts on two partial Re-ID datasets, including Partial-REID and Partial-iLIDS. Rank-1, -3 accuracy (%) and mINP (%) are reported.</figDesc><table><row><cell>Method</cell><cell>R1</cell><cell cols="3">Partial-REID R3 mINP</cell><cell>R1</cell><cell cols="2">Partial-iLIDS R3 mINP</cell></row><row><cell>DSR [232] CVPR18</cell><cell cols="2">50.7</cell><cell>70.0</cell><cell>-</cell><cell cols="2">58.8</cell><cell>67.2</cell><cell>-</cell></row><row><cell>SFR [249] ArXiv18</cell><cell cols="2">56.9</cell><cell>78.5</cell><cell>-</cell><cell cols="2">63.9</cell><cell>74.8</cell><cell>-</cell></row><row><cell>VPM [67] CVPR19</cell><cell cols="2">67.7</cell><cell>81.9</cell><cell>-</cell><cell cols="2">67.2</cell><cell>76.5</cell><cell>-</cell></row><row><cell>BagTricks [122] CVPR19W</cell><cell cols="2">62.0</cell><cell>74.0</cell><cell>45.4</cell><cell cols="2">58.8</cell><cell>73.9</cell><cell>68.7</cell></row><row><cell>AGW</cell><cell cols="2">69.7</cell><cell>80.0</cell><cell>56.7</cell><cell cols="2">64.7</cell><cell>79.8</cell><cell>73.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8 :</head><label>8</label><figDesc></figDesc><table><row><cell cols="7">Comparison with the state-of-the-arts on cross-</cell></row><row><cell cols="7">modality visible-infrared Re-ID. Rank-1 accuracy (%), mAP</cell></row><row><cell cols="7">(%) and mINP (%) are reported on two public datasets.</cell></row><row><cell></cell><cell cols="2">RegDB [60]</cell><cell></cell><cell cols="2">SYSU-MM01 [21]</cell><cell></cell></row><row><cell></cell><cell cols="2">Visible-Thermal</cell><cell cols="2">All Search</cell><cell cols="2">Indoor Search</cell></row><row><cell>Method</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>Zero-Pad [21] ICCV17</cell><cell>17.75</cell><cell>18.90</cell><cell>14.8</cell><cell cols="3">15.95 20.58 26.92</cell></row><row><cell>HCML [186] AAAI18</cell><cell>24.44</cell><cell>20.08</cell><cell cols="4">14.32 16.16 24.52 30.08</cell></row><row><cell>eBDTR [142] TIFS19</cell><cell>34.62</cell><cell>33.46</cell><cell cols="4">27.82 28.42 32.46 42.46</cell></row><row><cell>HSME [187] AAAI19</cell><cell>50.85</cell><cell>47.00</cell><cell cols="2">20.68 23.12</cell><cell>-</cell><cell>-</cell></row><row><cell>D 2 RL [189] CVPR19</cell><cell>43.4</cell><cell>44.1</cell><cell>28.9</cell><cell>29.2</cell><cell>-</cell><cell>-</cell></row><row><cell>AlignG [190] ICCV19</cell><cell>57.9</cell><cell>53.6</cell><cell>42.4</cell><cell>40.7</cell><cell>45.9</cell><cell>54.3</cell></row><row><cell cols="2">Hi-CMD [191] CVPR20 70.93</cell><cell>66.04</cell><cell>34.9</cell><cell>35.9</cell><cell></cell><cell></cell></row><row><cell>AGW (Ours)</cell><cell cols="2">70.05 mINP = 50.19 66.37</cell><cell cols="4">47.50 47.65 54.17 62.97 mINP =35.30 mINP = 59.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE R1</head><label>R1</label><figDesc>? 10 ?5 to 3.5 ? 10 ?4 . The learning rate lr(t) at epoch t can be computed as:</figDesc><table><row><cell cols="18">: Comparison with the state-of-the-arts on four video-based Re-ID datasets, including MARS, DukeVideo,</cell></row><row><cell cols="14">PRID2011 and iLIDS-VID. Rank-1, -5, -10 accuracy (%), mAP (%) and mINP (%) are reported.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Venue</cell><cell>R1</cell><cell>R5</cell><cell cols="2">MARS mAP mINP</cell><cell>R1</cell><cell cols="3">DukeVideo R5 mAP mINP</cell><cell>R1</cell><cell cols="3">PRID2011 R5 R20 mINP</cell><cell>R1</cell><cell cols="3">iLIDS-VID R5 R20 mINP</cell></row><row><cell>ETAP [144]</cell><cell>CVPR18</cell><cell cols="3">80.7 92.0 67.3</cell><cell>-</cell><cell cols="3">83.6 94.5 78.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DRSA [133]</cell><cell>CVPR18</cell><cell>82.3</cell><cell>-</cell><cell>65.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>93.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Snippet [134]</cell><cell>CVPR18</cell><cell cols="3">86.3 94.7 76.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">93.0 99.3</cell><cell>-</cell><cell>-</cell><cell cols="2">85.4 96.7</cell><cell>-</cell><cell>-</cell></row><row><cell>STA [135]</cell><cell>AAAI18</cell><cell cols="3">86.3 95.7 80.8</cell><cell>-</cell><cell cols="3">96.2 99.3 94.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VRSTC [20]</cell><cell>CVPR19</cell><cell cols="3">88.5 96.5 82.3</cell><cell>-</cell><cell cols="3">95.0 99.1 93.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">83.4 95.5 99.5</cell><cell>-</cell></row><row><cell>ADFD [110]</cell><cell>CVPR19</cell><cell cols="3">87.0 95.4 78.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">93.9 99.5 100</cell><cell>-</cell><cell cols="3">86.3 97.4 99.7</cell><cell>-</cell></row><row><cell>GLTR [136]</cell><cell>ICCV19</cell><cell cols="3">87.0 95.7 78.4</cell><cell>-</cell><cell cols="3">96.2 99.3 93.7</cell><cell>-</cell><cell cols="3">95.5 100.0 -</cell><cell>-</cell><cell cols="2">86.0 98.0</cell><cell>-</cell><cell>-</cell></row><row><cell>CoSeg [132]</cell><cell>ICCV19</cell><cell cols="3">84.9 95.5 79.9</cell><cell>57.8</cell><cell cols="3">95.4 99.3 94.1</cell><cell>89.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">79.6 95.3 99.3</cell><cell>-</cell></row><row><cell cols="5">BagTricks [122] CVPR19W 85.8 95.2 81.6</cell><cell>62.0</cell><cell cols="3">92.6 98.9 92.4</cell><cell>88.3</cell><cell cols="3">84.3 93.3 98.0</cell><cell>88.5</cell><cell cols="3">74.0 93.3 99.1</cell><cell>82.2</cell></row><row><cell>AGW</cell><cell>-</cell><cell cols="3">87.0 95.7 82.2</cell><cell>62.8</cell><cell cols="3">94.6 99.1 93.4</cell><cell>89.2</cell><cell cols="3">87.8 96.6 98.9</cell><cell>91.7</cell><cell cols="3">78.0 97.0 99.5</cell><cell>85.5</cell></row><row><cell>AGW+</cell><cell>-</cell><cell cols="3">87.6 85.8 83.0</cell><cell>63.9</cell><cell cols="3">95.4 99.3 94.9</cell><cell>91.9</cell><cell cols="3">94.4 98.4 100</cell><cell>95.4</cell><cell cols="3">83.2 98.3 99.7</cell><cell>89.0</cell></row><row><cell cols="2">? ? ? ? increased from 3.5 lr(t) = 3.5 ? 10 ?5 ? t 10 3.5 ? 10 ?4 3.5 ? 10 ?5 ? ? ? 3.5 ? 10 ?6</cell><cell cols="4">if t ? 10 if 10 &lt; t ? 40 if 40 &lt; t ? 70 if 70 &lt; t ? 120.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE R2 :</head><label>R2</label><figDesc>Comparison with the state-of-the-arts on SYSU-MM01 dataset. Rank at r accuracy (%), mAP (%) and mINP (%) are reported. (Single-shot query setting<ref type="bibr" target="#b20">[21]</ref> for experiments). "*" represents methods published after the paper submission.</figDesc><table><row><cell>Settings</cell><cell></cell><cell></cell><cell></cell><cell>All Search</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Indoor Search</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Venue</cell><cell>r = 1</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>mINP</cell><cell>r = 1</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>mINP</cell></row><row><cell>One-stream [21]</cell><cell>ICCV17</cell><cell>12.04</cell><cell>49.68</cell><cell>66.74</cell><cell>13.67</cell><cell>-</cell><cell>16.94</cell><cell>63.55</cell><cell>82.10</cell><cell>22.95</cell><cell>-</cell></row><row><cell>Two-stream [21]</cell><cell>ICCV17</cell><cell>11.65</cell><cell>47.99</cell><cell>65.50</cell><cell>12.85</cell><cell>-</cell><cell>15.60</cell><cell>61.18</cell><cell>81.02</cell><cell>21.49</cell><cell>-</cell></row><row><cell>Zero-Pad [21]</cell><cell>ICCV17</cell><cell>14.80</cell><cell>54.12</cell><cell>71.33</cell><cell>15.95</cell><cell>-</cell><cell>20.58</cell><cell>68.38</cell><cell>85.79</cell><cell>26.92</cell><cell>-</cell></row><row><cell>TONE [186]</cell><cell>AAAI18</cell><cell>12.52</cell><cell>50.72</cell><cell>68.60</cell><cell>14.42</cell><cell>-</cell><cell>20.82</cell><cell>68.86</cell><cell>84.46</cell><cell>26.38</cell><cell>-</cell></row><row><cell>HCML [186]</cell><cell>AAAI18</cell><cell>14.32</cell><cell>53.16</cell><cell>69.17</cell><cell>16.16</cell><cell>-</cell><cell>24.52</cell><cell>73.25</cell><cell>86.73</cell><cell>30.08</cell><cell>-</cell></row><row><cell>BDTR [142]</cell><cell>IJCAI18</cell><cell>27.32</cell><cell>66.96</cell><cell>81.07</cell><cell>27.32</cell><cell>-</cell><cell>31.92</cell><cell>77.18</cell><cell>89.28</cell><cell>41.86</cell><cell>-</cell></row><row><cell>eBDTR [142]</cell><cell>TIFS19</cell><cell>27.82</cell><cell>67.34</cell><cell>81.34</cell><cell>28.42</cell><cell>-</cell><cell>32.46</cell><cell>77.42</cell><cell>89.62</cell><cell>42.46</cell><cell>-</cell></row><row><cell>HSME [187] D 2 RL [189]</cell><cell>AAAI19 CVPR19</cell><cell>20.68 28.9</cell><cell>32.74 70.6</cell><cell>77.95 82.4</cell><cell>23.12 29.2</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>MAC [261]</cell><cell>MM19</cell><cell>33.26</cell><cell>79.04</cell><cell>90.09</cell><cell>36.22</cell><cell>-</cell><cell>36.43</cell><cell>62.36</cell><cell>71.63</cell><cell>37.03</cell><cell>-</cell></row><row><cell>MSR [262]</cell><cell>TIP19</cell><cell>37.35</cell><cell>83.40</cell><cell>93.34</cell><cell>38.11</cell><cell>-</cell><cell>39.64</cell><cell>89.29</cell><cell>97.66</cell><cell>50.88</cell><cell>-</cell></row><row><cell>AlignGAN [190] X-Modal  *  [263] Hi-CMD  *  [191] cm-SSFT  *  [264] DDAG  *  [192] HAT  *  [188]</cell><cell>ICCV19 AAAI-20 CVPR20 CVPR20 ECCV20 TIFS20</cell><cell>42.4 49.9 34.9 47.7 54.75 55.29</cell><cell>85.0 89.8 77.6 -90.39 92.14</cell><cell>93.7 96.0 --95.81 97.36</cell><cell>40.7 50.7 35.9 54.1 53.02 53.89</cell><cell>----39.62 -</cell><cell>45.9 ---61.02 62.10</cell><cell>87.6 ---94.06 95.75</cell><cell>94.4 ---98.41 99.20</cell><cell>54.3 ---67.98 69.37</cell><cell>----62.61 -</cell></row><row><cell>AGW</cell><cell>-</cell><cell>47.50</cell><cell>84.39</cell><cell>92.14</cell><cell>47.65</cell><cell>35.30</cell><cell>54.17</cell><cell>91.14</cell><cell>95.98</cell><cell>62.97</cell><cell>59.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE R4 :</head><label>R4</label><figDesc>Comparison with the state-of-the-arts on two partial Re-ID datasets, including Partial-REID and Partial-iLIDS. Rank-1, -3 accuracy (%) and mINP (%) are reported.</figDesc><table><row><cell>Method</cell><cell>R1</cell><cell cols="3">Partial-REID R3 mINP</cell><cell>R1</cell><cell cols="2">Partial-iLIDS R3 mINP</cell></row><row><cell>DSR [232] CVPR18</cell><cell cols="2">50.7</cell><cell>70.0</cell><cell>-</cell><cell cols="2">58.8</cell><cell>67.2</cell><cell>-</cell></row><row><cell>SFR [249] ArXiv18</cell><cell cols="2">56.9</cell><cell>78.5</cell><cell>-</cell><cell cols="2">63.9</cell><cell>74.8</cell><cell>-</cell></row><row><cell>VPM [67] CVPR19</cell><cell cols="2">67.7</cell><cell>81.9</cell><cell>-</cell><cell cols="2">67.2</cell><cell>76.5</cell><cell>-</cell></row><row><cell cols="3">BagTricks [122] CVPR19W 62.0</cell><cell>74.0</cell><cell>45.4</cell><cell cols="2">58.8</cell><cell>73.9</cell><cell>68.7</cell></row><row><cell>AGW</cell><cell cols="2">69.7</cell><cell>80.0</cell><cell>56.7</cell><cell cols="2">64.7</cell><cell>79.8</cell><cell>73.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Person reidentification by camera correlation aware feature augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="392" to="408" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1528" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Re-id done right: towards good practices for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05339</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aggregating deep pyramidal representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">Luca</forename><surname>Foresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Specific person retrieval via incomplete text description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="547" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Identity-aware textual-visual matching with latent co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1890" to="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person re-identification with discriminatively trained viewpoint invariant dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4516" to="4524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving person re-identification by viewpoint cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zaidenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boulay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-scale learning for low-resolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3765" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Resource aware person reidentification across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="8042" to="8051" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Illumination-invariant person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="365" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving person re-identification via pose-aware multi-shot matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Spindle net: Person reidentification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="420" to="429" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarially occluded samples for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5098" to="5107" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vrstc: Occlusion-free video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7183" to="7192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rgb-infrared cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5380" to="5389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1179" to="1188" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Continuous adaptation of multi-camera person identification models through sparse non-redundant representative selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="66" to="78" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal model adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="858" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminant context information analysis for post-ranking person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1650" to="1665" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards open-world person re-identification by one-shot group-based verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="591" to="606" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active image pair selection for continuous person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4263" to="4267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalizable person re-identification by domain-invariant mapping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Consistent re-identification in a camera network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="330" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person re-identification by contour sketch under moderate clothing change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical gaussian descriptor for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1363" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Person re-identification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relaxed pairwise learned metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?stinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="780" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient psd constrained asymmetric metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3685" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification by deep asymmetric metric embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A survey of open-world person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learningbased methods for person re-identification: A comprehensive review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-A</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Survey on deep learning techniques for person re-identification task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Serj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ullah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05284</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Intelligent multi-camera video surveillance: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Survey of pedestrian detection for advanced driver assistance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geronimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1239" to="1258" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6457" to="6465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Features for multi-target multi-camera tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6036" to="6046" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Domain transfer support vector ranking for person re-identification without target camera label information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3567" to="3574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Person reidentification via ranking aggregation of similarity pulling and dissimilarity pushing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia (TMM)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2553" to="2566" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Person recognition system based on a combination of body images from visible light and thermal cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">605</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">One-pass person reidentification by sketch online discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="237" to="250" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Robust depth-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2588" to="2603" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1345" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Semi-supervised coupled dictionary learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3550" to="3557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Shape and appearance context modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Towards unsupervised open-set person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="769" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fast open-world person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2286" to="2300" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="475" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07220</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A spatiotemporal appearance representation for viceo-based pedestrian re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3810" to="3818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Video person reidentification by temporal residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1366" to="1377" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deeply-learned partaligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep representation learning with part loss for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="480" to="496" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Person re-identification using cnn features learned from combination of attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2428" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Joint learning of single-image and cross-image representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1288" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3800" to="3808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Robust anchor embedding for unsupervised video person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="170" to="186" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Multi-scale deep learning architectures for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5399" to="5408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Attention driven person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="143" to="155" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2285" to="2294" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="365" to="381" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">End-to-end deep kronecker-product matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6886" to="6895" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Person re-identification with cascaded pairwise convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1470" to="1478" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Self-critical attention learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9637" to="9646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Dual attention matching network for context-aware feature sequence based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5363" to="5372" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Re-identification with consistent attentive siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5735" to="5744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Discriminative feature learning with consistent attention regularization for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8040" to="8049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="8649" to="8658" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Spectral feature transformation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4976" to="4985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="402" to="419" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Deeply-learned partaligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Person reidentification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Learning deep context-aware features over body and latent parts for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Pose-driven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3960" to="3969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Attentionaware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2119" to="2128" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Beyond human parts: Dual part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3642" to="3651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Dissecting person re-identification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="608" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Secondorder non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3760" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9317" to="9326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Aanet: Attribute attention network for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Attribute-driven feature disentangling and temporal aggregation for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4913" to="4922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jingya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiatian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shaogang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2275" to="2284" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2109" to="2118" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">View confusion feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Aware loss with angular regularization for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Consistent-aware deep learning for person re-identification in a camera network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5771" to="5780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Pose transferrable person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="4099" to="4108" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Pose-normalized image generation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="650" to="667" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5157" to="5166" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2138" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Imageimage domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="994" to="1003" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Cross-dataset person re-identification via unsupervised pose disentanglement and adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7919" to="7929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">A strong baseline and batch normneuralization neck for deep person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08332</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3691" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Domain adaptation through synthesis for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="189" to="205" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Person reidentification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Recurrent convolutional network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez Del Rincon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">A two stream siamese convolutional neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tahboub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1983" to="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Person reidentification via recurrent feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="701" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">See the forest for the trees: Joint spatial and temporal recurrent neural networks for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4747" to="4756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Jointly attentive spatial-temporal pooling networks for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Co-segmentation inspired attention networks for video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="562" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="369" to="378" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="1169" to="1178" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Sta: Spatial-temporal attention for large-scale video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Global-local temporal representations for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3958" to="3967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Efficient and deep person reidentification using multi-level similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2335" to="2344" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3750" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Eliminating background-bias for robust person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5794" to="5803" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">A discriminatively learned cnn embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05666</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Bi-directional center-constrained top-ranking for visible thermal person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security (TIFS)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="407" to="419" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">An overview and empirical comparison of distance metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moutafis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="612" to="625" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person reidentification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="5177" to="5186" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Deep cosine metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in WACV</title>
		<imprint>
			<biblScope unit="page" from="748" to="756" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Spherereid: Deep hypersphere manifold embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIR</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="51" to="58" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02629</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Embedding deep metric for person re-identification: A study against large variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="732" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Point to set similarity based deep feature learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3741" to="3750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Hard-aware point-to-set deep metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="188" to="204" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Patch-based discriminative feature learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3633" to="3642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Deep reinforcement active learning for human-in-the-loop person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6122" to="6131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Easy identification from better constraints: Multi-shot person re-identification from reference constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5373" to="5381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Ranking optimization for person re-identification via similarity and dissimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia (ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1239" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Pop: Person reidentification post-rank optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Human-in-the-loop person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="405" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1846" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Re-ranking via metric fusion for object retrieval and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="740" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Query based adaptive re-ranking for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="397" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<title level="m" type="main">Efficient online local metric adaptation via negative samples for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2420" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Query-adaptive late fusion for image search and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">Shape: A novel graph theoretic algorithm for making consensus-based decisions in person reidentification systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Associating groups of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Person re-identification by manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3567" to="3571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">A systematic evaluation and benchmark for person reidentification: Features, metrics, and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rates-Borras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="523" to="536" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="737" to="753" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Regionbased quality estimation network for large-scale person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hetang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="page" from="7347" to="7354" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Abd-net: Attentive but diverse person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Mixed high-order attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing humanlevel performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">Dynamic label graph matching for unsupervised video re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5142" to="5150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">advpattern: Physical-world attacks on deep person reidentification via adversarially transformable patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8341" to="8350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Multi-shot pedestrian reidentification via sequential decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="6781" to="6789" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Recurrent attention models for depth-based person identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1229" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Reinforced temporal attention and split-rate transfer for depth-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karianakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="715" to="733" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Re-identification with rgb-d sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">B</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Improving deep visual representation for person reidentification by global and local image-language association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="54" to="70" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="686" to="701" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Deep adversarial graph attention convolution network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Visible thermal person re-identification via dual-constrained top-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1092" to="1099" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Hierarchical discriminative learning for visible thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="page" from="7501" to="7508" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Hsme: Hypersphere manifold embedding for visible thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8385" to="8392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Visible-infrared person reidentification via homogeneous augmented tri-modal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIFS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Learning to reduce dual-level discrepancy for infrared-visible person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Rgbinfrared cross-modality person re-identification via joint pixel and feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3623" to="3632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Hi-cmd: Hierarchical cross-modality disentanglement for visible-infrared person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Dynamic dual-attentive aggregation learning for visible-infrared person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Cascaded sr-gan for scale-adaptive low resolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="3891" to="3897" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Recover and identify: A generative dual model for cross-resolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8090" to="8099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Neural person search machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="493" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Learning context graph for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2158" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Query-guided end-to-end person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Munjal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Re-id driven localization refinement for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9814" to="9823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning attention selection for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<monogr>
		<title level="m" type="main">Spatiotemporal person retrieval via natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1453" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3539" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Locality aware appearance metric for multi-target multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12037</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Person re-identification by unsupervised l1 graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="178" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<monogr>
		<title level="m" type="main">Stepwise metric promotion for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2429" to="2438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Dynamic graph co-matching for unsupervised video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2976" to="2990" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">Exploiting global camera network constraints for unsupervised video person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10486</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Hierarchical clustering with hard-batch triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification by camera-aware similarity consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6922" to="6931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Unsupervised graph association for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8321" to="8330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<title level="m" type="main">One-shot metric learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<monogr>
		<title level="m" type="main">Learning person re-identification models from videos with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Raychaudhuri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10631</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="172" to="188" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Adaptive transfer network for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7202" to="7211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Sbsgan: Suppression of inter-domain background shift for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9527" to="9536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Instance-guided context rendering for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<monogr>
		<title level="m" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Surpassing real-world source training data: Random 3d characters for generalizable person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2020</title>
		<imprint>
			<biblScope unit="page" from="3422" to="3430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">A novel unsupervised camera-aware domain adaptation framework for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8080" to="8089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8222" to="8231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<monogr>
		<title level="m" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7948" to="7956" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Interpretable and generalizable person re-identification with query-adaptive convolution and temporal lifting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<monogr>
		<title level="m" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="994" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Style normalization and restitution for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Multiple expert brainstorming for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Augmentation invariant and instance spreading feature for softmax embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4678" to="4686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7073" to="7082" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Foreground-aware pyramid reconstruction for alignment-free occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8450" to="8459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Pose-guided feature alignment for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="542" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Robust person re-identification by modelling feature uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Purifynet: A robust person reidentification model with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIFS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Adversarial open-world person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="280" to="296" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">On the error-reject tradeoff in biometric verification systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Golfarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="786" to="796" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<monogr>
		<title level="m" type="main">Group re-identification via unsupervised transfer of sparse features encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luca Foresti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2449" to="2458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Matching groups of people by covariance descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Takala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2744" to="2747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Group reidentification: Leveraging and integrating multi-grain information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="192" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Dotgnn: Domain-transferred graph neural network for group reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1888" to="1896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Person reidentification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="486" to="504" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Unsupervised adaptive re-identification in open world dynamic camera networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7054" to="7063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Human re-identification in crowd videos using personal, social and environmental constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Assari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="119" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7794" to="7803" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Multisimilarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5022" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<monogr>
		<title level="m" type="main">Recognizing partial biometric patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07399</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Clothing change aware person identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Katipally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Zon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2112" to="2120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<monogr>
		<title level="m" type="main">When person re-identification meets changing clothes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04070</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Exploiting transitivity for learning person re-identification models on a budget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="7064" to="7072" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<monogr>
		<title level="m" type="main">Person re-identification in the 3d space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04569</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Cross dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="650" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<monogr>
		<title level="m" type="main">Decentralised learning from independent multi-domain labels for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b255">
	<monogr>
		<title level="m" type="main">Incremental learning in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06281</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Part-based deep hashing for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4806" to="4817" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<monogr>
		<title level="m" type="main">Fast person reidentification via cross-camera semantic binary transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3873" to="3882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Faster person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="275" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">Distilled person reidentification: Towards a more scalable system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1187" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Modality-aware collaborative learning for visible thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="347" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Learning modality-specific representations for visible-infrared person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="579" to="590" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Infrared-visible cross-modal person re-identification with an x modality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint>
			<biblScope unit="page" from="4610" to="4617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">Crossmodality person re-identification with shared-specific feature transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="379" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
