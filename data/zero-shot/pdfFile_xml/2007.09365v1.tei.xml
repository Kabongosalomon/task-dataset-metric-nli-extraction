<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Malleable 2.5D Convolution: Learning Receptive Fields along the Depth-axis for RGB-D Scene Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-18">18 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">The Chinese</orgName>
								<orgName type="institution">University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Malleable 2.5D Convolution: Learning Receptive Fields along the Depth-axis for RGB-D Scene Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-18">18 Jul 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>RGB-D Scene Parsing</term>
					<term>Geometry in CNN</term>
					<term>Malleable 25D Convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Yajie Xing 1[0000?0002?1226?1529] , Jingbo Wang 2[0000?0001?9700?6262] , and Gang Zeng 1[0000?0002?9575?4651]</p><p>Abstract. Depth data provide geometric information that can bring progress in RGB-D scene parsing tasks. Several recent works propose RGB-D convolution operators that construct receptive fields along the depth-axis to handle 3D neighborhood relations between pixels. However, these methods pre-define depth receptive fields by hyperparameters, making them rely on parameter selection. In this paper, we propose a novel operator called malleable 2.5D convolution to learn the receptive field along the depth-axis. A malleable 2.5D convolution has one or more 2D convolution kernels. Our method assigns each pixel to one of the kernels or none of them according to their relative depth differences, and the assigning process is formulated as a differentiable form so that it can be learnt by gradient descent. The proposed operator runs on standard 2D feature maps and can be seamlessly incorporated into pre-trained CNNs. We conduct extensive experiments on two challenging RGB-D semantic segmentation dataset NYUDv2 and Cityscapes to validate the effectiveness and the generalization ability of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent progresses <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> in CNN have achieved great success in scene parsing tasks such as semantic segmentation. Depth data provide geometric information that is not captured by the color channels and therefore can assist feature extraction and improve segmentation performance. With the availability of commercial RGB-D sensors such as Kinect, there comes an increasing interest in exploiting the additional depth data and incorporating geometric information into CNNs.</p><p>Many works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref> take RGB images and depth maps or HHA <ref type="bibr" target="#b8">[9]</ref> encodings as two separate inputs and adopt two-stream style networks to process them. These methods only take depth information as features and keep the fixed <ref type="figure">Fig. 1</ref>. Illustration of the malleable 2.5D convolution with 3 kernels. Best view in color. The blue, green, and red colors respectively represent the receptive field of each kernel. The malleable 2.5D convolution arranges its kernels sequentially along the depth-axis and adopts differentiable functions to construct soft depth receptive fields for its kernels. It samples pixels on the 2D plane, the same as standard 2D convolution. And it assigns pixels to its kernels according to depth data and its depth receptive fields. During the training process, the depth receptive fields can be learnt to compress or stretch automatically according to the dataset, making the convolution "malleable" geometric structures of 2D CNN, which neglects the available 3D geometric relations between pixels. The most direct method to leverage the 3D relations between pixels is to project RGB-D data into 3D pointclouds <ref type="bibr" target="#b22">[23]</ref> or volumes <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref>, and exploit 3D networks to handle geometry. However, 3D networks are computationally more expensive than 2D CNN and cannot benefit from prevalent ImageNet-pretrained models.</p><p>Recently, some works turn to introduce geometric information into the 2D convolution operators. Depth-aware CNN <ref type="bibr" target="#b30">[31]</ref> augments the standard convolution with a depth similarity term. It applies masks to suppress the contribution of pixels whose depths are different from the center of the kernel. Consequently, it constructs a soft receptive field along the depth-axis where more distant pixels are partially occluded by the mask. 2.5D convolution <ref type="bibr" target="#b33">[34]</ref> moves a step forward to utilizing more kernels to capture richer geometric relations. It builds a grid receptive field in 3D space, assigns each pixel to one of the kernels according to the relative depth differences with the center of the kernel, and thus mimics a real 3D convolution kernel. These methods successfully leverage geometric information by introducing a receptive field along the depth-axis. And they can be easily incorporated into pre-trained CNNs while not bring much computational cost. However, in these methods, the depth receptive fields are determined by pre-defined hyperparameters.</p><p>In different environments, the scene structures and depth quality can be very different. For example, NYUDv2 <ref type="bibr" target="#b26">[27]</ref> consists of indoor scenes and captures depth data by Kinect, which has good accuracy. Cityscapes <ref type="bibr" target="#b6">[7]</ref> uses a stereo camera to capture outdoor street scenes, resulting in a much longer depth range and noisier depth data. Naturally, the receptive field along the depth-axis should not be the same across different environment settings. If we artificially pre-define depth receptive fields for different environments, it would bring many parameteradjusting works, and the selected parameters are possibly still not suitable for the specific environment. Therefore, we need a method that can not only build a receptive field along the depth-axis to handle 3D geometry, but also flexibly learn the receptive field for different environments.</p><p>To address the aforementioned problems, in this paper, we propose a novel convolution operator called malleable 2.5D convolution (illustrated in <ref type="figure">Fig. 1</ref>). Similar to 2.5D convolution, the malleable 2.5D convolution can have either one or more 2D convolution kernels sequentially arranged along the depth-axis. To determine the depth receptive field of each kernel, we adopt a softmax classification to assign each pixel to kernels according to pixels' relative depth differences. The assigning process is differentiable and can be learnt by gradient descent. We also introduce learnable "kernel rebalancing weights" parameters to rebalance the output scale of each kernel, since pixels are not distributed evenly in each class. The malleable 2.5D convolution can flexibly learn the depth receptive field for different environments (as shown in <ref type="figure" target="#fig_3">Fig. 4</ref> while only introducing a small number of additional parameters (2k + 3 parameters if it has k kernels). Meanwhile, because malleable 2.5D convolutions are based on 2D convolution kernels, they can be easily incorporated into pre-trained 2D CNNs by simply replacing standard 2D convolutions.</p><p>Our contributions can be summarized as follows:</p><p>-We propose a novel convolution operator called malleable 2.5D convolution that has learnable receptive fields along the depth-axis. -Two techniques are proposed in the malleable 2.5D convolution: 1) We propose a differentiable pixel assigning method to achieve learnable depth receptive field; 2) We introduce "kernel rebalancing weights" parameters to rebalance the uneven pixel distribution in the kernels. -We conduct extensive experiments on both indoor RGB-D semantic segmentation dataset NYUDv2 <ref type="bibr" target="#b26">[27]</ref> and outdoor dataset Cityscapes <ref type="bibr" target="#b6">[7]</ref>, and validate the effectiveness and generalization ability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>RGB-D Scene Parsing Benefiting from the great success of deep convolutional networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11]</ref>, Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b20">[21]</ref> and its successors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36]</ref> have achieved promising results for RGB semantic segmentation. RGB-D segmentation extends RGB semantic segmentation by providing additional depth data. A widely applied method is to encode depth into HHA features <ref type="bibr" target="#b8">[9]</ref>: horizontal disparity, height above the ground and angle with gravity direction. It is usually used in two-stream style networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref> to process RGB images and HHA images and fuse the features or predictions.</p><p>Other methods attempt to exploit geometric clues from depth data instead of treating them as features. Some works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b22">23]</ref> transform RGB-D images into 3D data and use 3D networks to handle geometry. Other works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> take advantage of the fact that the scales of objects in images are inversely proportional to the depths, and change the receptive fields of convolutions according to the depth information.</p><p>Depth-aware CNN <ref type="bibr" target="#b30">[31]</ref> and 2.5D Convolution <ref type="bibr" target="#b33">[34]</ref> are two methods most related to our work. Depth-aware CNN applies a mask to construct a soft receptive field along the depth-axis where more distant pixels are weakened and nearer pixels have more contribution to the output. 2.5D convolution seeks to mimic a 3D convolution kernel with several 2D convolution kernels on 2D plane. It builds a grid receptive field in 3D space, and accordingly assigns each pixel to one of the kernels in a similar way a 3D convolution does. Both methods pre-define the receptive field along the depth-axis by hyperparameters, making them rely on parameter adjusting and therefore more difficult to be applied in different environments. Our method solves this problem by building a learnable depth receptive field, and yields better effectiveness and generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutions with Learnable Receptive Fields</head><p>In 2D CNNs, there are several works attempting to break the fixed grid kernel structure of convolution and construct learnable receptive fields. Spatial Transformer Networks <ref type="bibr" target="#b11">[12]</ref> warps feature maps with a learned global spatial transformation. SAC <ref type="bibr" target="#b36">[37]</ref> learns the scale of receptive fields according to the contents of local features. PAC <ref type="bibr" target="#b37">[38]</ref> moreover learns convolution kernel shapes according to the perspective. Deformable convolution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b40">41]</ref> learns 2D offsets and adds them to the regular grid sampling locations in the standard convolution. These works change their receptive field on the 2D plane, which can be implemented by linear transformation and interpolation. However, along the depth-axis, we are facing a very different problem. We cannot determine a sample point and then calculate the corresponding feature because pixels in 3D space are very sparse. We have to do it inversely, assigning pixels to sample points. And that makes the process more difficult to be differentiable. Despite the difficulty, our method manages to construct a learnable receptive field along the depth-axis in a novel way. Another difference is that in this work, we aim to learn the depth receptive field suitable for the given dataset instead of each pixel.</p><p>Neural Architecture Search Some neural architecture search works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30]</ref> try to determine the best convolution kernel shapes (or their combination) for the specific dataset. They use different methods including searching techniques or differentiable optimization. Our method can also be considered as searching the kernel shapes of malleable 2.5D convolutions, but we only search the receptive fields along the depth-axis and we achieve it through learning by gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Malleable 2.5D Convolution</head><p>In this section, we describe and analyze the proposed malleable 2.5D convolution. We firstly introduce the method to construct learnable receptive fields along the depth-axis. Then we introduce the kernel rebalancing mechanism in the operator. Finally, we give some analysis of malleable 2.5D convolution and present how we integrate it into pre-trained CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Receptive Fields along the Depth-axis</head><p>Relative Depth Differences First of all, we define the relative depth differences that are used when we construct receptive fields along the depth-axis. For a convolution kernel, if input map has a downsampling rate r down to the original input image, and the dilation rate is r dilate , then the distance between two neighborhood points sampled by the convolution is ?d p = r down * r dilate . Note that ?d p is a distance on the original input image plane.</p><p>For a pixel p i whose coordinate at the input image is c i , and the depth at</p><formula xml:id="formula_0">p i is d(c i ), the coordinate of p i in 3D space is? i = (c i ? (c x , c y )) * d(c i )/f , where c x ,</formula><p>c y are the coordinates of the principal point and f is the focal length. c x , c y , f are all given by camera parameters (usually two focal lengths f x , f y are given, but in most cases they are very close and we can approximately treat them as the same value).</p><p>If we project the local sampling grid of the convolution kernel around p i to 3D space, then the distance between two neighborhood points would be ?d s (c i ) = ?d p * d(c i )/f . The grid size along the depth-axis should be the same with the other two directions. Therefore, we define ?d s (c i ) as the unit of relative depth difference at image coordinate c i . And the relative depth difference between pixel at c j and c i within the local grid centered at c i is given by</p><formula xml:id="formula_1">d(c i , c j ) = (d(c i ) ? d(c j ))/?d s (c i )<label>(1)</label></formula><p>Depth Receptive Field Functions For each pixel p i whose coordinate at the input image is c i , a standard 2D convolution performs a weighted sum within the receptive field around p i :</p><formula xml:id="formula_2">y(c i ) = cp?Rp w(c p ) ? x(c i + c p ),<label>(2)</label></formula><p>where R p is the local grid that describes the 2D receptive field around p i in the input x, and w is the convolution kernel. Typically, R p is a regular grid defined by kernel size and dilation rate. A malleable 2.5D convolution has K convolution kernels whose receptive fields are sequentially arranged along the depth-axis. And the pixels are assigned to the kernels according to depth map d: where g k is the assigning function for kernel k. g k defines the depth receptive field of kernel k, and satisfies</p><formula xml:id="formula_3">y(c i ) = K k=1 cp?Rp g k (d(c i ), d(c i + c p )) ? w k (c p ) ? x(c i + c p ),<label>(3)</label></formula><formula xml:id="formula_4">K k=1 g k (d(c i ), d(c i + c p )) ? 1, ?c p ? R p .<label>(4)</label></formula><p>To make the assigning functions differentiable, we implement them as a softmax classification:</p><formula xml:id="formula_5">g k (d(c i ), d(c i + c p )) = exp(h k (d(c i ), d(c i + c p ))) K+1 m=0 exp(h m (d(c i ), d(c i + c p )) .<label>(5)</label></formula><p>Here h 0 and h K+1 are the functions for the two classes outside of all receptive fields (in front of and behind). For k = 1, 2, ? ? ? , K, h k are defined by the relative depth difference as</p><formula xml:id="formula_6">h k (d(c i ), d(c i + c p )) = ?(d(c i , c i + c p ) ? a k ) 2 /t,<label>(6)</label></formula><p>where a k is a learnable parameter that determines the center of the kernel's depth receptive field, and t is a learnable temperature parameter that can sharpen/soften the activation of softmax. h 0 and h K+1 are defined as</p><formula xml:id="formula_7">h 0 = ?sgn(d(c i , c i + c p ) ? a 0 ) ? (d(c i , c i + c p ) ? a 0 ) 2 /t, h K+1 = sgn(d(c i , c i + c p ) ? a K+1 ) ? (d(c i , c i + c p ) ? a K+1 ) 2 /t.<label>(7)</label></formula><p>Here sgn denotes the signum function, which is used to make h 0 and h K+1 monotonic so that they construct borders of the receptive field. The constructed depth receptive fields are controlled by parameters t and a k , k = 0, 1, ? ? ? , K + 1. And both sets of parameters are learnable by gradient descent. <ref type="figure" target="#fig_0">Fig 2 gives</ref> an example of the image of h k and g k .  The ratio of pixels assigned to each kernel, before and after rebalance. The figure shows the malleable 2.5D convolution at res5 stage of a trained model. We count the sum of g k and s k ?g k for each kernel across the whole NYUDv2 dataset and calculate the ratio. More cases will be presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Kernel Rebalancing</head><p>Through the receptive field functions g k , we can assign pixels to convolution kernels. However, pixels are not evenly distributed along the depth-axis. In other words, the expectation</p><formula xml:id="formula_8">E[g k (d(c i ), d(c i + c p ))]</formula><p>are not equal for different parameters of g k . This implies that the outputs of different kernels might have different value scales, and the scales change with parameters of g k . Theoretically, the scale change can be adjusted by w k , but implicitly learning a scale factor in convolution kernel weights could increase learning difficulty. Therefore, we introduce a scale factor s k to rebalance the output scales of different kernel. Then Eq. 3 is modified as</p><formula xml:id="formula_9">y(c i ) = K k=1 cp?Rp s k ? g k (d(c i ), d(c i + c p )) ? w k (c p ) ? x(c i + c p ),<label>(8)</label></formula><p>And s k -s must satisfy s k ? 0, ?k = 1, 2, ? ? ? , K and K k=1 s k = 1. Note that K k=1 s k can actually equal to any constant considering that in modern CNNs, a convolution layer is almost always followed by a normalization layer. To ensure that s k -s satisfy the conditions, we implement them as</p><formula xml:id="formula_10">s k = exp(b k ) K k=1 exp(b k )<label>(9)</label></formula><p>And the malleable 2.5D convolution learns b k by gradient descent. <ref type="figure">Fig. 9</ref> shows the effect of kernel rebalancing. We can see that the kernel rebalancing mechanism meets our expectation and successfully rebalance the pixel distribution in different kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Understanding Malleable 2.5D Convolution</head><p>Comparisons with other RGB-D convolutions To get a better understanding of learnable depth receptive fields, we compare our malleable 2.5D convolution with the two previous RGB-D convolutions, Depth-aware Convolution and the 2.5D Convolution. These two convolutions can both be written as the form of Eq. 3. In Depth-aware Convolution, K is 1, and g 1 is defined as</p><formula xml:id="formula_11">g 1 (d(c i ), d(c i + c p )) = exp(??|d(c i ) ? d(c i + c p )|),<label>(10)</label></formula><p>where ? is a pre-defined constant. And in 2.5D convolution, K can be any positive integer, and g k is defined as  In <ref type="figure" target="#fig_3">Fig 4,</ref> we compare depth receptive fields of these two convolutions and our method. We visualize depth receptive fields in two typical cases in NYUDv2 and Cityscapes respectively. In the two datasets, both focal lengths and the depth ranges are different. The Depth-aware Convolution uses absolute depth difference to control the receptive field. When the depths and focal length changes, the 3D distance between two adjacent pixels on the image could be very different. And we can see that Depth-aware Convolution cannot fit both cases with the same parameter setting. 2.5D convolution adopts relative depth differences, which makes it more robust to varying depth and focal length. However, 2.5D convolution's receptive field along the depth-axis is also pre-determined and cannot adapt to different datasets without handcraft adjusting. In contrast, malleable 2.5D convolution not only can avoid the influence of varying depth and focal length, but also automatically learn the depth receptive field for different environments. As an outdoor dataset, Cityscapes has a larger range of depth. And the depth maps in Cityscapes are relatively noisy, not as sharp as those in NYUDv2. Intuitively, the depth receptive field for Cityscapes should be larger and less sharp. We can see from the figure that malleable 2.5D convolution learns a wider depth receptive field for Cityscapes than that for NYUDv2, and the edges drop slower. In <ref type="table" target="#tab_1">Table 1</ref>, we compare the FLOPs and parameter number of different convolutions. We present ResNet-based DeepLabv3+ <ref type="bibr" target="#b3">[4]</ref> as the baseline, and replace the 3 ? 3 convolution with a RGB-D convolution in the first residual unit in each stage of the ResNet. It shows that when using the same number of kernels, the malleable 2.5D convolution only brings very minor additional computational cost to achieve learnable depth receptive fields.</p><formula xml:id="formula_12">g k (d(c i ), d(c i + c p )) = ? ? ? 1, k ? 1 ? K 2 ? d(c i , c i + c p ) &lt; k ? K 2 0, otherwise<label>(11)</label></formula><p>Usage of Malleable 2.5D Convolution Malleable 2.5D convolutions can be easily incorporated into CNNs by replacing standard 2D convolutions. The inputs of malleable 2.5D convolutions are standard feature maps, depth maps, and camera parameters. And the outputs of malleable 2.5D convolutions are the same with standard 2D convolutions.</p><p>In RGB-D semantic segmentation, utilizing pre-trained models is essential to attain good performance. When replacing standard convolutions with malleable 2.5D convolutions, we do not abandon the pre-trained weights of the original convolutions but adopt a simple parameter loading strategy to make use of them. We duplicate the pre-trained weights and load them into each kernel of the malleable 2.5D convolution. in finetuning time, the k kernels start from the same initialization and gradually learn to model geometric relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our method on two popular RGB-D scene parsing datasets: NYUDv2 <ref type="bibr" target="#b26">[27]</ref> and Cityscapes <ref type="bibr" target="#b6">[7]</ref>. These two datasets respectively contains indoor and outdoor scenes and the depth sources are different. Therefore, the depth ranges, object sizes and the quality of depth data are very different in them. Evaluating on these two datasets is challenging and can validate the robustness and generalization ability of RGB-D scene parsing methods. We will introduce these two datasets in detail in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Model Implementation DeepLabv3+ <ref type="bibr" target="#b3">[4]</ref> is a widely recognized state-of-theart method for semantic segmentation. We adopt ResNet-based DeepLabv3+ pre-trained on ImageNet <ref type="bibr" target="#b23">[24]</ref> as our baseline network. And for NYUDv2, we moreover adopt a multi-stage merging block inspired by previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref>. Details of the network structures will be illustrated in the supplementary material. To evaluate the our method, we replace the 3 ? 3 convolution with a malleable 2.5D convolution in the first residual unit in each stage of the ResNet.</p><p>Our implementation is based on PyTorch <ref type="bibr" target="#b21">[22]</ref>. Synchronized batch normalization are adopted for better batch statistics. By default, we use malleable 2.5D convolution with 3 kernels, and the parameter [a 0 , a 1 , a 2 , a 3 , a 4 ] are initialized as [?2, ?1, 0, 1, 2], t is initialized as 1, and b k -s are all initialized as 0.</p><p>Training Settings We use SGD optimizer with momentum to train our model. The momentum is fixed as 0.9 and the weight decay is set to 0.0001. We employ a "poly" learning rate policy where the initial learning rate is multiplied by (1 ? iter max iter ) power . The initial learning rate is set to 0.01 and the power is set to 0.9. We use batch size of 16 and train our model for 40k iterations for NYUDv2 and 60k iterations for Cityscapes. For data augmentation, we use random cropping, random horizontal flipping and random scaling with scale ? {0.75, 1, 1.25, 1.5, 1.75, 2}. Besides, we adopt the bootstrapped cross-entropy loss as in <ref type="bibr" target="#b31">[32]</ref> for Cityscapes experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>First of all, we compare our method with previous RGB-D convolutions to validate the benefits of learnable receptive fields along the depth-axis. The results are shown in <ref type="table">Table 2</ref>. We compare Depth-aware, 2.5D and malleable 2.5D convolution on both NYUDv2 and Cityscapes datasets. When implementing the  <ref type="bibr" target="#b30">[31]</ref> VGG-16 ?2 43.9 -CFN(RefineNet-152) <ref type="bibr" target="#b17">[18]</ref> ResNet-152 ?2 47.7 -2.5D <ref type="bibr" target="#b33">[34]</ref> ResNet-101 ?1 48.4 75.3 2.5D+HHA <ref type="bibr" target="#b33">[34]</ref> ResNet-101 ?2 49.1 75.9 RDF-101 <ref type="bibr" target="#b15">[16]</ref> ResNet-101 ?2 49.1 75.6 RDF-152 <ref type="bibr" target="#b15">[16]</ref> ResNet-152 ?2 50.1 76.0 Idempotent <ref type="bibr" target="#b34">[35]</ref> ResNet other two methods, we use the default parameters provided in their papers <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref> which are tuned on NYUDv2. All evaluations are conducted without multi-scale and flip testing tricks. For fairness, we compare malleable 2.5D convolution with other convolutions in the case that they have the same kernel number. And the single-kernel version of malleable 2.5D convolution's parameter [a 0 , a 1 , a 2 ] are initialized as [?1, 0, 1]. The results show that the malleable 2.5D convolution consistently outperforms other methods and baselines. It is worth noting that Depth-aware and 2.5D convolutions fail on Cityscapes. They decrease the performance even depth information is incorporated. However, our malleable 2.5D convolution still works well on Cityscapes with the same parameter initialization on NYUDv2. It adaptively learns the depth receptive field that fits the outdoor environment and improves segmentation performance. <ref type="figure">Fig. 5</ref> visualizes qualitative comparison results between the baseline and our method on NYUv2 test set. NYUDv2 provides high-quality depth data, and most RGB-D scene parsing methods take it as the main benchmark. Therefore, we then compare with more state-of-the-art RGB-D scene parsing methods on NYUDv2, shown in <ref type="table">Table 3</ref>. As other works do <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, we adopt test-time multi-scale and flip inference to obtain best performance. We evaluate models based on both ResNet-50 and ResNet-101 backbones. And we use malleable 2.5D convolutions with 3 kernels. Different from many works that adopt two-stream network design to process RGB+HHA inputs, we do not use HHA input and thus do not double the computational cost brought by backbones. The results show that even without doubling the backbone, our model achieves promising performance and outperforms other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>In this subsection, we conduct ablation studies on NYUDv2 dataset to validate the efficacy of our method. <ref type="table" target="#tab_4">Table 4</ref>, we conduct experiments that compare different numbers of convolutional kernels in malleable 2.5D convolutions. While the performance improves when the number increases from 1 to 3, the performance of the 5-kernel case drops slightly compared to the 3-kernel case. We suppose the reason is that 3 kernels are enough to model 3D relations between pixels, and in the 5-kernel case there are too few pixels assigned to the two outer kernels and they cannot be sufficiently trained. <ref type="table" target="#tab_5">Table 5</ref>, we evaluate the effects of introduced learnable parameters. We fix part of the introduced parameters {a k , t, b k } at the initialized values, and set rest of them learnable. The results demonstrate that when we fix all the parameters and thus fix the depth receptive fields, the performance is the worst. And the fixed malleable 2.5D convolution gets a very similar result to 2.5D convolution. When only the kernel center a k is learnable, the performance stays almost the same. And when both a k and t are learnable, the mIoU improves from 48.33% to 48.62% and pixel accuracy improves from 75.74% to 75.91%. We argue that this is because the temperature term T controls the sharpness of depth receptive fields, and therefore it is crucial to set both a k and t learnable for assigning pixels to different kernels more accurately. When the kernel rebalancing parameter b k is introduced, the performance moreover improves to the result of complete malleable 2.5D convolution. These results validate that the improvement is indeed brought by the introduced learnable parameters. Initialization of introduced parameters The parameter a k serves as the center of kernel k's receptive field along the depth-axis. It is the most direct factor that controls the learnt depth receptive field. In <ref type="table" target="#tab_8">Table 8</ref>, we evaluate the effects of different initialization of a k for 3-kernel malleable 2.5D convolution. When we use large values to initialize a k , the performance drops slightly. However, both results outperform the baseline and 2.5D convolution. This validates the effectiveness and the robustness of the learnable depth receptive field in our method. More experiments of different parameter initializations will be included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of kernels In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learnable parameters In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Malleable 2.5D Convolution in Different Layers</head><p>To reveal the effects of using malleable 2.5D convolution in different locations in the backbone network, we conduct a series of experiments. The results are shown in <ref type="table" target="#tab_7">Table 7</ref>. When using more malleable 2.5D convolution, the network gains more capability to handle 3D geometric information and achieves better segmentation performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel RGB-D convolution operator called malleable 2.5D convolution, which has learnable receptive fields along the depth-axis. By learning the receptive field along the depth-axis, malleable 2.5D convolution can learn to adapt to different environments without handcraft parameter adjusting and improve RGB-D scene parsing performance. And malleable 2.5D convolution can be easily incorporated into pre-trained standard CNNs. Our extensive experiments on NYUDv2 and Cityscapes validate the effectiveness and the generalization ability of our method.</p><p>NYUDv2 NYUDv2 <ref type="bibr" target="#b26">[27]</ref> is an indoor RGB-D semantic segmentation dataset. It contains 1449 RGB-D images with pixel-wise labels. And it provides depth maps captured by Kinect and the corresponding camera intrinsic parameters for all images. We follow the 40-class setting and the standard split which consists of 795 training images and 654 testing images.</p><p>Cityscapes Cityscapes <ref type="bibr" target="#b6">[7]</ref> is an urban scene understanding dataset that contains outdoor scene in different cities. The dataset has 5,000 stereo frames, each frame containing an 2048 ? 1024 RGB image, a disparity map, a set of camera parameters, and a fine-annotated 19-category ground truth label map. There are 2,979 images in training set, 500 images in validation set and 1,525 images in test set. We use camera parameters and disparity maps to calculate depth maps. The quality of depth data in this dataset is not as good as NYUDv2, and the scenes have wider ranges and more complicated structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">More experiments of the initialization of introduced parameters</head><p>In <ref type="table" target="#tab_8">Table 8</ref>, we compare more different initialization settings of the introduced parameters a k and t. When we change the initialization settings, the performance may slightly drop, but still outperforms the baseline and 2.5D convolution. This validates the effectiveness and the robustness of the learnable depth receptive field in our method. Small initialization values of a k seems to have relatively obvious harm on the performance. We suppose that it is because in this case the receptive fields of different kernels are largely overlapped at the initial state, and it brings difficulty for learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Images of Assigning Functions h k and g k</head><p>To give a clear illustration of the assigning functions h k and g k , we present several real-case images of h k and g k in <ref type="figure" target="#fig_5">Fig. 6</ref>. We draw the images of the initialization state of h k and g k , and we also draw the images of h k and g k in a model trained on NYUDv2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparisons of Depth Receptive Functions</head><p>In <ref type="figure">Fig. 7</ref> and <ref type="figure">Fig. 8</ref>, we compare depth receptive fields of depth-aware, 2.5D and malleable 2.5D convolution in NYUDv2 and Cityscapes. In NYUDv2, pixels are closer in 3D space than those in Cityscapes since they are respectively indoor   and outdoor scenes. Therefore, it is intuitive that convolutions should have larger depth receptive fields on Cityscapes than NYUDv2. From the figures we can see that malleable 2.5D convolution indeed learns wider depth receptive fields for Cityscapes, while depth-aware and 2.5D convolutions cannot automatically fit different environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Effects of Kernel Rebalancing</head><p>In <ref type="figure">Fig. 9</ref>, we present the effects of kernel rebalancing. We show the kernel rebalancing results of all four malleable 2.5D convolutions in the model trained on NYUDv2. As we know, in earlier stages, local geometric features play a more important role. and in later stages, the importance of capturing context increases. The rebalancing parameters in early stages only fix part of the imbalance problem and keep the two further kernels decayed compared to the center kernel, which makes the convolution sensitive to local depth changes. When comes to the later stages, the rebalancing parameters tend to balance the kernels well and therefore let the convolution able to handle long-distance relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Visualization of feature maps</head><p>In <ref type="figure">Fig. 10</ref>, we visualize the feature maps generated by different kernels of a malleable 2.5D convolution. We save the output feature maps of the malleable 2.5D convolution in res2 stage of a trained ResNet-101-based model, and select two channels to draw figures. Generally, the three kernels respectively handle pixels that are "in front of", "around the same depth with" and "behind" the center pixel of a local receptive field. From the feature maps, we can see that the three kernels indeed learn different relations and can activate accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Network Structures</head><p>In <ref type="figure">Fig. 11</ref>, we present the network structures we use in NYUDv2 and Cityscapes respectively. We adopt ResNet-based DeepLabv3+ as our baseline network. To evaluate the effect of our method, we replace the 3 ? 3 convolution with a malleable 2.5D convolution in the first residual unit in each stage of the ResNet. For the NYUDv2 dataset, we adopt a multi-stage merging block on the backbone network. And for Cityscapes, we keep the original DeepLabv3+ structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative Depth Difference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Res4</head><p>Res5 Res3 Res2 <ref type="figure">Fig. 8</ref>. Comparison of depth receptive field functions g k on Cityscapes where the depth d(ci) = 20m. We compare depth-aware, 2.5D and malleable 2.5D convolution at each stages of ResNet after training. Note that we scale the y-axis to see better. The overall scale does not affect output results because of batch normalizations Ratio (h) After rebalance (at res5) <ref type="figure">Fig. 9</ref>. The ratio of pixels assigned to each kernel, before and after rebalance. We count the sum of g k and s k ? g k for each kernel across the whole NYUDv2 dataset and calculate the ratio.  <ref type="figure">Fig. 10</ref>. Visualization of the feature maps generated by different kernels in a malleable 2.5D convolution. We draw 2 feature maps for each kernel and each input image. The feature maps of "kernel 1" are determined by what is in front of the center pixel of a local receptive field. The feature maps of "kernel 2" are determined by what is around the same depth with the center pixel of a local receptive field. The feature maps of "kernel 3" are determined by what is behind the center pixel of a local receptive field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Image of h k and g k when K = 3, [a0, a1, a2, a3, a4] = [?2, ?1, 0, 1, 2] and t = 1. Best view in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The ratio of pixels assigned to each kernel, before and after rebalance. The figure shows the malleable 2.5D convolution at res5 stage of a trained model. We count the sum of g k and s k ?g k for each kernel across the whole NYUDv2 dataset and calculate the ratio. More cases will be presented in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of depth receptive field functions g k . Best view in color. We compare the depth receptive fields at res5 stage after training. The first row shows a case in NYUDv2 where the depth d(ci) = 1m. The second row shows a case in Cityscapes where the depth d(ci) = 20m. Note that we scale the y-axis to see better. The overall scale does not affect output results because of batch normalizations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>g k at res5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Images of h k and g k . (a) and (b) are the initialization of h k and g k . And the rest subfigures are h k -s and g k -s of each malleable 2.5D convolution at different ResNet stages after training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the computational cost of different convolutions. The input size is 768 ? 768. "kernels" means the kernel number of used RGB-D convolutions</figDesc><table><row><cell>Method</cell><cell>kernels</cell><cell>FLOPs(G)</cell><cell>Params(M)</cell></row><row><cell>Baseline</cell><cell>1</cell><cell>215.673</cell><cell>59.468</cell></row><row><cell>Depth-aware[31]</cell><cell>1</cell><cell>215.675</cell><cell>59.468</cell></row><row><cell>Malleable 2.5D</cell><cell>1</cell><cell>215.679</cell><cell>59.468</cell></row><row><cell>2.5D[34]</cell><cell>3</cell><cell>234.701</cell><cell>65.734</cell></row><row><cell>Malleable 2.5D</cell><cell>3</cell><cell>234.711</cell><cell>65.734</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Comparison with other RGB-D convolutions on NYUDv2 and Cityscapes. The backbone model is ResNet-50. "kernels" means the kernel number of used RGB-D convolutions Comparison with state-of-the-art RGB-D scene parsing methods on NYUDv2. Multi-scale and flipping inference strategies are used when evaluating our method.</figDesc><table><row><cell>Method</cell><cell>kernels</cell><cell cols="4">NYUDv2 mIoU(%) pixel Acc(%) mIoU(%) pixel Acc(%) Cityscapes</cell></row><row><cell>Baseline</cell><cell>1</cell><cell>44.56</cell><cell>73.01</cell><cell>79.94</cell><cell>96.34</cell></row><row><cell>Depth-aware[31]</cell><cell>1</cell><cell>46.69</cell><cell>74.27</cell><cell>79.01</cell><cell>96.32</cell></row><row><cell>Malleable 2.5D</cell><cell>1</cell><cell>47.08</cell><cell>75.13</cell><cell>80.26</cell><cell>96.40</cell></row><row><cell>2.5D[34]</cell><cell>3</cell><cell>48.23</cell><cell>75.73</cell><cell>78.63</cell><cell>96.29</cell></row><row><cell>Malleable 2.5D</cell><cell>3</cell><cell>48.80</cell><cell>76.03</cell><cell>80.81</cell><cell>96.51</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Backbone</cell><cell>mIoU(%)</cell><cell>pixel Acc(%)</cell></row><row><cell>FCN+HHA[25]</cell><cell></cell><cell cols="2">VGG-16 ?2</cell><cell>34.0</cell><cell>65.4</cell></row><row><cell cols="2">Depth-aware+HHA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study for number of convolution kernels. The backbone is ResNet-50</figDesc><table><row><cell>Kernel numbers</cell><cell>mIoU(%)</cell><cell>pixel Acc(%)</cell></row><row><cell>1</cell><cell>47.08</cell><cell>75.13</cell></row><row><cell>3</cell><cell>48.80</cell><cell>76.03</cell></row><row><cell>5</cell><cell>48.17</cell><cell>75.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation study for introduced learnable parameters in malleable 2.5D convolution. The backbone model is ResNet-50</figDesc><table><row><cell>Method</cell><cell>Learnable params</cell><cell>mIoU(%)</cell><cell>pixel Acc(%)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>44.56</cell><cell>73.01</cell></row><row><cell>2.5D[34]</cell><cell>-</cell><cell>48.23</cell><cell>75.73</cell></row><row><cell></cell><cell>None</cell><cell>48.28</cell><cell>75.68</cell></row><row><cell>Malleable 2.5D</cell><cell>a k a k , t</cell><cell>48.33 48.62</cell><cell>75.74 75.91</cell></row><row><cell></cell><cell>a k , t, b k</cell><cell>48.80</cell><cell>76.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Results of different initialization of a k . The backbone model is ResNet-50</figDesc><table><row><cell>Method</cell><cell>Initialization</cell><cell>mIoU(%)</cell><cell>pixel Acc(%)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>44.56</cell><cell>73.01</cell></row><row><cell>2.5D[34]</cell><cell>-</cell><cell>48.23</cell><cell>75.73</cell></row><row><cell>Malleable 2.5D</cell><cell>[?4, ?2, 0, 2, 4] [?2, ?1, 0, 1, 2]</cell><cell>48.66 48.80</cell><cell>75.94 76.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Results of using malleable 2.5D convolutions in different layers.</figDesc><table><row><cell>"Replaced</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Results of different initialization of a k and t. The backbone model is ResNet-50</figDesc><table><row><cell>Method</cell><cell>a k</cell><cell>t</cell><cell>mIoU(%)</cell><cell>pixel Acc(%)</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell>44.56</cell><cell>73.01</cell></row><row><cell>2.5D[34]</cell><cell>-</cell><cell>-</cell><cell>48.23</cell><cell>75.73</cell></row><row><cell></cell><cell>[?4, ?2, 0, 2, 4]</cell><cell>1</cell><cell>48.66</cell><cell>75.94</cell></row><row><cell>Malleable 2.5D</cell><cell>[?2, ?1, 0, 1, 2]</cell><cell>1</cell><cell>48.80</cell><cell>76.03</cell></row><row><cell></cell><cell>[?1, ?0.5, 0, 0.5, 1]</cell><cell>1</cell><cell>48.42</cell><cell>75.78</cell></row><row><cell></cell><cell>[?2, ?1, 0, 1, 2]</cell><cell>0.5</cell><cell>48.69</cell><cell>75.81</cell></row><row><cell>Malleable 2.5D</cell><cell>[?2, ?1, 0, 1, 2]</cell><cell>1</cell><cell>48.80</cell><cell>76.03</cell></row><row><cell></cell><cell>[?2, ?1, 0, 1, 2]</cell><cell>2</cell><cell>48.74</cell><cell>75.83</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="8713" to="8724" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3d neighborhood convolution: Learning depthaware features for RGB-D and RGB semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Locality-sensitive deconvolution networks with gated fusion for RGB-D indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1475" to="1483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.350</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.350" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="345" to="360" />
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FuseNet: incorporating depth into semantic segmentation via fusion-based CNN architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV (1)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">10111</biblScope>
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. In: NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth-adaptive deep neural network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2478" to="2490" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="956" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">RDFNet: RGB-D multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LSTM-CF: unifying context modeling and fusion with lstms for RGB-D scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="541" to="557" />
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cascaded feature network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="1320" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">RefineNet: multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">3d graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="5209" to="5218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semantic segmentation via structured patch prediction, context CRF and guidance CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="5178" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (5)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7576</biblScope>
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mixconv: Mixed depthwise convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1907.09595</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Depth-aware CNN for RGB-D segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (11)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11215</biblScope>
			<biblScope unit="page" from="144" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<idno>abs/1604.04339</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">2.5 d convolution for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1410" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coupling two-stream rgb-d semantic segmentation network by idempotent mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1850" to="1854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.224</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.224" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2050" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Perspective-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="909" to="924" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">3d geometry-aware semantic labeling of outdoor street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="2343" to="2349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deformable convnets V2: more deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Comparison of depth receptive field functions g k on NYUDv2 where the depth d(ci) = 1m. We compare depth-aware, 2.5D and malleable 2.5D convolution at each stages of ResNet after training. Note that we scale the y-axis to see better</title>
	</analytic>
	<monogr>
		<title level="m">The overall scale does not affect output results because of batch normalizations</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
