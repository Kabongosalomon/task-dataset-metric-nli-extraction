<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Target Representations for Masked Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingbin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Target Representations for Masked Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Masked autoencoders have become popular training paradigms for self-supervised visual representation learning. These models randomly mask a portion of the input and reconstruct the masked portion according to the target representations. In this paper, we first show that a careful choice of the target representation is unnecessary for learning good representations, since different targets tend to derive similarly behaved models. Driven by this observation, we propose a multi-stage masked distillation pipeline and use a randomly initialized model as the teacher, enabling us to effectively train high-capacity models without any efforts to carefully design target representations. Interestingly, we further explore using teachers of larger capacity, obtaining distilled students with remarkable transferring ability. On different tasks of classification, transfer learning, object detection, and semantic segmentation, the proposed method to perform masked knowledge distillation with bootstrapped teachers (dBOT) outperforms previous self-supervised methods by nontrivial margins. We hope our findings, as well as the proposed method, could motivate people to rethink the roles of target representations in pre-training masked autoencoders. The code and pre-trained models are publicly available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Masked Image Modeling (MIM) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref> has recently become an active research topic in the field of visual representation learning and establishes strong performance for vision recognition tasks, e.g., image classification, object detection, and semantic segmentation, which also surpasses traditional supervised learning <ref type="bibr" target="#b33">[34]</ref> mechanism. To be specific, MIM randomly masks a portion of the input and then reconstructs the masked portion according to the transformed target, formulated as where " " means element-wise product; M is the patch mask; "x M " represents "unmasked patches" and vice versa; f ? (?) is the learnable network to be pre-trained; T is the transformation function generating the reconstructed target; M(?, ?) is the similarity measurement, e.g., l2distance <ref type="bibr" target="#b17">[18]</ref>. A masked image passed through the network f ? (x M ) to reconstruct the visual representation of the intact image with transformation T (x (1 ? M )).</p><formula xml:id="formula_0">min ? E x?D M(T (x (1 ? M )), f ? (x M )),<label>(1)</label></formula><p>A crucial problem of MIM is how to choose the reconstructed target, i.e., T (?) in Eq. <ref type="bibr" target="#b0">(1)</ref>. In this work, we paraphrase a term Masked Knowledge Distillation (MKD) to focus our discussion on a special case of MIM where target is generated by a parameterized network (teacher network), i.e., T (?) = h ? (?). Previous methods use disparate teacher networks to generate the reconstruction target. BEiT <ref type="bibr" target="#b2">[3]</ref> employs a pre-trained DALL-E <ref type="bibr" target="#b30">[31]</ref> as the teacher network. MaskFeat <ref type="bibr" target="#b36">[37]</ref> uses HOG <ref type="bibr" target="#b9">[10]</ref> and DINO <ref type="bibr" target="#b5">[6]</ref> features; MVP <ref type="bibr" target="#b37">[38]</ref> employs a multi-modality model, CLIP <ref type="bibr" target="#b29">[30]</ref>, which is pre-trained by rich image-text pairs. MAE <ref type="bibr" target="#b17">[18]</ref> uses image pixels as the target, which functions likewise to a randomly initialized teacher network, as demonstrated in Appendix B.1. iBOT <ref type="bibr" target="#b44">[45]</ref> and data2vec <ref type="bibr" target="#b1">[2]</ref> use the exponential moving average (EMA) strategy to update teacher's parameters ?. Though different methods differ in their architectural designs and optimization, the choice of the teacher network lies crucial for each method and calls for a systematic study.</p><p>The purpose of our work is to investigate whether a careful design of the teacher network for MKD matters. Such exploration is nontrivial given that different teacher networks contain different knowledge we endued into the teacher network, which may induce diverse behaviors for the student networks. To this end, we compare four teacher networks with different computation pipelines, i.e., DINO <ref type="bibr" target="#b5">[6]</ref> for contrastive learning, MAE <ref type="bibr" target="#b17">[18]</ref> for masked autoencoding, DeiT <ref type="bibr" target="#b33">[34]</ref> for supervised learning, and DALL-E <ref type="bibr" target="#b30">[31]</ref> for autoregressive generation. To our surprise, although the behaviors of the teacher networks are very different, the distilled student networks share similar characters after several stages of masked knowledge distillation: (i) the performance variance between student networks distilled from different teachers rapidly decreases.</p><p>(ii) the model weights and outputs across layers within the networks share similar properties.</p><p>Such observations indicate that the design of target representation is not essential for learning good visual representations when pre-trained with multi-stage, i.e., teacher networks do not matter with multi-stage distillation.. Exceptionally, we use a randomly initialized model as teacher to perform multi-stage masked knowledge distillation, and find that it performs as well as those initialized by pretrained models with the exactly same settings! Using a random model as teachers not only avoids an extra pre-training stage, but also alleviates the painstaking selection of the target representations.</p><p>Based on the above studies and observations, we naturally propose to perform masked knowledge distillation with bootstrapped teachers, short as dBOT . Specifically, masked knowledge distillation is performed repeatedly in multiple stages. At the end of each stage, we assign the student's weight to the teacher and re-initialize the student's weight to continue distillation. To reveal our method's potential to serve as a strong baseline for learning universal visual representations, we highlight the following results:</p><p>(i) With simple yet effective design that enables pretraining starting from randomly initialized teachers, dBOT achieves 84.5%, 86.6%, and 87.4% top-1 fine-tuning accuracy on ImageNet-1K <ref type="bibr" target="#b10">[11]</ref> with ViT-B/16, ViT-L/16, and ViT-H/14, respectively, significantly surpassing previous states of the art, MAE. Beyond that, dBOT achieves 52.7 and 56.0 AP box for object detection on COCO <ref type="bibr" target="#b23">[24]</ref> with ViT-B and ViT-L respectively. These results indicate that the choice of teacher networks is not important for learning a good representation in MKD.</p><p>(ii) dBOT is generalizable taking teachers of larger capacity to perform distillation. As shown in <ref type="table">Table 7</ref>, when distilled from bigger teachers, students can have boosted performances, especially in dense downstream tasks. When distilled from data-richer teachers, e.g., CLIP <ref type="bibr" target="#b29">[30]</ref>, the distilled student achieves 89.1% fine-tuning accuracy with ViT-H and 56.2 segmentation mIoU with ViT-L, setting new state-of-the-art results for self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Self-Supervised Visual Learning</head><p>Self-supervised learning is an active research topic recently. Early practices revolve around contrastive learning <ref type="bibr">[5-7, 17, 19]</ref> where the model output features of similar images are pulled together. With the development of Masked Language Modeling (MLM) in language pre-training <ref type="bibr" target="#b11">[12]</ref>, researchers also introduce the training strategy of masked reconstruction to visual pre-training. BEiT <ref type="bibr" target="#b2">[3]</ref> uses the DALL-E <ref type="bibr" target="#b30">[31]</ref> to encode an image patch as the target for model reconstruction. iBOT <ref type="bibr" target="#b44">[45]</ref> uses an on-line teacher shifting the target from offline to online to make the target semantic meaningful. In addition to using the token obtained from offline or online model as reconstruct target, MAE <ref type="bibr" target="#b17">[18]</ref>, SimMIM <ref type="bibr" target="#b40">[41]</ref>, and MaskFeat <ref type="bibr" target="#b36">[37]</ref> achieve good performance in masked-image reconstruction using low-level pixels or HOG <ref type="bibr" target="#b9">[10]</ref> features. Among them, MAE uses an asymmetric encoder-decoder structure greatly increasing the training efficiency. data2vec <ref type="bibr" target="#b1">[2]</ref> demonstrates good generalizations on three modalities (vision, speech and language) by reconstructing multiple neural network layer representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Knowledge Distillation</head><p>Knowledge distillation (KD) is widely employed in model knowledge compression <ref type="bibr" target="#b19">[20]</ref>, which improves the performance of the smaller student model by distilling the knowledge learned from a well-trained large teacher network. Further study on e.g. relational KD <ref type="bibr" target="#b28">[29]</ref>, contrastive KD <ref type="bibr" target="#b32">[33]</ref>, and latent feature KD <ref type="bibr" target="#b31">[32]</ref> is conducted to improve the performance of vanilla KD. Beyond its prominence in the field of supervised learning, KD recently cuts a figure in self-supervised learning. Concurrent work manages to adopt conventional feature distillation <ref type="bibr" target="#b38">[39]</ref> to match contrastive models with MIM-trained ones. Nevertheless, it shows negligible gains on MIM-trained models such as MAE. BEiT <ref type="bibr" target="#b2">[3]</ref>, MaskFeat <ref type="bibr" target="#b36">[37]</ref> and MVP <ref type="bibr" target="#b37">[38]</ref> could be seen as distilling knowledge from dVAE <ref type="bibr" target="#b30">[31]</ref>, HOG features <ref type="bibr" target="#b9">[10]</ref> and language-induced model CLIP <ref type="bibr" target="#b29">[30]</ref> within the discourse of MKD, respectively. Until now, there exists no work conferring a system-level study on the importance of how to choose adequate target representation or teacher networks to guide the learning of MKD. Beyond that, we propose using a randomly initialized model as the teacher and bootstraps the teacher for stages, demonstrating a superiority over other practices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Does h ? (?) Matter in MKD?</head><p>Given the general form of masked knowledge distillation as shown in Eq. (1), in this section, we aim to investigate whether the careful design of the target, i.e., teacher network h ? (?), matters. Specifically, we want to answer three questions as follows:</p><p>? Whether models distilled from different h ? (?) differ in terms of their transfer performances?</p><p>? Whether distilled models differ in terms of their weights and outputs?</p><p>? If h ? (?) does not matter, what matters more to close the gap between students distilled from different h ? (?)?</p><p>To answer these questions, we employ the standard masked autoencoder framework <ref type="bibr" target="#b17">[18]</ref>   <ref type="table">Table 1</ref>. The top-1 classification accuracy on ImageNet-1K, object detection AP-box on COCO with Cascade Mask R-CNN, and semantic segmentation mIoU on ADE20K with UperNet of dBOT using different models as the initialized teacher network. Note that all models are pre-trained on ImageNet-1K, including DALL-E, for a fair comparison. We perform distillation in each stage for 800 epochs. In the 1 st stage, we distill from initialized teacher to obtain a student. In the subsequent (i.e., 2 nd , 3 rd , etc.) stages, the obtained students are leveraged as bootstrapped teacher to distill a new student.</p><p>Common setup. The architectural settings strictly follow <ref type="bibr" target="#b17">[18]</ref>. For the teacher network, we use the vanilla ViT <ref type="bibr" target="#b12">[13]</ref> with intact input. For the student network with masked input, we use the asymmetric encoder-decoder structure. The student's output is further projected to a dimension same as that of teacher's embedding. During pre-training, we use Smooth L1 loss <ref type="bibr" target="#b13">[14]</ref> for the optimization of the student network, and the teacher network is kept fixed. Detailed settings are delayed to Appendix A.1. We pre-train models on ImageNet-1K <ref type="bibr" target="#b10">[11]</ref> and conduct evaluation under classification on ImageNet, object detection on COCO <ref type="bibr" target="#b23">[24]</ref>, and semantic segmentation on ADE20K <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary Study</head><p>We first investigate the effect of using networks initialized differently as teachers for masked knowledge distillation. Four canonical methods as pre-trained teachers are substantiated, each from a category distinguished based on their computation pipelines, i.e., DeiT <ref type="bibr" target="#b33">[34]</ref> for supervised learning, DINO <ref type="bibr" target="#b5">[6]</ref> for contrastive learning, DALL-E <ref type="bibr" target="#b30">[31]</ref> for autoregressive generation, and MAE <ref type="bibr" target="#b17">[18]</ref> for autoencoding. The results of initialized teacher at the 0 th stage and of its distilled student at the 1 st stage are shown in <ref type="table">Table 1</ref>.</p><p>Different h ? (?) lead to similarly performed students. After the first stage of masked knowledge distillation, the student consistently outperforms teacher as shown in <ref type="table">Table 1</ref>, yielding 1.8%, 1.0%, 2.4%, and 0.7% performance gains for four different h ? (?) respectively, demonstrating the effectiveness of masked knowledge distillation for visual representation learning. Although the performance order of different h ? (?) is reserved after the first stage of distillation, the students distilled from different h ? (?) have closer downstream performances compared to the original h ? (?). Take MAE and DALL-E being the initialized teachers as an example, the performance range in classification, drops from 2.5% (83.6% vs. 81.1%) to 0.8% (84.3% vs. 83.5%), indicating that the performance gap is narrowed after the first stage. The conclusion holds true for experiments on object detection and semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Distillation with Multiple Stages</head><p>Given the observations that better teacher generally induces better outperforming student, we are motivated to use the trained student as teacher to train new student repeatedly and study whether similar trend endures. If so, we would like to seek in what stage the performances saturate for different downstream tasks, as well as the discrepancy among the results incurred by different initialized teachers.</p><p>h ? (?) does not matter with multi-stage distillation. The performance gain is valid but decreases with multi-stage and eventually vanishes. Take MAE being the initialized teacher as an example, students outperform teachers by +0.7%, +0.1%, -0.1% for classification, +2.3, -0.2, -0.2 points for object detection, and +1.5, +0.8, -0.6 points, for semantic segmentation, from the 0 th to the 3 rd stage. Other teachers and downstream tasks share the same conclusion. Moreover, the performance gaps of students learned from different teachers decreases, especially after multi-stage, as shown by the performance variance at different stages in the last row of <ref type="table">Table 1</ref>. Take classification tasks for an instance, the variance decreases along with the training stage, i.e., 2.24, 0.37, 0.07, 0.04, which reveals that the choice of h ? (?) exerts little influence on the downstream performance. See <ref type="table">Table 1</ref> for results of more downstream tasks. To demonstrate models' differences in terms of weights and outputs, we conduct a property analysis in Sec. 7.</p><p>A random h ? (?) works surprisingly well. Since the choice of h ? (?) does not matter, an intuitive experiment is to see what will happen when we employ a random teacher, in which the parameters is randomly initialized at the 0 th stage. To our surprise, using a random teacher achieves performances comparably with other pre-trained teachers. Compared to a randomly initialized models, distilled students with multiple stages achieve 6.1%, 20.4, and 21.3 performance gain on classification, object detection and se- : Exponential moving average is applied to correlate the parameters of the student and teacher networks, constructing an online teacher. (c): dBOT uses a multi-stage distillation pipeline, i.e., the parameters of the teacher network are frozen except at breakpoints, where we assign student parameters to the teacher and re-initialize the student network. mantic segmentation respectively. Empirically, object detection and semantic segmentation requires one more stage to saturate compared to classification. The saturated results are on par with those induced by pre-trained teachers, which enables us to train a state-of-the-art model more efficiently, without the need of an extra pre-training stage for the initialized teacher (e.g., contrastive learning as DINO).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MKD with Bootstrapped Teachers</head><p>The study in Sec. 3 motivates us to propose a multi-stage distillation pipeline for pre-training. The entire pre-training undergoes multiple stages split by breakpoints. For each stage, we fix the teacher network to obtain a stable visual representation, guiding the learning of the student network. The pre-trained model is then used as a stronger teacher and distill its knowledge to a new subsequent student, providing richer visual representations. We re-initialize the student network at each breakpoint. The above process repeats itself -the teachers keep bootstrapped from the students, until a performance saturation on downstream tasks is observed. Hence, our strategy is to perform distillation with bootstrapped teachers. We illustrate our framework in <ref type="figure" target="#fig_0">Fig. 1c</ref> and the conceptual relations with the other two paradigms in <ref type="figure" target="#fig_0">Fig. 1</ref>. By noting m as the momentum which indicates how fast the teacher's parameters ? t is updated from student's parameters ? s , i.e., ? t = m ? ? t + (1 ? m) ? ? s , we present the following discussions.</p><p>Relations with previous methods. One group of works leverages pre-trained teacher as in <ref type="figure" target="#fig_0">Fig. 1a</ref>, i.e., BEiT <ref type="bibr" target="#b2">[3]</ref>, MaskFeat [37] * . The teacher requires an extra stage of pre-training and is kept fixed with m = 1. Ideally, pretrained teachers bear additional knowledge which is prone to be more semantic meaningful, prompting student's learning. Nonetheless, the pre-training of these teachers entail * The HOG feature adopted in MaskFeat is also pre-designed, so here we use pre-trained teacher for simple description. a completely different computation pipeline <ref type="bibr" target="#b36">[37]</ref> and often additional data <ref type="bibr" target="#b37">[38]</ref>, complicating its practical use. Another group as in <ref type="figure" target="#fig_0">Fig. 1b</ref> works with random teacher in dispense with pre-trained ones. Starting from randomness, the teachers in iBOT <ref type="bibr" target="#b44">[45]</ref> and data2vec <ref type="bibr" target="#b1">[2]</ref>, however, are bootstrapped from the student typically with m ? (0, 1), e.g., 0.9998 as in <ref type="bibr" target="#b1">[2]</ref>. Although bootstrap induces improving quality of the teacher's representation, the pipeline is plagued by its optimization instability and sensitivity towards hyper-parameters. We note that MAE uses identity mapping of pixels as the target, which is observed to function similarly as a fixed random teacher with m = 1, as shown in Appendix B.1. Despite its simplicity, such practice eludes synergy between the teacher and the student. Comparatively, dBOT suits both random teacher (our default as in Secs. 5 and 6.1) and pre-trained teacher (Sec. 6.2), and is with m = 0 for every breakpoint and m = 1 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pre-training</head><p>Architecture. We use different capacity Vision Transformers <ref type="bibr" target="#b12">[13]</ref>, i.e., ViT-B/16, ViT-L/16, and ViT-H/14 for dBOT. The input image of size 224?224 is first divided by a linear projection head into non-overlapping patch tokens total of 196 for ViT-B and ViT-L, and 256 for ViT-H. We exactly follow the common setup demonstrated in Sec. 3, e.g., a student with asymmetric encoder-decoder architecture, a teacher with intact input, etc. On a specific note, the experiments to distill from teachers of large capacity (Sec. 6) are based on architectural designs with nuance. When distilling from bigger teachers, we use students and teachers of the different sizes, e.g., with ViT-H as the teacher to distill ViT-B. When distilling from data-richer teachers, e.g., CLIP <ref type="bibr" target="#b29">[30]</ref>, we find that not using the decoder for the student model as in BEiT lies crucial in getting advanced results. Detailed architectural designs are illustrated in Sec. <ref type="bibr" target="#b5">6</ref> Optimization. The learning rate is first linearly increased to the initial learning rate for the first 40 epochs and then cosine annealed to 0. The initial learning rate is set as 1.5e-4 ? batch size / 256, with batch size being 4096 for all models. We use the AdamW optimizer <ref type="bibr" target="#b25">[26]</ref> and Smooth L1 loss <ref type="bibr" target="#b13">[14]</ref> to optimize the parameters of student network. Stochastic drop rate are applied, 0.2 for ViT-B, 0.2 for ViT-L, and 0.3 for ViT-H. We use only center-crop and flipping for data augmentation. As shown in <ref type="table">Table 1</ref>, the performance of different downstream tasks saturates at different stages. By default, we pre-train all models for classification with 2 stages, for object detection and semantic segmentation with 3 stages. Different stages and training recipes are applied for experiments in Sec. 6, illustrated there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ImageNet Results</head><p>We primarily focus on the end-to-end fine-tuning performance and report the top-1 validation accuracy on ImageNet-1K <ref type="bibr" target="#b10">[11]</ref> dataset.</p><p>Evaluation setup. We sweep the base learning rate within a range with a batch size being 1024. We warm up the learning rate during the first 5 epochs to the initial learning rate and use a cosine schedule for the rest of the epochs. We average all the patch tokens output from the last transformer block and pass them into a linear projection head for classification. We fine-tune ViT-B for 100 epochs and ViT-L and ViT-H for 50 epochs in total.</p><p>Comparison with previous results. We report the finetuning results on ImageNet-1K, mainly focusing on the comparison of the self-supervised and supervised methods. Supervised denotes the results reported in the MAE. As shown in <ref type="table" target="#tab_2">Table 2</ref> different model capacities, demonstrating its scalability. We achieved top-1 evaluation accuracy of 84.5%, 86.6%, and 87.4% with ViT-B, ViT-L, and ViT-H, yielding gains of 0.9%, 0.7%, and 0.5% compared to MAE. When fine-tuned with an image size of 448, dBOT further achieves an accuracy of 88.0%. The results of dBOT? using CLIP as teachers are listed to showcase an upper-bound, the details of which are delayed to Sec. 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Downstream Tasks</head><p>To further demonstrate the effectiveness, we consider dense prediction tasks: object detection, semantic segmentation, instance segmentation, as well as classification tasks that transfer to smaller datasets.</p><p>Objection detection and instance segmentation. We consider Cascade Mask R-CNN <ref type="bibr" target="#b3">[4]</ref> as the task head for object detection and instance segmentation with ViT-B and ViT-L on COCO <ref type="bibr" target="#b23">[24]</ref>. We report AP box and AP mask for object detection and instance segmentation respectively. dBOT outperforms the previous self-supervised and supervised methods by a large margin, setting a new state-of-the-art result with both ViT-B and ViT-L. With ViT-B, dBOT achieves a AP box of 52.7 and a AP mask of 45.7, outperforming the supervised baseline pre-training by 2.9 and 2.5 points, respectively. With ViT-L, such improvement is more prominent with 4.8 and 3.6 points respectively, showing the high scalability of dBOT for model capacity in downstream dense prediction tasks. Further improvements exist when distilling from bigger (dBOT ? ) and data-richer (dBOT?) teachers, the details of which are delayed to Secs. 6.1 and 6.2.</p><p>Semantic segmentation. We adapt UperNet <ref type="bibr" target="#b39">[40]</ref> as the task head for semantic segmentation with ViT-B and ViT-L on ADE20K <ref type="bibr" target="#b43">[44]</ref>. We report the mIoU and mAcc for semantic segmentation, and the results are demonstrated in <ref type="table">Table 4</ref>. We achieve the best performances on semantic seg-method mIoU mAcc ViT-B ViT-L ViT-B ViT-L supervised <ref type="bibr" target="#b17">[18]</ref> 47.4 49.9 --iBOT <ref type="bibr" target="#b44">[45]</ref> 48  mentation compared to previous self-supervised methods by nontrivial margin. dBOT improves mIoU from 47.4 to 49.5 with ViT-B, and 49.9 to 54.5 with ViT-L, compared to the supervised baseline. We delay experiments with largecapacity teachers to Sec. 6.</p><p>Transfer learning. We study transfer learning performance by fine-tuning the pre-trained models on smaller datasets, including CIFAR10 [23] (Cif 10 ), CIFAR100 <ref type="bibr" target="#b22">[23]</ref> (Cif 100 ), iNaturalist18 <ref type="bibr" target="#b34">[35]</ref> (iNa 18 ), iNaturalist19 <ref type="bibr" target="#b34">[35]</ref> (iNa <ref type="bibr" target="#b18">19</ref> ), Flowers <ref type="bibr" target="#b27">[28]</ref> (Flwrs), and Cars <ref type="bibr" target="#b21">[22]</ref>. The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. dBOT achieves comparable, if not better, performances compared to previous best methods. Specifically, the improvement is significant on relatively larger datasets like iNaturalist18 and iNaturalist19, with 4.7% and 3.3% respectively compared to the supervised baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>Stage split number. We study stage number by splitting a total training epochs of 1600 into varying distillation stages, from 0 to 2. Results are shown in <ref type="table">Table 6a</ref>. 2-stage distillation works the best (for classification), while all splitting strategies obtain a top-1 accuracy higher than 83.6%, indicating its generalizability.</p><p>Epoch for each stage. Momentum update. We use in dBOT a multi-stage distillation pipeline, which is to distill from a momentum encoder with m being 0 for every breakpoint and 1 otherwise. We further investigate into other momentum update strategies commonly used in self-supervised learning. Results are shown in <ref type="table">Table 6c</ref>. The vanilla strategy works the best.</p><p>Target normalization. We study whether patch tokens obtained by the self-attention blocks to be used as target representation should be passed through the Layer Normalization <ref type="bibr" target="#b0">[1]</ref> layer <ref type="bibr">[LN]</ref>. The accuracy of models after 2-stage distillation is shown in <ref type="table">Table 6d</ref>. Without passing through [LN], the patch tokens directly obtained from the transformer block make them less suitable as target representations to guide students' learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Student initialization.</head><p>We study whether the weight of students should remain when entering the next stage of distillation. Specifically, we either keep the student's weight unchanged or to re-initialize the student at each breakpoint. As shown in <ref type="table">Table 6e</ref>, re-initializing the student's weight works the best.</p><p>Mask ratio. <ref type="table">Table 6f</ref> shows the influence of the mask ratio on end-to-end fine-tuning. The optimal mask ratio for dBOT is 75%, the same as that in MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">MKD with Teachers of Larger Capacity</head><p>In this section, we perform distillation with teachers of larger capacity, i.e., larger model capacity in Sec. 6.1 and larger data source in Sec. 6.2, showcasing the potential of masked knowledge distillation in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Distill from Bigger Teachers</head><p>Inspired by canonical practices in knowledge distillation <ref type="bibr" target="#b19">[20]</ref>, we use larger teachers to distill smaller students. Specifically, we attempt to use ViT-L/H as teacher networks to distill ViT-B, and ViT-H as the teacher network to distill ViT-L. All larger teachers are first distilled for 2 stages with the default setup. We resize the image to 196?196 for ViT-H/14 to keep the length of its output the same as that of ViT-B/L. While we do not find substantial gains on classification results, the results by distilling from ViT-H are significantly better for dense prediction tasks compared to the default setup, i.e., +0.8 points of AP box and +1.3 points of mIoU with ViT-B as the student. The performance gain in distilling ViT-L from ViT-H is diminished but still valid, i.e., +0.1 AP box and +0.7 mIoU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Distill from Data-Richer Teachers</head><p>We explore to use models pre-trained with richer data (i.e., CLIP <ref type="bibr" target="#b29">[30]</ref> with 400M Image-Text pairs) as the initialized teacher to seek a potential upper-bound of MKD. Compared to the default setup, there exist two major disparities of the pre-training recipes for models distilled from stronger teachers (e.g., DINO and CLIP): (i) For the architecture, we find that not using the asymmetric encoder-decoder architecture <ref type="bibr" target="#b17">[18]</ref> is optimal, as shown in ViT-H 87.4 --CLIP-L <ref type="bibr" target="#b29">[30]</ref> 88.5 (+1.1) -- metric architecture generates momentum for bootstrapping models similar to <ref type="bibr" target="#b16">[17]</ref>, which lies crucial for distillation with random teachers, it hurts the performance when distilling with stronger pre-trained teachers. We therefore leverage the vanilla architecture. (ii) For optimization, we find that 1-stage distillation with longer epochs works comparatively, if not better, compared to multi-stage distillation, as analysed in Appendix B.4. We therefore perform 1stage MKD. Such practice is summarized as recipe? detailed in <ref type="table">Table A1</ref>. Results are shown in <ref type="table" target="#tab_10">Table 9</ref>. ViT-B distilled from CLIP-B achieve an 85.7% top-1 accuracy and a 52.9 mIoU, surpassing all previous arts. With CLIP-L as the teacher, ViT-H achieves an 89.1% top-1 accuracy, setting a new state-of-the-art image recognition result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Property Analysis</head><p>We investigate the properties of models distilled from different teachers under certain criteria, analyzing models' weights and outputs. Further, training efficiency is briefly discussed with previous methods.     Averaged attention distance. We compute averaged attention distance <ref type="bibr" target="#b12">[13]</ref>, averaged over ImageNet-1K val set, for each attention head of different blocks to understand how local and global information flows into Transformers. Partial results are shown in <ref type="figure" target="#fig_2">Fig. 2</ref> with the full results in <ref type="figure" target="#fig_0">Fig. C1</ref>. Although the average attention distance of disparate initialized teachers varies greatly, their distilled students after multi-stage distillation exhibits similar behaviors, e.g., models' attention toward local or global contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Singular value decomposition.</head><p>We computed the percentage of top-k singular values <ref type="bibr" target="#b35">[36]</ref> of the embedding w.r.t each layer. The results are averaged over the ImageNet-1K val set. We showcase the results with k varying from 1 to 5. Partial results are shown in <ref type="figure" target="#fig_4">Fig. 3</ref> with the full results in <ref type="figure" target="#fig_2">Fig. C2</ref>. The higher the percentage, models' output over a image is less correlated, indicating larger redundancy of it spatial representations thus less suitability for compression. Intuitively, random models at the 0 th stage has the method data2vec <ref type="bibr" target="#b1">[2]</ref> BEiT <ref type="bibr" target="#b2">[3]</ref> MAE <ref type="bibr" target="#b17">[18]</ref> dBOT asym.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>enc-dec</head><p>ViT <ref type="table" target="#tab_2">-B  169  166  79  157 109  ViT-L  431  356  125  380 200  ViT-H  960  751  240  892 416   Table 11</ref>. Training time (s) per epoch for different self-supervised methods with ViT-B/16, ViT-L/16, and ViT-H/14. All entries are tested on the same setting, i.e., 32 NVIDIA A100-80G GPUs. largest percentage given that pixels are merely randomly projected. The student networks distilled from different initialized teachers exhibits similar behaviors.</p><p>Unsupervised object detection. We use unsupervised object localization to quantitatively evaluate the visual representation obtained by different models. We follow the evaluation practice proposed in <ref type="bibr" target="#b26">[27]</ref> with Correct Localization (CorLoc) on POC-VOC 2012 trainval sets, except that we conduct feature decomposition via SVD instead of Laplacian since we observe more stable behaviors with SVD.</p><p>Quantitative results are demonstrated in <ref type="table" target="#tab_11">Table 10</ref>. dBOT using different teachers achieves very similar results, with students consistently outperforming their teachers.</p><p>Training efficiency. We compute the training time per epoch for different methods in <ref type="table">Table 11</ref>. With an asymmetric encoder-decoder architecture as the default setup, dBOT performs slower than MAE, but much faster than data2vec and BEiT. Such advantage turns more significant with models of larger size. A vanilla architecture without the asymmetric design (colored gray), which lies crucial in distillation with data-richer teachers, yields a comparable pretraining efficiency with BEiT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>As a special case of MIM, we formulate MKD upon which an empirical investigation is conducted about the influence of different target representations on self-supervised masked autoencoders. The study concludes that it is not necessary to carefully choose the target representation to learn good visual representations if distillation is performed in multiple stages (i.e., with bootstrapped teachers). On one hand, instead of initializing teachers with pre-trained models, we resort to random ones for simple practice, achieving favorable results in a wide range of downstream tasks without an extra stage of pre-training. On the other hand, we apply bigger or data-richer teachers as general cases of MKD, seeking to pursue the frontier for the distilled models. Generalizable to various setups, dBOT sets new state-of-the-art results on image classification and semantic segmentation. We hope our study and method will provide timely insights for self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>A.1. Pre-Training Default setup. We show our default pre-training setup in the second colum of <ref type="table">Table A1</ref>. We use Xavier Uniform <ref type="bibr" target="#b14">[15]</ref> to initialize the Vision Transformer <ref type="bibr" target="#b12">[13]</ref>. Note that we use asymmetry stochastic drop path rate for students and teachers.</p><p>Setup for distillation from bigger teachers. We follow the default setup, except that we use a different setup for stages. We first train larger-size teachers for 2 stages (in all downstream tasks) and use those to distill new students for 1 stage (in all downstream tasks).</p><p>Setup for distillation from data-richer teachers. We use recipe? to distill from data-richer teachers as showcased in the third column of <ref type="table">Table A1</ref>. The recipe is specifically tailored for the CLIP <ref type="bibr" target="#b29">[30]</ref> model. Except that using a symmetric encoder-decoder architecture as in <ref type="bibr" target="#b2">[3]</ref> lies crucial for obtaining advanced results, other configuration can be tweaked around for other data-richer teachers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Classification</head><p>Default setup. The default end-to-end fine-tuning recipe is shown in the second column of <ref type="table" target="#tab_2">Table A2</ref>, following the common recipes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> of ViT tuning for self-supervised models. The same recipe is applied when distilling from bigger teachers.</p><p>Setup for distillation from data-richer teachers. We use recipe? to distill from data-richer teachers as showcased in the third column of <ref type="table">Table A1</ref>. We find that smaller learning rates and drop path suit better for such setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Object Detection and Instance Segmentation</head><p>We adopt the vanilla ViT with Cascade Mask R-CNN <ref type="bibr" target="#b3">[4]</ref> as the task head on COCO <ref type="bibr" target="#b23">[24]</ref> dataset for object detection and instance segmentation, following the common setup <ref type="bibr" target="#b44">[45]</ref>. The default recipe is shown in <ref type="table">Table A3</ref>. To cope with versatile image sizes, we add relative position embedding instead of interpolating the absolute position embedding obtained during pre-training. For a fair comparison, we applied the same setup and sweep the learning rate and stochastic drop path rate for different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Semantic Segmentation</head><p>We use vanilla ViT and UperNet <ref type="bibr" target="#b39">[40]</ref> as the task head on ADE20K <ref type="bibr" target="#b43">[44]</ref> dataset for semantic segmentation, following the common setup <ref type="bibr" target="#b2">[3]</ref>. The default recipe is shown in <ref type="table">Table A4</ref>. To cope with versatile image sizes, we add relative position embedding instead of interpolating the absolute position embedding obtained during pre-training. For a fair comparison, we applied the same setup and sweep the learning rate and layer-wise decay for different methods. It can be derived that using the patch token obtained by a randomly initialized network as the target can achieve comparable results with a pixel as a target. A similar result proves that patch tokens obtained by a randomly initialized can also serve as a good reconstruction target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Object Detection with Mask R-CNN</head><p>Additionally, we use Mask R-CNN structure with FPN for object detection and instance segmentation on COCO datasets. The results are shown in <ref type="table" target="#tab_5">Table B5</ref>. dBOT outperforms other methods by a large margin, which is similar to the results using Cascade Mask R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Semi-Supervised Learning.</head><p>To investigate the label efficiency of dBOT, we also show the semi-supervised results under different labeled  <ref type="table">Table B7</ref>. ImageNet-1K classification results of 1 stage masked knowledge distillation with different teachers. Total epochs are shown in the format of (stages?epochs per stage). denotes performance gaps between entries of 2?800 and 1?1600.</p><p>data availability in <ref type="table">Table B6</ref> . We focus on the comparison with self-supervised learning methods. The label-fraction sampling strategy follows <ref type="bibr" target="#b6">[7]</ref>. dBOT outperforms MAE by 1.7 and 1.4 points using 1% and 10% of the labels, respectively, showing a higher label efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. 1-Stage MKD</head><p>We use different models as teachers to distill student for one stage with longer epochs, i.e., 1600. Results are shown in <ref type="table">Table B7</ref>. Empirically, the performance gains for multi-stage MKD over 1-stage MKD decrease as teachers' fine-tuning performance increase. Stronger teachers, such as DINO and MAE, induce similarly performed students with 1-stage MKD (1?1600) compared to 2-stage MKD (2?800). Specifically, when using CLIP as the pre-trained teacher, the performance for 2-stage MKD is, to our surprise, 0.9% lower than that of 1-stage MKD. Understandably, although the fine-tuning result of the student after 1stage distillation is better than that of CLIP, the student is essentially trained on IN1K and may not contain faith-fully data information stored in the CLIP model. Therefore, strong teachers work well with 1-stage MKD, especially for models pre-trained on extra richer data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Averaged Attention Distance</head><p>Average attention distance for dBOT using DeiT, DINO, MAE and random as teachers are illustrated in <ref type="figure" target="#fig_0">Fig. C1</ref>. dBOT shifts the averaged attention distance to the same distribution from various teachers' results. Additionally, dBOT achieves more local attention than previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Singular Value Decomposition</head><p>Singular value decomposition for dBOT using DeiT, DINO, MAE and random as teachers are shown in <ref type="figure" target="#fig_2">Fig. C2</ref>. The same distribution is achieved by dBOT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Unsupervised Object Detection.</head><p>We first compute singular value decomposition for the patch feature obtained by the ViT-B last block. Then a sign operation is applied on the first eigenvector, obtaining a binary mask of an image. We then take the bounding box around the largest connected component, which is more like the foreground object instead of the background. Correct localization (CorLoc) is used to measure the results, evaluated on POC-VOC 2012 trainval sets. A box is considered to have correctly identified an object if it has more than 50% intersection-over-union with a ground truth bounding box.  <ref type="figure" target="#fig_0">Figure C1</ref>. Average attention distance of different heads w.r.t layer number of ViT-B with different distilling teachers and their corresponding student distilled for 2 stages. The first row showcases the teachers while the second showcases the 2 th stage distilled student. Models using different teachers achieve the same result. The distilled students obtain comparatively more local attention compared to the teachers.  <ref type="figure" target="#fig_2">Figure C2</ref>. Singular value decomposition of different layers of ViT-B with different distilling teachers and their corresponding student distilled for 2 stages. The first row showcases the teachers while the second showcases the 2 th stage distilled student. Models using different teachers achieve the same result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Conceptual comparison of three masked image modeling paradigms. The difference between the three paradigms is how the parameters of the teacher network are updated. (a): The parameters of the teacher network are frozen during the whole training process, constructing an offline teacher. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Average attention distance of different heads w.r.t layer number on IN1K with different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Top-k (varying from 1 to 5) singular value percentage w.r.t layer number on IN1K with different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>th 1 st 2 nd 3 rd 0 th 1 st 2 nd 3 rd 4 th 0 th 1 st 2 nd 3 rd 4 th</figDesc><table><row><cell>computation</cell><cell>initialized</cell><cell>classification</cell><cell>object detetion</cell><cell></cell><cell>semantic segmentation</cell></row><row><cell cols="4">pipeline 0 Supervised teacher DeiT [34] 81.8 83.6 84.3 84.3 49.1 50.5 52.5 52.4</cell><cell>-</cell><cell>46.4 49.2 50.4 49.9</cell><cell>-</cell></row><row><cell>Contrastive</cell><cell>DINO [6]</cell><cell cols="2">83.2 84.2 84.5 84.4 50.1 52.5 52.9 52.7</cell><cell>-</cell><cell>46.8 49.7 50.4 49.4</cell><cell>-</cell></row><row><cell cols="4">Autoregressive DALL-E [31] 81.1 83.5 84.4 84.3 31.9 51.0 52.7 52.5</cell><cell>-</cell><cell>31.9 47.4 49.6 49.3</cell><cell>-</cell></row><row><cell cols="2">Autoencoding MAE [18]</cell><cell cols="2">83.6 84.3 84.4 84.3 50.6 52.9 52.7 52.5</cell><cell>-</cell><cell>48.1 49.6 50.4 49.8</cell><cell>-</cell></row><row><cell>-</cell><cell>random</cell><cell cols="5">77.3 83.4 84.5 84.3 29.2 49.6 52.4 52.7 52.4 25.7 47.0 49.1 49.5 49.5</cell></row><row><cell cols="2">performance variance</cell><cell cols="2">2.24 0.37 0.07 0.04 9.54 1.23 0.17 0.12</cell><cell>-</cell><cell>9.19 1.15 0.54 0.23</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">to give a system-level study,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>introduced next.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison result of the previous methods on ImageNet-1K. We evaluate by the end-to-end fine-tuning protocol. All results are based on an image size of 224, except for ViT-H with an extra result with 448 image size. We perform distillation in each stage for 800 epochs and with 2 stages (our default) in total.?: We pre-train with 1 stage for 1600 epochs with CLIP as the initialized teacher. The best results for each model are bolded.</figDesc><table><row><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Transfer classification accuracy on various datasets. We report the results of ViT-B. Sup. denotes the supervised baseline. The average results (avg.) are shown in the rightmost column.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6b</head><label>6b</label><figDesc>With the 2 nd stage distilling for 800 epochs, longer epochs for the 1 st stage induces 0.2% improvement (84.3% vs. 84.5%). With the 1 st stage distilling for 800 epochs, 800 epochs are enough for the 2 nd stage since 1200 epochs incur no gain.</figDesc><table /><note>studies proper epochs needed for each stage in a 2-stage distillation pipeline.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .Table 8 .</head><label>68</label><figDesc>Ablation study with ViT-B/16 on ImageNet-1K validation set. We report with the end-to-end fine-tuning top-1 accuracy (%). Ablation study is conducted with randomly initialized teachers. We note that models distilled from the pre-trained teachers generally share similar trends. Default settings are marked in gray . vanilla denotes m being 0 at breakpoint and 1 otherwise. cosine(a,b) denotes m is cosine annealed from value a to b. Image classification on IN1K with DINO and CLIP as initialized teachers, as well as random ones. Students with DINO and CLIP as teachers are distilled for 1 stage.</figDesc><table><row><cell cols="3">teacher student</cell><cell>cls.</cell><cell>det.</cell><cell>seg.</cell></row><row><cell>ViT-B</cell><cell></cell><cell></cell><cell>84.5</cell><cell>52.7</cell><cell>49.5</cell></row><row><cell>ViT-L</cell><cell cols="2">ViT-B</cell><cell cols="2">84.6 (+0.1) 53.1 (+0.4) 50.1 (+0.6)</cell></row><row><cell>ViT-H</cell><cell></cell><cell></cell><cell cols="2">84.6 (+0.1) 53.5 (+0.8) 50.8 (+1.3)</cell></row><row><cell cols="3">ViT-L ViT-L ViT-H</cell><cell cols="2">86.6 86.8 (+0.2) 56.1 (+0.1) 55.2 (+0.7) 56.0 54.5</cell></row><row><cell cols="5">Table 7. Results of classification (cls.) on IN1K, object detection</cell></row><row><cell cols="5">(det.) on COCO, and semantic segmentation (seg.) on ADE20K.</cell></row><row><cell cols="5">For same-size teachers (colored gray), students are pre-trained</cell></row><row><cell cols="5">with default settings. For bigger teachers, students are pre-trained</cell></row><row><cell cols="5">for 1 stage from 2-stage distilled teachers.</cell></row><row><cell cols="2">initialized teacher</cell><cell></cell><cell cols="2">pre-training data</cell><cell>asym. enc-dec</cell><cell>acc</cell></row><row><cell>random</cell><cell></cell><cell></cell><cell>IN1K</cell><cell>84.5</cell></row><row><cell>random</cell><cell></cell><cell></cell><cell>IN1K</cell><cell>83.8</cell></row><row><cell cols="2">DINO [6]</cell><cell></cell><cell>IN1K</cell><cell>84.4</cell></row><row><cell cols="2">DINO [6]</cell><cell></cell><cell>IN1K</cell><cell>84.8</cell></row><row><cell cols="2">CLIP [30]</cell><cell cols="3">IN1K + 400M ITp.</cell><cell>84.9</cell></row><row><cell cols="2">CLIP [30]</cell><cell cols="3">IN1K + 400M ITp.</cell><cell>85.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>While an asym-</figDesc><table><row><cell>initialized teacher</cell><cell>student</cell><cell>cls.</cell><cell>det.</cell><cell>seg.</cell></row><row><cell>random CLIP-B [30]</cell><cell>ViT-B</cell><cell cols="3">84.5 85.7 (+1.2) 53.6 (+0.9) 52.9 (+3.4) 52.7 49.5</cell></row><row><cell>random CLIP-L [30]</cell><cell>ViT-L</cell><cell cols="3">86.6 87.8 (+1.2) 56.8 (+0.8) 56.2 (+1.7) 56.0 54.5</cell></row><row><cell>random</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Results of classification (cls.) on IN1K, object detection (det.) on COCO, and semantic segmentation (seg.) on ADE20K with CLIP [30] as the teacher. Students are distilled for 1 stage. The det. results with CLIP as teachers are with absolute position embedding.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>The results of unsupervised object detection on Pascal VOC 2012 with CorLoc based on SVD decomposition.</figDesc><table><row><cell cols="6">model DeiT DINO DALL-E MAE random</cell></row><row><cell>0 th</cell><cell>13.8</cell><cell>36.1</cell><cell>36.5</cell><cell>36.6</cell><cell>23.6</cell></row><row><cell>2 nd</cell><cell>36.6</cell><cell>36.6</cell><cell>36.6</cell><cell>36.6</cell><cell>36.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A1 .Table A2 .Table A3 .Table A4 .</head><label>A1A2A3A4</label><figDesc>Pre-training setup. recipe? is the pre-training recipe for dBOT? . cos. denotes cosine distance. c., d., and s. denotes downstream tasks of classification, object detection, and semantic segmentation respectively. drop path is for the students. End-to-end fine-tuning setup. recipe? is the pretraining recipe for dBOT? .B. Additional ExperimentsB.1. Pixels vs. Random Mapping of Pixels MAE performs masked image modeling using the image pixel as the reconstruction target. We directly alter the target to patch tokens obtained from the image fed into a randomly initialized network. We select two patch tokens as the reconstruction target, one is the token obtained using the last transformer block, and the other is the token ob-Object detection and instance segmentation setup. Semantic segmentation setup.</figDesc><table><row><cell></cell><cell>config</cell><cell></cell><cell>value</cell><cell>config</cell><cell>default</cell><cell>recipe?</cell></row><row><cell cols="2">optimizer</cell><cell cols="2">AdamW [26]</cell><cell>optimizer</cell><cell cols="2">AdamW [26]</cell></row><row><cell cols="2">optim. momentum</cell><cell cols="2">? 1 , ? 2 = 0.9, 0.999</cell><cell>optim. momentum ? 1</cell><cell>0.9</cell></row><row><cell cols="2">peak learning rate</cell><cell></cell><cell>1e-4</cell><cell>optim. momentum ? 2</cell><cell>0.95</cell><cell>0.98</cell></row><row><cell cols="2">batch size</cell><cell></cell><cell>16</cell><cell>loss</cell><cell>Smooth L1</cell><cell>negative cos.</cell></row><row><cell cols="2">layer-wise decay</cell><cell></cell><cell>0.75</cell><cell>peak learning rate</cell><cell>2.4e-3</cell><cell>3e-3</cell></row><row><cell cols="2">weight decay</cell><cell></cell><cell>0.05</cell><cell>learning rate schedule</cell><cell cols="2">cosine decay [25]</cell></row><row><cell cols="2">learning schedule</cell><cell></cell><cell>step</cell><cell>batch size</cell><cell cols="2">4096</cell></row><row><cell></cell><cell>epochs</cell><cell></cell><cell>12</cell><cell>weight decay</cell><cell>0.05</cell></row><row><cell cols="2">step epochs</cell><cell></cell><cell>8, 11</cell><cell>stages</cell><cell>2 (c.), 3 (d./s.)</cell><cell>1</cell></row><row><cell cols="2">drop path [21]</cell><cell></cell><cell>0.2</cell><cell>epochs per stage</cell><cell>800</cell><cell>1600</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>warmup epochs [16]</cell><cell>40</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>augmentation</cell><cell cols="2">RandomResizedCrop</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>aug. input scale</cell><cell>(0.2, 1)</cell><cell>(0.4, 1)</cell></row><row><cell></cell><cell>config</cell><cell></cell><cell>value</cell><cell>asym. enc-dec [18]</cell><cell></cell></row><row><cell cols="2">optimizer</cell><cell cols="2">AdamW [26]</cell><cell>drop path [21]</cell><cell cols="2">0.2 (B/L), 0.3 (H) 0.1 (B/L/H)</cell></row><row><cell cols="2">optim. momentum peak learning rate</cell><cell cols="2">? 1 , ? 2 = 0.9, 0.999 {0.3,0.5,0.8,1,3}e-4</cell><cell>target w/ [LN] mask ratio</cell><cell>0.75</cell><cell>0.4</cell></row><row><cell cols="2">batch size</cell><cell></cell><cell>16</cell><cell></cell><cell></cell></row><row><cell cols="2">layer-wise decay</cell><cell cols="2">{0.65,0.75,0.85.0.95}</cell><cell></cell><cell></cell></row><row><cell cols="2">weight decay</cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell></row><row><cell cols="2">learning schedule</cell><cell></cell><cell>cosine</cell><cell></cell><cell></cell></row><row><cell></cell><cell>steps</cell><cell></cell><cell>16000</cell><cell></cell><cell></cell></row><row><cell cols="2">warmup steps</cell><cell></cell><cell>1500</cell><cell></cell><cell></cell></row><row><cell cols="2">drop path [21]</cell><cell cols="2">0.1(B), 0.2(L)</cell><cell>config</cell><cell>default</cell><cell>recipe?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>optimizer</cell><cell cols="2">AdamW [26]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">peak learning rate {0.8,1.2,1.6,2}e-3</cell><cell>{1,2,3,4}e-4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>weight decay</cell><cell>0.05</cell></row><row><cell cols="4">tained using linear projection, i.e., without any transformer</cell><cell>optim. momentum layer-wise decay</cell><cell cols="2">? 1 , ? 2 = 0.9, 0.999 0.75</cell></row><row><cell cols="4">block. After 400 epoch pre-training of ViT-B, the top-1 ac-</cell><cell>batch size</cell><cell>1024</cell></row><row><cell cols="4">curacy of the model on ImageNet-1K obtained by the three</cell><cell>learning schedule</cell><cell cols="2">cosine decay</cell></row><row><cell cols="3">different target is shown below.</cell><cell></cell><cell>warmup epochs</cell><cell>5</cell></row><row><cell>epoch</cell><cell>pixel</cell><cell>0 th block</cell><cell>12 th block</cell><cell>epochs</cell><cell cols="2">100 (B), 50 (L/H)</cell></row><row><cell>400</cell><cell>83.3</cell><cell>83.2</cell><cell>83.2</cell><cell>augmentation</cell><cell cols="2">RandAug (9, 0.5) [9]</cell></row><row><cell>1600</cell><cell>83.6</cell><cell>83.6</cell><cell>83.6</cell><cell>label smoothing</cell><cell>0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>mixup [43]</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>cutmix [42]</cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">drop path [21] 0.2 (B/L), 0.3 (H) 0.1 (B), 0.2 (L), 0.3 (H)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Table B5. Object detection and instance segmentation results on COCO using Mask R-CNN. We report the result both with ViT-B and ViT-H. All results are based on our implementation with official released pre-trained model.</figDesc><table><row><cell>method</cell><cell></cell><cell cols="2">AP box ViT-B ViT-L</cell><cell cols="3">AP mask ViT-B ViT-L</cell></row><row><cell>supervised</cell><cell></cell><cell>47.9</cell><cell>49.3</cell><cell cols="2">42.9</cell><cell>43.9</cell></row><row><cell>iBOT [45]</cell><cell></cell><cell>48.6</cell><cell>50.6</cell><cell cols="2">43.1</cell><cell>44.7</cell></row><row><cell cols="2">data2vec [2]</cell><cell>41.1</cell><cell>46.1</cell><cell cols="2">37.0</cell><cell>41.0</cell></row><row><cell>MAE [18]</cell><cell></cell><cell>50.2</cell><cell>53.5</cell><cell cols="2">44.8</cell><cell>47.4</cell></row><row><cell>dBOT</cell><cell></cell><cell>51.4</cell><cell>54.0</cell><cell cols="2">45.8</cell><cell>48.0</cell></row><row><cell>method</cell><cell></cell><cell>arch</cell><cell></cell><cell>1%</cell><cell cols="2">10%</cell></row><row><cell cols="2">data2vec [2]</cell><cell>ViT-B</cell><cell></cell><cell>48.7</cell><cell cols="2">71.2</cell></row><row><cell>MAE [18]</cell><cell></cell><cell>ViT-B</cell><cell></cell><cell>53.1</cell><cell cols="2">73.1</cell></row><row><cell>dBOT</cell><cell></cell><cell>ViT-B</cell><cell></cell><cell>54.8</cell><cell cols="2">74.5</cell></row><row><cell cols="7">Table B6. Semi-supervised learning on ImageNet-1K with dif-</cell></row><row><cell cols="7">ferent self-supervised models. 1% and 10% represents the label</cell></row><row><cell>fraction.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pre-training</cell><cell>ran-</cell><cell>DALL-E</cell><cell>DeiT</cell><cell>DINO</cell><cell>MAE</cell><cell>CLIP</cell></row><row><cell>epochs</cell><cell>dom</cell><cell>[31]</cell><cell>[34]</cell><cell>[6]</cell><cell>[18]</cell><cell>[30]</cell></row><row><cell>0</cell><cell>77.3</cell><cell>81.1</cell><cell cols="4">81.8 83.2 83.6 84.8</cell></row><row><cell>1?1600</cell><cell>83.6</cell><cell>83.6</cell><cell cols="4">83.6 84.4 84.4 84.9</cell></row><row><cell>2?800</cell><cell>84.5</cell><cell>84.4</cell><cell cols="4">84.3 84.5 84.4 84.0</cell></row><row><cell cols="3">+0.9 +0.8</cell><cell cols="4">+0.7 +0.1 +0.0 -0.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BEiT: BERT pretraining of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cascade R-CNN: high quality object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ed Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q Le</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAIS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeruIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVGIP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Singular value decomposition and principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis M</forename><surname>Rechtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A practical approach to microarray data analysis</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05175</idno>
		<title level="m">MVP: Multimodality-guided visual pre-training</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Contrastive learning rivals masked image modeling in fine-tuning via feature distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14141</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SimMIM: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">iBOT: Image bert pretraining with online tokenizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
