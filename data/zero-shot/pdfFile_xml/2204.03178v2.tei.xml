<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3M: Multi-loss, Multi-path and Multi-level Neural Networks for speech recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Feng</surname></persName>
							<email>shulinfeng@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Su</surname></persName>
							<email>dansu@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Bellevue</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3M: Multi-loss, Multi-path and Multi-level Neural Networks for speech recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Conformer</term>
					<term>mixture of experts</term>
					<term>Multi-loss</term>
					<term>Multi-level</term>
					<term>Multi-path</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Conformer based CTC/AED model has become a mainstream architecture for ASR. In this paper, based on our prior work, we identify and integrate several approaches to achieve further improvements for ASR tasks, which we denote as multi-loss, multi-path and multi-level, summarized as "3M" model. Specifically, multi-loss refers to the joint CTC/AED loss and multi-path denotes the Mixture-of-Experts(MoE) architecture which can effectively increase the model capacity without remarkably increasing computation cost. Multi-level means that we introduce auxiliary loss at multiple level of a deep model to help training. We evaluate our proposed method on the public WenetSpeech dataset and experimental results show that the proposed method provides 12.2% ? 17.6% relative CER improvement over the baseline model trained by Wenet toolkit. On our large scale dataset of 150k hours corpus, the 3M model has also shown obvious superiority over the baseline Conformer model. Code is publicly available at https://github.com/tencentailab/3m-asr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>End-to-End(E2E) automatic speech recognition(ASR) have gained large improvements in recent years. As a combination of convolution module and self-attention mechanism, Conformer <ref type="bibr" target="#b0">[1]</ref> can model local and global dependencies of an audio sequence and achieve a better performance, making it become a favored choice to train benchmarks for many E2E ASR toolkits(e.g. Espnet <ref type="bibr" target="#b1">[2]</ref>, Wenet <ref type="bibr" target="#b2">[3]</ref>). CTC/attention-based encoder-decoder, also known as CTC/AED, is a popular E2E ASR framework, which effectively utilizes advantages of both architectures to improve robustness and achieve faster convergence. Previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> have proved this multi-loss training can significantly improve the performance of E2E ASR systems.</p><p>Scaling up to a larger model has been an effective way towards a flexible and powerful E2E ASR system. It has shown that a large Conformer model(e.g. SpeechStew <ref type="bibr" target="#b7">[8]</ref>) can achieve state-of-the-art(SoTA) results across a wide variety of tasks. However, developing large models in real-world application is seriously hindered by the expensive computation cost of both training and inference time. Several techniques have been proposed to reduce the model complexity, such as knowledge distillation, low-rank decomposition, quantization and pruning, but these methods inevitably suffer from performance degradation.</p><p>Recently, mixture of experts (MoE) based approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> have been intensively investigated and applied in different * Equal contribution. tasks such as language modeling <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, image classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, and speech recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. MoE models can easily increase the model capacity by increasing the number of experts. With the introduction of the sparsely-gated mixtureof-experts layer <ref type="bibr" target="#b18">[19]</ref>, MoE models can dynamically route inputs to corresponding expert networks, which enables us to satisfy training and inference efficiency by having sub-network activated on per-example basis.</p><p>Based on the joint CTC/AED multi-loss training framework, we explore the MoE approach on the Conformer model, named Conformer-MoE, where the second Macaron-style halfstep feed-forward layer of each Conformer block is modified into the MoE layer. With sparsely-gated mechanism, the MoE layer will route their inputs to the top-1 expert with largest route probability, thus keeping the computational cost roughly constant. The Conformer-MoE activate different sub-networks when dealing with different input samples and we call this property as multi-path. Additionally, we further propose to integrate separate attention decoders at multiple intermediate level of encoder layers, which effectively accelerate convergence and get a better performance. The multi-level property, along with multipath and multi-loss, makes up our final 3M model The rest of the paper is organized as follows. Section 2 reviews the previous work of Conformer and SpeechMoE, and Section 3 presents our proposed method 3M. The experimental results are reported in Section 4. Finally, we conclude this paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous works</head><p>This section mainly describes previous work of Conformer and SpeechMoE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conformer</head><p>The Conformer is proposed by <ref type="bibr" target="#b0">[1]</ref>, and each Conformer block comprises two Feed Forward modules sandwiching the Multi-Head self-attention module and the Convolution module. Given the input x, we can get the output y of Conformer block:</p><formula xml:id="formula_0">x = x + 1 2 F F N (x)<label>(1)</label></formula><p>x =x + M HSAN (x) (2)  <ref type="figure">Figure 1</ref>: illustration for the architecture of 3M model, which is composed of a shared encoder with several 3M blocks, a CTC decoder, an Attention Decoder and an embedding network.</p><formula xml:id="formula_1">x = x + Conv( x)<label>(3)</label></formula><formula xml:id="formula_2">y = LN (x + 1 2 F F N (x))<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">SpeechMoE</head><p>The SpeechMoE is proposed in <ref type="bibr" target="#b16">[17]</ref>, which comprises multiple MoE layers, non-expert layers and a shared embedding network. Each MoE layer consists of n experts and a router. It takes the output of the previous layer and the shared embedding as input and routes each speech frame to the top-1 expert with the largest route probability. Let W l r , e c and o l?1 be the router weights of the l-th layer, shared embedding and the output of the previous layer, then the router probability can be defined as follows:</p><formula xml:id="formula_3">r l = W l r ? Concat(e c ; o l?1 )<label>(5)</label></formula><formula xml:id="formula_4">p l i = exp r l i n j=1 exp r l j<label>(6)</label></formula><p>Then, the selected expert's output is also gated by router probability to get the output of the MoE layer,</p><formula xml:id="formula_5">y l = p l i E l i<label>(7)</label></formula><p>Since only one expert is active in each layer, the Speech-MoE can keep the computational cost constant while scaling up to a very large model. To achieve better sparsity and balance among different experts, the sparsity L1 loss Ls and mean importance loss Lm are added into the loss function:</p><formula xml:id="formula_6">Ls = 1 k k j=1 pj 1 (8) Lm = n n i=1 ( 1 k k j=1 pij) 2<label>(9)</label></formula><p>wherepj stands for the unit-normalized router probability distribution of frame j, and k is the number of frames in this minibatch. pij stands for the router probability on expert i of frame j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">3M model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-loss</head><p>To date, CTC/AED is commonly used in E2E ASR, which effectively utilizes the advantages of both architectures in training and decoding. On one hand, the attention lacks left-to-right constraints, making it difficult to generate proper alignments in the case of noisy data and/or long input sequences. Augmenting the constrained CTC loss based on the forward-backward algorithm can greatly reduce the number of irreuglarly aligned utterances.</p><p>On the other hand, CTC imposes the conditional independence constraint that output predictions are independent, which is not true for ASR. Attention objective can help eliminate the conditional independence constraint and back propagate the context information to rectify the probability distribution output by CTC. When employing decoding, both attention-based scores and CTC scores can be combined in a rescoring/one-pass beam search to improve the performance. We regard the CTC/AED joint training as multi-loss property. As shown in <ref type="figure">Figure 1</ref>, the proposed model has a CTC branch and an attention branch with a shared encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-path</head><p>Here, we propose to apply MoE architecture on Conformer model. Specifically, as shown in <ref type="figure">Figure 1</ref>, we extend the second Macaron-style feed-forward module to the MoE layer, which consists of N FFN module and a router. Similar to SpeechMoE <ref type="bibr" target="#b16">[17]</ref>, each FFN module stands for one expert network and the router takes the shared embedding and the output from the previous module as input and routes each speech frame to the top-1 expert with the largest route probability. Expert outputs are also gated by the corresponding router probability. With this sparse-gated mechanism, the proposed model can activate different sub-networks for different samples, which is regarded as multi-path property. We have also attempted to modify both FFN modules in the Conformer block into MoE layers, but it fails to achieve a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-level</head><p>As we know, the lower layers of the Conformer encoder also have a low correlation with grapheme information. Generally, the input of the attention decoder is the top layer of the Conformer encoder. In order to make the lower layers of encoder have a earlier access to grapheme information from the decoder, we propose to integrate separate attention decoders at intermediate layers of the encoder, as shown in green box part of <ref type="figure">Figure  1</ref>. In our practices, two additional decoders are applied to intermediate layers at 1/3 depth and 2/3 depth of the encoder, which are only used for training with AED loss and ignored during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training objective</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">MoE training loss</head><p>We have introduced the mentioned Ls and Lm loss to make the router probabilities sparse and diverse. To provide enough distinction information for routers, the shared embedding network which converts low-level features to high-level embedding is necessary to help the router attain better selecting effect. The embedding training loss provides reliable embeddings for the routers. The training loss of MoE is defined as:</p><formula xml:id="formula_7">LMoE = ?Ls(x) + ?Lm(x) + ?Le(x; y)<label>(10)</label></formula><p>Where Ls and Lm are the mentioned sparsity L1 loss and mean importance loss, used to encourage sparsity and diversity of the model. The embedding loss Le is the CTC loss. ?, ?, and ? is the scale for Ls, Lm and Le respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Joint CTC/AED training loss</head><p>In this paper, the CTC loss and multi-level AED loss are combined in the training of the 3M model:</p><formula xml:id="formula_8">LJoint = ?Lc(x; y) + (1 ? ?) K j=1 La j (x; y)<label>(11)</label></formula><p>Among these items, Lc is the CTC loss. K is the number of level. La j is the AED loss for the transformer decoder of j-th level. ? are the interpolated weight for CTC loss and AED loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">Loss function</head><p>Given the input x, grapheme target y, the complete loss function of our method is defined as:</p><formula xml:id="formula_9">L(x; y) = LMoE + LJoint<label>(12)</label></formula><p>4. EXPERIMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">WenetSpeech Task</head><p>We first evaluate our proposed method on the WenetSpeech <ref type="bibr" target="#b19">[20]</ref> Dataset, which a is multi-domain Mandarin corpus consisting of 10005 hours of high-quality labeled speech. Evaluation is performed in terms of word error rate (WER) on the Dev, Test Net and Test Meeting, described in <ref type="bibr" target="#b19">[20]</ref>. The input speech uses 80-dimension log-Mel filterbank features, which are computed with a 25ms window and shifted every 10ms. Spec-Augment is applied 2 frequency masks with maximum frequency mask (F = 30) and 2 time masks with maximum time mask (T = 50) to alleviate over-fitting. A global mean and variance normalization is used as data preparation. The max number of epochs is 26. We set the checkpoint with the lowest CTC loss as the final model. We use the benchmarks from Kaldi, Espnet and Wenet listed in <ref type="bibr" target="#b19">[20]</ref> as our baselines. Our Conformer model uses the same setup as Espnet and Wenet benchmarks, which consists of 12-block Conformer encoder(d f f = 2048, H = 8, d att = 512, CN N kernel = 15) and 6-block transformer decoder(d f f = 2048, H = 8). As for the MoE layer, we set the number of experts to be 16, 32 and 64, noted as 16e, 32e and 64e respectively. The shared embedding network is a static model without MoE layer but a similar structure to the baseline, which contains 6 Conformer encoder layers and is pretrained with CTC objective before used for MoE training. The whole model is trained with CTC and attention objectives, along with our mentioned auxiliary losses. For all experiments on MoE models, we set the hyper-parameters ? = 0.15, ? = 0.15, ? = 0.01, ? = 0.3. A set of 5535 Mandarin characters and 26 English letters is used as the modeling units. For decoding, we generate the N-Best hypotheses by the CTC decoder and rescore them by the attention decoder to get the final results.</p><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, our Conformer-MoE models, equipped with the multi-path property, achieve consistent performance improvements on three test sets over Kaldi, Espnet and Wenet benchmarks. Moreover, by increasing the number of experts, Conformer-MoE gets a better performance on Dev but 64e doesn't get consistent improvements on Test Net and Test Meeting, which have more difficult domain and unmatched data. In total, Conformer-MoE(32e) achieves the best performance on these test sets. Compared with Wenet benchmark, it provides 12.2% ? 17.6% relative character error rate(CER) reduction. Our 3M model, Comformer-MoE with additional multi-level property, doesn't get a better performance over Conformer-MoE so we did not post its results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">150,000 hours Task</head><p>In this experiment, we scale up the size of training corpus to 150000 hours to further identify the effectiveness of our proposed method. The training corpus is collected from different application domains. In order to improve the system robustness, we simulate six types of different mixing conditions like voice processing(vp), acoustic echo cancellation(aec), reverberation(rir) and signal-to-noise-ratio(SNR), etc. The SNR is set between 15 and 30 dB, and the reverberation time is set between 0 and 900 milliseconds. The test sets are collected from YouTube, consisting of 15 hours of audio. They add up to 65536 utterances and are divided into 10 domains, covering game commentary, documentary, comic dialogue and so on.</p><p>Our baseline Conformer model is implemented by Wenet toolkits and it consists of two convolution downsampling layers, a 18-block Conformer encoder and a 4-block bi-transformer decoder (2-block for forward and 2-block for backward). For MoE models, the number of experts are also set to be 16, 32 and 64. The shared embedding network is similar as before and has 7 Conformer blocks. For the multi-level setting, two separate bitransformer decoders are applied to the intermediate layers at 1/3 depth and 2/3 depth of the encoder. Since decoders at intermediate level are only used for training, there is no increase computational cost in decoding for 3M models. All model are trained with CTC and attention objectives and hyper-parameters settings are the same as before. We use the floating point operations(FLOPs) for a one-second example to evaluate the inference computation cost and <ref type="table" target="#tab_3">Table 3</ref> shows the comparison on parameters amount and FLOPs for Conformer, Conformer-MoE and 3M.</p><p>Experimental results are shown in <ref type="table" target="#tab_1">Table 1</ref>. It is clear to see from column 2 and column 5 that 3M (16e) achieves lower character error rate than the baseline Conformer model and the gain is 11.5% on average. With the increase of data amount, MoE models with more experts are easier to get a better per-  To analyze the efficacy of 3M model, we must study how much the multi-path and multi-level contribute to the performance individually. We train Conformer-MoE (multi-path) and Conformer-MLevel (multi-level) model by only keeping the multi-path and the multi-level in 3M model, respectively. In <ref type="table" target="#tab_1">Table 1</ref>, we see that a significant gain benefits from both the multi-level and multi-path. Especially, compared with the baseline model, the gain is 9.2% (from column 2 and column 4) for the multi-path and 5.6% (from column 2 and column 3) for the multi-level. It suggests that both the multi-level and multi-path are critical to the quality of the proposed 3M model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>In this paper, we introduce the 3M model for speech recognition. Specifically, based on the CTC/AED E2E ASR framework, we apply MoE on the Conformer model and apply AED at multiple levels of the model. We demonstrated the effectiveness of our proposed method on both the public WenetSpeech dataset and a large scale dataset. Experimental results show that the Conformer-MoE provides up to 17.6% relative CER improvement compared with the Wenet benchmark on the Wenet-Speech dataset. The mutli-level property can also contribute to a better performance. Overall, our 3M model achieves 14.9% relative CER improvement on average over the baseline Conformer model on the large scale dataset. Future work includes increasing the number of experts by one or two orders of magnitudes, and exploring the proposed 3M model with other endto-end training framework such as transformer transducers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of Conformer, Conformer-MLevel, Conformer-MoE and 3M.</figDesc><table><row><cell>Testset</cell><cell cols="6">Confomer Conformer-MLevel Conformer-MoE (16e) 3M (16e) 3M (32e) 3M (64e)</cell></row><row><cell>cctv</cell><cell>1.60</cell><cell>1.52</cell><cell>1.48</cell><cell>1.47</cell><cell>1.43</cell><cell>1.42</cell></row><row><cell>deyunshe</cell><cell>16.48</cell><cell>16.01</cell><cell>15.56</cell><cell>15.20</cell><cell>14.78</cell><cell>14.31</cell></row><row><cell>liyongle</cell><cell>4.63</cell><cell>3.96</cell><cell>3.71</cell><cell>3.53</cell><cell>3.50</cell><cell>3.37</cell></row><row><cell>luoxiang</cell><cell>3.93</cell><cell>3.51</cell><cell>3.30</cell><cell>3.11</cell><cell>3.10</cell><cell>3.08</cell></row><row><cell>luozhengyu</cell><cell>2.78</cell><cell>2.55</cell><cell>2.4</cell><cell>2.30</cell><cell>2.27</cell><cell>2.19</cell></row><row><cell>qichezhijia</cell><cell>6.89</cell><cell>6.58</cell><cell>6.42</cell><cell>6.34</cell><cell>6.30</cell><cell>6.25</cell></row><row><cell>shenghuo</cell><cell>9.84</cell><cell>9.49</cell><cell>9.16</cell><cell>9.03</cell><cell>8.41</cell><cell>8.13</cell></row><row><cell>tianxiazuqiu</cell><cell>3.82</cell><cell>3.77</cell><cell>3.59</cell><cell>3.52</cell><cell>3.46</cell><cell>3.37</cell></row><row><cell>wangzhe</cell><cell>11.75</cell><cell>11.29</cell><cell>10.87</cell><cell>10.63</cell><cell>10.43</cell><cell>10.36</cell></row><row><cell>xiaoaidashu</cell><cell>5.94</cell><cell>5.83</cell><cell>5.81</cell><cell>5.74</cell><cell>5.70</cell><cell>5.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>CER results on WenetSpeech for public benchmarks and Conformer-MoE</figDesc><table><row><cell>Toolkit</cell><cell cols="3">Dev Test Net Test Meeting</cell></row><row><cell>Kaldi</cell><cell>9.07</cell><cell>12.83</cell><cell>24.72</cell></row><row><cell>Espnet</cell><cell>9.70</cell><cell>8.90</cell><cell>15.90</cell></row><row><cell>Wenet</cell><cell>8.88</cell><cell>9.70</cell><cell>15.59</cell></row><row><cell cols="2">Conformer-MoE (16e) 7.67</cell><cell>8.28</cell><cell>13.96</cell></row><row><cell cols="2">Conformer-MoE (32e) 7.49</cell><cell>7.99</cell><cell>13.69</cell></row><row><cell cols="2">Conformer-MoE (64e) 7.19</cell><cell>8.36</cell><cell>13.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>A comparsion of Conformer, Conformer-MoE and 3M. We can see that 3M model achieves consistent improvements from 16e to 64e on all test sets in 150000 hours task, which is different from the WenetSpeech results. In total, 3M(64e) provides 14.9% relative CER improvement on average over the baseline Conformer model.</figDesc><table><row><cell>Model</cell><cell cols="2">Params FLOPs</cell></row><row><cell>Conformer</cell><cell>120M</cell><cell>8.3B</cell></row><row><cell>Conformer-MoE (16e)</cell><cell>425M</cell><cell>12.3B</cell></row><row><cell>3M (16e)</cell><cell>500M</cell><cell>12.3B</cell></row><row><cell>3M (32e)</cell><cell>775M</cell><cell>12.3B</cell></row><row><cell>3M (64e)</cell><cell>1.37B</cell><cell>12.3B</cell></row><row><cell>formance.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00015</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Wenet: Production first and production ready end-to-end speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2102</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Advances in joint ctc-attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<idno>abs/1706.02737</idno>
		<ptr target="http://arxiv.org/abs/1706.02737" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving hybrid ctc/attention architecture with time-restricted self-attention ctc for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">4639</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hybrid ctc/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint ctc/attention decoding for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Speechstew: Simply mix all available speech recognition data to train one large neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02133</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical mixtures of experts and the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="214" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16668</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hard mixtures of experts for large scale weakly supervised vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6865" to="6873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Network of experts for large-scale image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep mixture of experts via shallow embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence. PMLR</title>
		<imprint>
			<biblScope unit="page" from="552" to="562" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic routing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Speechmoe: Scaling to large acoustic models with dynamic routing mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03036</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<idno type="arXiv">arXiv:2111.11831</idno>
		<title level="m">Speechmoe2: Mixture-of-experts model with improved routing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
