<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HIDA: Towards Holistic Indoor Understanding for the Visually Impaired via Semantic Instance Segmentation with a Wearable Solid-State LiDAR Sensor</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyu</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HIDA: Towards Holistic Indoor Understanding for the Visually Impaired via Semantic Instance Segmentation with a Wearable Solid-State LiDAR Sensor</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Independently exploring unknown spaces or finding objects in an indoor environment is a daily but challenging task for visually impaired people. However, common 2D assistive systems lack depth relationships between various objects, resulting in difficulty to obtain accurate spatial layout and relative positions of objects. To tackle these issues, we propose HIDA, a lightweight assistive system based on 3D point cloud instance segmentation with a solid-state LiDAR sensor, for holistic indoor detection and avoidance. Our entire system consists of three hardware components, two interactive functions (obstacle avoidance and object finding) and a voice user interface. Based on voice guidance, the point cloud from the most recent state of the changing indoor environment is captured through an on-site scanning performed by the user. In addition, we design a point cloud segmentation model with dual lightweight decoders for semantic and offset predictions, which satisfies the efficiency of the whole system. After the 3D instance segmentation, we post-process the segmented point cloud by removing outliers and projecting all points onto a top-view 2D map representation. The system integrates the information above and interacts with users intuitively by acoustic feedback. The proposed 3D instance segmentation model has achieved state-of-the-art performance on ScanNet v2 dataset. Comprehensive field tests with various tasks in a user study verify the usability and effectiveness of our system for assisting visually impaired people in holistic indoor understanding, obstacle avoidance and object search.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For sighted people, when they enter an unfamiliar indoor environment, they can observe and perceive the surrounding environments through their vision. However, such a global scene understanding is challenging for people with visual impairments. They often need to approach and touch the objects in the room one by one to distinguish their categories * Corresponding author (e-mail: kailun.yang@kit.edu). and get familiar with their locations. This is not only inconvenient but also creates some risks for visually impaired people in their everyday living and travelling tasks. In this work, we develop a system to help vision-impaired people understand unfamiliar indoor scenes.</p><p>Some assistance systems leverage various sensors (such as radar, ultrasonic, and range sensors) to help the visionimpaired avoid obstacles <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b65">65]</ref>. With the development of deep learning, vision tasks like object detection and image segmentation can yield precise scene perception. Different vision-based systems were proposed towards environment perception and navigation assistance for visually impaired people. However, most 2D image semanticsegmentation-based systems <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b56">56]</ref> and 3D-visionbased systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b58">58]</ref> cannot provide a holistic understanding, because these systems only process the current image or image with depth information captured by the camera, rather than a complete scan of the surroundings.</p><p>Compared with 2D images, 3D point clouds contain more information and are suitable for reconstruction of the surrounding environment. Thereby, in this work, we propose HIDA, an assistance system for Holistic Indoor Detection and Avoidance based on semantic instance segmentation. The main structure of HIDA is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. When the user enters an unfamiliar room, the user can wake up the system by voice, and then the system will help the user to scan the surroundings by running Simultaneous Localization and Mapping (SLAM). Then the obtained point cloud will be delivered into an instance segmentation network, where each point will be attached with instance information. We adapt PointGroup <ref type="bibr" target="#b22">[23]</ref> in our instance segmentation network and modify the structure to acquire 3D semantic perception with higher precision. We enable a crossdimension understanding by converting the point cloud with instance-aware semantics to a usable and suitable representation, i.e., 2D top-view segmentation, for assistance. After reading the current user location, the system will inform vision-impaired people through voice about the obstacles around and suggest the safe passable direction. In addition, the user can specify a certain type of object in the room, and our system will tell the user the distance and direction Our segmentation model is trained and evaluated on the ScanNet v2 dataset <ref type="bibr" target="#b8">[9]</ref>, which yields an accurate and robust surrounding perception. During field experiment and various pre-tests, the learned model performed satisfactorily in our indoor usage scenarios, which makes it suitable for real-world applications. Even when an object was partially scanned, this network is still able to recognize and render relatively accurate classification. To evaluate the assistance functions of our system, we designed different tasks. Our system has achieved significant results in reducing collisions with obstacles. According to the questionnaire survey, the users believe that our system will help vision-impaired people in indoor scene understanding. To the best of our knowledge, we are the first to use 3D semantic instance segmentation for assisting the visually impaired.</p><p>In summary, we deliver the following contributions: ? We propose HIDA, a wearable system with a solidstate LiDAR sensor, which helps the visually impaired to obtain a holistic understanding of indoor surroundings with object detection and obstacle avoidance. ? We designed a 3D instance segmentation model, achieving the state-of-the-art in mAP on ScanNet v2. ? We convert the point cloud with semantic instance information to a usable top-view representation suitable for assisting vision-impaired people. ? We conducted user studies to evaluate obstacle avoidance and object search, verifying the usability and benefit of the proposed assistance system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Segmentation for Visual Assistance. Since the surge of deep learning particularly the concept of fully convolutional networks <ref type="bibr" target="#b34">[35]</ref>, semantic segmentation can be performed end-to-end, which enables a dense surrounding understanding. Thereby, semantic segmentation has been introduced into vision-based navigational perception and assistance systems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b66">66]</ref>. Yang et al. <ref type="bibr" target="#b56">[56]</ref> put forth a real-time semantic segmentation architecture to enable universal terrain perception, which has been integrated in a pair of stereo-camera glasses and coupled with depth-based close obstacle segmentation. Mao et al. <ref type="bibr" target="#b37">[38]</ref> designed a panoptic lintention network to reduce the computation complexity in panoptic segmentation for efficient navigational perception, which unifies semantic and instance-specific understanding. Semantic segmentation has also been leveraged to address intersection perception with lightweight network designs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>, whereas most instance-segmentation-based assistance systems <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b61">61]</ref> directly use the classic Mask R-CNN model <ref type="bibr" target="#b14">[15]</ref> and rely on sensor fusion to output distance information.</p><p>Compared to 2D segmentation-driven assistance, 3D scene parsing systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b62">62]</ref> fall behind, as these classical point cloud segmentation works focus on designing principled algorithms for walkable area detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b62">62]</ref> or stairs navigation <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b59">59]</ref>. In this work, we devise a 3D semantic instance segmentation system for helping visually impaired people perceive the entire surrounding and provide a top-view understanding, which is critical for various indoor travelling and mapping tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>. 3D Semantic and Instance Segmentation. With the appearance of large-scale indoor 3D segmentation datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>, point cloud semantic instance segmentation becomes increasingly popular. It allows to go beyond 2D segmentation and render both point-wise and instance-aware understanding, which is appealing for assisting the visually impaired. Early works follow two mainstreams. One is based on object detection that first extracts 3D bounding boxes and then predicts point-level masks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b60">60]</ref>. Another prominent paradigm is segmentation-driven, which first infers semantic labels and then groups points into instances by using point embedding representations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52]</ref>.</p><p>Recently, PointGroup <ref type="bibr" target="#b22">[23]</ref> is designed to enable better grouping of points into semantic objects and separation of adjacent instances. DyCo3D <ref type="bibr" target="#b15">[16]</ref> employs dynamic convolution customized for 3D instance segmentation. Oc-cuSeg <ref type="bibr" target="#b13">[14]</ref> relies on multi-task learning by coupling embedding learning and occupancy regression. 3D-MPA <ref type="bibr" target="#b10">[11]</ref> generates instance proposals in an object-centric manner, followed by a graph convolutional model enabling higher-level interactions between nearby instances. Additional methods use panoptic fusion <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b54">54]</ref>, transformers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b67">67]</ref> and omni-supervision <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b57">57]</ref> towards complete understanding.</p><p>In this work, we build a holistic semantic instance-aware scene parsing system to help visually impaired people understand the entire surrounding. We augment instance segmentation with a lightweight dual-decoder design to better predict semantic-and offset features. Differing from other cross-dimension <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref> and cross-view <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43]</ref> sensing platforms, we aim for top-view understanding and our holistic instance-aware solution directly leverages 3D point cloud segmentation results and aligns them onto 2D topview representations for generating assistive feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HIDA: Proposed System</head><p>The entire architecture of HIDA is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>, including hardware components, user interfaces, and the algorithm pipeline. Designed to maximize the stability of the portable system, only very few parts are integrated into our prototype. As shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>), the system is composed of three hardware components. First, a lightweight solid-state LiDAR sensor is attached to a belt for collecting point clouds. The RealSense L515, as the world's smallest high-resolution LiDAR depth camera, is well suitable as part of wearable devices. In addition, the scanning range up to 9m is suitable for most indoor scenes. A laptop placed in a backpack is the second component of our system. The laptop with a GPU processor ensures that the instance segmentation can be performed in an online manner. As for input and output interfaces, a bone-conduction headset with a microphone is the third component. The audio commands from users can be recognized by the user interface. Also beneficially, thanks to the bone-conduction earphones, internal acoustic cues from our system and external environmental sounds can be separately perceived by the users, which is safety-critical for assisting the visually impaired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Environment Scan</head><p>The users will collect independently the point cloud under the audio guidance. At the same time, the system also needs to obtain the user's position in the point cloud map. These can be achieved through Simultaneous Localization and Mapping (SLAM) with odometry <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b49">49]</ref>. There are many mature SLAM frameworks. However, for the visually impaired, collecting point clouds in an unfamiliar environment is different from conventional ways. Specifically, the entire room is usually scanned at the entrance, so the user's movement is limited in a small range and those movement will mostly be the in-situ rotation. In addition, the motion of the human body is more unstable compared to robots. In this case, the odometry of SLAM may lose tracking. If this happens, the camera is required to move back to the previous position of the keyframe for loop closing, which is hard for vision-impaired people. Therefore, we value more the robustness of the mapping process. In our field test, RTAB-Map <ref type="bibr" target="#b25">[26]</ref> achieved a reliable performance under such requirements. <ref type="figure" target="#fig_1">Fig. 2</ref> shows two scanning results in real-world scenes using RTAB-Map. It can be seen that even in a limited range of movement, HIDA can obtain point clouds with rich and dense object information. After proficiency, users can complete the collection of dense point clouds in most cases by their own, even if they cannot check the collection of point clouds through the display at the same time.</p><p>In our system, once the "start" instruction signal is recognized by the voice user interface, the environment scan process will begin. The odometry will update the current position and direction of the user at the same time. Under the audio guidance, the user can slowly turn around to scan the surrounding environment. In practice, the default scan time of a whole room is set as 20 seconds, which can obtain a sufficient amount of keyframes from different directions. After that, the captured point cloud will be sent to the segmentation network for 3D instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Instance Segmentation</head><p>In the data preprocessing stage, if the point cloud is too large, the segmentation time and memory requirement will be significantly increased. Considering the efficiency of 3D segmentation, we restrict the amount of points under  200, 000. If the total number of points in the point cloud exceeds 200, 000, we will downsample them evenly for saving running time and memory usage. Besides, some noise points will be removed as outliers which is caused by the influence of natural lighting.</p><p>Inspired by PointGroup <ref type="bibr" target="#b22">[23]</ref>, we design a 3D instance segmentation architecture. PointGroup uses a 7-layer sparse convolution U-Net <ref type="bibr" target="#b46">[46]</ref> to extract features. Each layer consists of an encoder and a decoder. The extracted feature will be decoded into 2 branches: semantic branch and offset branch. The semantic branch builds the clusters that have the same semantic labels. The offset branch predicts perpoint offset vectors to shift each point towards the instance centroid and builds shifted point clusters that belong to the same instances. Those cluster proposals will be voxelized and then scored. In the inference, Non-Maximum Suppression (NMS) is performed to make final instance predictions.</p><p>Different from <ref type="bibr" target="#b22">[23]</ref>, our key adaptation lies in the backbone structure in order to enable a clear separation of dense instances and reach a better performance on semantic and offset predictions, which are essential in 3D instance segmentation. The architecture of our modified model is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Specifically, we use one encoder and dual decoders in each layer, including a decoder for semantic features and another for offset features. They are extracted separately in the early stage of the network. In addition, this dual-decoder U-Net consists of only 5 layers instead of 7, which can reduce parameters and speed up the infer- ence. More details are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. This architecture has a clearly better performance compared with the original backbone, while maintaining a low amount of parameters.</p><p>As mentioned before, many objects may only be partially scanned. Even so, the model can still accurately identify most of the objects in the point cloud. We show some examples of instance segmentation results in <ref type="figure" target="#fig_1">Fig. 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">User Interface</head><p>After capturing point cloud and obtaining semantic and instance information, we can actually provide a variety of information to vision-impaired people. Although we have already downsampled the point cloud before, the calculation with the whole point cloud is still very time-consuming. In order to reduce the processing time, the proposed system only traverses the entire point cloud once to extract information of interest of the users. The proposed implementation is as follows: First, all points will be projected onto a plane parallel to the ground (XY -plane). Then, the current user location will be updated. For each instance, 5 feature points will be searched in points belonging to the same segmented instance: one point closest to the user and four coordinate extreme points (corner points) of this object. The direction will be calculated according to the camera pose information. We build the 2D camera coordinate X Y relative to the XY -plane, and then transform the feature points coordinate (x i , y i ) into (x i , y i ). +x corresponds to the front of the user, and +y corresponds to the left. The coordinates are not intuitive for the user, so our designed user interface will represent instances with distance and direction relative to the user. The directions will be divided into 12 areas, as shown in <ref type="figure" target="#fig_4">Fig. 5a</ref>). In this way, the location of each instance will be replaced by the distance and direction of the point on the instance that is the closest to the user. Besides, in order to help the user bypass the obstacle, the direction of the "corner points" will also be denoted as the direction "occupied" by the object. An example is shown in <ref type="figure" target="#fig_4">Fig. 5b</ref>). Obstacle avoidance. Furthermore, we proposed two interactive functions. The first function is the obstacle avoidance. The user will set an obstacle avoidance range. The system will primarily eliminate the direction occupied by obstacles within this range to find passable area. If all directions in the scanned area are already occupied by the obstacles, not scanned area will be directed and suggested to users as potential passable area. The passable area and all objects information within the detection range will be output. <ref type="figure" target="#fig_5">Fig. 6</ref> is a functional example in an indoor scene. Object finding. The second function is the object search. The user specifies an object category of interest through voice commands, such as a "Find a desk" instruction. Then, the system will search for the corresponding instance and return the object position through acoustic cues. For instance, "Found a desk, distance 2.2 meters, direction in directly forward" will be output as speech via the boneconducted headset. In addition, in order to help users navigate to the object, our system can also alert obstacles in the direction leading to the object. For example, "Attention, a chair in this direction, distance 1.3 meters". The detailed schematic view is in <ref type="figure">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Quantitative results of segmentation model</head><p>We trained our instance segmentation model on ScanNet v2 dataset <ref type="bibr" target="#b8">[9]</ref>, containing 1613 scans of indoor scenes. The dataset is spitted into 1201, 312, and 100 scans in the train- <ref type="figure">Figure 7</ref>: Object search in an example scene. ing, validation, and testing subsets, respectively. We set the cluster voxel size as 0.02m, and cluster radius as 0.03m and the minimum cluster point number as 50. In the training process, we uses Adam optimizer <ref type="bibr" target="#b24">[25]</ref> with a learning rate of 0.001. Our model learned through the entire training set for 360 times with a batch size of 8. We report the quantitative performance in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>. Several qualitative visualizations of the segmentation results are shown in <ref type="figure" target="#fig_6">Fig. 8</ref>. In these three scenes, the instances are placed quite densely (e.g., many chairs are placed side by side in the third scenes), and some instances have not been completely scanned. But our model can still separate them well, which is beneficial for upper-level assistance. Following previous researches, mean Average Precision (mAP) is leveraged in our work as the main evaluation metric (see <ref type="table" target="#tab_0">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref>). Specifically, AP <ref type="bibr" target="#b24">25</ref> and AP 50 denote the AP scores with IoU threshold set to 25% and 50%. Also, AP averages the scores with IoU threshold set from 50% to 95%, with a step size of 5%. We first assess the  performance on the validation set of ScanNet v2, as shown in <ref type="table" target="#tab_1">Table 2</ref>. Here, we also compare the performance with different backbone sizes: "*-S" denotes a smaller backbone with m = 16 and "*-L" denotes a large backbone with m = 32. If we focus on the smaller-size backbone and preform a fair comparison with the original PointGroup model <ref type="bibr" target="#b22">[23]</ref> and the recent DyCo3D model <ref type="bibr" target="#b15">[16]</ref>, the proposed architecture clearly exceeds them, i.e., by 2.8% compared to PointGroup and 2.6% compared to DyCo3D. We also report the class-wise performance of our 3D point cloud instance segmentation model on the testing set of ScanNet v2, as listed in <ref type="table" target="#tab_0">Table 1</ref>. Compared with state-ofthe-art methods, our DD-UNet+Group model has achieved the best performance measured in mAP (43.6%). Among the 11 architectures, our method reaches high scores on many classes relevant for assisting the visually impaired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Runtime analysis of point cloud segmentation</head><p>We tested this system on a laptop (Intel Core i7-7700HQ, NVIDIA GTX 1050Ti). We scanned four real-world scenes including a densely-scanned meeting room, a bedroom, a corridor, and an office. The scan time and point cloud sizes are different. We counted the pre-processing time, instance segmentation time (specific to each part of the network), and the extraction time of object information, listed in the <ref type="table" target="#tab_3">Table 3</ref>. It can be seen that the processing of the point cloud is still computationally complex, which inevitably leads to a short waiting time for the user. In the scenario of helping the visually impaired gather complete scene understanding, search for interested objects, or travel in unfamiliar indoor scenes, a short waiting time is understandable. Yet, a more  powerful mobile processor and a more efficient large-scale point cloud instance segmentation method are expected to allow users travelling instantly after the scanning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">User Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Function test</head><p>Two tasks are designed to test HIDA with 7 participants including 5 males and 2 females. Their ages are within 24-30. Due to COVID-19 restrictions especially the social-distancing regulation challenging for the visually impaired <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b47">47]</ref>, the voluntary participants were sighted, and they were blindfolded during the tests. The first task focuses on the functionality of obstacle avoidance, whereas the second tests the functionality of instance locating and guidance ability for assisting users to find their interested objects. When each participant was executing their tasks, we recorded the whole process for subsequent analysis. Task 1, find passable direction: The scenario of the first task is: The user enters a wide corridor (in most directions passable), within three meters around the entrance, we place different kinds of obstacles and leave a "gap" in a random direction. The users were asked to find and leave this area with obstacles. Each user performed this task first using only the white cane and then performed using both the white cane and our system. The arrangement and position of obstacles and the gap will be changed after each task has been successfully executed.  <ref type="figure" target="#fig_7">Fig. 9a</ref>) illustrates some of our qualitative analysis results of task 1. It can be seen that in the case of using only a white cane, users generally do global search without guidance, trying to recognize the surrounding environment and find passable areas by a brute-force like approach. While using HIDA, participants can directly find the correct direction of the gap. The collisions with obstacles were significantly reduced, providing a safe walking condition in indoor environments with obstacles. However, few problems among different users are found. For example, the visionimpaired user sometimes can not completely scan the surrounding environment. If our system did not find a "gap" between the scanned obstacles, it will suggest two directions pointed to unscanned area. The last row of <ref type="figure" target="#fig_7">Fig. 9a</ref>) represents this case. Although there is a correct direction among two suggested directions, it is hard for users to go back to the previous position after they primarily tried in the wrong direction. This indicates that one should have more practice in order to maximize the system's effect.</p><p>Task 2, find a specific object: We chose an office for the second task since objects placement in this office seems more complex, indicating a kind of difficulty level for testing our system. First, users will be taken to different start points. Then, the users were asked to find a random chair and sit down. Each user fulfilled this task twice, the first time with the white cane, and the second time with the white cane and our system. We only changed the position of part objects each time. In order to avoid the user becoming more familiar with the room when entering the room for the second time, half of the users first performed the task with white cane and then used both the system and cane, the other half on the contrary. <ref type="figure" target="#fig_7">Fig. 9b</ref>) visualizes part of the results. Similar to task 1, when the user only relies on the white cane, the user perceived the surrounding objects one by one through touching them and feeling their shape, showing a low efficiency for finding the object. In the case of using our system as supplement, the users walked directly toward the chair, discarding the process of object shape recognition by users through touching, indicating a higher efficiency in searching objects. And they could bypass the obstacles on the path. As shown in the last row in <ref type="figure" target="#fig_7">Fig. 9b</ref>), some objects may not be correctly classified since the accuracy of our segmentation network is not perfect. A network with an even higher precision is expected to be designed and deployed in the future.</p><p>Efficiency analysis. We also counted the average time the user costed to complete these tasks in <ref type="table" target="#tab_5">Table 4</ref>. As it can be seen from the data, using our system did not significantly reduce the whole duration time. The reason is that scanning of the surrounding environment, segmentation of the point cloud, and audio interaction with users are still timeconsuming for the current method. If we only measure the time from the user leaving the starting area to completing the task, the differences are clear, indicating the potential and superiority of the proposed system, which gave a correct direction and significantly reduced wrong attempts.  Overall For vision impaired people, this system helps. <ref type="table">Table 5</ref>: Questionnaires: For each statement, users will select a score among 1-5, 5 means strongly agree, and 1 means strongly disagree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Feedback</head><p>Questionnaire. A questionnaire from multiple aspects, listed in <ref type="table">Table 5</ref>, is designed in order to further evaluate the user experience of our system. After the participants completed the above two tasks, each participant answered these questions. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the feedback from the participants regarding our system. Participants make positive comments in terms of understanding, complexity, and comfort of the functionality of our system. They consider, that the system has an intuitive and smooth interface, and it is much easier to find the object with this system. However, the functionality and the operability still need to be improved. In summary, most participants think that the functions of HIDA are indeed very helpful in case of visual impairment and wearing the current hardware will not have too much impact on their daily lives. But they expect more functions, in addition, they think that some training are necessary in order to become familiar with the system, then they will accomplish the tasks more smoothly. User comments. We also collected some comments from the participants. Some positive comments include that our system did help them to understand the environment, it provided a very smooth interface, it is easy to wear, and the bone-conduction headset is also very suitable for such a system. However, some participants still put forward some improvement directions of this system. In the hardware aspect, one participant hopes to use a lighter processor instead of laptop in the future, which will make our system more wearable. In the functional aspect, two participants both suggested that if the real-time objects information could be provided, it will help the visually impaired more. In the interaction aspect, one participant mentioned that the current voice interaction is still a bit time-consuming, especially when there are many obstacles, the output of information is somewhat lengthy. In addition, some participants thought this system should have a clearer guidance on how to scan a room completely or give more training before the usage. These comments are very constructive for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Works</head><p>In this work, HIDA, a 3D vision-based assistance system is proposed to help visually impaired people gather a holistic indoor understanding with object detection and obstacle avoidance. By using point cloud obtained with a wearable solid-state LiDAR sensor, it provides obstacle detection and specific object searching. Visual SLAM and instance-aware 3D scene parsing are combined, where point-wise semantics are converted to yield a dense top-view surrounding perception. The devised 3D instance segmentation model attains state-of-the-art performance on ScanNet v2 dataset, thanks to separate predictions of semantic-and offset features. The overall system is verified to be useful and helpful for indoor travelling and object searching, and it is promising for more assisted living tasks.</p><p>However, there are some limitations of our system. First, the surrounding environment scanning and point cloud segmentation are time-consuming, making a real-time processing not possible. In addition, the accuracy of the current point cloud instance segmentation model still has much space to be improved. Last but not least, although we have obtained a satisfactory point cloud map with semantic instance information, due to the visual odometry get lost easily when the movement range is too large, the real-time interaction with the map has not been fully investigated.</p><p>In the future, we intend to address the above points and introduce improvements to the system such as leveraging multi-sensor fusion to speed up the scanning of the surrounding environment and improving the odometry to obtain the location of the user on the map in real time. Moreover, designing a higher-precision segmentation network structure will also significantly improve the accuracy of holistic guidance delivered by our system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>a) Hardware overview. b) System overview. of this object and warn possible obstacles on the path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Left: Capturing point cloud within a limited moving range. Right: Instance segmentation results (red colored indices wrongly classified instances).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of our 3D point cloud segmentation model with dual-decoder U-Net in the backbone structure to separately predict semantic-and offset features, and subsequent adaptations for holistic instance-aware semantic understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Backbone architecture. X ? m indicates the number of encoder output channels. We set m = 16 or m = 32.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Representation of instance distance and direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Obstacle avoidance in an example scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results on ScanNetV2 validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>a): Visualization of task 1; b): Visualization of task 2. For each visualization, floor maps are plotted according to recorded videos and saved point clouds, and trajectories are manually plotted according to the recorded videos. Instance segmentation and the system output visualization are on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Questionnaire feedback from the participants, error bars indicate standard error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>10.6 3.7 13.5 32.1 2.8 33.9 11.6 46.6 9.3 3D-BoNet [55] 25.3 48.8 68.7 51.9 32.4 25.1 13.7 34.5 3.1 41.9 6.9 16.2 13.1 5.2 20.2 33.8 14.7 30.1 30.3 65.1 17.8 MASC [32] 25.4 44.7 61.5 46.3 24.9 11.3 16.7 41.2 0.0 34.5 7.3 17.3 24.3 13.0 22.8 36.8 16.0 35.6 20.8 71.1 13.6 48.4 29.9 27.7 59.1 4.7 33.2 21.2 21.7 27.8 19.3 41.3 41.0 19.5 57.4 35.2 35.2 84.9 21.3 SSEN [64] 38.4 57.5 72.4 85.2 49.4 19.2 22.6 64.8 2.2 39.8 29.9 27.7 31.7 23.1 19.4 51.4 19.6 58.6 44.4 84.3 18.4 PE [63] 39.6 64.5 77.6 66.7 46.7 44.6 24.3 62.4 2.2 57.7 10.6 21.9 34.0 23.9 48.7 47.5 22.5 54.1 35.0 81.8 27.3 PointGroup [23] 40.7 63.6 77.8 63.9 49.6 41.5 24.3 64.5 2.1 57.0 11.4 21.1 35.9 21.7 42.8 66.0 25.6 56.2 34.1 86.0 29.1 DyCo3D [16] 39.5 64.1 76.1 64.2 51.8 44.7 25.9 66.6 5.0 25.1 16.6 23.1 36.2 23.2 33.1 53.5 22.9 58.7 43.8 85.0 31.7 Ours (DD-UNet+Group) 43.6 63.5 76.4 63.0 50.8 48.0 31.0 62.4 6.5 63.8 17.4 25.6 38.4 19.4 42.8 75.9 28.9 57.4 40.0 84.9 29.1 Per class mAP 3D instance segmentation results on ScanNet v2 [9] testing set. mAP, AP 50 and AP 25 are reported.</figDesc><table><row><cell cols="3">mAP AP50 AP25</cell><cell>bathhub</cell><cell>bed</cell><cell>bookshelf</cell><cell>cabinet</cell><cell>chair</cell><cell>counter</cell><cell>curtain</cell><cell>desk</cell><cell>door</cell><cell>otherfu.</cell><cell>picture</cell><cell>refrigerator</cell><cell>s.curtain</cell><cell>sink</cell><cell>sofa</cell><cell>table</cell><cell>toilet</cell><cell>window</cell></row><row><cell cols="20">3D-SIS [17] 16.1 8.8 SALoss-ResNet [30] 26.2 38.2 55.8 40.7 15.5 6.8 4.3 34.6 0.1 13.4 0.5 45.9 69.5 66.7 33.5 6.7 12.3 42.7 2.2 28.0 5.8 21.6 21.1 3.9 14.2 51.9 10.6 33.8 31.0 72.1 13.8</cell></row><row><cell>MTML [27] 28.2</cell><cell>54.9</cell><cell cols="18">73.1 57.7 38.0 18.2 10.7 43.0 0.1 42.2 5.7 17.9 16.2 7.0 22.9 51.1 16.1 49.1 31.3 65.0 16.2</cell></row><row><cell>3D-MPA [11] 35.5</cell><cell cols="2">67.2 74.2 mAP</cell><cell>AP 50</cell><cell></cell><cell cols="2">AP 25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MTML [27]</cell><cell>20.3</cell><cell></cell><cell>40.2</cell><cell></cell><cell cols="2">55.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D-MPA [11]</cell><cell>35.3</cell><cell></cell><cell>59.1</cell><cell></cell><cell cols="2">72.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointGroup [23]</cell><cell>35.2</cell><cell></cell><cell>57.1</cell><cell></cell><cell cols="2">71.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DyCo3D-S [16]</cell><cell>35.4</cell><cell></cell><cell>57.6</cell><cell></cell><cell cols="2">72.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DyCo3D-L [16]</cell><cell>40.6</cell><cell></cell><cell>61.0</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours-S</cell><cell>38.0</cell><cell></cell><cell>58.5</cell><cell></cell><cell cols="2">72.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours-L</cell><cell>42.4</cell><cell></cell><cell>60.3</cell><cell></cell><cell cols="2">74.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on ScanNet v2 [9] validation set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Analysis of inference time (s). * denotes pre-</cell></row><row><cell>processed point cloud; Total denotes the time from scan</cell></row><row><cell>finished to detection done; PRE denotes outliers removing</cell></row><row><cell>and downsampling; BB denotes backbone + two branches;</cell></row><row><cell>CL denotes the clustering part; S&amp;P denotes ScoreNet and</cell></row><row><cell>final instance prediction; DET denotes information extrac-</cell></row><row><cell>tion from instances map.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the time required to complete the task. * denotes ignoring the processing time.</figDesc><table><row><cell></cell><cell>Questionnaires</cell></row><row><cell>Understanding</cell><cell>I can understand an indoor environment by</cell></row><row><cell></cell><cell>using this system.</cell></row><row><cell>Movement</cell><cell>I know where the obstacle located.</cell></row><row><cell>Functionality</cell><cell>The system meets my expectations of what a</cell></row><row><cell></cell><cell>navigation system can do.</cell></row><row><cell>Complexity</cell><cell>I do not have to concentrate all my attention</cell></row><row><cell></cell><cell>on the subtle changes of sound.</cell></row><row><cell>Operability</cell><cell>I can get familiar with this system easily;</cell></row><row><cell></cell><cell>The audio guidance is clear.</cell></row><row><cell>Comfort</cell><cell>Wearing this device has no negative effect on</cell></row><row><cell></cell><cell>my other daily actions.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Navigation assistance for the visually impaired using RGB-D sensor with range expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Aladren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>L?pez-Nicol?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josechu</forename><forename type="middle">J</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Systems Journal</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smart guiding glasses for visually impaired people in indoor environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguo</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enhancing perception for the visually impaired with deep learning techniques and low-cost wearable sensors. Pattern Recognition Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuria</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmanuel</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Gomez-Donoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Cazorla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rapid detection of blind roads and crosswalks by using a lightweight semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengcai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchu</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computer vision for the visually impaired: the sound of vision system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simona</forename><surname>Caraiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anca</forename><surname>Morar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Owczarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Burlacu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dariusz</forename><surname>Rzeszotarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolae</forename><surname>Botezatu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Herghelegiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Florica Moldoveanu, Pawel Strumillo, and Alin Moldoveanu</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>ICCVW</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic MapNet: Building allocentric SemanticMaps and representations from egocentric views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Cartillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Panoramic annular SLAM with loop closure and global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ScanNet: richly-annotated 3D reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">V-Eye: A visionbased navigation system for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Jung</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Cheng</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Yu Fan</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Ju</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Wen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D-MPA: Multi-proposal aggregation for 3D semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Omni-supervised point cloud segmentation via gradual receptive field component reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PCT: Point cloud transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><forename type="middle">R</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">OccuSeg: Occupancy-aware 3D instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DyCo3D: Robust instance segmentation of 3D point clouds through dynamic convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D-SIS: 3D semantic instance segmentation of RGB-D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Outdoor walking guide for the visually-impaired people based on semantic segmentation and depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Hsuan</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Chu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Hsiang</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-June</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPAI, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Development of a wearable guide device based on convolutional neural network for blind or visually impaired persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zeng</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Syun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Xiong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An indoor positioning framework based on panoramic visual odometry for visually impaired people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement Science and Technology</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional projection network for cross dimension scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Panoramic panoptic segmentation: Towards complete surrounding understanding via unsupervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Jaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<idno>IV, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PointGroup: Dual-set point grouping for 3D instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Safe local navigation for visually impaired users with a timeof-flight and haptic feedback device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">K</forename><surname>Katzschmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RTAB-Map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3D instance segmentation via multi-task metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">R</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ISANA: Wearable contextaware indoor assistive navigation with obstacle avoidance for the blind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Pablo</forename><surname>Mu?oz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejian</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aries</forename><surname>Arditi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semantic scene mapping with spatiotemporal deep neural network for robotic applications. Cognitive Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbing</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huosheng</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3D instance embedding learning with a structure-aware loss function for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning based wearable assistive system for visually impaired people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguo</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MASC: Multi-scale affinity with sparse convolution for 3D instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Indoor topological localization based on a novel deep learning technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbing</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">3D-to-2D distillation for indoor scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unifying obstacle detection, recognition, and fusion based on millimeter wave radar and RGB-depth sensors for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningbo</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Scientific Instruments</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Computer vision-based assistance system for the visually impaired using mobile edge artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">T</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><forename type="middle">K</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchendra</forename><forename type="middle">M</forename><surname>Nivedha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhandarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Panoptic lintention network: Towards efficient navigational perception for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RCAR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Helping the blind to get through COVID-19: Social distancing assistant using real-time semantic segmentation on RGB-D video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Lidegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Prasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stephen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The semantic paintbrush: Interactive 3D mapping and recognition in large outdoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">PanopticFusion: Online volumetric semantic mapping at the level of stuff and things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaku</forename><surname>Narita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Seno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohsuke</forename><surname>Kaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-view semantic segmentation for sensing surroundings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankai</forename><surname>Bowen Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho Yin Tiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">MASS: Multi-attentional semantic segmentation of LiDAR data for dense top-view understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncong</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Heidenreich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Stairs detection with odometry-aided traversal from a wearable RGB-D camera. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>P?rez-Yus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Guti?rrez-G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>L?pez-Nicol?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Guerrero</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint semantic-instance segmentation of 3D point clouds with multi-task pointwise networks and multi-value conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Active crowd analysis for pandemic risk mitigation for blind or visually impaired persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samridha</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daohan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John-Ross</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">H</forename><surname>Seiple</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Porfiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Enabling independent navigation for visually impaired people through a wearable vision-based feedback system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">K</forename><surname>Katzschmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santani</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Giarr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Lightweight 3-D localization and mapping for solid-state LiDAR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An environmental perception and navigational assistance system for visually impaired persons based on semantic stixels and sound interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SMC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">SGPN: Similarity group proposal network for 3D point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Associatively segmenting instances and semantics in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Footprints and free space from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">SceneGraphFusion: Incremental 3D scene graph prediction from RGB-D sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tombari</surname></persName>
		</author>
		<idno>CVPR, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning object bounding boxes for 3D instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unifying terrain awareness through real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">Miguel</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Omnisupervised omnidirectional semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A new approach of point cloud processing and scene segmentation for guiding the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunmin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICBISP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">3-D object recognition of a robotic navigation aid for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">GSPN: Generative shape proposal network for 3D instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyuk</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Content-aware video analysis to guide visually impaired walking on the street</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ervin</forename><surname>Yohannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">K</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IVIC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Ilyes Mendili, and Soedji Ablam Edoh Barnabe. Ego-semantic labeling of scene from depth image for visually impaired and blind people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chayma</forename><surname>Zatout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slimane</forename><surname>Larabi</surname></persName>
		</author>
		<editor>IC-CVW</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Point cloud instance segmentation using probabilistic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Spatial semantic embedding network: Fast 3D instance segmentation with deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junha</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><forename type="middle">Min</forename><surname>Kim</surname></persName>
		</author>
		<idno>arXiv, 2020. 6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An indoor wayfinding system based on geometric features aided graph SLAM for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Perception framework through realtime semantic segmentation and scene recognition on a wearable system for the visually impaired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RCAR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Point transformer. arXiv, 2020. 3</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
