<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Span-based Joint Entity and Relation Extraction with Transformer Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>Marginally revised version 2</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Eberts</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
						</author>
						<title level="a" type="main">Span-based Joint Entity and Relation Extraction with Transformer Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">Marginally revised version 2</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.3233/FAIA200321)</idno>
					<note>This work was published in the proceedings of ECAI 2020 (</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce SpERT, an attention model for span-based joint entity and relation extraction. Our key contribution is a lightweight reasoning on BERT embeddings, which features entity recognition and filtering, as well as relation classification with a localized, marker-free context representation. The model is trained using strong within-sentence negative samples, which are efficiently extracted in a single BERT pass. These aspects facilitate a search over all spans in the sentence.</p><p>In ablation studies, we demonstrate the benefits of pre-training, strong negative sampling and localized context. Our model outperforms prior work by up to 2.6% F1 score on several datasets for joint entity and relation extraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transfomer networks such as BERT <ref type="bibr" target="#b7">[8]</ref>, GPT <ref type="bibr" target="#b25">[26]</ref>, Transformer-XL <ref type="bibr" target="#b6">[7]</ref>, RoBERTa <ref type="bibr" target="#b18">[19]</ref> or MASS <ref type="bibr" target="#b29">[30]</ref> have recently attracted strong attention in the NLP research community. These models use multihead self-attention as a key mechanism to capture interactions between tokens <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32]</ref>. This way, context-sensitive embeddings can be obtained that disambiguate homonyms and express semantic and syntactic patterns. Transformer networks are commonly pre-trained on large document collections using language modelling objectives. The resulting models can then be transferred to target tasks with relatively small supervised training data, resulting in state-of-the-art performance in many NLP tasks such as question answering <ref type="bibr" target="#b36">[37]</ref> or contextual emotion detection <ref type="bibr" target="#b4">[5]</ref>.</p><p>This work investigates the use of Transformer networks for relation extraction: Given a pre-defined set of target relations and a sentence such as "Leonardo DiCaprio starred in Christopher Nolan's thriller Inception", our goal is to extract triplets such as ("Leonardo DiCaprio", Plays-In, "Inception") or ("Inception", Director, "Christopher Nolan"). The task comprises of two subproblems, namely the identification of entities (entity recognition) and relations between them (relation classification). While common methods tackle the two problems separately <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref>, more recent work uses joint models for both steps <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref>. The latter approach seems promising, as on the one hand knowledge about entities (such as the fact that "Leonardo DiCaprio" is a person) is of interest when choosing a relation, while knowledge of the relation (Director) can be useful when identifying entities. We present a model for joint entity and relation extraction that utilizes the Transformer network BERT as its core. A span-based ap-proach is followed: Any token subsequence (or span) constitutes a potential entity, and a relation can hold between any pair of spans. Our model performs a full search over all these hypotheses. Unlike previous work based on BIO/BILOU labels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>, a spanbased approach can identify overlapping entities such as "codeine" within "codeine intoxication". Since Transformer models like BERT are computationally expensive, our approach conducts only a single forward pass per input sentence and performs a light-weight reasoning on the resulting embeddings. In contrast to other recent approaches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref>, our model features a much simpler downstream processing using shallow entity/relation classifiers. We use a local context representation without using particular markers, and draw negative samples from the same sentence in a single BERT pass. These aspects facilitate an efficient training and a full search over all spans. We coin our model "Span-based Entity and Relation Transformer" (SpERT) <ref type="bibr" target="#b2">3</ref> . In summary, our contributions are:</p><p>? We present a novel approach towards span-based joint entity and relation extraction. Our approach appears to be simple but effective, consistently outperforming prior work by up to 2.6% (relation extraction F1 score). ? We investigate several aspects crucial for the success of our model, showing that (1) negative samples from the same sentence yield a training that is both efficient and effective, and a sufficient number of strong negative samples appears to be vital. (2) A localized context representation is beneficial, especially for longer sentences.</p><p>(3) We also study the effects of pre-training and show that finetuning a pre-trained model yields a strong performance increase over training from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Traditionally, relation extraction is tackled by using separate models for entity detection and relation classification, whereas neural networks constitute the state of the art. Various approaches for relation classification have been investigated such as RNNs <ref type="bibr" target="#b38">[39]</ref>, recursive neural networks <ref type="bibr" target="#b28">[29]</ref> or CNNs <ref type="bibr" target="#b37">[38]</ref>. Also, Transformer models have been used for relation classification <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35]</ref>: The input text is fed once through a Transformer model and the resulting embeddings are classified. Note, however, that pre-labeled entities are assumed to be given. In contrast to this, our approach does not rely on labeled entities and jointly detects entities and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Entity and Relation Extraction</head><p>Since entity detection and relation classification may benefit from exploiting interrelated signals, models for the joint detection of entities and relations have recently drawn attention (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b15">16]</ref>). Most approaches detect entities by sequence-to-sequence learning: Each token is tagged according to the well-known BIO scheme (or its BILOU variant). Miwa and Sasaki <ref type="bibr" target="#b22">[23]</ref>  <ref type="table">tackle joint entity and relation extraction as  a table-filling problem, where each cell of the table corresponds to a  word pair of the sentence. The diagonal of the table is filled with</ref> the BILOU tag of the token itself and the off-diagonal cells with the relations between the respective token pair. Relations are predicted by mapping the entities' last words. The table is filled with relation types by minimizing a scoring function based on several features such as POS tags and entity labels. A beam search is employed to find an optimal table-filling solution. Gupta et al. <ref type="bibr" target="#b9">[10]</ref> also formulate joint entity and relation extraction as a table-filling problem. Unlike Miwa and Sasaki they employ a bidirectional recurrent neural network to label each word pair.</p><p>Miwa and Bansal <ref type="bibr" target="#b21">[22]</ref> use a stacked model for joint entity and relation extraction. First, a bidirectional sequential LSTM tags the entities according to the BILOU scheme. Second, a bidirectional tree-structured RNN operates on the dependency parse tree between an entity pair to predict the relation type. Zhou et al. <ref type="bibr" target="#b41">[42]</ref> utilize a BILOU-based combination of a bidirectional LSTM and a CNN to extract a high level feature representation of the input sentence. Since named entity extraction is only performed for the most likely relations, the approach predicts a lower number of labels compared to the table-filling approaches. Zheng et al. <ref type="bibr" target="#b40">[41]</ref> first encode input tokens with a bidirectional LSTM. Another LSTM then operates on each encoded word representation and outputs the entity boundaries (akin to BILOU scheme) alongside their relation type. Conditions where one entity is related to multiple other entities are not considered. Bekoulis et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> also employ a bidirectional LSTM to encode each word of the sentence. They use character embeddings alongside Word2Vec embeddings as input representations. Entity boundaries and tags are extracted with a Conditional Random Field (CRF). In contrast to Zheng et al. <ref type="bibr" target="#b40">[41]</ref>, Bekoulis et al. also detect cases in which a single entity is related to multiple others.</p><p>While the above approaches heavily rely on LSTMs, our approach uses an attention-based Transformer type network. The attention mechanism has also been used in joint models: Nguyen and Verspoor <ref type="bibr" target="#b23">[24]</ref> use a BiLSTM-CRF-based model for entity recognition. Token representations are shared with the relation classification task, and embeddings for BILOU entity labels are learned. In relation classification, entities interact via a bi-affine attention layer. Chi et al. <ref type="bibr" target="#b5">[6]</ref> use similar BiLSTM representations. They detect entities with BIO tags and train with an auxiliary language modeling objective. Relation classifiers attend into the BiLSTM encodings. Note, however, that neither of the two works utilize Transformer type networks.</p><p>More similar to our work is the recent approach by Li et al. <ref type="bibr" target="#b17">[18]</ref>, who also apply BERT as their core model and use a question answering setting, where entity-and relation-specific questions guide the model to head and tail entities. The model requires manually defined (pseudo-)question templates per relation, such as "find a weapon which is owned by &lt;?&gt;". Entities are detected by a relation-wise labeling with BILOU-type tags, based on BERT embeddings. In contrast to this approach, our model requires no explicit formulation of questions. Also, our approach is span-based instead of BILOU.</p><p>Span-based Approaches As BIO/BILOU-based models only assign a single tag to each token, a token cannot be part of multiple entities at the same time, such that situations with overlapping (often nested) entities cannot be covered. Think of the sentence "Ford's Chicago plant employs 4,000 workers", where both "Chicago" and "Chicago plant" are entities. Here, span-based approaches -which perform an exhaustive search over all spans and offer the fundamental benefit of covering overlapping entities -have been investigated. Applications include coreference resolution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, semantic role labeling <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b11">12]</ref>, and the improvement of language modeling by learning to predict spans instead of single words <ref type="bibr" target="#b12">[13]</ref>.</p><p>Recently, some span-based models towards joint entity and relation extraction have been proposed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9]</ref>, using span representations derived from a BiLSTM over concatenated ELMo, word and character embeddings. These representations are then shared across the downstream tasks. While Dixit and Al-Onaizan <ref type="bibr" target="#b8">[9]</ref> focus on joint entity and relation extraction, Luan et al. <ref type="bibr" target="#b19">[20]</ref> conduct a beam search over the hypothesis space, estimating which spans participate in entity classes, relations and coreferences.</p><p>Luan et al.'s follow-up model DyGIE <ref type="bibr" target="#b20">[21]</ref> adds a graph propagation step to capture the interaction of spans. A dynamic span graph is constructed, in which embeddings are propagated using a learned gated mechanism. Using this refinement of span representations, further improvements are demonstrated. More recently, Wadden et al.'s DyGIE++ <ref type="bibr" target="#b33">[34]</ref> has replaced the BiLSTM encoder with BERT. Dy-GIE++ constitutes the only Transformer-based span approach towards joint entity and relation extraction yet. In contrast to DyGIE and DyGIE++, our model utilizes a much simpler downstream processing, omitting any graph propagation and using shallow entity and relation classifiers. Instead, we found localized context representation and strong negative sampling to be of vital importance. We include a quantitative comparison with DyGIE++ in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>Our model uses a pre-trained BERT <ref type="bibr" target="#b7">[8]</ref> model as its core, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>: An input sentence is tokenized, obtaining a sequence of n byte-pair encoded (BPE) tokens <ref type="bibr" target="#b27">[28]</ref>. Byte-pair encoding represents infrequent words (such as treehouse) by common subwords (tree and house) and is utilized in BERT to limit the vocabulary size and to map out-of-vocabulary words. The BPE tokens are passed through BERT, obtaining an embedding sequence (e1, e2, ...en, c) of length n + 1 (the last token c represents a special classifier token capturing the overall sentence context). Unlike classical relation classification, our approach detects entities among all token subsequences (or spans). For example, the token sequence (we,will,rock,you) maps to the spans (we), (we,will), (will,rock,you), etc. . We classify each span into entity types (a), filter non-entities (b), and finally classify all pairs of remaining entities into relations (c).</p><p>(a) Span Classification Our span classifier takes an arbitrary candidate span as input. Let s := (ei, ei+1, ..., e i+k ) denote such a span. Also, we assume E to be a pre-defined set of entity categories such as person or organization. The span classifier maps the span s to a class out of E?{none}. none represents spans that do not constitute entities.</p><p>The span classifier is displayed in detail in the dashed box in Figure 1 (see Step (a)). Its input consists of three parts:</p><p>? The span's BERT embeddings (red) are combined using a fusion, f (ei, ei+1, ..., e i+k ). Regarding the fusion function f , we found max-pooling to work best, but will investigate other options in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERT (fine-tuned)</head><p>cls width embeddings ? Given the span width k + 1, we look-up a width embedding w k+1 (blue) from a dedicated embedding matrix, which contains a fixedsize embedding for each span width 1, 2, ... <ref type="bibr" target="#b13">[14]</ref>. These embeddings are learned by backpropagation, and allow the model to incorporate a prior over the span width (note that spans which are too long are unlikely to represent entities).</p><p>This yields the following span representation (whereas ? denotes concatenation):</p><formula xml:id="formula_0">e(s) := f (ei, ei+1, ..., e i+k ) ? w k+1 .<label>(1)</label></formula><p>Finally, we add the classifier token c <ref type="figure" target="#fig_0">(Figure 1</ref>, green), which represents the overall sentence (or context). Context forms an important source of disambiguation, as keywords (such as spouse or says) are strong indicators for entity classes (such as person). The final input to the span classifier is:</p><formula xml:id="formula_1">x s := e(s) ? c<label>(2)</label></formula><p>This input is fed into a softmax classifier:</p><formula xml:id="formula_2">y s = softmax W s ? x s + b s<label>(3)</label></formula><p>which yields a posterior for each entity class (incl. none).</p><p>(b) Span Filtering By looking at the highest-scored class, the span classifier's output (Equation 3) estimates which class each span belongs to. We use a simple approach and filter all spans assigned to the none class, leaving a set of spans S which supposedly constitute entities. Note that -unlike prior work [23, 20] -we do not perform a beam search over the entity/relation hypotheses. We pre-filter spans longer than 10 tokens, limiting the cost of span classification to O(n).</p><p>(c) Relation Classification Let R be a set of pre-defined relation classes. The relation classifier processes each candidate pair (s1, s2) of entities drawn from S?S and estimates if any relation from R holds. The input to the classifier consists of two parts:</p><p>1. To represent the two entity candidates s1, s2, we use the fused BERT/width embeddings e(s1), e(s2) (Eq. 1). 2. Obviously, words from the context such as spouse or president are important indicators of the expressed relation. One possible context representation would be the classifier token c. However, we found c to be unsuitable for long sentences expressing a multitude of relations. Instead, we use a more localized context drawn from the direct surrounding of the entities: Given the span ranging from the end of the first entity to the beginning of the second entity <ref type="figure" target="#fig_0">(Figure 1</ref>, yellow), we combine its BERT embeddings by max-pooling, obtaining a context representation c(s1, s2). If the range is empty (e.g., in case of overlapping entities), we set c(s1, s2) = 0.</p><p>Just like for the span classifier, the input to the relation classifier is obtained by concatenating the above features. Note that -since relations are asymmetric in general -we need to classify both (s1, s2) and (s2, s1), i.e. the input becomes</p><formula xml:id="formula_3">x r 1 := e(s1) ? c(s1, s2) ? e(s2) x r 2 := e(s2) ? c(s1, s2) ? e(s1)</formula><p>.</p><p>Both x r 1 and x r 2 are passed through a single-layer classifier:</p><formula xml:id="formula_4">y r 1/2 := ? W r ? x r 1/2 + b r<label>(4)</label></formula><p>where ? denotes a sigmoid of size #R. Any high response in the sigmoid layer indicates that the corresponding relation holds between s1 and s2. Given a confidence threshold ?, any relation with a score ?? is considered activated. If none is activated, the sentence is assumed to express no known relation between the two entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training</head><p>We learn the size embeddings w <ref type="figure" target="#fig_0">(Figure 1</ref>, blue) as well as the span/relation classifiers' parameters (W s , b s , W r , b r ) and fine-tune BERT in the process. Our training is supervised: Given sentences with annotated entities (including their entity types) and relations, we define a joint loss function for entity classification and relation classification:</p><formula xml:id="formula_5">L = L s + L r ,</formula><p>whereas L s denotes the span classifier's loss (cross-entropy over the entity classes including none) and L r denotes the binary crossentropy over relation classes. Both losses are averaged over each batches' samples. No class weights are applied. A training batch consists of B sentences, from which we draw samples for both classifiers:</p><p>? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We compare SpERT with other joint entity/relation extraction models and investigate the influence of several hyperparameters. The evaluation is conducted on three publicly available datasets:  <ref type="bibr" target="#b19">[20]</ref>.</p><p>? ADE: The ADE dataset <ref type="bibr" target="#b10">[11]</ref> consists of 4, 272 sentences and 6, 821 relations extracted from medical reports that describe the adverse effects arising from drug use. It contains a single relation type Adverse-Effect and the two entity types Adverse-Effect and Drug. As in previous work, we conduct a 10-fold cross validation.</p><p>We evaluate SpERT on both entity recognition and relation extraction. An entity is considered correct if its predicted span and entity label match the ground truth. A relation is considered correct if its relation type as well as the two related entities are both correct (in span and type). Only for SciERC, entity type correctness is not considered when evaluating relation extraction, which is in line with prior work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>. Following previous work, we measure the precision, recall and F1 score for entities and relations, and report microaveraged values for the SciERC dataset. On CoNLL04 and ADE 4 , some prior work does not explicitly state if scores where micro-or macro-averaged over types, which is why we report both metrics for future reference. For ADE, the metrics are averaged over the folds.</p><p>For most of our experiments we use the BERTBASE (cased) model 5 as a sentence encoder, pre-trained on English language <ref type="bibr" target="#b7">[8]</ref>. On the SciERC dataset, just like DyGIE++ <ref type="bibr" target="#b33">[34]</ref>, we replace BERT with SciBERT (cased) <ref type="bibr" target="#b3">[4]</ref>, a BERT model pre-trained on a large corpus of scientific papers. The weights of BERT (or SciBERT) are updated during the training process. We initialize our classifiers' weights with normally distributed random numbers (?=0, ?=0.02). We use the Adam Optimizer with a linear warmup and linear decay learning rate schedule and a peak learning rate of 5e?5, a dropout before the entity and relation classifier with a rate of 0.1 (both according to <ref type="bibr" target="#b7">[8]</ref>), a batch size of B=2, and width embeddings w of 25 dimensions. No further optimizations were conducted on those parameters. We choose the number of epochs <ref type="bibr" target="#b19">(20)</ref>, the relation filtering threshold (? = 0.4), as well as the number of negative entity and relation samples per sentence (Ne=Nr=100) based on the CoNLL04 development set. We do not specifically tune our model for the other two datasets but use the same hyperparameters instead. <ref type="table">Table 1</ref> shows the test set evaluation results for the three datasets. We report the average over 5 runs for each dataset except ADE. SpERT consistently outperforms the state-of-the-art for both entity and relation extraction on all datasets. While entity recognition performance increased for all datasets, e.g. by 1.1% (CoNLL04) and 2.8% (Sci-ERC) F1 respectively, we observe even stronger performance increases in relation extraction: Compared to Li et al. <ref type="bibr" target="#b17">[18]</ref> ("Multiturn QA" in <ref type="table">Table 1</ref>), who also rely on BERT as a sentence encoder but use a BILOU approach for entity extraction, our model improves the state-of-the-art on the CoNLL04 dataset by 2.6% (micro) F1. On the challenging and domain-specific SciERC dataset, SpERT outperforms the DyGIE++ model of Wadden et al. <ref type="bibr" target="#b33">[34]</ref> by about 2.4% using SciBERT as a sentence encoder. When BERT is used instead, the performance drops by 4.4%, confirming that in-domain language model pre-training is beneficial, which is in line with findings of Wadden et al. <ref type="bibr" target="#b33">[34]</ref>. While for SciERC previous work does not consider entity types for relation extraction, we report these values as a reference for future work (40.51 precision, 36.82 recall, 38.57 F1 using SciBERT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with state of the art</head><p>On the ADE dataset, SpERT achieves an improvement of about 2% (SpERT (without overlap) in <ref type="table">Table 1</ref> "Relation-Metric" model by Tran and Kavuluru <ref type="bibr" target="#b30">[31]</ref>. Note that ADE also contains 120 instances of relations with overlapping entities, which can be discovered by span-based approaches like SpERT (in contrast to BILOU-based models). These have been filtered in prior work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref>. As a reference for future work on overlapping entity recognition, we also present results on the full dataset (including the overlapping entities). When including this additional challenge, our model performs only marginally worse (?0.4%) compared to not considering overlapping entities. Out of the 120 relations with overlapping entities, 65 were detected correctly (?54%). Examples of relations between overlapping entities correctly predicted by SpERT are included in <ref type="table" target="#tab_3">Table 4</ref> (top).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Candidate selection and negative sampling</head><p>We also study the effect of the number and sampling of negative training examples. <ref type="figure" target="#fig_2">Figure 2</ref> shows the F1 score (relations and entities) for the CoNLL04 and SciERC development sets, plotted against the number of negative samples Ne/Nr per sentence. We see that a sufficient number of negative samples is essential: When using only a single negative entity and relation (Ne=Nr=1) per sentence, relation F1 is about 10.5% (CoNLL04) and 9.7% (SciERC). With a high number of negative samples, the performance stagnates for both datasets. However, we found our results to be more stable when using a sufficiently high Ne and Nr (we chose Ne=Nr=100 in all other experiments).</p><p>For relation classification, we also assess the effect of using weak instead of strong negative relation samples: Instead of using the entity classifier as a filter for entity candidates S and drawing strong negative training samples from S?S, we omit span filtering and sample random training span pairs not matching any ground truth relation. With these weak samples, our model retains a high recall (84.4%) on the CoNLL04 development set, but the precision decreases drastically to about 4.3%. We observed that the model tends to predict subspans of entities to be in relation when using weak samples: For example, in the sentence "[John Wilkes Booth]head, who assassinated [President Lincoln]tail, was an actor", the pairs ("John", "President") or ("Wilkes", "Lincoln") are chosen. Additionally, pairs where one entity is correct and the other one incorrect are also favored by the model. Obviously, span filtering is not only beneficial in terms of training and evaluation speed, but is also vital for accurate localization in SpERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Localized context</head><p>Despite advances in detecting long distance relations using LSTMs or the attention mechanism, the noise induced with increasing context remains a challenge. By using a localized context, i.e. the context between entity candidates, the relation classifier can focus on the sentence's section that is often most discriminative for the relation type. To assess this effect, we compare localized context with two other context representations that use the whole sentence: We evaluate the three options on the CoNLL04 development set ( <ref type="figure" target="#fig_4">Figure 3</ref>): When employing SpERT with a localized context, the model reaches an F1 score of 71.0%, which significantly outperforms a max pooling over the whole sentence (65.8%) and using the classifier token (63.9%). <ref type="figure" target="#fig_4">Figure 3</ref> also displays results with respect to the sentence length: We split the CoNLL04 development set into four different parts, namely sentences with &lt;20, 20 ? 34, 35 ? 50 and &gt;50 tokens. Obviously, localized context leads to comparable or better results for all sentence lengths, particularly for very long sentences: Here, it reaches an F1 score of 57.3%, while the performance drastically decreases to 44.9/38.5% when using the other options. <ref type="table" target="#tab_3">Table 4</ref> (middle) shows an example of a long sentence with multiple entities: By using a localized context the model correctly predicts the three Located-In relations, while relying on the full context leads to many false positive relations such as ("Jackson", Located-In, "Colo.") or ("Wyo.", Located-In, "McAllen"). This shows that guiding the model towards relevant sections of the input sentence is vital. An interesting direction for future work is to learn the relevant context with respect to the entity candidates, and to incorporate precomputed syntactical information into SpERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Pre-training and entity representation</head><p>Next, we assess the effect of BERT's language modeling pre-training. It seems intuitive that pre-training on large-scale datasets helps the model to learn semantic and syntactic relations that are hard to capture on a limited-scale target dataset. Therefore, we test three variants of pre-training:</p><p>1. Full: We use the fully pre-trained BERT model (LM Pre-trained, our default setting). 2. -Layers: We retain pre-trained token embeddings but train the layers from scratch (using the default initalization <ref type="bibr" target="#b7">[8]</ref>). 3. -Layers,Embeddings: We train layers and token embeddings from scratch (again, using the default initialization).</p><p>As <ref type="table">Table 2</ref> shows, training the BERT layers from scratch results in a performance drop of about 17.0% and 29.4% (macro) F1 for entity and relation extraction respectively.</p><p>Further, training the token embeddings from scratch results in an even stronger drop in F1. These results suggest that pre-training a large network like BERT is challenging on the fairly small joint entity and relation extraction datasets. Therefore, language modeling  pre-training is vital for generalization and to obtain a competitive performance.</p><p>Finally, we investigate different options for the entity span representation e(s) other than conducting a max pooling over the entity's tokens, namely a sum and average pooling (note that a size embedding and a context representation is again concatenated to obtain the final entity representation (Equation 1)). <ref type="table">Table 3</ref> shows the CoNLL04 (macro) F1 with respect to the different entity representations: We found the averaging of the entity tokens to be unsuitable for both entity (69.2%) and relation extraction (44.8%). Sum pooling improves the performance to 80.3/68.2%. Max pooling, however, outperforms this by another increase of 3.8% and 2.8% respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Error inspection</head><p>Although SpERT yields strong results on joint entity and relation extraction, we observed several common errors which leave room for further research.  or missing a word annotated in the ground truth. This error occurs especially often in the domain specific ADE and SciERC datasets. ? Syntax: Another frequently encountered error is the prediction of a relation (here: Work-For) between two entities, which could possibly be related based on their entity types ("Yevhen Saburov", a person, and "Black Sea Fleet", an employer), but are not related in the sentence. ? Logical: Sometimes, a relation is not explicitly stated in the sentence, but can logically be inferred based on the context. In the depicted case, it is not stated that "Linda Schmitt" is indeed a product manager of "Becton Dickinson", but it is obvious due to her speaking for the company. ? Classification: In some rare cases (especially in the SciERC dataset), SpERT correctly predicted two related entities, but assigned a wrong relation type. ? Missing annotation: Finally, there are also some cases where a correct prediction is missing in the ground truth. Here, in addition to correctly predicting ("Norton Winfried Simon", Live-In, "Portland, Ore."), SpERT also outputs ("Norton Winfried Simon", Live-In, "San Francisco"), which is correct but not labeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have presented SpERT, a span-based model for joint entity and relation extraction that relies on the pre-trained Transformer network BERT as its core. We show that with strong negative sampling, span filtering, and a localized context representation, a search over all spans in an input sentence becomes feasible. Our results suggest that span-based approaches perform competitive to BILOU-based models and may be the more promising approach for future research due to their ability to identify overlapping entities.</p><p>In the future, we plan to investigate more elaborate forms of con-text for relation classifiers. Currently, our model simply employs the span between the two entities, which proved superior to the full context. Employing additional syntactic features or learned contextwhile maintaining an efficient exhaustive search -appears to be a promising challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Our approach towards joint entity and relation extraction SpERT first passes a token sequence through BERT. Then, (a) all spans within the sentence are classified into entity types, as illustrated for three sample spans s 1 , s 2 , s 3 (red). (b) Spans classified as non-entites (here, s 1 ) are filtered. (c) All pairs of remaining entities (here, (s 2 , s 3 )) are combined with their context (the span between the entities, yellow) and classified into relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For the span classifier, we utilize all labeled entities S gt as positive samples, plus a fixed number Ne of random non-entity spans as negative samples. For example, given the sentence "In 1913, Olympic legend [Jesse Owens]People was born in [Oakville, Alabama]Location." we draw negative samples such as "Owens" or "born in".? To train the relation classifier, we use ground truth relations as positive samples, and draw Nr negative samples from those entity pairs S gt ?S gt that are not labeled with any relation. For example, given a sentence with the two relations ("Marge", Mother, "Bart") and ("Bart", Teacher, "Skinner"), the unconnected entity pair ("Marge", *, "Skinner") constitutes a negative sample for any relation. We found such strong negative samples -in contrast to sampling random span pairs -to be of vital importance.Note that the above process samples training examples per sentence:Instead of generating samples scattered over multiple sentenceswhich would require us to feed all those sentences through the deep and computationally expensive BERT model -we run each sentence only once through BERT (single-pass). This way, multiple positive/negative samples pass a single shallow linear layer for the entity and relation classifier respectively, which speeds-up the training process substantially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The accuracy of entity and relation classification (F1 on CoNLL04 and SciERC development set) increases significantly with the number of negative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Macro F1 scores of relation classification on the CoNLL04 development set when using different context representations. Localized context (red) performs best overall (left), particularly on long sentences with &gt;50 tokens (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Table 4 .</head><label>4</label><figDesc>SpERT relation extraction examples showing that (a) as a span-based approach, our model can deal with overlapping entities, and (b) localized context yields better precision for long sentences compared to using the full sentence as context. (c) showcases various common sources of error. green [*] = true positive relation, blue [*] = false positive relation, red [*] = false negative relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Effect of BERT pre-training on entity and relation extraction (CoNLL04 development set). A fully pre-trained BERT model significantly outperforms two BERTs in which the self-attention layers (-Layers) or the layers and the BPE input token embeddings (-Layers,Embeddings) are trained from scratch. Investigation of different entity span representations e(s) (summing and averaging of entity's tokens) on the CoNLL04 development set.</figDesc><table><row><cell>Pre-training</cell><cell cols="3">Entity F1 Relation F1</cell></row><row><cell>Full</cell><cell></cell><cell>84.04</cell><cell>70.98</cell></row><row><cell>-Layers</cell><cell></cell><cell>67.06</cell><cell>41.58</cell></row><row><cell cols="2">-Layers,Embeddings</cell><cell>50.84</cell><cell>25.22</cell></row><row><cell>Pooling</cell><cell cols="3">Entity F1 Relation F1</cell></row><row><cell>Max</cell><cell>84.04</cell><cell>70.98</cell></row><row><cell>Sum</cell><cell>80.26</cell><cell>68.16</cell></row><row><cell>Average</cell><cell>69.21</cell><cell>44.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>(bottom) contains examples of five error cases we found to be common in the evaluated datasets:? Incorrect spans: One common source of error is the prediction of slightly incorrect entity spans, e.g. by adding a nearby word(a) Examples of Overlapping Entities Six days after starting acyclovir she exhibited signs of [[lithium] toxicity]. A diagnosis of masked [[theophylline] poisoning] should be considered in similar situations involving a rapid decrease of insulin requirements. Dickinson] sells needle containers to doctors and hospitals but may develop a container for home use, said [Linda Schmitt], an assistant product manager.</figDesc><table><row><cell cols="2">(b) Effect of Localized Context</cell></row><row><cell>localized context</cell><cell>Temperatures around the nation at 2 a.m. EST ranged from 2 degrees at [Jackson]1, [Wyo.]1, and [Gunnison]2,</cell></row><row><cell></cell><cell>[Colo.]2, to 89 degrees at [McAllen]3, [Texas]3.</cell></row><row><cell>full context</cell><cell>Temperatures around the nation at 2 a.m. EST ranged from 2 degrees at [[Jackson]1]2, [Wyo.]3, and [Gunnison]4,</cell></row><row><cell></cell><cell>[[Colo.]4]1, to 89 degrees at [[McAllen]5]3, [[Texas]5]2.</cell></row><row><cell>(c) Error Cases</cell><cell></cell></row><row><cell>incorrect spans</cell><cell>[Delayed [bowel injury]] is an infrequently observed complication of [[chromic phosphate]] administration.</cell></row><row><cell>syntax</cell><cell>Ambassador Miller is also scheduled to meet with Crimean Deputy [Yevhen Saburov] and [[Black Sea Fleet]]</cell></row><row><cell></cell><cell>Commander [Eduard Baltin].</cell></row><row><cell>logical</cell><cell>[Becton</cell></row></table><note>classification Finally, we briefly describe an experiment which we have done in extending the [[n-best speech / language inte- gration architecture]rel:Used-For]rel:Evaluate-For to improving [[OCR accuracy]rel:Used-For]rel:Evaluate-For.missing annotation [[Norton Winfred Simon]] was born on Feb. 5, 1907, in [Portland, Ore.], and spent his teenage years in [San Francisco] .</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">RheinMain University of Applied Sciences, Germany, {markus.eberts, adrian.ulges}@hs-rm.de 2 Because of new insights into evaluation metrics used in related work, we updated Table 1 and report both micro/macro averaged entity values for the ADE dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The code for reproducing our results is available at https://github.com/markus-eberts/spert.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">for ADE, the relation performance is not affected by the average method since the dataset contains only one relation type. 5 using 12 layers, 768-dimensional embeddings, 12 heads per layer, resulting in a total 110M parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? Full context: Instead of performing a max pooling over the context between entity candidates, a max pooling over all tokens in the sentence is conducted. ? Cls token: Just like in the entity classifier(Figure 1, green), we use a special classifier token as context, which is able to attend to the whole sentence.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was funded by German Federal Ministry of Education and Research (Program FHprofUnt, Project DeepCA (13FH011PX6)).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="2830" to="2836" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint entity recognition and relation extraction as a multi-head selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2019 Task 3: EmoContext Contextual Emotion Detection in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedhar</forename><surname>Nath Narahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghana</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>of the 13th International Workshop on Semantic Evaluation<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhancing Joint Entity and Relation Extraction with Language Modeling and Hierarchical Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjun</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. APWeb-WAIM</title>
		<meeting>APWeb-WAIM</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11641</biblScope>
			<biblScope unit="page" from="314" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&apos;, CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT 2019</title>
		<meeting>of NAACL-HLT 2019<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Span-level model for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpit</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="5308" to="5314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Table Filling Multi-Task Recurrent Neural Network for Joint Entity and Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2016 Organizing Committee</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
	<note>Proc. of COLING 2016</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Development of a Benchmark Corpus to Support the Automatic Extraction of Drugrelated Adverse Effects from Medical Case Reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><forename type="middle">Mateen</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="885" to="892" />
			<date type="published" when="2012-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="364" to="369" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Endto-end Neural Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2017</title>
		<meeting>of EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Higher-Order Coreference Resolution with Coarse-to-Fine Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT 2018</title>
		<meeting>of NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A neural joint model for entity and relation extraction from biomedical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint Models for Extracting Adverse Drug Events from Biomedical Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI 2016, IJCAI&apos;16</title>
		<meeting>of IJCAI 2016, IJCAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2838" to="2844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Entity-Relation Extraction as Multi-Turn Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2019</title>
		<meeting>of ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1340" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename></persName>
		</author>
		<title level="m">Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2018</title>
		<meeting>of EMNLP 2018<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A General Framework for Information Extraction using Dynamic Span Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT 2019</title>
		<meeting>of NAACL-HLT 2019<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2016</title>
		<meeting>of ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling Joint Entity and Relation Extraction with Table Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction using deep biaffine attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECIR 2019</title>
		<meeting>of ECIR 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Span Selection Model for Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="1630" to="1642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Linear Programming Formulation for Global Inference in Natural Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL 2004 at HLT-NAACL 2004</title>
		<meeting>of CoNLL 2004 at HLT-NAACL 2004<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2004-05-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic Compositionality Through Recursive Matrixvector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL 2012</title>
		<meeting>of EMNLP-CoNLL 2012<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">MASS: Masked Sequence to Sequence Pre-training for Language Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1905.02450</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural Metric Learning for Fast End-to-End Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Kavuluru</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-HLT 2018</title>
		<meeting>of ACL-HLT 2018</meeting>
		<imprint>
			<date type="published" when="2018-01" />
			<biblScope unit="page" from="872" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Entity, Relation, and Event Extraction with Contextualized Span Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>abs/1909.03546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saloni</forename><surname>Potdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2019</title>
		<meeting>of ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1371" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Survey on Recent Advances in Named Entity Recognition from Deep Learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th International Conference on Computational Linguistics</title>
		<meeting>of the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="2145" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-End Open-Domain Question Answering with BERTserini</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aileen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL 2019 (Demonstrations)</title>
		<meeting>of NAACL 2019 (Demonstrations)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Relation Classification via Convolutional Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and ACL</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Relation Classification via Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1508.01006</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-End Neural Relation Extraction with Global Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP 2017</title>
		<meeting>of EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL 2017</title>
		<meeting>of ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Joint Extraction of Multiple Relations and Entities by Using a Hybrid Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CCL 2017</title>
		<meeting>of CCL 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
