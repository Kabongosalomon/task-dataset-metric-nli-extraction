<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised novelty detection using ensembles with regularized disagreement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-14">March 14, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>ifrea</roleName><forename type="first">Alexandru</forename><forename type="middle">T</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Stavarache</surname></persName>
							<email>ericst@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Yang</surname></persName>
							<email>fan.yang@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised novelty detection using ensembles with regularized disagreement</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-14">March 14, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks often predict samples with high confidence even when they come from unseen classes and should instead be flagged for expert evaluation. Current novelty detection algorithms cannot reliably identify such near OOD points unless they have access to labeled data that is similar to these novel samples. In this paper, we develop a new ensemble-based procedure for semi-supervised novelty detection (SSND) that successfully leverages a mixture of unlabeled ID and novel-class samples to achieve good detection performance. In particular, we show how to achieve disagreement only on OOD data using early stopping regularization. While we prove this fact for a simple data distribution, our extensive experiments suggest that it holds true for more complex scenarios: our approach significantly outperforms state-of-the-art SSND methods on standard image data sets (SVHN/CIFAR-10/CIFAR-100) and medical image data sets with only a negligible increase in computation cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Novelty detection is challenging since X-rays of novel diseases are remarkably similar to known conditions. The unlabeled batch of inference-time data can be used to adapt a semi-supervised novelty detection approach to emerging novel diseases.</p><p>While the experts are examining the peculiar X-rays over the course of the next week, the novelty detection model helps to collect more instances of the same new condition and can request human review for these patients. The human experts can then label these images and include them in the labeled training set to update both the diagnostic prediction and the novelty detection systems. This process repeats each week and enables both diagnostic and novelty detection models to adjust to new emerging diseases.</p><p>Note that, in this example, the novelties are a particular kind of out-of-distribution samples with two properties. First, several novel-class samples may appear in the unlabeled batch at the end of a week, e.g. a contagious disease will lead to several people in a small area to be infected. This situation is different from cases when outliers are assumed to be singular, e.g. anomaly detection problems. Second, the novel-class samples share many features in common with the ID data, and only differ from known classes in certain minute details. For instance, both ID and OOD samples are frontal chest X-rays, with the OOD samples showing distinctive signs of a pneumonia caused by a new virus. In what follows, we use the terms novelty detection and OOD samples to refer to data with these characteristics.</p><p>Automated diagnostic prediction systems (Task I) can already often have satisfactory performance <ref type="bibr" target="#b4">[5]</ref>. In contrast, novelty detection (Task II) still poses a challenging problem in these scenarios. Many prior approaches can be used for semi-supervised novelty detection (SSND), when a batch of unlabeled data that may contain OOD samples is available, like in <ref type="figure">Figure 1</ref>. 1 However, all of these methods fail to detect novel-class data when used with complex models, like neural networks.</p><p>Despite showing great success on simple benchmarks like SVHN vs CIFAR10, SOTA unsupervised OOD detection methods perform poorly on near OOD data <ref type="bibr" target="#b50">[51]</ref> where OOD inputs are similar to the training samples. Furthermore, even though unlabeled data can benefit novelty detection <ref type="bibr" target="#b44">[45]</ref>, existing SSND methods for deep neural networks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b53">54]</ref> cannot improve upon unsupervised methods on near OOD data sets. Even methods that violate fundamental OOD detection assumptions by using known test OOD data for hyperparameter tuning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54]</ref> fail to work on challenging novelty detection tasks. Finally, large pretrained models seem to solve near OOD detection <ref type="bibr" target="#b10">[11]</ref>, but they only work for extremely specific OOD data sets (see Section 5 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This situation naturally raises the following question:</head><p>Can we improve semi-supervised novelty detection for neural networks?</p><p>In this paper, we introduce a new method that successfully leverages unlabeled data to obtain diverse ensembles for novelty detection. Our contributions are as follows:</p><p>? We propose to find Ensembles with Regularized Disagreement (ERD), that is, disagreement only on OOD data. Our algorithm produces ensembles just diverse enough to be used for novelty detection with a disagreement test statistic (Section 2).</p><p>? We prove that training with early stopping leads to regularized disagreement, for data that satisfies certain simplifying assumptions (Section 3).</p><p>? We show experimentally that ERD significantly outperforms existing methods on novelty detection tasks derived from standard image data sets, as well as on medical image benchmarks (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed method</head><p>In this section we first introduce our proposed method to obtain Ensembles with Regularized Disagreement (ERD) and describe how they can be used for novelty detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training ensembles with regularized disagreement (ERD)</head><p>Recall from <ref type="figure">Figure 1</ref> that we have access to both a labeled training set S = {(x i , y i )} n i=1 ? P , with covariates x i ? X ID and discrete labels y i ? Y, and an unlabeled set U , which contains both ID and unknown OOD samples. Moreover, we initialize the models of the ensemble using the weights of a predictor with good in-distribution performance, pretrained on S. In the scenarios we consider, such a well-performing pretrained classifier is readily available, as it solves Task I in <ref type="figure">Figure 1</ref>.</p><p>The entire training procedure is described in Algorithm 1. For training a single model in the ensemble, we assign a label c ? Y to all the unlabeled points in U , resulting in the c-labeled set that we denote as (U, c) := {(x, c) : x ? U }. We then fine-tune a classifier f c on the union S ? (U, c) of the correctly-labeled training set S, and the unlabeled set (U, c). In particular, we choose an early stopping time at which validation accuracy is high and training error on S ? (U, c) is low. We create a diverse ensemble of K classifiers f c by choosing a different artificial label c ? Y for every model.</p><p>Intuitively, encouraging each model in the ensemble to fit different labels to the unlabeled set U promotes disagreement, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In the next sections, we elaborate on how to use diverse ensembles for novelty detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ensemble disagreement for novelty detection</head><p>We now discuss how we can use ensembles with disagreement to detect OOD samples and why the right amount of diversity is crucial. Note that we can cast the novelty detection problem as a hypothesis test with the null hypothesis H 0 : x ? X ID .</p><p>As usual, we test the null hypothesis by comparing a test statistic with a threshold t 0 : The null hypothesis is rejected and we report x as OOD (positive) if the test statistic is larger than t 0 (Section 4.3 elaborates on the choice of t 0 ). In particular, we use as test statistic the following disagreement score, which computes the average distance between the softmax outputs of the K models in the ensemble:</p><formula xml:id="formula_0">(Avg ? ?)(f 1 (x), ..., f K (x)) := 2 i =j ? (f i (x), f j (x)) K(K ? 1) ,</formula><p>where ? is a measure of disagreement between the softmax outputs of two predictors, for example the total variation distance ? TV (f i (x), f j (x)) = 1 2 f i (x) ? f j (x) 1 used in our experiments 2 . We provide a thorough discussion on the soundness of this test statistic for disagreeing models and compare it with previous metrics in Appendix B.</p><p>Even though previous work like <ref type="bibr" target="#b53">[54]</ref> used a similar disagreement score, their detection performance is notably worse. The reason lies in the lack of diversity in their trained ensemble (see <ref type="figure" target="#fig_8">Figure 8a</ref> in Appendix B). On the other hand Algorithm 1 without early stopping would lead to a too diverse ensemble, that also disagrees on ID points, and hence, has a high false positive rate (see Appendix K).</p><p>In the next section, we explain why novelty detection with this test statistic crucially relies on the right amount of ensemble diversity and how ensembles may achieve this goal if they are trained to have regularized disagreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Desired ensemble diversity via regularized disagreement</head><p>For simplicity of illustration, let us first assume a training set with binary labels and a semisupervised novelty detection setting as depicted in <ref type="figure" target="#fig_0">Figure 2</ref> a). For an ensemble with two models, like in <ref type="figure" target="#fig_0">Figure 2</ref> b), the model predictions agree on the blue and red areas and disagree on the gray area depicted in <ref type="figure" target="#fig_0">Figure 2</ref> c). Note that the two models in <ref type="figure" target="#fig_0">Figure 2</ref> are just diverse enough to obtain both high power (flag true OOD as OOD) and low false positive rate (avoid flagging true ID as OOD) at the same time.</p><p>Previous methods that try to leverage unlabeled data to obtain more diverse ensembles either do not work with deep neural networks <ref type="bibr">[3,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b20">21]</ref> or do not disagree enough on OOD data <ref type="bibr" target="#b53">[54]</ref>, leading to subpar novelty detection performance (see <ref type="figure" target="#fig_8">Figure 8a</ref> in Appendix B).</p><p>To obtain the right amount of diversity, it is crucial to train ensembles with regularized disagreement on the unlabeled set: The models should disagree on the unlabeled OOD samples, but agree on the unlabeled ID points <ref type="figure" target="#fig_1">(Figure 3c</ref>). Thus, we avoid having too little disagreement as in <ref type="figure" target="#fig_1">Figure 3a</ref>), which results in low power, or too much diversity, resulting in high false positive rate as in <ref type="figure" target="#fig_1">Figure 3b</ref>). In particular, if models f c predict the correct label on ID points and the label c on OOD data, we can effectively use disagreement to detect novel-class samples. Since classifiers with good ID generalization need to be smooth, we expect the model predictions on holdout OOD data from the same distributions to be in line with the predictions on the unlabeled set.</p><p>In Section 3 we argue that the training procedure in Algorithm 1 successfully induces regularized disagreement and prove it in a synthetic setting. Our experiments in Section 4 further corroborate our theoretical statements. Finally, we note that one could also use other regularization techniques like dropout or weight decay. However, running a grid search to select the right hyperparameters can be more computationally expensive than simply using one run of the training process to select the optimal stopping time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Provable regularized disagreement via early stopping</head><p>In this section, we show how using early stopping in Algorithm 1 prevents fitting the incorrect artificial label on the unlabeled ID samples. Albeit for a simplified setting, this result provides a rigorous proof of concept and intuition for why ERD ensembles achieve the right amount of diversity necessary for good novelty detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary definitions</head><p>We first introduce necessary definitions to prepare the mathematical statement. Recall that in our approach, in addition to the correct labels of the ID training set S, each member of the ensemble tries to fit one label c to the entire unlabeled set U that can be further partitioned into</p><formula xml:id="formula_1">(U, c) = (U ID , c) ? (U OOD , c) = {(x, c) : x ? U ID } ? {(x, c) : x ? U OOD },</formula><p>where U ID := U ? X ID and U OOD := U \ U ID . Moreover, assuming that the label of an ID input x is deterministically given by y * (x), we can partition the set (U ID , c) (see <ref type="figure" target="#fig_1">Figure 3b</ref>) into a subset of effectively "correctly labeled" samples (U c ID , c) and "incorrectly labeled" samples (U ?c ID , c):</p><formula xml:id="formula_2">(U ?c ID , c) := {(x, c) : x ? U ID with y * (x) = c} (U c ID , c) := {(x, c) : x ? U ID with y * (x) = c}. Note that (U ?c ID , c)</formula><p>can be viewed as the subset of noisy samples from the entire training set S ? (U, c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Main result</head><p>We now prove that there exists indeed an optimal stopping time at which a two-layer neural network trained with gradient descent does not fit the incorrectly labeled subset (U ?c ID , c), under mild distributional assumptions.</p><p>For the formal statement, we assume that the artificially labeled set S ? (U, c) is clusterable, i.e. the points can be grouped in K clusters of similar sizes. Each class may comprise several clusters, but every cluster contains only samples from one class. Any cluster may include at most a fraction ? ? [0, 1] of samples with label noise, e.g. (U ?c ID , c). We denote by c 1 , ..., c K the cluster centers and define the matrix C := [c 1 , ..., c K ] T ? R K?d . Further, let ? N N C be a measure of how well a two-layer neural network can separate the cluster centers (? N N C = 0 if c i = c j for some i, j ? K). Under these assumptions we have the following:  The precise assumptions for the proposition can be found in Appendix A. On a high level, the reasoning follows from two simple insights: 1. When the artificial label is not equal to the true label, the ID samples in the unlabeled set can be seen as noisy samples in the set S ? (U, c). 2. It is well known that early stopping prevents models from fitting incorrect labels since noisy samples with incorrect labels are often fit later during training (see e.g. theoretical and empirical evidence here <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b32">33]</ref>). In particular, our proof heavily relies on Theorem 2.2 of <ref type="bibr" target="#b28">[29]</ref> which shows that early stopped predictors are robust to label noise. Proposition 3.1 gives a flavor of the theoretical guarantees that ERD enjoys. Albeit simple, the clusterable data model actually includes data with non-linear decision boundaries. On the other hand, the requirement that the clusters are balanced seems rather restrictive. In our experiments we show that this condition is in fact more stringent than it should. In particular, our method still works when the number of OOD samples |U OOD | is considerably smaller than the number of ID samples from any given class, as we show in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Choosing the early stopping time</head><p>In practice, we avoid computing the exact value of T by using instead a heuristic for picking the early stopping iteration with the highest validation accuracy (indicated by the vertical line in <ref type="figure" target="#fig_3">Figure 4</ref>). As shown in the figure, the model fits the noisy training points, i.e. (U ?c ID , c), late during fine-tuning, which causes the validation accuracy to decrease, since the model will also predict the incorrect label c on some validation ID samples. In Appendix J we show that the trend in <ref type="figure" target="#fig_3">Figure 4</ref> is consistent across data sets. <ref type="table">Table 1</ref>. AUROC and TNR@95 for ERD and various baselines (we highlight the best baseline). Numbers in square brackets indicate the ID/OOD classes. Asterisks mark methods proposed in this paper. Mahal, nnPU and MCD ( ? ) use oracle information about the OOD data. Repeated runs of ERD show a small variance ? 2 &lt; 0.01 in the detection metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other settings SSND</head><formula xml:id="formula_3">ID data OOD data Vanilla Ensembles Gram DPN OE Mahal. ? nnPU ? MCD ? Mahal-U Bin.</formula><p>Classif. * ERD *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data sets</head><p>Our experiments focus on novel-class detection scenarios where the ID and OOD data share many similar features and only differ in a few characteristics. We use standard image data sets (e.g. CIFAR10/CIFAR100) and consider half of the classes as ID, and the other half as novel. We also assess ERD's performance on a medical image benchmark <ref type="bibr" target="#b5">[6]</ref>, where near OOD data consists of novel unseen diseases (e.g. X-rays of the same body part from patients with different conditions; see Appendix E for details). Further, we also include far OOD data sets (e.g. CIFAR10/CIFAR100 vs SVHN) for completeness.</p><p>For all scenarios, we used a labeled training set (e.g. 40K samples for CIFAR10), a validation set with ID samples (e.g. 10K samples for CIFAR10) and an unlabeled test set where half of the samples are ID and the other half are OOD (e.g. 5K ID samples and 5K OOD samples for CIFAR10 vs SVHN). For evaluation, we use a holdout set containing ID and OOD samples in the same proportions as the unlabeled set. Moreover, in Appendix F.5 we present results obtained with a smaller unlabeled set of only 1K samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our method against a wide range of baselines that are applicable in the SSND setting.</p><p>Semi-supervised novelty detection. We primarily compare ERD to SSND approaches that are designed to incorporate a small set of unlabeled ID and novel samples.</p><p>The MCD method <ref type="bibr" target="#b53">[54]</ref> trains an ensemble of two classifiers such that one model gives high-entropy and the other yields low entropy predictive distributions on the unlabeled samples. Furthermore, nnPU <ref type="bibr" target="#b23">[24]</ref> considers a binary classification setting, in which the labeled data comes from one class (i.e. ID samples, in our case), while the unlabeled set contains a mixture of samples from both classes. Notably, both methods require oracle knowledge that is usually unknown in the regular SSND setting: MCD uses test OOD data for hyperparameter tuning while nnPU requires oracle knowledge of the ratio of OOD samples in the unlabeled set.</p><p>In addition to these baselines, we also propose two natural extensions to the SSND setting of two existing methods. Firstly, we present a version of the Mahalanobis approach (Mahal-U ) that is calibrated using the unlabeled set, instead of using oracle OOD data. Secondly, since nnPU requires access to the OOD ratio of the unlabeled set, we also consider a less burdensome alternative: a binary classifier trained to separate the training data from the unlabeled set and regularized with early stopping like our method.</p><p>Unsupervised novelty detection (UND). Naturally, one may ignore the unlabeled data and use UND approaches. The current SOTA UND method on the usual benchmarks is the Gram method <ref type="bibr" target="#b42">[43]</ref>. Other UND approaches include vanilla ensembles <ref type="bibr" target="#b25">[26]</ref>, deep generative models (which tend to give undesirable results for OOD detection <ref type="bibr" target="#b22">[23]</ref>), or various Bayesian approaches (which are often poorly calibrated on OOD data <ref type="bibr" target="#b37">[38]</ref>).</p><p>Preliminary analyses revealed that generative models and methods trained with a contrastive loss <ref type="bibr" target="#b50">[51]</ref> or with one-class classification <ref type="bibr" target="#b46">[47]</ref> perform poorly on near OOD data sets (see Appendix F.2 for a comparison; we use numbers reported by the authors for works where we could not replicate their results).</p><p>Other methods. We also compare with Outlier Exposure <ref type="bibr" target="#b19">[20]</ref> and Deep Prior Networks (DPN) <ref type="bibr" target="#b33">[34]</ref> which use TinyImages as known outliers during training, irrespective of the OOD set used for evaluation. On the other hand, the Mahalanobis baseline <ref type="bibr" target="#b27">[28]</ref> is tuned on samples from the same OOD distribution used for evaluation. Finally, we also consider large transformer models pretrained on ImageNet21k and fine-tuned on the ID training set <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation details</head><p>Baseline hyperparameters. For all the baselines, we use the default hyperparameters suggested by their authors on the respective ID data set (see Appendix D for more details). For the binary classifier, nnPU, ViT, and vanilla ensembles, we choose the hyperparameters that optimize the loss on an ID validation set. ERD details. <ref type="bibr">3</ref> We follow the procedure in Algorithm 1 to fine-tune each model in the ERD ensemble starting from weights that are pretrained on the labeled ID set S. <ref type="bibr" target="#b3">4</ref> Unless otherwise specified, we train K = 3 ResNet20 models <ref type="bibr" target="#b16">[17]</ref> using 3 randomly chosen class labels for (U, c) and note that even ensembles of two models produce good results (see Appendix F.9). We stress that whenever applicable, our choices disadvantage ERD for the comparison with the baselines, e.g. vanilla ensembles use K = 5, and for most of the other approaches we use the larger WideResNet-28-10. We select the early stopping time and other standard hyperparameters so as to maximize validation accuracy.</p><p>Evaluation. As in standard hypothesis testing, choosing different thresholds for rejecting the null hypothesis leads to different false positive and true positive rates (FPR and TPR, respectively). The ROC curve follows the FPR and TPR for all possible threshold values and the area under the curve  (AUROC; larger values are better) captures the performance of a statistical test without having to select a specific threshold. In addition, we also report the TNR at a TPR of 95% (TNR@95; larger values are better). 5</p><p>Computation cost. We only need to fine-tune two-model ensembles to get good performance with ERD (see Appendix F.9). For instance, in applications like the one in <ref type="figure">Figure 1</ref>, ERD fine-tuning introduces little overhead and works well even with scarce resources (e.g. it takes around 5 minutes on 2 GPUs for the settings in <ref type="table">Table 1</ref>). In contrast, other ensemble diversification methods require training different models for each hyperparameter choice and have training losses that cannot be easily parallelized (e.g. <ref type="bibr" target="#b53">[54]</ref>). Moreover, the only other approach that achieves comparable performance to our method on some near OOD data uses large transformer models pretrained on a large and conveniently chosen data set <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main results</head><p>We summarize the main empirical results in <ref type="table">Table 1</ref>. While most methods achieve near-perfect detection for far OOD, ERD has a clear edge over the baselines for novel-class detection within the same dataset -even compared to methods ( ?) that use oracle OOD information. For completeness, we present in Appendix F.2 a comparison with more related works. These methods either show unsatisfactory performance on near OOD tasks, or seem to work well only on certain specific data sets. We elaborate on the potential causes of failure for these works in Section 5.</p><p>For the medical novelty detection benchmark we show in <ref type="figure" target="#fig_5">Figure 5a</ref> the average AUROC achieved by some representative baselines taken from <ref type="bibr" target="#b5">[6]</ref>. Our method improves the average AUROC from 0.85 to 0.91, compared to the best baseline. We refer the reader to <ref type="bibr" target="#b5">[6]</ref> for precise details on the methods. Appendix G contains more results, as well as additional baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation studies and limitations</head><p>We also perform extensive experiments to understand the importance of specific design choices and hyperparameters, and refer the reader to the appendix for details. <ref type="table">Table 1</ref> we evaluate our approach on a holdout test set that is drawn from the same distribution as the unlabeled set U used for fine-tuning. However, we provide experiments in Appendix F.10 that show that novelty detection with ERD continues to perform well even when the test set and U come from different distributions (e.g. novel-class data in the test set also suffers from corruptions). Further, even though our main focus is novel-class detection, our experiments (Appendix F.4) indicate that ERD can also successfully identify near OOD samples that suffer from only mild covariate shift compared to the ID data (e.g. CIFAR10 vs corrupted CIFAR10 <ref type="bibr" target="#b17">[18]</ref> or CIFAR10v2 <ref type="bibr" target="#b38">[39]</ref>). Finally, Appendix F.1 shows that ERD ensembles also perform well in a transductive setting <ref type="bibr" target="#b43">[44]</ref>, where the test set coincides with U .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relaxing assumptions on OOD samples. In</head><p>Relaxing the assumptions of Proposition 3.1. Our theoretical results require that the ID classes in the training set and the novel classes in U have similar cardinality. In fact, this condition is unnecessarily strong as we show in our empirical analysis: In all experimental settings we have significantly fewer OOD than ID training points. We further investigate the impact of the size of the unlabeled set and of the ratio of novel samples in it ( |U OOD | |U ID |+|U OOD | ) and find that ERD in fact maintains good performance for a broad range of ratios in <ref type="figure" target="#fig_5">Figure 5b</ref>.</p><p>Sensitivity to hyperparameter choices. We point out that ERD ensembles are particularly robust to changes in the hyperparameters like batch size or learning rate (Appendix H), or the choice of the arbitrary label assigned to the unlabeled set (Appendix F.9). Further, we note that ERD ensembles with as few as two models already show remarkable novelty detection performance and refer to Appendix F.9 for experiments with larger ensemble sizes. Moreover, ERD performance improves with larger neural networks (Appendix F.8), meaning that ERD will benefit from any future advances in architecture design.</p><p>Choice of disagreement score. We show in <ref type="table">Table 3</ref> in Appendix B, that the training procedure alone (Algorithm 1) does not suffice for good novelty detection. For optimal results, ERD ensembles need to be combined with a disagreement-based score like the one introduced in Section 2.3. Finally, we show how the distribution of the disagreement score changes during training for ERD (Appendix K) and explain why regularizing disagreement is more challenging for near OOD data, compared to easier, far OOD settings (Appendix J).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations.</head><p>Despite the advantages of ERD, like all prior SSND methods, our approach is not a good fit for online (real-time) novelty detection tasks. Moreover, ERD ensembles are not tailored to anomaly detection, where outliers are particularly rare, since the unlabeled set should contain at least a small number of samples from the novel classes (see <ref type="figure" target="#fig_5">Figure 5b</ref> and Appendix I). However, ERD ensembles are an ideal candidate for applications that require highly accurate, offline novelty detection, like the one illustrated in <ref type="figure">Figure 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>In this section, we present an overview of different types of related methods that are in principle applicable for solving semi-supervised novelty detection. In particular, we indicate caveats of these methods based on their categorization with respect to 1) data availability and 2) the surrogate objective they try to optimize. This taxonomy may also be of independent interest to navigate the zoo of ND methods. We list a few representative approaches in <ref type="table" target="#tab_0">Table 2</ref> and refer the reader to surveys such as <ref type="bibr" target="#b3">[4]</ref> for a thorough literature overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Taxonomy according to data availability</head><p>In this section we present related novelty detection methods that use varying degrees of labeled OOD data for training. We call test OOD the novel-class data that we want to detect at test time.</p><p>In a scenario like the one in <ref type="figure">Figure 1</ref>, one can apply unsupervised novelty detection (UND) methods that ignore the unlabeled batch and only uses ID data during training <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b35">36]</ref>. However, these approaches lead to poor novelty detection performance, especially on near OOD data.</p><p>There are methods that suggest to improve UND performance by using additional data. For example, during training one may use synthetically generated outliers (e.g. <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b46">47]</ref>) or a different OOD data set that may be available (e.g. OE and DPN use TinyImages) with samples known to be outliers. However, in order for these augmented unsupervised ND (A-UND) methods to work, they require that the OOD data used for training is similar to test OOD samples. When this condition is not satisfied, A-UND performance deteriorates drastically (see <ref type="table">Table 1</ref>). However, by definition, novel data is unknown and the only information about the OOD data that is realistically available is in the unlabeled set like in SSND. Therefore, it is unknown what an appropriate choice of the training OOD data is for A-UND methods.</p><p>Another line of work uses pretrained models to incorporate additional data that is close to test OOD samples, i.e. pretrained UND (P-UND). <ref type="bibr" target="#b10">[11]</ref> use large transformer models pretrained on ImageNet21k and achieve good near OOD detection performance when ID and OOD data are similar to ImageNet samples (e.g. CIFAR10/CIFAR100). However, our experiments in Appendix F.3 reveal that this method performs poorly on all other near OOD data sets, including unseen FashionMNIST or SVHN classes and X-rays of unknown diseases. This unsatisfactory performance is apparent when ID and OOD data do not share visual features with the pretraining data (i.e. ImageNet21k). Since collecting such large troves of "similar" data for pre-training is often not possible in practical applications (as medical imaging), the use case of their method is rather limited.</p><p>Furthermore, a few popular methods use test OOD data for calibration or hyperparameter tuning <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42]</ref>, which is not applicable in practice. Clearly, knowing the test OOD distribution a priori turns the problem into supervised ND (SND), and hence, violates the fundamental assumption that OOD data is unforeseeable.</p><p>As we have already seen, current SSND approaches (e.g. MCD, nnPU) perform poorly for complex models such as neural networks. We note that SSND is similar to using unlabeled data for learning with augmented classes (U-LAC) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">57]</ref> and is related to transductive novelty detection <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b14">15]</ref>, where the test set coincides with the unlabeled set used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Taxonomy according to probabilistic perspective</head><p>Apart from data availability, the methods that we can use in a practical SSND scenario implicitly or explicitly use a different principle based on a probabilistic model. For example, novel-class samples are a subset of the points that are out-of-distribution in the literal sense, i.e. P X (x) &lt; ?. One can hence learn P X from unlabeled ID data, which is however notoriously difficult in high dimensions.</p><p>Similarly, from a Bayesian viewpoint, the predictive variance is larger for OOD samples with P X (x) &lt; ?. Hence, one could instead compute the posterior P X (y|x) and flag points with large variance (i.e. high predictive uncertainty). This circumvents the problem with estimating P X . However, Bayesian estimates of uncertainty that accompany NN predictions tend to not be accurate on OOD data <ref type="bibr" target="#b37">[38]</ref>, resulting in poor novelty detection performance.</p><p>When the labels are available for the training set, we can instead partially learn P X using y. For instance, one could use generative modeling to estimate the set of x for which P X (x) &gt; ? via P X (x|y) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">43]</ref>. Alternatively, given a loss and function space, we may use the labels indirectly, like in ERD, and use properties of the approximated population error that imply small or large P X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In summary, we propose an SSND procedure that exploits unlabeled data effectively to generate an ensemble with regularized disagreement, which achieves remarkable novelty detection performance. Our SSND method does not need labeled OOD data during training unlike many other related works summarized in <ref type="table" target="#tab_0">Table 2</ref>.</p><p>We leave as future work a thorough investigation of the impact of the labeling scheme of the unlabeled set on the sample complexity of the method, as well as an analysis of the trade-off governed by the complexity of the model class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Theoretical statements</head><formula xml:id="formula_4">Definition A.1 (( , ?)-clusterable data set). We say that a data set D = {(x i , y i )} n i=1 is ( , ?)- clusterable for fixed &gt; 0 and ? ? [0, 1] if there exists a partitioning of it into subsets {C 1 , ..., C K },</formula><p>which we call clusters, each with their associated unit-norm cluster center c i , that satisfy the following conditions:</p><formula xml:id="formula_5">? K i=1 C i = D and C i ? C j = ?, ?i, j ? [K];</formula><p>? all the points in a cluster lie in the -neighborhood of their corresponding cluster center, i.e. ||x ? c i || 2 ? for all x ? C i and all i ? [K];</p><p>? a fraction of at least 1 ? ? of the points in each cluster C i have the same label, which we call the cluster label and denote y * (c i ). The remaining points suffer from label noise;</p><p>? if two cluster C i and C j have different labels, then their centers are 2 far from each other, i.e.</p><formula xml:id="formula_6">||c i ? c j || 2 ? 2 ; ? the clusters are balanced i.e. for all i ? [K], ? 1 n K ? |C i | ? ? 2 n K ,</formula><p>where ? 1 and ? 2 are two positive constants.</p><p>In our case, for a fixed label c ? Y, we assume that the set S ? (U, c) is ( , ?)-clusterable into K clusters. We further assume that each cluster C i only includes a few noisy samples from (U ?c ID , c), i.e.</p><formula xml:id="formula_7">|C i ?(U ?c ID ,c)| |C i | ? ? and that for clusters C i whose cluster label is not c, i.e. y * (c i ) = c, it holds that C i ? (U OOD , c) = ?.</formula><p>We define the matrices C :</p><formula xml:id="formula_8">= [c 1 , ..., c K ] T ? R K?d and ? := (CC T ) E g [? (Cg)? (Cg) T ], with g ? N (0, I d ) and</formula><p>where denotes the elementwise product. We use ? and ? min (?) to denote the spectral norm and the smallest eigenvalue of a matrix, respectively.</p><p>For prediction, we consider a 2-layer neural network model with p hidden units, where p</p><formula xml:id="formula_9">K 2 C 4 ? min (?) 4 .</formula><p>We can write this model as follows:</p><formula xml:id="formula_10">x ? f (x; W ) = v T ?(W x),<label>(1)</label></formula><p>The first layer weights W are initialized with random values drawn from N (0, 1), while the last layer weights v have fixed values: half of them are set to 1/p and the other half is ?1/p. We consider activation functions ? with bounded first and second order derivatives, i.e. |? (x)| ? ? and ? (x) ? ?. We use the squared loss for training, i.e. L(W ) = 1 2 n i=0 (y i ? f (x i ; W )) 2 and take gradient descent steps to find the optimum of the loss function, i.e.</p><formula xml:id="formula_11">W ? +1 = W ? ? ??L(W ? ), where the step size is set to ? K n C 2 .</formula><p>We can now state the following proposition:</p><formula xml:id="formula_12">Proposition A.1. Assume that ? ? ?/8 and ? ??? min (?) 2 /K 2 , where ? is a constant such that ? ? 2</formula><p>|Y?1| and ? is a constant that depends on ?. Then it holds with high probability 1 ? 3/K 100 ? Ke ?100d over the initialization of the weights that the neural network trained on S ? (U, c) perfectly fits S,</p><formula xml:id="formula_13">(U c ID , c) and (U OOD , c), but not (U ?c ID , c), after T = c 4 C 2 ? min (?) iterations.</formula><p>This result shows that there exists an optimal stopping time at which the neural network predicts the correct label on all ID points and the label c on all the OOD points. As we will see later in the proof, the proposition is derived from a more general result which shows that the early stopped model predicts these labels not only on the points in U but also in an -neighborhood around cluster centers. Hence, an ERD ensemble can be used to detect holdout OOD samples similar to the ones in U , after being tuned on U . This follows the intuition that classifiers regularized with early stopping are smooth and generalize well.</p><p>The clusterable data model is generic enough to include data sets with non-linear decision boundaries. Moreover, notice that the condition in Proposition A.1 is satisfied when S ? (U ID , c) is ( , ?)-clusterable and (U OOD , c) is -clusterable and if the cluster centers of (U OOD , c) are at distance at least 2 from the cluster centers of S ? (U ID , c). A situation in which these requirements are met is, for instance, when the OOD data comes from novel classes, when all classes (including the unseen ones that are not in the training set) are well separated, with cluster centers at least 2 away in Euclidean distance. In addition, in order to limit the amount of label noise in each cluster, it is necessary that the number of incorrectly labeled samples in (U ?c ID , c) is small, relative to the size of S.</p><p>In practice, we only need that the decision boundary separating (U OOD , c) from S is easier to learn than the classifier required to interpolate the incorrectly labeled (U ?c ID , c), which is often the case, provided that (U OOD , c) is large enough and the OOD samples come from novel classes.</p><p>We now provide the proof for Proposition A.1:</p><p>Proof. We begin by restating a result from <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_14">Theorem A.1 ([29]). Let D := {(x i , y i )} ? R d ? Y be an ( , ?)-clusterable training set, with ? c 1 ?? min (?) 2 /K 2 and ? ? ?/8, where ? is a constant that satisfies ? ? 2 |Y|?1 .</formula><p>Consider a two-layer neural network as described above, and train it with gradient descent starting from initial weights sampled i.i.d. from N (0, 1). Assume further that the step size is ? = c 2 K n C 2 and that the number of hidden units p is at least c 3 <ref type="bibr" target="#b3">4</ref> . Under these conditions, it holds with probability at least 1 ? 3/K 100 ? Ke ?100d over the random draws of the initial weights, that after T = c 4</p><formula xml:id="formula_15">K 2 C 4 ? min (?)</formula><formula xml:id="formula_16">C 2 ? min (?)</formula><p>gradient descent steps, the neural network x ? f (x; W T ) predicts the correct cluster label for all points in the -neighborhood of the cluster center, namely:</p><formula xml:id="formula_17">arg max y?Y |f (x; W T ) ? ?(y)| = y * (c i ), for all x with x ? c i 2 ? and all clusters i ? [K],<label>(2)</label></formula><p>where ? : Y ? {0, 1} |Y| yields one-hot embeddings of the labels. The constants c 1 , c 2 , c 3 , c 4 depend only on ?. </p><formula xml:id="formula_18">? ?) with ?(f 1 (x), f 2 (x)) = 1 sgn(f1(x)) =sgn(f2(x)) (Right).</formula><p>The two metrics achieve similar TPRs, but using (H ? Avg) instead of our score, (Avg ? ?), leads to more false positives, since the former simply flags as OOD a band around the averaged model (solid black line) and does not take advantage of the ensemble's diversity.</p><p>Notice that, under the assumptions introduced above, the set S ? (U, c) is ( , ?)-clusterable, since the incorrectly labeled ID points in (U ?c ID , c) constitute at most a fraction ? of the clusters they belong to. As a consequence, Proposition A.1 follows directly from Theorem A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Disagreement score for novelty detection</head><p>As we argue in Section 3, Algorithm 1 produces an ensemble that disagrees on OOD data, and hence, we want to devise a scalar score that reflects this model diversity. Previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38]</ref> first average the softmax predictions of the models in the ensemble and then use the entropy as a metric, i.e.</p><formula xml:id="formula_19">(H ? Avg)(f 1 (x), ..., f K (x)) := ? |Y| i=1 (f (x)) i log(f (x)) i where f (x) := 1 K K i=1 f i (x) and (f (x)) i is the i th element of f (x) ? [0, 1] |Y|6 .</formula><p>We argue later that averaging discards information about the diversity of the models.</p><p>Recall that our average pairwise disagreement between the outputs of K models in an ensemble reads: 7</p><formula xml:id="formula_20">(Avg ? ?)(f 1 (x), ..., f K (x)) := 2 K(K ? 1) i =j ? (f i (x), f j (x)) ,<label>(3)</label></formula><p>where ? is a measure of disagreement between the softmax outputs of two predictors, for example the total variation distance ? We briefly highlight the reason why averaging softmax outputs first like in previous works relinquishes all the benefits of having a more diverse ensemble, as opposed to the proposed pairwise score in <ref type="bibr">Equation 3</ref>. Recall that varying thresholds yield different true negative and true positive rates (TNR and TPR, respectively) for a given statistic. In the sketch in <ref type="figure" target="#fig_6">Figure 6</ref> we show that the score we propose, (Avg ? ?), achieves a higher TNR compared to (H ? Avg), for a fixed TPR, which is a common way of evaluating statistical tests. Notice that the detection region for (H ? Avg) is always limited to a band around the average model for any threshold value t 0 . In order for the (H ? Avg) to have large TPR, this band needs to be wide, leading to many false positives. Instead, our disagreement score exploits the diversity of the models to more accurately detect OOD data.</p><formula xml:id="formula_21">TV (f i (x), f j (x)) = 1 2 f i (x) ? f j (x) 1 used in our experiments.</formula><p>We now provide further quantitative evidence to support the intuition presented in <ref type="figure" target="#fig_6">Figure 6</ref>. The aggregation metric is tailored to exploit ensemble diversity, which makes it particularly beneficial for ERD. On the other hand, Vanilla Ensembles only rely on the stochasticity of the training process and the random initializations of the weights to produce diverse models, which often leads to classifiers that are strikingly similar as we show in <ref type="figure" target="#fig_7">Figure 7</ref> for a few 2D data sets. As a consequence, using our disagreement score (Avg ? ?) for Vanilla Ensembles can sometimes hurt novelty detection performance. To see this, consider the extreme situation in which the models in the ensemble are identical, i.e. f 1 = f 2 . Then it follows that (Avg ? ?)(f 1 (x), f 2 (x)) = 0, for all test points x and for any function ? that satisfies the distance axioms.</p><p>We note that the disagreement score that we propose takes a form that is similar to previous diversity scores, e.g. <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b53">54]</ref>. In the context of regression, one can measure uncertainty using the variance of the outputs metric previously employed in works such as <ref type="bibr" target="#b11">[12]</ref>. However, we point out that using the output variance requires that the ensemble is the result of sampling from a random process (e.g. sampling different training data for the models, or sampling different parameters from a posterior). In our framework, we obtain the ensemble by solving a different optimization problem for each of the models by assigning a different label to the unlabeled data. Therefore, despite their similarities, our disagreement score and the output variance are, on a conceptual level, fundamentally different metrics. <ref type="table">Table 3</ref> shows that (Avg ? ?) leads to worse novelty detection performance for Vanilla Ensembles, compared to using the entropy of the average softmax score, (H ? Avg), which was proposed in prior work. However, if the ensembles are indeed diverse, as we argue is the case for our method ERD (see Section 3), then there is a clear advantage to using a score that, unlike (H ? Avg), takes diversity into account, as shown in <ref type="table">Table 3</ref> for 5-model ERD ensembles. <ref type="table">Table 3</ref>. The disagreement score that we propose (Avg ? ?) exploits ensemble diversity and benefits in particular ERD ensembles. Novelty detection performance is significantly improved when using (Avg ? ?) compared to the previously proposed (H ? Avg) metric. Since Vanilla Ensemble are not diverse enough, a score that relies on model diversity can hurt novelty detection performance. We highlight the AUROC and the TNR@95 obtained with the score function that is best for Vanilla Ensemble and the best for ERD. We highlight once again that other methods that attempt to obtain diverse ensembles, such as MCD, fail to train models with sufficient disagreement, even when they use oracle OOD for hyperparameter tuning <ref type="figure" target="#fig_8">(Figure 8a</ref>). </p><formula xml:id="formula_22">ID data OOD data Vanilla Ensembles (H ? Avg) Vanilla Ensembles (Avg ? ?) ERD (H ? Avg) ERD (Avg ? ?) AUROC ? / TNR@95 ? SVHN<label>CIFAR10</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Taxonomy of OOD detection methods according to overall objective</head><p>We now provide more details regarding the categorization of OOD detection approaches based on the different surrogate objectives that they use in order to detect OOD samples.</p><p>Learning the ID marginal P X . We loosely define OOD samples as all x for which P X (x) &lt; ?, for a small constant ? &gt; 0. Therefore, if we had access to the marginal training distribution P X , we would have perfect OOD detection. Realistically, however, P X is unknown, and we need to resort to estimating it. Explicit density estimation with generative models <ref type="bibr">[1,</ref><ref type="bibr" target="#b35">36]</ref> is inherently difficult in high dimensions. Alternatively, one-class classification <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref> and PU learning approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref> try to directly learn a discriminator between ID and OOD data in the presence of known (e.g. A-UND) or unknown (e.g. SSND) OOD data. However, these methods tend to produce indistinguishable representations for inliers and outliers when the ID distribution consists of many diverse classes.</p><p>Learning P X using label information (ours). Since in a prediction problem, the ID training set has class labels, one can take advantage of that additional information to distinguish points in the support of P X from OOD data. For instance, <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">43]</ref> propose to use the intermediate representations of neural networks trained for prediction to detect OOD data. Often, the task is to also simultaneously predict well on ID data, a problem known as open-set recognition <ref type="bibr" target="#b12">[13]</ref> and tackled by approaches like OpenHybrid <ref type="bibr" target="#b54">[55]</ref>.</p><p>Learning uncertainty estimates for P Y |X . In the prediction setting, calibrated uncertainty estimates error could naturally be used to detect OOD samples. Many uncertainty quantification methods are based on a Bayesian framework <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref> or calibration improvement <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16]</ref>. However, neither of them perform as well as other OOD methods mentioned above <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experiment details D.1 Baselines</head><p>In this section we describe in detail the baselines with which we compare our method and describe how we choose their hyperparameters. For all baselines we use the hyperparameters suggested by the authors for the respective data sets (e.g. different hyperparameters for CIFAR10 or ImageNet). For all methods, we use pretrained models provided by the authors. However, we note that for the novel-class settings, pretraining on the entire training set means that the model is exposed to the OOD classes as well, which is undesirable. Therefore, for these settings we pretrain only on the split of the training set that contains the ID classes. Since the classification problem is similar to the original one of training on the entire training set, we use the same hyperparameters that the authors report in the original papers.</p><p>Moreover, we point out that even though different methods use different model architectures, that is not inherently unreasonable when the goal is novelty detection, since it is not clear if a complex model is more desirable than a smaller model. For this reason, we use the model architecture recommended by the authors of the baselines and which was used to produce the good results reported in their published works. For Vanilla Ensembles and for ERD we show results for different architectures in Appendix F.8.</p><p>? Vanilla Ensembles <ref type="bibr" target="#b25">[26]</ref>: We train an ensemble on the training set according to the true labels. For a test sample, we average the outputs of the softmax probabilities predicted by the models, and use the entropy of the resulting distribution as the score for the hypothesis test described in Section 2.3. We use ensembles of 5 models, with the same architecture and hyperparameters as the ones used for ERD. Hyperparameters are tuned to achieve good validation accuracy.</p><p>? Gram method <ref type="bibr" target="#b42">[43]</ref>: The Gram baseline is similar to the Mahalanobis method in that both use the intermediate feature representations obtained with a deep neural network to determine whether a test point is an outlier. However, what sets the Gram method apart is the fact that it does not need any OOD data for training or calibration. We use the pretrained models provided by the authors, or train our own, using the same methodology as described for the Mahalanobis baseline. For OOD detection, we use the code published by the authors. We note that for MLP models, the Gram method is difficult to tune and we could not find a configuration that works well, despite our best efforts and following the suggestions proposed during our communication with the authors.</p><p>? Deep Prior Networks (DPN) <ref type="bibr" target="#b33">[34]</ref>: DPN is a Bayesian Method that trains a neural network (Prior Network) to parametrize a Dirichlet distribution over the class probabilities. We train a WideResNet WRN-28-10 for 100 epochs using SGD with momentum 0.9, with an initial learning rate of 0.01, which is decayed by 0.2 at epochs 50, 70, and 90. For MNIST, we use EMINST/Letters as OOD for tuning. For all other settings, we use TinyImages as OOD for tuning.</p><p>? Outlier Exposure <ref type="bibr" target="#b19">[20]</ref>: This approach makes a model's softmax predictions close to the uniform distribution on the known outliers, while maintaining a good classification performance on the training distribution. We use the WideResNet architecture (WRN). For fine-tuning, we use the settings recommended by the authors, namely we train for 10 epochs with learning rate 0.001. For training from scratch, we train for 100 epochs with an initial learning rate of 0.1. When the training data set is either CIFAR10/CIFAR100 or ImageNet, we use the default WRN parameters of the author's code, namely 40 layers, 2 widen-factor, droprate 0.3. When the training dataset is SVHN, we use the author's recommended parameters of 16 layers, 4 widen-factor and droprate 0.4. All settings use the cosine annealing learning rate scheduler provided with the author's code, without any modifications. For all settings, we use TinyImages as known OOD data during training. In Section F.6 we show results for known OOD data that is similar to the OOD data used for testing.</p><p>? Mahalanobis <ref type="bibr" target="#b27">[28]</ref>: The method pretrains models on the labeled training data. For a test data point, it uses the intermediate representations of each layer as "extracted features". It then performs binary classification using logistic regression using these extracted features. In the original setting, the classification is done on "training" ID vs "training" OOD samples (which are from the same distribution as the test OOD samples). Furthermore, hyperparameter tuning for the optimal amount of noise is performed on validation ID and OOD data. We use the WRN-28-10 architecture, pretrained for 200 epochs. The initial learning rate is 0.1, which is decayed at epochs 60, 120, and 160 by 0.2. We use SGD with momentum 0.9, and the standard weight decay of 5 ? 10 ?4 . The code published for the Mahalanobis method performs a hyperparameter search automatically for each of the data sets.</p><p>The following baselines attempt to leverage the unlabeled data that is available in applications such as the one depicted in <ref type="figure">Figure 1</ref>, similar to ERD.</p><p>? Non-negative PU learning (nnPU) <ref type="bibr" target="#b23">[24]</ref>: The method trains a binary predictor to distinguish between a set of known positives (in our case the ID data) and a set that contains a mixture of positives and negatives (in our case the unlabeled set). To prevent the interpolation of all the unlabeled samples, <ref type="bibr" target="#b23">[24]</ref> proposes a regularized objective. It is important to note that most training objectives in the PU learning literature require that the ratio between the positives and negatives in the unlabeled set is known or easy to estimate. For our experiments we always use the exact OOD ratio to train the nnPU baseline. Therefore, we obtain an upper bound on the AUROC/TNR@95. If the ratio is estimated from finite samples, then estimation errors may lead to slightly worse OOD detection performance. We perform a grid search over the learning rate and the threshold that appears in the nnPU regularizer and pick the option with the best validation accuracy measured on a holdout set with only positive samples (in our case, ID data).</p><p>? Maximum Classifier Discrepancy (MCD) <ref type="bibr" target="#b53">[54]</ref>: The MCD method trains two classifiers at the same time, and makes them disagree on the unlabeled data, while maintaining good classification performance. We use the WRN-28-10 architecture as suggested in the paper. We did not change the default parameters which came with the author's code, so weight decay is 10 ?4 , and the optimizer is SGD with momentum 0.9. When available (for CIFAR10 and CIFAR100), we use the pretrained models provided by the authors. For the other training datasets, we use their methodology to generate pretrained models: We train a WRN-28-10 for 200 epochs. The learning rate starts at 0.1 and drops by a factor of 10 at 50% and 75% of the training progress.</p><p>? Mahalanobis-U: This is a slightly different version of the Mahalanobis baseline, for which we use early-stopped logistic regression to distinguish between the training set and an unlabeled set with ID and OOD samples (instead of discriminating a known OOD set from the inliers). The early stopping iteration is chosen to minimize the classification errors on a validation set that contains only ID data (recall that we do not assume to know which are the OOD samples).</p><p>In addition to these approaches that have been introduced in prior work, we also propose a strong novel baseline that that bares some similarity to PU learning and to ERD.</p><p>? Binary classifier The approach consists in discriminating between the labeled ID training set and the mixed unlabeled set, that contains both ID and OOD data. We use regularization to prevent the trivial solution for which the entire unlabeled set is predicted as OOD. Unlike PU learning, the binary classifier does not require that the OOD ratio in the test distribution is known. The approach is similar to a method described in <ref type="bibr" target="#b43">[44]</ref> which also requires that the OOD ratio of the unlabeled set is known. We tune the learning rate and the weight of the unlabeled samples in the training loss by performing a grid search and selecting the configuration with the best validation accuracy, computed on a holdout set containing only ID samples. We note that the binary classifier that appears in Section G in the medical benchmark, is not the same as this baseline. For more details on the binary classifier that appears in the medical data experiments we refer the reader to <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Training configuration for ERD</head><p>For ERD we always use hyperparameters that give the best validation accuracy when training a model on the ID training set. In other words, we pick hyperparameter values that lead to good ID generalization and do not perform further hyperparameter tuning for the different OOD data sets on which we evaluate our approach. We point out that, if the ID labeled set is known to suffer from class imbalance, subpopulation imbalance or label noise, any training method that addresses these issues can be used instead of standard empirical risk minimization to train our ensemble (e.g. see <ref type="bibr" target="#b28">[29]</ref>).</p><p>For MNIST and FashionMNIST, we train ensembles of 3-layer MLP models with ReLU activations. Each intermediate layer has 100 neurons. The models are optimized using Adam, with a learning rate of 0.001, for 10 epochs.</p><p>For SVHN, CIFAR10/CIFAR100 and ImageNet, we train ensembles of ResNet20 <ref type="bibr" target="#b16">[17]</ref>. The models are initialized with weights pretrained for 100 epochs on the labeled training set. We fine-tune each model for 10 epochs using SGD with momentum 0.9, and a learning rate of 0.001. The weights are trained with an 2 regularization coefficient of 5e ? 4. We use a batch size of 128 for all scenarios, unless explicitly stated otherwise. We used the same hyperparameters for all settings.</p><p>For pretraining, we perform SGD for 100 epochs and use the same architecture and hyperparameters as described above, with the exception of the learning rate that starts at 0.1, and is multiplied by 0.2 at epochs 50, 70 and 90.</p><p>Apart from ERD, which fine-tunes the ensemble models starting from pretrained weights, we also present in the Appendix results for ERD++. This variant of our method trains the models from random initializations, and hence needs more iterations to converge, making it more computationally expensive than ERD. We train all models in the ERD++ ensembles for 100 epochs with a learning rate that starts at 0.1, and is multiplied by 0.2 at epochs 50, 70 and 90. All other hyperparameters are the same as for ERD ensembles.</p><p>For the medical data sets, we train a Densenet-121 as the authors do in the original paper <ref type="bibr" target="#b5">[6]</ref>. For ERD++, we do not use random weight initializations, but instead we start with the ImageNet weights provided with Tensorflow. The training configuration is exactly the same as for ResNet20, except that we use a batch size of 32 due to GPU memory restrictions, and for fine tuning we use a constant learning rate of 10 ?5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Computational considerations for ERD</head><p>We note that ERD models reach the optimal stopping time within the first 10 epochs on all the data sets that we consider, which amounts to around 6 minutes of training time if the models in the ensemble are fine-tuned in parallel on NVIDIA 1080 Ti GPUs. This is substantially better than the cost of fine-tuning a large ViT transformer model (which takes about 1 hour for 2500 iterations on the same hardware). Moreover, since the loss we use to train the ensemble decouples over the models, it allows for easy parallelization, unlike objectives like MCD where the ensemble models are intertwined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ID and OOD data sets E.1 Data sets</head><p>For evaluation, we use the following image data sets: MNIST <ref type="bibr" target="#b26">[27]</ref>, Fashion MNIST <ref type="bibr" target="#b51">[52]</ref>, SVHN <ref type="bibr" target="#b36">[37]</ref>, CIFAR10 and CIFAR100 <ref type="bibr" target="#b24">[25]</ref>.</p><p>For the experiments using MNIST and FashionMNIST the training set size is 50K, the validation size is 10K, and the test ID and test OOD sizes are both 10K. For SVHN, CIFAR10 and CIFAR100, the training set size is 40K, the validation size is 10K, and the unlabeled set contains 10K samples: 5K are ID and 5K are OOD. For evaluation, we use a holdout set of 10K examples (half ID, half OOD). For the settings that use half of the classes as ID and the other half as OOD, all the sizes are divided by 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Samples for the settings with novel classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More experiments</head><p>We now present more experimental results that provide additional insights about the proposed approach. We note that, unless otherwise specified, we use 5-model ERD ensembles in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Evaluation on the unlabeled set</head><p>In the main text we describe how one can leverage the unlabeled set U to obtain an novelty detection algorithm that accurately identifies outliers at test time that similar to the ones in U . It is, however, possible to also use our method ERD to flag the OOD samples contained in the same set U used for fine-tuning the ensemble. In <ref type="table" target="#tab_2">Table 4</ref> we show that the novelty detection performance of ERD is similar regardless of whether we use U for evaluation, or a holdout test set T drawn from the same distribution as U . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Comparison with other related works</head><p>We compare 5-model ERD ensembles to more OOD detection approaches. For various reasons we did not run these methods ourselves on the data sets for which we evaluate our method in Section 4 (e.g. code not available, unable to replicate published results, poor performance reported by the authors etc). We collected the AUROC numbers presented in <ref type="table">Table 5</ref> from the papers that introduce each method. We note that our approach shows an excellent overall performance, being consistently better than or on par with the related works that we consider. While the method of <ref type="bibr" target="#b10">[11]</ref> performs significantly better than all other baselines on CIFAR10/CIFAR100 tasks, we argue in Appendix F.3 that this is primarily due to the convenient choice of the data set used for pretraining the transformer models (i.e. Imagenet21k) which is strikingly similar to the ID and OOD data.</p><p>OpenHybrid <ref type="bibr" target="#b54">[55]</ref> is an open set recognition approach which reports great near OOD detection performance. We note that, despite our best efforts, we did not manage to match in our own experiments the results reported in the paper, even after communicating with the authors and using the code that they have provided. Moreover, we point out that the performance of OpenHybrid seems to deteriorate significantly when the ID data consists of numerous classes, as is the case for CIFAR100.</p><p>Furthermore, we note that generative models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr">1]</ref> and one-class classification approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b46">47]</ref> showed generally bad performance, in particular on near OOD data. When the ID training set is made up of several diverse classes, it is difficult to represent accurately all the ID data, and only the ID data. <ref type="table">Table 5</ref>. AUROC numbers collected from the literature for a number of relevant OOD detection methods. We note that the method of <ref type="bibr" target="#b10">[11]</ref> ( ? ) uses a large scale visual transformer models pretrained on a superset of the OOD data, i.e. ImageNet21k, while the method of <ref type="bibr" target="#b45">[46]</ref> ( * ) uses oracle OOD samples for training from the same data set as test OOD. For the settings with random classes, the numbers are averages over 5 draws and the standard deviation is always strictly smaller than 0.01 for our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Shortcomings of pretrained ViT models for novelty detection</head><p>In this section we provide further experimental results pointing to the fact that large pretrained transformer models <ref type="bibr" target="#b10">[11]</ref> can only detect near OOD samples from certain specific data sets, and do not generalize well more broadly.</p><p>Implementation details. We fine-tune visual transformer (ViT) models pretrained on Ima-genet21k according the methodology described in <ref type="bibr" target="#b10">[11]</ref>. We report results using the ViT-S-16 architecture ( 22 million trainable parameters) which we fine-tune for 2500 iterations on labeled ID data. We use the hyperparameters suggested by the authors and always ensure that the prediction accuracy of the fine-tuned model on ID data is in the expected range. The code published by the authors uses three different test statistics to detect OOD data: the maximum softmax probability <ref type="bibr" target="#b18">[19]</ref>, the vanilla Mahalanobis distance <ref type="bibr" target="#b27">[28]</ref> and a recently proposed variant of the Mahalanobis approach <ref type="bibr" target="#b39">[40]</ref>. In <ref type="table">Table 6</ref> we present only the metrics obtained with the best-performing test statistic for ViT. We stress that this favors the ViT method significantly, as different test statistics seem to perform better on different data sets. Since test OOD data is unknown, it is not possible to select which test statistic to use a priori, and hence, we use oracle knowledge to give ViT models an unfair advantage.</p><p>Experimental results. In <ref type="table">Table 6</ref> we compare pretrained visual transformers with 5-model ERD and ERD++ ensembles. Notably, the data sets can be partitioned in two clusters, based on ViT novelty detection performance. On the one hand, if the ID or OOD data comes from CIFAR10 or CIFAR100, ViT models can detect novel-class samples well. Perhaps surprisingly, ViT fails short of detecting OOD data perfectly (i.e. AUROC and TNR@95 of 1) on easy tasks such as CIFAR10 vs SVHN or CIFAR100 vs SVHN, unlike ERD and a number of other baseline approaches.</p><p>On the other hand, ViT shows remarkably poor performance on all other data sets, when neither the ID nor the OOD data come from CIFAR10/CIFAR100. This includes some of the novel disease use cases from the medical OOD detection benchmark (see Appendix G for more details about the data sets). This unsatisfactory performance persists even for larger ViT models (we have tried ViT-S-16 and ViT-B-16 architectures), when fine-tuning for more iterations (we have tried both 2500 and 10000 iterations), or when varying hyperparameters such as the learning rate.</p><p>Intuition for why ViT fails. We conjecture that the novelty detection performance with pretrained ViT models relies heavily on the choice of the pretraining data set. In particular, we hypothesize that, since CIFAR10/CIFAR100 classes are included in the Imagenet21k data set used for pretraining, the models learn features that are useful for distinguishing ID and OOD classes when the ID and/or OOD data comes from CIFAR10/CIFAR100. Hence, this would explain the good performance of pretrained models on the data sets at the top of <ref type="table">Table 6</ref>. On the other hand, when ID and OOD data is strikingly different from the pretraining data, both ID and OOD samples are projected to the same concentrated region of the representation space, which makes it difficult to detect novel-class points. Moreover, the process of fine-tuning as it is described in <ref type="bibr" target="#b10">[11]</ref> seems to not help to alleviate this problem. This leads to the poor performance observed on the near OOD data sets at the bottom of <ref type="table">Table 6</ref>.</p><p>In conclusion, having a large pretraining data set seems to be beneficial when the OOD data shares many visual and semantic features in common with the pretraining data. However, in realworld applications it is often difficult to collect such large data sets, which makes the applicability of pretrained ViT models limited to only certain specific scenarios. <ref type="table">Table 6</ref>. Pretrained ViT models tend to perform well when the ID and OOD data is semantically similar to (or even included in) the pretraining data, e.g. CIFAR10, CIFAR100 (top part), and their detection performance deteriorates drastically otherwise (bottom part). We compare ViT-S-16 models pretrained on Imagenet21k with 5-model ERD and ERD++ ensembles and highlight the best method. See Appendix G for more details about the medical data sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 OOD detection for data with covariate shift</head><p>In this section we evaluate the baselines and the method that we propose on settings in which the OOD data suffers from covariate shift. The goal is to identify all samples that come from the shifted distribution, regardless of how strong the shift is. Notice that mild shifts may be easier to tackle by domain adaptation, but when the goal is OOD detection they pose a more difficult challenge.</p><p>We want to stress that in practice one may not be interested in identifying all samples with distribution shift as OOD, since a classifier may still produce correct predictions on some of them. In contrast, when data suffers from covariate shift we can try to learn predictors that perform well on both the training and the test distribution, and we may use a measure of predictive uncertainty to identify only those test samples on which the classifier cannot make confident predictions. Nevertheless, we use these covariate shift settings as a challenging OOD detection benchmark and show in <ref type="table" target="#tab_6">Table 8</ref> that our method ERD does indeed outperform prior baselines on these difficult settings.</p><p>We use as outliers corrupted variants of CIFAR10 and CIFAR100 <ref type="bibr" target="#b17">[18]</ref>, as well as a scenario where ImageNet <ref type="bibr" target="#b8">[9]</ref> is used as ID data and ObjectNet <ref type="bibr">[2]</ref> as OOD, both resized to 32x32. <ref type="figure">Figure 10</ref> shows samples from these data sets. The Gram and nnPU baselines do not give satisfactory results on the difficult CIFAR10/CIFAR100 settings in <ref type="table">Table 1</ref> and thus we do not consider them for the covariate shift cases. For the SSND methods (e.g. MCD, Mahal-U and ERD/ERD++) we evaluate on the same unlabeled set that is used for training (see the discussion in Section F.1). <ref type="figure">Figure 10</ref>. Left: Samples from ImageNet and ObjectNet taken from the original paper by <ref type="bibr">[2]</ref>. Right: Data samples for the corrupted CIFAR10-C data set. Furthermore, we present results on distinguishing between CIFAR10 <ref type="bibr" target="#b24">[25]</ref> and CIFAR10v2 <ref type="bibr" target="#b38">[39]</ref>, a data set meant to be drawn from the same distribution as CIFAR10 (generated from the Tiny Images collection). In <ref type="bibr" target="#b38">[39]</ref>, the authors argue that CIFAR10 and CIFAR10v2 come from very similar distributions. They provide supporting evidence by training a binary classifier to distinguish between them, and observing that the accuracy that is obtained of 52.9% is very close to random.</p><p>Our experiments show that the two data sets are actually distinguishable, contrary to what previous work has argued. First, our own binary classifier trained on CIFAR10 vs CIFAR10v2 obtains a test accuracy of 67%, without any hyperparameter tuning. The model we use is a ResNet20 trained for 200 epochs using SGD with momentum 0.9. The learning rate is decayed by 0.2 at epochs 90, 140, 160 and 180. We use 1600 examples from each data set for training, and we validate using 400 examples from each data set. Our OOD detection experiments (presented in <ref type="table" target="#tab_5">Table 7</ref>) show that most baselines are able to distinguish between the two data sets, with ERD achieving the highest performance. The methods which require OOD data for tuning (Outlier Exposure and DPN) use CIFAR100. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Results with a smaller unlabeled set</head><p>We now show that our method performs well even when the unlabeled set is significantly smaller.</p><p>In particular, we show in the table below that ERD maintains a high AUROC and TNR@95 even when only 1,000 unlabeled samples are used for fine-tuning (500 ID and 500 OOD). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.6 More results for Outlier Exposure</head><p>The Outlier Exposure method needs access to a set of OOD samples during training. The numbers we report in the rest of paper for Outlier Exposure are obtained by using the TinyImages data set as the OOD samples that are seen during training. In this section we explore the use of an OOD train data set that is more similar to the OOD data observed at test time. This is a much easier setting for the Outlier Exposure method: the closer OOD train is to OOD test , the easier it will be for the model tuned on OOD train to detect the test OOD samples.</p><p>In the table below we focus only on the settings with corruptions. For each corruption type, we use the lower severity corruption as OOD train and evaluate on the higher severity data and vice versa. We report for each metric the average taken over all corruptions (A), and the value for the worst-case setting (W).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID data</head><p>OOD data OE (trained on sev5) OE (trained on sev2)</p><formula xml:id="formula_23">AUROC ? CIFAR10 CIFAR10-C sev 2 (A) 0.89 N/A CIFAR10 CIFAR10-C sev 2 (W) 0.65 N/A CIFAR10 CIFAR10-C sev 5 (A) N/A 0.98 CIFAR10 CIFAR10-C sev 5 (W) N/A 0.78 CIFAR100 CIFAR100-C sev 2 (A) 0.85 N/A CIFAR100 CIFAR100-C sev 2 (W) 0.59 N/A CIFAR100 CIFAR100-C sev 5 (A) N/A 0.97 CIFAR100 CIFAR100-C sev 5 (W) N/A 0.67</formula><p>Average 0.87 0.98 <ref type="table">Table 10</ref>. Results for Outlier Exposure, when using the same corruption type, but with a higher/lower severity, as OOD data seen during training.  <ref type="figure">01 1.00 / 1.00 1.00 / 1.00 1.00 / 1.00 1.00 / 1.00 1</ref>  <ref type="bibr">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>  For FashionMNIST we chose this particular split (i.e. classes 0,2,3,7,8 vs classes 1,4,5,6,9) because the two partitions are more similar to each other. This makes novelty detection more difficult than the 0-4 vs 5-9 split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.7 Results on MNIST and FashionMNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.8 Vanilla and ERD Ensembles with different architectures</head><p>In this section we present OOD detection results for 5-model Vanilla and ERD ensembles with different architecture choices, and note that the better performance of our method is maintained across model classes. Moreover, we observe that ERD benefits from employing more complex models, like the WideResNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.9 Impact of the ensemble size and of the choice of arbitrary label</head><p>In this section we show novelty detection results with our method using a smaller number of models for the ensembles. We notice that the performance is not affected substantially, indicating that the computation cost of our approach could be further reduced by fine-tuning smaller ensembles. <ref type="table">Table 13</ref>. Results obtained with smaller ensembles for ERD. The numbers for K &lt; 5 are averages over 3 runs, where we use a different set of arbitrary labels for each run to illustrate our method's stability with respect the choice of labels to be assigned to the unlabeled set. We note that the standard deviations are small (? ? 0.01 for the AUROC values and ? ? 0.08 for the TNR@95 values). Impact of the choice of arbitrary labels. Furthermore, we note that in the table we report averages over 3 runs of our method, where for each run we use a different subset of Y to assign arbitrary labels to the unlabeled data. We do this in order to assess the stability of ERD ensembles to the choice of the arbitrary labels and notice that the novelty detection performance metrics do not vary significantly. Concretely, the standard deviations are consistently below 0.01 for all data sets for the AUROC metric, and below 0.07 for the TNR@95 metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.10 Detection performance on different OOD data</head><p>In this section we investigate whether the proposed method maintains its good novelty detection performance when the test-time OOD data comes from a different data set compared to the OOD data that is present in the unlabeled set used for fine-tuning. In particular, we are interested if our approach can still identify outliers in situations when they suffer from various corruptions. This scenario can sometimes occur in practice, when machine failure or uncurated data can lead to mild distribution shift.</p><p>Concretely, we focus on the difficult near OOD scenarios and take as ID half of the CIFAR10 or CIFAR100 classes, while the other half is OOD. For this experiment, we fine-tune the ERD ensembles using clean OOD data from the other half of CIFAR10 and CIFAR100, respectively. For evaluation, we use clean ID data and corrupted OOD samples from CIFAR10-C and CIFAR100-C, respectively. We give more details on these corrupted data sets in Appendix F.4. We consider corruptions of severity 2 and 5 from all corruptions types.</p><p>In <ref type="table" target="#tab_2">Table 14</ref> we show the average AUROC and the worst AUROC over all corruption types for vanilla and ERD ensembles. Note that our approach maintains a similar performance compared to the numbers presented in <ref type="table">Table 1</ref> for same test-time OOD data. It is also noteworthy that all the average AUROC values are consistently larger than the baselines in <ref type="table">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Medical OOD detection benchmark</head><p>The medical OOD detection benchmark is organized as follows. There are four training (ID) data sets, from three different domains: two data sets with chest X-rays, one with fundus imaging and one with histology images. For each ID data set, the authors consider three different OOD scenarios:</p><p>1. Use case 1: The OOD data set contains images from a completely different domain, similar to our category of easy OOD detection settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Use case 2:</head><p>The OOD data set contains images with various corruptions, similar to the hard covariate shift settings that we consider in Section F.4.</p><p>3. Use case 3: The OOD data set contains images that come from novel classes, not seen during training.  AUROC averaged over all scenarios in the medical OOD detection benchmark <ref type="bibr" target="#b5">[6]</ref>. The values for all the baselines are computed using code made available by the authors of <ref type="bibr" target="#b5">[6]</ref>. Notably, most of the baselines assume oracle knowledge of OOD data at training time. In addition, in <ref type="figure" target="#fig_1">Figure 13</ref> we present the average taken over only the novel-class settings in the medical benchmark. We observe that the performance of all methods is drastically affected, all of them performing much worse than the average presented in <ref type="figure" target="#fig_0">Figure 12</ref>. This stark decrease in AUROC and TNR@95 indicates that novelty detection is indeed a challenging task for OOD detection methods even in realistic settings. Nevertheless, 2-model ERD ensembles maintain a better performance than the baselines.</p><p>In <ref type="figure" target="#fig_3">Figures 14, 15, 16</ref> we present AUROC and AUPR (Area under the Precision Recall curve) for ERD for each of the training data sets, and each of the use cases. <ref type="figure" target="#fig_0">Figure 12</ref> presents averages over all settings that we considered, for all the baseline methods in the benchmark. Notably, ERD performs well consistently across data sets. The baselines are ordered by their average performance on all the settings (see <ref type="figure" target="#fig_0">Figure 12</ref>).</p><p>For all medical benchmarks, the unlabeled set is balanced, with an equal number of ID and OOD samples (subsampling the bigger data set, if necessary). We use the unlabeled set for evaluation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Effect of learning rate and batch size</head><p>We show now that ERD ensembles are not too sensitive to the choice of hyperparameters. We illustrate this by varying the learning rate and the batch size, the hyperparameters that we identify as most impactful. As <ref type="figure" target="#fig_7">Figure 17</ref> shows, many different configurations lead to similar novelty detection performance. <ref type="figure" target="#fig_7">Figure 17</ref>. AUROCs obtained with an ensemble of WRN-28-10 models, as the initial learning rate and the batch size are varied. We used the hardest setting, CIFAR100:0-50 as ID, and CIFAR100:50-100 as OOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Additional figure showing the dependence on the unlabeled set configuration</head><p>The configuration of the unlabeled set (i.e. the size of the unlabeled set, the ratio of OOD samples in the unlabeled set) influences the performance of our method, as illustrated in <ref type="figure" target="#fig_5">Figure 5b</ref>. Below, we show that the same trend persists for different data sets too, e.g. when we consider CIFAR10 as ID data and SVHN as OOD data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Learning curves for other data sets</head><p>In addition to <ref type="figure" target="#fig_3">Figure 4</ref>, we present in this section learning curves for other data sets as well. The trend that persists throughout all figures is that the arbitrary label is learned first on the unlabeled OOD data. Choosing a stopping time before the validation accuracy starts to deteriorate prevents the model from fitting the arbitrary label on unlabeled ID data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of near OOD data on training ERD ensembles.</head><p>The learning curves illustrated in <ref type="figure" target="#fig_9">Figure 19</ref> provide insight into what happens when the OOD data is similar to the ID training samples and the impact that has on training the proposed method. In particular, notice that for CIFAR10[0-4] vs CIFAR10 <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> in <ref type="figure" target="#fig_9">Figure 19d</ref>, the models require more training epochs before reaching an accuracy on unlabeled OOD samples of 100%. The learning of the arbitrary label on the OOD samples is delayed by the fact that the ID and OOD data are similar, and hence, the bias of the correctly labeled training set has a strong effect on the predictions of the models on the OOD inputs. Since we early stop when the validation accuracy starts deteriorating (e.g. at around epoch 8 in <ref type="figure" target="#fig_9">Figure 19d</ref>), we end up using models that do not interpolate the arbitrary label on the OOD samples. Therefore, the ensemble does not disagree on the entirety of the OOD data in the unlabeled set, which leads to lower novelty detection performance. Importantly, however, our empirical evaluation reveals that the drop in performance for ERD ensembles is substantially smaller than what we observe for other OOD detection methods, even on near OOD data sets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Evolution of disagreement score during fine-tuning</head><p>In this section we illustrate how the distribution of the disagreement score changes during fine-tuning for ID and OOD data, for a 5-model ERD ensemble. Thus, we can further understand why the performance of the ERD ensembles is impacted by near OOD data. <ref type="figure" target="#fig_0">Figure 20</ref> reveals that for far OOD data (the left column) the disagreement scores computed on OOD samples are well separated from the disagreement scores on ID data (note that disagreement on OOD data is so concentrated around the maximum value of 2 that the boxes are essentially reduced to a line segment). On the other hand, for near OOD data (the right column) there is sometimes significant overlap between the disagreement scores on ID and OOD data, which leads to the slightly lower AUROC values that we report in <ref type="table">Table 1</ref>.</p><p>The figures also illustrate how the disagreement on the ID data tends to increase as we fine-tune the ensemble for longer, as a consequence of the models fitting the arbitrary labels on the unlabeled  ID samples. Conversely, in most instances one epoch suffices for fitting the arbitrary label on the OOD data.</p><p>We need to make one important remark: While in the figure we present disagreement scores for the ensemble obtained after each epoch of fine-tuning, we stress that the final ERD ensemble need not be selected among these. In particular, since each model for ERD is early stopped separately, potentially at a different iteration, it is likely that the ERD ensemble contains models fine-tuned for a different number of iterations. Since we select the ERD ensembles from a strictly larger set, the final ensemble selected by the our proposed approach will be at least as good at distinguishing ID and OOD data as the best ensemble depicted in <ref type="figure" target="#fig_0">Figure 20</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Left: Sketch of the SSND setting. Middle and Right: Novelty detection with a diverse ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>a) Ensembles with too little disagreement fail to detect OOD samples. b) An ensemble of two models trained on S ? (U, c) disagrees on both ID and OOD data. b) Regularization prevents models from fitting (U ?c ID , c), limiting disagreement to only OOD samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposition 3 . 1 .</head><label>31</label><figDesc>(informal) It holds with high probability over the initialization of the weights that a two-layer neural network trained on S ? (U, c) perfectly fits S, (U c ID , c) and (U OOD , c), but not (U ?c ID , c), after T C 2 ? N N C iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Accuracy during fine-tuning a model pretrained on S (epoch 0 indicates values obtained with the initial pretrained weights). The samples in (U OOD , c) are fit first, while the model reaches high accuracy on (U ID , c) much later. We fine-tune for at least one epoch and then early stop when the validation accuracy starts decreasing after 7 epochs (vertical line). The model is trained on SVHN[0:4] as ID and SVHN[5:9] as OOD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Novelty detection performance on medical data (b) Effect of OOD proportion on detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Left: AUROC averaged over all scenarios in the medical novelty detection benchmark. The values for the baselines are computed using the code from [6]. Right: The AUROC of a 3-model ERD ensemble as the number and proportion of ID (CIFAR10[0:4]) and OOD (CIFAR10[5:9]) samples in the unlabeled set are varied (see also Appendix I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Cartoon illustration showing a diverse ensemble of linear binary classifiers. We compare novelty detection performance for two aggregation scores: (H ? Avg) (Left) and (Avg</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Relying only on the randomness of SGD and of the weight initialization to diversify models is not enough, as it often yields similar classifiers. Each column shows a different predictor trained from random initializations with Adam. All models have the same 1-hidden layer MLP architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>(a) Not enough diversity (MCD) (b) Regularized diversity (ERD) Distribution of disagreement scores on ID and OOD data for an ensemble that is not diverse enough (Left), and an ensemble with regularized disagreement (Right). Note that MCD is early-stopped using oracle OOD data. ID=CIFAR10[0:4], OOD=CIFAR10[5:9].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>(a) Data samples for the MNIST/FashionMNIST splits. (b) Data samples for the CIFAR10/SVHN splits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>0.64 / 0.07 0.77 / 0.15 0.66 / 0.12 0.77 / 0.20 0.95 / 0.71 0.78 / 0.30 0.82 / 0.39 0.95 / 0.66 0.94 / 0.67 0.94 / 0.68 Average 0.82 / 0.30 0.94 / 0.78 0.82 / 0.51 0.94 / 0.79 0.98 / 0.92 0.94 / 0.76 0.95 / 0.83 0.98 / 0.90 0.98 / 0.91 0.98 / 0.91</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Samples from the medical image benchmark. There are 3 ID data sets containing frontal and lateral chest X-rays and retinal images. Hard OOD samples contain images of diseases that are not present in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. AUROC averaged over all scenarios in the medical OOD detection benchmark [6]. The values for all the baselines are computed using code made available by the authors of [6]. Notably, most of the baselines assume oracle knowledge of OOD data at training time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .</head><label>13</label><figDesc>AUROC averaged over the novel-class scenarios in the medical OOD detection benchmark<ref type="bibr" target="#b5">[6]</ref>, i.e. only use case 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Comparison between ERD and the various baselines on the NIH chest X-ray data set, for use case 1 (top), use case 2 (middle) and use case 3 (bottom). Baselines ordered as inFigure 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .</head><label>15</label><figDesc>Comparison between ERD and the various baselines on the PC chest X-ray data set, for use case 1 (top), use case 2 (middle) and use case 3 (bottom). Baselines ordered as inFigure 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 .</head><label>16</label><figDesc>Comparison between ERD and the various baselines on the DRD fundus imaging data set, for use case 1 (top), use case 2 (middle) and use case 3 (bottom). Baselines ordered as inFigure 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 .</head><label>18</label><figDesc>The AUROC of a 3-model ERD ensemble as the number and proportion of ID (CIFAR10) and OOD (SVHN) samples in the unlabeled set are varied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>(a) ID = SVHN; OOD = CIFAR10. (b) ID = SVHN[0-4]; OOD = SVHN[5-9]. (c) ID = CIFAR10; OOD = SVHN. (d) ID = CIFAR10[0-4]; OOD = CIFAR10[5-9]. (e) ID = CIFAR100; OOD = SVHN. (f) ID = CIFAR100[0-49]; OOD = CIFAR100[50-99].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 19 .</head><label>19</label><figDesc>Accuracy measured while fine-tuning a model pretrained on S (epoch 0 indicates values obtained with the initial pretrained weights). The samples in (U OOD , c) are fit first, while the model reaches high accuracy on (U ID , c) much later. We fine-tune for at least one epoch and then early stop when the validation accuracy starts decreasing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>(a) ID = SVHN; OOD = CIFAR10. (b) ID = SVHN[0-4]; OOD = SVHN[5-9]. (c) ID = CIFAR10; OOD = SVHN. (d) ID = CIFAR10[0-4]; OOD = CIFAR10[5-9]. (e) ID = CIFAR100; OOD = SVHN. (f) ID = CIFAR100[0-49]; OOD = CIFAR100[50-99].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 20 .</head><label>20</label><figDesc>The distribution of the disagreement score measured during fine-tuning on ID and OOD data (blue and orange boxes, respectively). The box indicates the lower and upper quartiles of the distribution, while the middle line represents the median and the whiskers show the extreme values. Notice that the distributions of the scores are easier to distinguish for far OOD data (left column), and tend to overlap more for near OOD settings (right column).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Taxonomy of novelty detection methods, categorized according to data availability (horizontal axis) and probabilistic perspective (vertical axis). We highlight the ensemble-based methods.</figDesc><table><row><cell></cell><cell>UND</cell><cell></cell><cell>SSND</cell><cell>Different OOD A-UND</cell><cell cols="2">Synthetic OOD A-UND</cell><cell></cell><cell>P-UND</cell><cell>SND</cell></row><row><cell>Learn</cell><cell>Generative</cell><cell>e.g.</cell><cell>nnPU [KNPS17]</cell><cell></cell><cell>OC</cell><cell>classif.</cell><cell>[SLYJP21,</cell><cell>[RH21]</cell><cell>[GKRB13, DKT19,</cell></row><row><cell>P X</cell><cell cols="2">[AAB18], OC classif.</cell><cell></cell><cell></cell><cell cols="2">TMJS20]</cell><cell></cell><cell></cell><cell>RVGBM20]</cell></row><row><cell></cell><cell>e.g. [SPSSW01]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Learn P X using y</cell><cell cols="2">Gram [SO19], Open-Hybrid [ZLGG20]</cell><cell>ERD (Ours), SSND for shallow models [MBGBM10, BLS10], U-</cell><cell></cell><cell cols="3">Data contrastive loss [TMJS20, augmentation for</cell><cell>ViT [FRL20]</cell><cell>Mahala. MCD [YA19] [LLLS18],</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LAC [DYZ14, ZZMZ20]</cell><cell></cell><cell>LA20]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Uncertainty of P Y |X</cell><cell cols="2">e.g. [GG16], Vanilla Bayesian methods</cell><cell>-</cell><cell>[HMD19] DPN [MG18], OE</cell><cell cols="3">noise [HTLI19] or uniform GAN outputs [LLLS18],</cell><cell></cell><cell>ODIN [LLS18]</cell></row><row><cell></cell><cell cols="2">Ensemble [LPB17]</cell><cell></cell><cell></cell><cell cols="3">samples ([JLMG20])</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison between the novelty detection performance of ERD when using a holdout test set T for evaluation, or the same unlabeled set U that was used for fine-tuning the models.</figDesc><table><row><cell>ID data</cell><cell>OOD data</cell><cell>ERD (eval on T )</cell><cell>ERD (eval on U )</cell></row><row><cell></cell><cell></cell><cell cols="2">AUROC ? / TNR@95 ?</cell></row><row><cell>SVHN</cell><cell>CIFAR10</cell><cell>1.00 / 0.99</cell><cell>1.00 / 0.99</cell></row><row><cell>CIFAR10</cell><cell>SVHN</cell><cell>1.00 / 1.00</cell><cell>1.00 / 1.00</cell></row><row><cell>CIFAR100</cell><cell>SVHN</cell><cell>1.00 / 1.00</cell><cell>1.00 / 1.00</cell></row><row><cell>FMNIST[0,2,3,7,8]</cell><cell>FMNIST[1,4,5,6,9]</cell><cell>0.94 / 0.67</cell><cell>0.94 / 0.67</cell></row><row><cell>SVHN[0:4]</cell><cell>SVHN[5:9]</cell><cell>0.95 / 0.74</cell><cell>0.96 / 0.79</cell></row><row><cell>CIFAR10[0:4]</cell><cell>CIFAR10[5:9]</cell><cell>0.93 / 0.70</cell><cell>0.93 / 0.69</cell></row><row><cell>CIFAR100[0:49]</cell><cell>CIFAR100[50:99]</cell><cell>0.82 / 0.44</cell><cell>0.80 / 0.36</cell></row><row><cell cols="2">Average</cell><cell>0.95 / 0.79</cell><cell>0.95 / 0.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>OOD detection performance on CIFAR10 vs CIFAR10v2</figDesc><table><row><cell>ID data OOD data</cell><cell>Vanilla Ensembles</cell><cell>DPN</cell><cell>OE</cell><cell>Mahal.</cell><cell>MCD</cell><cell>Mahal-U</cell><cell>ERD</cell><cell>ERD++</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AUROC ? / TNR@95 ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CIFAR10 CIFAR10v2 0.64 / 0.13</cell><cell>0.63 / 0.09</cell><cell>0.64 / 0.12</cell><cell>0.55 / 0.08</cell><cell>0.58 / 0.10</cell><cell>0.56 / 0.07</cell><cell>0.76 / 0.26</cell><cell>0.91 / 0.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>OOD detection performance on data with covariate shift. For ERD and vanilla ensembles, we train 5 ResNet20 models for each setting. The evaluation metrics are computed on the unlabeled set.</figDesc><table><row><cell>ID data</cell><cell>OOD data</cell><cell>Vanilla Ensembles</cell><cell>DPN</cell><cell>OE</cell><cell>Mahal.</cell><cell>MCD</cell><cell>Mahal-U</cell><cell>ERD</cell><cell>ERD++</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AUROC ? / TNR@95 ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CIFAR10</cell><cell>CIFAR10-C sev 2 (A)</cell><cell>0.68 / 0.20</cell><cell>0.73 / 0.31</cell><cell>0.70 / 0.20</cell><cell>0.84 / 0.53</cell><cell>0.82 / 0.50</cell><cell>0.75 / 0.38</cell><cell>0.96 / 0.86</cell><cell>0.99 / 0.95</cell></row><row><cell>CIFAR10</cell><cell>CIFAR10-C sev 2 (W)</cell><cell>0.51 / 0.05</cell><cell>0.47 / 0.03</cell><cell>0.52 / 0.06</cell><cell>0.58 / 0.08</cell><cell>0.52 / 0.06</cell><cell>0.55 / 0.07</cell><cell>0.68 / 0.19</cell><cell>0.86 / 0.41</cell></row><row><cell>CIFAR10</cell><cell>CIFAR10-C sev 5 (A)</cell><cell>0.84 / 0.49</cell><cell>0.89 / 0.60</cell><cell>0.86 / 0.54</cell><cell>0.94 / 0.80</cell><cell>0.95 / 0.84</cell><cell>0.88 / 0.63</cell><cell>1.00 / 0.99</cell><cell>1.00 / 1.00</cell></row><row><cell>CIFAR10</cell><cell>CIFAR10-C sev 5 (W)</cell><cell>0.60 / 0.10</cell><cell>0.72 / 0.10</cell><cell>0.63 / 0.11</cell><cell>0.78 / 0.27</cell><cell>0.60 / 0.08</cell><cell>0.68 / 0.12</cell><cell>0.98 / 0.86</cell><cell>1.00 / 1.00</cell></row><row><cell>CIFAR100</cell><cell>CIFAR100-C sev 2 (A)</cell><cell>0.68 / 0.20</cell><cell>0.62 / 0.18</cell><cell>0.65 / 0.19</cell><cell>0.82 / 0.48</cell><cell>0.72 / 0.29</cell><cell>0.67 / 0.22</cell><cell>0.94 / 0.76</cell><cell>0.97 / 0.86</cell></row><row><cell>CIFAR100</cell><cell cols="2">CIFAR100-C sev 2 (W) 0.52 / 0.06</cell><cell>0.32 / 0.03</cell><cell>0.52 / 0.06</cell><cell>0.55 / 0.07</cell><cell>0.52 / 0.06</cell><cell>0.55 / 0.06</cell><cell>0.71 / 0.19</cell><cell>0.86 / 0.44</cell></row><row><cell>CIFAR100</cell><cell>CIFAR100-C sev 5 (A)</cell><cell>0.78 / 0.37</cell><cell>0.74 / 0.36</cell><cell>0.76 / 0.37</cell><cell>0.92 / 0.72</cell><cell>0.91 / 0.65</cell><cell>0.84 / 0.55</cell><cell>0.99 / 0.97</cell><cell>1.00 / 0.99</cell></row><row><cell>CIFAR100</cell><cell cols="2">CIFAR100-C sev 5 (W) 0.64 / 0.14</cell><cell>0.49 / 0.12</cell><cell>0.62 / 0.13</cell><cell>0.71 / 0.19</cell><cell>0.60 / 0.10</cell><cell>0.63 / 0.13</cell><cell>0.96 / 0.71</cell><cell>0.98 / 0.89</cell></row><row><cell cols="2">Tiny ImageNet Tiny ObjectNet</cell><cell>0.82 / 0.49</cell><cell>0.70 / 0.32</cell><cell>0.79 / 0.37</cell><cell>0.75 / 0.26</cell><cell>0.99 / 0.98</cell><cell>0.72 / 0.25</cell><cell>0.98 / 0.88</cell><cell>0.99 / 0.98</cell></row><row><cell></cell><cell>Average</cell><cell>0.67 / 0.23</cell><cell>0.63 / 0.23</cell><cell>0.67 / 0.23</cell><cell>0.76 / 0.38</cell><cell>0.74 / 0.39</cell><cell>0.70 / 0.27</cell><cell>0.91 / 0.71</cell><cell>0.96 / 0.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Experiments with a test set of size 1,000, with an equal number of ID and OOD test samples. For ERD and vanilla ensembles, we train 5 ResNet20 models for each setting. The evaluation metrics are computed on the unlabeled set.</figDesc><table><row><cell>ID data</cell><cell>OOD data</cell><cell>Vanilla Ensembles</cell><cell>DPN</cell><cell>OE</cell><cell>Mahal.</cell><cell>MCD</cell><cell>Mahal-U</cell><cell>ERD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AUROC ? / TNR@95 ?</cell><cell></cell><cell></cell></row><row><cell>SVHN</cell><cell>CIFAR10</cell><cell>0.97 / 0.88</cell><cell>1.00 / 1.00</cell><cell>1.00 / 1.00</cell><cell>0.99 / 0.98</cell><cell>0.97 / 0.85</cell><cell>0.99 / 0.95</cell><cell>1.00 / 0.99</cell></row><row><cell>CIFAR10</cell><cell>SVHN</cell><cell>0.92 / 0.78</cell><cell>0.95 / 0.85</cell><cell>0.97 / 0.89</cell><cell>0.99 / 0.96</cell><cell>1.00 / 0.98</cell><cell>0.99 / 0.96</cell><cell>1.00 / 1.00</cell></row><row><cell>CIFAR100</cell><cell>SVHN</cell><cell>0.84 / 0.48</cell><cell>0.77 / 0.44</cell><cell>0.82 / 0.50</cell><cell>0.98 / 0.90</cell><cell>0.97 / 0.73</cell><cell>0.98 / 0.92</cell><cell>0.99 / 1.00</cell></row><row><cell>SVHN[0:4]</cell><cell>SVHN[5:9]</cell><cell>0.92 / 0.69</cell><cell>0.87 / 0.19</cell><cell>0.85 / 0.52</cell><cell>0.92 / 0.71</cell><cell>0.91 / 0.51</cell><cell>0.91 / 0.63</cell><cell>0.97 / 0.86</cell></row><row><cell>CIFAR10[0:4]</cell><cell>CIFAR10[5:9]</cell><cell>0.80 / 0.39</cell><cell>0.82 / 0.32</cell><cell>0.82 / 0.41</cell><cell>0.79 / 0.27</cell><cell>0.69 / 0.25</cell><cell>0.64 / 0.13</cell><cell>0.87 / 0.50</cell></row><row><cell cols="2">CIFAR100[0:49] CIFAR100[50:99]</cell><cell>0.78 / 0.35</cell><cell>0.70 / 0.26</cell><cell>0.74 / 0.31</cell><cell>0.72 / 0.20</cell><cell>0.70 / 0.26</cell><cell>0.72 / 0.19</cell><cell>0.79 / 0.38</cell></row><row><cell>CIFAR10</cell><cell>CIFAR10-C sev 2 (A)</cell><cell>0.68 / 0.20</cell><cell>0.73 / 0.31</cell><cell>0.70 / 0.20</cell><cell>0.84 / 0.53</cell><cell>0.82 / 0.50</cell><cell>0.75 / 0.38</cell><cell>0.91 / 0.71</cell></row><row><cell>CIFAR10</cell><cell>CIFAR10-C sev 2 (W)</cell><cell>0.51 / 0.05</cell><cell>0.47 / 0.03</cell><cell>0.52 / 0.06</cell><cell>0.58 / 0.08</cell><cell>0.52 / 0.06</cell><cell>0.55 / 0.07</cell><cell>0.57 / 0.09</cell></row><row><cell>CIFAR10</cell><cell>CIFAR10-C sev 5 (A)</cell><cell>0.84 / 0.49</cell><cell>0.89 / 0.60</cell><cell>0.86 / 0.54</cell><cell>0.94 / 0.80</cell><cell>0.95 / 0.84</cell><cell>0.88 / 0.63</cell><cell>0.99 / 0.95</cell></row><row><cell>CIFAR10</cell><cell>CIFAR10-C sev 5 (W)</cell><cell>0.60 / 0.10</cell><cell>0.72 / 0.10</cell><cell>0.63 / 0.11</cell><cell>0.78 / 0.27</cell><cell>0.60 / 0.08</cell><cell>0.68 / 0.12</cell><cell>0.92 / 0.67</cell></row><row><cell>CIFAR100</cell><cell>CIFAR100-C sev 2 (A)</cell><cell>0.68 / 0.20</cell><cell>0.62 / 0.18</cell><cell>0.65 / 0.19</cell><cell>0.82 / 0.48</cell><cell>0.72 / 0.29</cell><cell>0.67 / 0.22</cell><cell>0.84 / 0.48</cell></row><row><cell>CIFAR100</cell><cell cols="2">CIFAR100-C sev 2 (W) 0.52 / 0.06</cell><cell>0.32 / 0.03</cell><cell>0.52 / 0.06</cell><cell>0.55 / 0.07</cell><cell>0.52 / 0.06</cell><cell>0.55 / 0.06</cell><cell>0.55 / 0.07</cell></row><row><cell>CIFAR100</cell><cell>CIFAR100-C sev 5 (A)</cell><cell>0.78 / 0.37</cell><cell>0.74 / 0.36</cell><cell>0.76 / 0.37</cell><cell>0.92 / 0.72</cell><cell>0.91 / 0.65</cell><cell>0.84 / 0.55</cell><cell>0.96 / 0.80</cell></row><row><cell>CIFAR100</cell><cell cols="2">CIFAR100-C sev 5 (W) 0.64 / 0.14</cell><cell>0.49 / 0.12</cell><cell>0.62 / 0.13</cell><cell>0.71 / 0.19</cell><cell>0.60 / 0.10</cell><cell>0.63 / 0.13</cell><cell>0.81 / 0.25</cell></row><row><cell></cell><cell>Average</cell><cell>0.75 / 0.37</cell><cell>0.72 / 0.34</cell><cell>0.75 / 0.38</cell><cell>0.82 / 0.51</cell><cell>0.78 / 0.44</cell><cell>0.77 / 0.42</cell><cell>0.87 / 0.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 .</head><label>11</label><figDesc>Results on MNIST/FashionMNIST settings. For ERD and vanilla ensembles, we train 5 3-hidden layer MLP models for each setting. The evaluation metrics are computed on the unlabeled set.</figDesc><table><row><cell>ID data</cell><cell>OOD data</cell><cell>Vanilla Ensembles</cell><cell>DPN</cell><cell>OE</cell><cell>Mahal.</cell><cell>nnPU</cell><cell>MCD</cell><cell>Mahal-U</cell><cell>Bin. Classif.</cell><cell>ERD</cell><cell>ERD++</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AUROC ? / TNR@95 ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MNIST</cell><cell>FMNIST</cell><cell>0.81 / 0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>Results with three different architectures for Vanilla and ERD ensembles. All ensembles comprise 5 models. For the corruption data sets, we report for each metric the average taken over all corruptions (A), and the value for the worst-case setting (W). The evaluation metrics are computed on the unlabeled set.</figDesc><table><row><cell>VGG16</cell><cell>ResNet20</cell><cell>WideResNet-28-10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 .</head><label>14</label><figDesc>Results obtained when evaluating on an OOD data set different from the one used for fine-tuning. All ERD ensembles are tuned on clean ID and OOD data and are evaluated on OOD data with corruptions.</figDesc><table><row><cell>ID data</cell><cell>OOD data in unlabeled set</cell><cell>Test-time OOD data</cell><cell>Vanilla Ensemble</cell><cell>ERD</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">AUROC ?</cell></row><row><cell>CIFAR10[0:4]</cell><cell>CIFAR10[5:9]</cell><cell>CIFAR10[5:9]-C sev 2 (A)</cell><cell>0.82</cell><cell>0.93</cell></row><row><cell>CIFAR10[0:4]</cell><cell>CIFAR10[5:9]</cell><cell>CIFAR10[5:9]-C sev 2 (W)</cell><cell>0.77</cell><cell>0.88</cell></row><row><cell>CIFAR10[0:4]</cell><cell>CIFAR10[5:9]</cell><cell>CIFAR10[5:9]-C sev 5 (A)</cell><cell>0.85</cell><cell>0.91</cell></row><row><cell>CIFAR10[0:4]</cell><cell>CIFAR10[5:9]</cell><cell>CIFAR10[5:9]-C sev 5 (W)</cell><cell>0.79</cell><cell>0.86</cell></row><row><cell>CIFAR100[0:49]</cell><cell>CIFAR100[50:99]</cell><cell>CIFAR100[50:99]-C sev 2 (A)</cell><cell>0.78</cell><cell>0.84</cell></row><row><cell>CIFAR100[0:49]</cell><cell>CIFAR100[50:99]</cell><cell>CIFAR100[50:99]-C sev 2 (W)</cell><cell>0.75</cell><cell>0.78</cell></row><row><cell>CIFAR100[0:49]</cell><cell>CIFAR100[50:99]</cell><cell>CIFAR100[50:99]-C sev 5 (A)</cell><cell>0.77</cell><cell>0.83</cell></row><row><cell>CIFAR100[0:49]</cell><cell>CIFAR100[50:99]</cell><cell>CIFAR100[50:99]-C sev 5 (W)</cell><cell>0.63</cell><cell>0.78</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also expect other distance metrics to be similarly effective.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our code is publicly available at https://github.com/ericpts/reto.<ref type="bibr" target="#b3">4</ref> In the appendix we also train the models from random initializations, i.e. ERD++, and obtain better novelty detection at the cost of more training iterations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In practice, choosing a good rejection threshold is important. A recent work<ref type="bibr" target="#b31">[32]</ref> proposes a criterion for selecting the threshold that is tailored specifically to the SSND setting. Alternatively, one can choose the threshold so as to achieve a desired FPR, which we can estimate using a validation set of ID samples.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We abuse notation slightly and denote our disagreement metric as (Avg ? ?) to contrast it with the ensemble entropy metric (H ? Avg), which first takes the average of the softmax outputs and only afterwards computes the score.<ref type="bibr" target="#b6">7</ref> We abuse notation slightly and denote our disagreement metric as (Avg ? ?) to contrast it with the ensemble entropy metric (H ? Avg), which first takes the average of the softmax outputs and only afterwards computes the score.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to C?lin Cruceru, Gideon Dresdner, Alexander Immer, Sidak Pal Singh and Armeen Taeb for feedback on the manuscript and to Ayush Garg for preliminary experiments. We also thank the anonymous reviewers for their helpful remarks.</p><p>The authors evaluate a number of methods on all these scenarios. The methods can be roughly categorized as follows:</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GANomaly: Semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ak?ay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9453" to="9463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting unlabeled data in ensemble methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demiriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Anomalous instance detection in deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06979</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning for chest X-ray analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sogancioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A benchmark of medical out of distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.04250</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning with augmented class by exploiting unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep variational semi-supervised novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurutach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04971</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of learning from positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring the limits of out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03004</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1050" to="1059" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recent advances in open set recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3614" to="3631" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Toward supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gornitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Brefeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="235" to="262" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Safe deep semi-supervised learning for unseen-class unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<meeting>the 37th International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Noise contrastive priors for functional uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximizing overall diversity for improved uncertainty estimates in deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4264" to="4271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Transfer learning for COVID-19 pneumonia detection and classification in chest X-ray images. medRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsamenis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Protopapadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voulodimos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Why normalizing flows fail to detect out-ofdistribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20578" to="20589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Positive-unlabeled learning with non-negative risk estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiryo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6402" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4313" to="4324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09070</idno>
		<title level="m">Hybrid discriminative-generative training via contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Open category detection with PAC guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garrepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3169" to="3178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7047" to="7058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised one-class support vector machines for classification of remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>M?noz-Mar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>G?mez-Chova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camp-Valls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="3188" to="3197" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? Evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13991" to="14002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Do ImageNet classifiers generalize to ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5389" to="5400" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A simple fix to Mahalanobis distance for improving near-OOD detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09022</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Likelihood ratios for out-of-distribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14707" to="14718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Detecting out-of-distribution examples with in-distribution examples and gram matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oore</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12510</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Transductive anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Novelty detection: Unlabeled data definitely help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Twelth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">{SSD}: A unified framework for self-supervised outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning and evaluating representations for deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A hybrid semi-supervised anomaly detection model for high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08059</idno>
		<title level="m">How does early stopping help generalization against label noise? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CSI: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11839" to="11852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Ledsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Macwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.05566</idno>
		<title level="m">Contrastive training for improved out-of-distribution detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heckel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09055</idno>
		<title level="m">Image recognition from raw labels collected without annotators</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised out-of-distribution detection by maximum classifier discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Hybrid models for open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploiting unlabeled data to enhance ensemble diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="98" to="129" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An unbiased risk estimator for learning with augmented classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10247" to="10258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Data-only methods: Fully non-parametric approaches like kNN</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<title level="m">Classifier-only methods: Methods that use a classifier trained on the training set</title>
		<imprint/>
	</monogr>
	<note>ODIN [30], Mahalanobis [28]. ERD falls into this category as well</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Methods with Auxiliary Models: Methods that use an autoencoder or a generative model, like a Variational Autoencoder or a Generative Adversarial Network. Some of these approaches can be expensive to train and difficult to optimize and tune</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">We stress the fact that for most of these methods the authors use (known) OOD data during training. Oftentimes the OOD samples observed during training come from a data set that is very similar to the OOD data used for evaluation. For exact details regarding the data sets and the methods used for the benchmark</title>
		<imprint/>
	</monogr>
	<note>we refer the reader to [6</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
