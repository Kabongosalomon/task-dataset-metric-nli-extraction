<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging Composite and Real: Towards End-to-end Deep Image Matting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhizi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>jing.zhang1@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
							<email>sjmaybank@dcs.bbk.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Information System</orgName>
								<orgName type="institution" key="instit1">Birkbeck College</orgName>
								<orgName type="institution" key="instit2">University of London</orgName>
								<address>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<postCode>2008</postCode>
									<settlement>Darlington</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhizi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
						</author>
						<title level="a" type="main">Bridging Composite and Real: Towards End-to-end Deep Image Matting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extracting accurate foregrounds from natural images benefits many downstream applications such as film production and augmented reality. However, the furry characteristics and various appearance of the foregrounds, e.g., animal and portrait, challenge existing matting methods, which usually require extra user inputs such as trimap or scribbles. To resolve these problems, we study the distinct roles of semantics and details for image matting and decompose the task into two parallel sub-tasks: high-level semantic segmentation and low-level details matting. Specifically, we propose a novel Glance and Focus Matting network (GFM), which employs a shared encoder and two separate decoders to learn both tasks in a collaborative manner for end-to-end natural image matting. Besides, due to the limitation of available natural images in the matting task, previous methods typically adopt composite images for training and evaluation, which result in limited generalization ability on real-world images. In this paper, we investigate the domain gap issue between composite images and real-world images systematically by conducting comprehensive analyses of various discrepancies between the foreground and background images. We find that a carefully designed composition route RSSN that aims to reduce the discrep-ancies can lead to a better model with remarkable generalization ability. Furthermore, we provide a benchmark containing 2,000 high-resolution real-world animal images and 10,000 portrait images along with their manually labeled alpha mattes to serve as a test bed for evaluating matting model's generalization ability on real-world images. Comprehensive empirical studies have demonstrated that GFM outperforms state-of-theart methods and effectively reduces the generalization error. The code and the datasets will be released at https://github.com/JizhiziLi/GFM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image matting refers to extracting the foreground alpha matte from an input image, requiring both hard labels for the explicit foreground or background and soft labels for the transition areas, which plays an important role in many applications, e.g., virtual reality, augmented reality, entertainment, etc. Typical foregrounds in image matting have furry details and diverse appearance, e.g., animal and portrait, which lay a great burden on image matting methods. How to recognize the semantic foregrounds or backgrounds as well as extracting the fine details for trimap-free natural image matting remains challenging in the image matting community.</p><p>For image matting, an image I is assumed to be a linear combination of foreground F and background B via a soft alpha matte ? ? [0, 1], i.e., due to the under-determined nature. To relieve the burden, previous matting methods adopt extra user inputs such as trimaps <ref type="bibr" target="#b31">(Xu et al., 2017)</ref> and scribbles <ref type="bibr" target="#b13">(Levin et al., 2007)</ref> as priors to decrease the degree of unknown. Based on sampling neighboring known pixels <ref type="bibr" target="#b30">(Wang and Cohen, 2007;</ref><ref type="bibr" target="#b24">Ruzon and Tomasi, 2000;</ref><ref type="bibr" target="#b29">Wang and Cohen, 2005)</ref> or defining an affinity matrix <ref type="bibr" target="#b38">(Zheng et al., 2008)</ref>, the known alpha values (i.e., foreground or background) are propagated to the unknown pixels. Usually, some edge-aware smoothness constraints are used to make the problem tractable <ref type="bibr" target="#b13">(Levin et al., 2007)</ref>. However, either the sampling or calculating affinity matrix is based on low-level color or structural features, which is not so discriminative at indistinct transition areas or fine edges. Consequently, their performance are sensitive to the size of unknown areas and may suffer from fuzzy boundaries and color blending. To address this issue, deep convolutional neural network (CNN)-based matting methods have been proposed <ref type="bibr" target="#b31">(Xu et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018;</ref><ref type="bibr" target="#b36">Zhang et al., 2019b;</ref><ref type="bibr" target="#b21">Qiao et al., 2020;</ref><ref type="bibr" target="#b18">Liu et al., 2020;</ref><ref type="bibr" target="#b33">Yu et al., 2021)</ref> to leverage their strong representative abilities to learn discriminative features <ref type="bibr">(Zhang and Tao, 2020)</ref>. Although CNN-based methods can achieve good matting results, the prerequisite trimaps or scribbles make them unlikely to be used in automatic applications such as the augmented reality of live streaming and film production.</p><p>To address this issue, end-to-end matting methods have been proposed <ref type="bibr" target="#b4">(Chen et al., 2018;</ref><ref type="bibr" target="#b36">Zhang et al., 2019b;</ref><ref type="bibr" target="#b25">Shen et al., 2016;</ref><ref type="bibr" target="#b21">Qiao et al., 2020;</ref><ref type="bibr" target="#b18">Liu et al., 2020)</ref> in recent years. Most of them can be categorized into two types. The first type shown in (i) of <ref type="figure" target="#fig_0">Figure 1</ref>(a) is a straightforward solution, which is to perform global segmentation <ref type="bibr" target="#b0">(Aksoy et al., 2018)</ref> and local matting sequentially. The former aims at trimap generation <ref type="bibr" target="#b4">(Chen et al., 2018;</ref><ref type="bibr" target="#b25">Shen et al., 2016)</ref> or foreground/background generation <ref type="bibr" target="#b36">(Zhang et al., 2019b)</ref> while the latter is image matting based on the trimap or other priors generated from the previous stage. The shortage of such a pipeline attributes to its sequential nature, since it may generate an erroneous semantic error which could not be corrected by the subsequent matting step. Besides, the separate training scheme in two stages may lead to a sub-optimal solution due to the mismatch between them. The second type is shown in (ii) of <ref type="figure" target="#fig_0">Figure 1</ref> (a), global information is provided as guidance while performing local matting. For example, coarse alpha matte is generated and used in the matting networks in  and in <ref type="bibr" target="#b21">(Qiao et al., 2020)</ref>, spatial-and channel-wise attention are adopted to provide global appearance filtration to the matting network. Such methods avoid the problem of stage-wise modeling and training but bring in new problems. Al-though global guidance is provided in an implicit way, it is challenging to generate alpha matte for both foreground/background areas and transition areas simultaneously in a single network due to their distinct appearance and semantics.</p><p>To solve the above problems, we study the distinct roles of semantics and details for natural image matting and explore the idea of decomposing the task into two parallel sub-tasks, semantic segmentation, and details matting. Specifically, we propose a novel end-toend matting model named Glance and Focus Matting network <ref type="bibr">(GFM)</ref>. It consists of a shared encoder and two separate decoders to learn both tasks in a collaborative manner for natural image matting, which is trained end-to-end in a single stage. Moreover, we also explore different data representation formats in the Glance Decoder and gain useful empirical insights in the semantictransition representations. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a)(iii), compared with previous methods, GFM is a unified model that models both sub-tasks explicitly and collaboratively in a single network.</p><p>Another challenge for image matting is the limitation of the available matting datasets. As shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref> ORI-Track, due to the laborious and costly labeling process, existing public matting datasets only have tens or hundreds of high-quality annotations <ref type="bibr" target="#b22">(Rhemann et al., 2009;</ref><ref type="bibr" target="#b25">Shen et al., 2016;</ref><ref type="bibr" target="#b31">Xu et al., 2017;</ref><ref type="bibr" target="#b36">Zhang et al., 2019b;</ref><ref type="bibr" target="#b21">Qiao et al., 2020)</ref>. They either only provide foregrounds and alpha mattes <ref type="bibr" target="#b31">(Xu et al., 2017;</ref><ref type="bibr" target="#b21">Qiao et al., 2020)</ref> as in (i) of <ref type="figure" target="#fig_0">Figure 1</ref>(b) ORI-Track, or provide fix-size and low-resolution (800 ? 600) portrait images with inaccurate alpha mattes <ref type="bibr" target="#b25">(Shen et al., 2016)</ref> generated by an ensemble of existing matting algorithms as in (ii) of <ref type="figure" target="#fig_0">Figure 1</ref>(b) ORI-Track. Due to the unavailability of real-world original images, as shown in (i) of <ref type="figure" target="#fig_0">Figure 1</ref>(b) COMP-Track, a common practice for data augmentation in matting is to composite one foreground with various background images by alpha blending according to Eq. (1) to generate large-scale composite data. The background images are usually choosing from existing benchmarks for image classification and detection, such as MS COCO <ref type="bibr" target="#b17">(Lin et al., 2014)</ref> and PASCAL VOC <ref type="bibr">(Everingham et al., 2010)</ref>. However, these background images are in low-resolution and may contain salient objects. In this paper, we point out that the training images following the above route have a significant domain gap with those natural images due to the composition artifacts, attributing to the resolution, sharpness, noise, and illumination discrepancies between the foreground and background images. The artifacts serve as cheap features to distinguish the foregrounds from the backgrounds and will mislead the The comparison between existing matting datasets and our proposed benchmark as well as the comparison between existing composition methods and our RSSN.</p><p>models during training, resulting in overfitted models with poor generalization on natural images. In this paper, we investigate the domain gap systematically and carry out comprehensive empirical analyses of the composition pipeline in image matting. We identify several kinds of discrepancies that lead to the domain gap and point out possible solutions to them. We then design a novel composition route named RSSN that can significantly reduce the domain gap arisen from the discrepancies of resolution, sharpness, noise, etc. Along with this, as shown in (ii) of <ref type="figure" target="#fig_0">Figure 1</ref>(b) COMP-Track, we propose a large-scale high-resolution clean background dataset (BG-20k) without salient foreground objects, which can be used in generating highresolution composite images. Extensive experiments show that the proposed composition route along with BG-20k can reduce the generalization error by 60% and achieve comparable performance as the model trained on original natural images. It opens an avenue for compositionbased image matting since obtaining foreground images and alpha mattes are much easier than those from original natural images by leveraging chroma keying.</p><p>To fairly evaluate matting models' generalization ability on real-world images, we make the first attempt to establish a large-scale benchmark consists of 2,000 high-resolution real-world animal images and 10,000 real-world portrait images along with manually carefully labeled fine alpha mattes. Comparing with previous datasets <ref type="bibr" target="#b31">(Xu et al., 2017;</ref><ref type="bibr" target="#b21">Qiao et al., 2020;</ref><ref type="bibr" target="#b25">Shen et al., 2016)</ref> as in (i) and (ii) of <ref type="figure" target="#fig_0">Figure 1</ref>(b) ORI-Track which only provide foreground images or low-resolution inaccurate alpha mattes, our benchmark includes all the high-resolution real-world original images and high-quality alpha mattes (more than 1080 pixels in the shorter side), which is beneficial to train models with better generalization on real-world images, and also suggests several new research problems which will be discussed later.</p><p>The contributions of this paper are four-fold: 1 ? We propose a novel model named GFM for endto-end image matting, which simultaneously generates global semantic segmentation and local alpha matte without any priors as input but a single image.</p><p>? We design a novel composition route RSSN to reduce various kinds of discrepancies and propose a large-scale high-resolution background dataset BG-20k to serve as better candidates for generating high-quality composite images.</p><p>? We construct a large-scale real-world images benchmark to benefit training a better model with good generalization by its large scale, diverse categories, and high-quality annotations.</p><p>? Extensive experiments on the benchmark demonstrate that GFM outperforms state-of-the-art (SOTA) matting models and can be a strong baseline for future research. Moreover, the proposed composition route RSSN demonstrates its value by reducing the generalization error by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Matting</head><p>Most classical image matting methods are using auxiliary inputs like trimaps <ref type="bibr" target="#b15">(Li et al., 2017;</ref><ref type="bibr" target="#b26">Sun et al., 2004;</ref><ref type="bibr" target="#b14">Levin et al., 2008;</ref><ref type="bibr" target="#b3">Chen et al., 2013;</ref><ref type="bibr" target="#b13">Levin et al., 2007)</ref>. They sample or propagate foreground and background labels to the unknown areas based on local smoothness assumptions. Recently, CNN-based methods improve them by learning discriminative features rather than relying on hand-crafted low-level color features <ref type="bibr" target="#b31">(Xu et al., 2017;</ref><ref type="bibr" target="#b19">Lu et al., 2019a;</ref><ref type="bibr" target="#b9">Hou and Liu, 2019;</ref><ref type="bibr" target="#b1">Cai et al., 2019;</ref><ref type="bibr" target="#b27">Tang et al., 2019)</ref>. Deep Matting <ref type="bibr" target="#b31">(Xu et al., 2017)</ref> employed an encoder-decoder structure to extract highlevel contextual features. IndexNet <ref type="bibr" target="#b19">(Lu et al., 2019a)</ref> focused on boundary recovery by learning the activation indices during down-sampling. However, trimap-based methods require user interaction, make them unlikely to be deployed in automatic applications. Recently, Chen et al. <ref type="bibr" target="#b4">(Chen et al., 2018)</ref> proposed an end-to-end model that first predicted the trimap then carried out matting. <ref type="bibr" target="#b36">Zhang et al. (Zhang et al., 2019b)</ref> also devised a twostage model that first segmented the foregrounds or the backgrounds and then refined them with a fusion net. Both methods separated the process of segmentation and matting into different stages, which may generate erroneous segmentation results that mislead the subsequent matting step. <ref type="bibr" target="#b21">Qiao et al. (Qiao et al., 2020)</ref> employed spatial and channel-wise attention to integrate appearance cues and pyramidal features while predicting, however, the distinct appearance and semantics of foreground/background areas and transition areas brought a heavy burden to a single-stage network and limited the quality of alpha matte prediction. <ref type="bibr" target="#b18">Liu et al. (Liu et al., 2020)</ref> proposed a network to perform human matting by predicting the coarse mask first, then adopting a refinement network to predict a more detailed one. Despite the necessity of stage-wise training and testing, a coarse mask was not enough for guiding the network to refine the details since the transition areas were not defined explicitly.</p><p>In contrast to previous methods, we devise a novel end-to-end matting model via multi-task learning, which addresses the segmentation and matting tasks simultaneously. It can learn both high-level semantic features and low-level structural features in a shared encoder, benefits the subsequent segmentation and matting decoders collaboratively. One close related work with ours is AdaMatting <ref type="bibr" target="#b1">(Cai et al., 2019)</ref>, which also has a structure of a shared encoder and two decoders. There are several significant differences: 1) AdaMatting requires a coarse trimap as an extra input while our GFM model only takes a single image as input without any priors; 2) the trimap branch in AdaMatting aims to refine the input trimap, which is much easier than generating a global representation in our case because the initial trimap actually serves as an attention mask for learning semantical features; 3) the adapted trimap generated from AdaMatting is serving as a guidance for alpha decoder via the following propagation unit, which is not suitable for end-to-end matting task since it lacks explicit collaboration for both decoders; 4) both the encoder and decoder structures of GFM are specifically designed for end-to-end matting, which differs from AdaMatting; and 5) we systematically investigate the semantic-transition representations in the glance decoder and gain useful empirical insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Matting Dataset</head><p>Existing matting datasets <ref type="bibr" target="#b22">(Rhemann et al., 2009;</ref><ref type="bibr" target="#b31">Xu et al., 2017;</ref><ref type="bibr" target="#b36">Zhang et al., 2019b;</ref><ref type="bibr" target="#b21">Qiao et al., 2020)</ref> only contain foregrounds and a small number of annotated alpha mattes, e.g., 27 training images and 8 test images in alphamatting <ref type="bibr" target="#b22">(Rhemann et al., 2009)</ref>, 431 training images and 50 test images in Comp-1k <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>, and 596 training images and 50 test images in HAttMatting <ref type="bibr" target="#b21">(Qiao et al., 2020)</ref>. DAPM <ref type="bibr" target="#b25">(Shen et al., 2016)</ref> proposes 2,000 real-world portrait images but at fix-size and low-resolution, together with limited quality alpha mattes generated by an ensemble of existing matting models. In contrast to them, we propose a highquality benchmark consists of 10,000 high-resolution real-world portrait images and 2,000 animal images and manually annotated alpha matte for each image. We empirically demonstrate that the model trained on our benchmark has a better generalization ability on realworld images than the one trained on composite images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Composition</head><p>As the inverse problem of image matting and the typical way of generating synthetic dataset, image com- position plays an important role in image editing. Researchers have been dedicated to improve the reality of composite images from the perspective of color, lighting, texture compatibility, and geometric consistency in the past years <ref type="bibr" target="#b32">(Xue et al., 2012;</ref><ref type="bibr" target="#b28">Tsai et al., 2017;</ref><ref type="bibr" target="#b2">Chen and Kae, 2019;</ref><ref type="bibr" target="#b6">Cong et al., 2020)</ref>. Xue et al. <ref type="bibr" target="#b32">(Xue et al., 2012)</ref> conducted experiments to evaluate how the image statistical measures including luminance, color temperature, saturation, local contrast, and hue determined the realism of a composite. Tsai et al. <ref type="bibr" target="#b28">(Tsai et al., 2017)</ref> proposed an end-to-end deep convolutional neural network to adjust the appearance of the foregrounds and backgrounds to be more compatible. Chen et al. <ref type="bibr" target="#b2">(Chen and Kae, 2019)</ref> proposed a generative adversarial network(GAN) architecture to learn geometrically and color consistent in the composites. <ref type="bibr" target="#b6">Cong et al. (Cong et al., 2020)</ref> contributed a large-scale image harmonization dataset and a network using a novel domain verification discriminator to reduce the inconsistency of the foregrounds and the backgrounds. Although they did a good job in harmonizing the composites to be more realistic, the domain gap still exists when fitting the synthetic data into the matting model, the reason is that a subjective agreed standard of harmonization by a human is not equivalent to a good training candidate for a machine learning model. Besides, such procedures may modify the boundary of the foregrounds and result in inaccuracy of the ground truth alpha mattes. In this paper, we alternatively focus on generating composite images that can be used to reduce the generalization error of matting models on natural images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GFM: Glance and Focus Matting Network</head><p>When tackling the image matting problem, we humans first glance at the image to quickly recognize the salient rough foreground or background areas and then focus on the transition areas to distinguish details from the background. It can be formulated as a segmentation stage and a matting stage roughly. Note that these two stages may be intertwined that there will be feedback from the second stage to correct the erroneous decision at the first stage, like some ambiguous areas caused by the protective coloration of animals or occlusions. To mimic the human experience and empower the matting model with proper abilities at both stages, it is reasonable to integrate them into a single model and explicitly model the collaboration. To this end, we propose a novel Glance and Focus Matting network for end-to-end image matting as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Shared Encoder</head><p>GFM has an encoder-decoder structure, where the encoder is shared by two subsequent decoders. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the encoder takes a single image as input and processes it through five blocks E 0 ? E 4 , where each reduces the resolution by half. We adopt the DenseNet-121 , <ref type="bibr">ResNet-34, or ResNet-101 (He et al., 2016)</ref> pre-trained on the Ima-geNet training set as our backbone encoder. Specifically, for DenseNet-121, we add a convolution layer to reduce the output feature channels to 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Glance Decoder (GD)</head><p>The glance decoder aims to recognize the easy semantic parts and leave the others as unknown areas. To this end, the decoder should have a large receptive field to learn high-level semantics. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we symmetrically stack five blocks D G 4 ? D G 0 as the decoder, each of which consists of three sequential 3 ? 3 convolutional layers and an upsampling layer. To enlarge the receptive field further, we add a pyramid pooling module (PPM) <ref type="bibr" target="#b37">(Zhao et al., 2017;</ref><ref type="bibr" target="#b19">Liu et al., 2019)</ref> after E 4 to extract global context, which is then connected to each decoder block D G i via element-wise sum. Loss Function The training loss for the glance decoder is a cross-entropy loss L CE defined as follows:</p><formula xml:id="formula_0">L CE = ? C c=1 G c g log G c p ,<label>(2)</label></formula><p>where G c p ? [0, 1] is the predicted probability for cth class, G c g ? {0, 1} is the ground truth label. The output of GD is a two-or three-channel (C = 2 or 3) class probability map depends on the semantic-transition representation, which will be detailed in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Focus Decoder (FD)</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, FD has the same basic structure as GD, i.e., symmetrically stacked five blocks D F 4 ? D F 0 . Different from GD, which aims to do roughly semantic segmentation, FD aims to extract details in the transition areas where low-level structural features are very useful. Therefore, we use a bridge block (BB) (Qin et al., 2019) instead of the PPM after E 4 to leverage local context in different receptive fields. Specifically, it consists of three dilated convolutional layers. The features from both E 4 and BB are concatenated and fed into D F 4 . We follow the U-net <ref type="bibr" target="#b23">(Ronneberger et al., 2015)</ref> style and add a shortcut between each encoder block E i and the decoder block D F i to preserve fine details. Loss Function The training loss for FD (L F D ) is composed of an alpha-prediction loss L T ? and a Laplacian loss L T lap in the unknown transition areas , i.e.,</p><formula xml:id="formula_1">L F D = L T ? + L T lap .<label>(3)</label></formula><p>Following <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>, the alpha loss L T ? is calculated as absolute difference between ground truth ? and predicted alpha matte ? F in the unknown transition region. It is defined as follows:</p><formula xml:id="formula_2">L T ? = i ? i ? ? F i ? W T i 2 + ? 2 i W T i ,<label>(4)</label></formula><p>where i denotes pixel index, W T i ? {0, 1} denotes whether pixel i belongs to the transition region or not. We add ? = 10 ?6 for computational stability. Following , the Laplacian loss L T lap is defined as the L1 distance between the Laplacian pyramid of ground truth and that of prediction.</p><formula xml:id="formula_3">L T lap = i W T i 5 k=1 (Lap k (? i ) ? Lap k (? F i ) 1 ,<label>(5)</label></formula><p>where Lap k denotes the kth level of the Laplacian pyramid. We use five levels in the Laplacian pyramid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">RoSTa: Representation of Semantic and Transition Areas</head><p>To investigate the impact of the representation format of the supervisory signal in our GFM, we adopt three kinds of Representations of Semantic and Transition areas (RoSTa) as the bridge to link GD and FD.</p><p>-GFM-TT We use the classical 3-class trimap T as the supervisory signal for GD, which is generated by dilation and erosion from ground truth alpha matte with a kernel size of 25. We use the ground truth alpha matte ? in the unknown transition areas as the supervisory signal for FD. -GFM-FT We use the 2-class foreground segmentation mask F as the supervisory signal for GD, which is generated by the erosion of ground truth alpha matte with a kernel size of 50 to ensure the left foreground part is correctly labeled. In this case, the area of I (? &gt; 0) ? F is treated as the transition area, where I (?) denotes the indicator function. We use the ground truth alpha matte ? in the transition area as the supervisory signal for FD. -GFM-BT We use the 2-class background segmentation mask B as the supervisory signal for glance decoder, which is generated by dilation of ground truth alpha matte with kernel size as 50 to ensure the left background part is correctly labeled. In this case, the area of B ?I (? &gt; 0) is treated as the transition area. We use the ground truth alpha matte ? in the transition area as the supervisory signal for FD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Collaborative Matting (CM)</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, CM merges the predictions from GD and FD to generate the final alpha prediction. Specifically, CM follows different rules when using different RoSTa as described in Section 3.4. In GFM-TT, CM replaces the transition area of the prediction of GD with the prediction of FD. In GFM-FT, CM adds the predictions from GD and FD to generate the final alpha matte. In GFM-BT, CM subtracts the prediction of FD from the prediction of GD as the final alpha matte. In this way, GD takes charge of recognizing rough foreground and background by learning global semantic features, and FD is responsible for matting details in the unknown areas by learning local structural features. Such task decomposition and specifically designed parallel decoders make the model simpler than the twostage ones in <ref type="bibr" target="#b4">(Chen et al., 2018;</ref><ref type="bibr" target="#b36">Zhang et al., 2019b)</ref>. Besides, both decoders are trained simultaneously that the losses can be back-propagated to each of them via the CM module. In this way, our model enables interaction between both decoders, so that the erroneous prediction can be corrected in time by the responsible branch. Obviously, it is expected to be more effective than the two-stage framework, where the erroneous segmentation in the first stage could not be corrected by the subsequent one and thus mislead it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>The training loss for collaborative matting (L CM ) consists of an alpha-prediction loss L ? , a Laplacian loss L lap , and a composition loss L comp , i.e.,</p><formula xml:id="formula_4">L CM = L ? + L lap + L comp .<label>(6)</label></formula><p>Here L ? and L lap are calculated according to Eq. (4) and Eq. (5) but in the whole alpha matte. Following <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>, the composition loss (L comp ) is calculated as the absolute difference between the composite images based on the ground truth alpha and the predicted alpha matte by referring to <ref type="bibr" target="#b13">(Levin et al., 2007)</ref>. It can be defined as follows:</p><formula xml:id="formula_5">L comp = i C(? i ) ? C(? CM i ) 2 + ? 2 N ,<label>(7)</label></formula><p>where C (?) denotes the composited image, ? CM is the predicted alpha matte by CM, and N denotes the number of pixels in the alpha matte.</p><p>To sum up, the final loss used during training is calculated as the sum of L CE , L F D and L CM , i.e.,</p><formula xml:id="formula_6">L = L CE + L F D + L CM .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RSSN: A Novel Composition Route</head><p>Since labeling alpha matte of real-world natural images is very laborious and costly, a common practice is to generate large-scale composite images from a few foreground images and the paired alpha mattes <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>. The prevalent matting composition route is to paste one foreground with various background images by alpha blending according to Eq. (1). However, since the foreground and background images are usually sampled from different distributions, there will be a lot of composition artifacts in the composite images, which leads to a large domain gap between the composite images and natural ones. The composition artifacts may mislead the model by serving as cheap features, resulting in overfitting on the composite images and producing large generalizing errors on natural images. In this section, we systematically analyze the factors that cause the composition artifacts including Resolution discrepancy, Semantic ambiguity, Sharpness discrepancy, and Noise discrepancy. To address these issues, we propose a new composition route named RSSN and a large-scale high-resolution background dataset named BG-20k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Resolution Discrepancy and Semantic Ambiguity</head><p>In the literature of image matting, the background images used for composition are usually chosen from existing benchmarks for image classification and detection, such as MS COCO <ref type="bibr" target="#b17">(Lin et al., 2014)</ref> and PASCAL VOC <ref type="bibr">(Everingham et al., 2010)</ref>. However, these background images are in low-resolution and may contain salient objects, causing the following two types of discrepancies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Resolution Discrepancy:</head><p>A typical image in MS COCO <ref type="bibr" target="#b17">(Lin et al., 2014)</ref> or Pascal VOC <ref type="bibr">(Everingham et al., 2010)</ref> has a resolution about 389 ? 466, which is much smaller compared to the high resolution foreground images in matting dataset such as Comp-1k <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>. The resolution discrepancy between the foreground and background images will result in obvious artifacts as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(b). 2. Semantic ambiguity: Images in MS COCO <ref type="bibr" target="#b17">(Lin et al., 2014)</ref> and Pascal VOC <ref type="bibr">(Everingham et al., 2010)</ref> are collected for classification and object detection tasks, which usually contain salient objects from different categories, including various animals, human, and objects. Directly pasting the foreground image with such background images will result in semantic ambiguity for end-to-end image matting. For example, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(b), there is a dog in the background which is beside the leopard in the composite image. Training with such images will mislead the model to ignore the background animal, i.e., probably learning few about semantics but more about discrepancies.</p><p>To address these issues, we collect a large-scale highresolution dataset named BG-20k to serve as good back-  <ref type="bibr" target="#b17">(Lin et al., 2014)</ref> with the foreground computed by <ref type="bibr" target="#b13">(Levin et al., 2007)</ref>. (c) Composite with a background from our proposed BG-20k by alpha blending of original image directly. (d) Composite with background from our proposed BG-20k with the foreground computed by <ref type="bibr" target="#b13">(Levin et al., 2007)</ref>. (e) Composite with the large-aperture effect.</p><p>ground candidates for composition. We only selected those images whose shortest side has at least 1080 pixels to reduce the resolution discrepancy. Moreover, we removed those images containing salient objects to eliminate semantic ambiguity. The details of constructing BG-20k are presented as follows.</p><p>1. We collected 50k high-resolution (HD) images using the keywords such as HD background, HD view, HD scene, HD wallpaper, abstract painting, interior design, art, landscape, nature, street, city, mountain, sea, urban, suburb from websites with open licenses 2 , removed those images whose shorter side has less than 1080 pixels and resized the left images to have 1080 pixels at the shorter side while keeping the original aspect ratio. The average resolution of images in BG-20k is 1180 ? 1539; 2. We removed duplicate images by a deep matching model <ref type="bibr" target="#b12">(Krizhevsky et al., 2012)</ref>. We adopted <ref type="bibr">YOLO-v3 (Redmon and Farhadi, 2018)</ref> and an object detection method  to detect salient objects and then manually double-checked to make sure each image has no salient objects. In this way, we built BG-20k containing 20,000 high-resolution clean images; 3. We split BG-20k into a disjoint training set <ref type="formula" target="#formula_3">(15k)</ref> and validation set (5k).</p><p>An composition example using the background image from BG-20k is shown in <ref type="figure" target="#fig_2">Figure 3</ref>(c) and <ref type="figure" target="#fig_2">Figure 3(d)</ref>. In (c), we use the foreground image computed by multiplying the ground truth alpha matte with the original image for alpha blending, in (d), we use the foreground image computed by the method in <ref type="bibr" target="#b13">(Levin et al., 2007)</ref> for alpha blending. As can be seen, there are obvious color artifacts in (c) that blend both colors of foreground and background in the fine details. The composite image in (d) is much more realistic than that in 2 https://unsplash.com/ and https://www.pexels.com/ (c). Therefore, we adopt the method in <ref type="bibr" target="#b13">(Levin et al., 2007)</ref> for computing the foreground images in our composition route. More examples of BG-20k are presented in <ref type="figure" target="#fig_3">Figure 4</ref> and the supplementary video. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sharpness Discrepancy</head><p>In photography, it is very common to highlight the sharp and salient foreground from the background context by adopting a large aperture and focal length on the foreground with the shallow depth-of-field and blurring the background with the out-of-focus effect. An example is shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a), where the leopard is the center of interest and the background is blurred. Previous composition methods dismiss this effect, producing a domain gap of sharpness discrepancy between the composite images and natural photos. Since we target the image matting task, where the foregrounds are usually salient in the images, we thereby investigate this effect in our composition route. Specifically, we simulate it by adopting the averaging filter in OpenCV with a kernel size chosen from 20, 30, 40, 50, 60 randomly to blur the background images. Since some natural photos may not have blurred backgrounds, we only use this technique in our composition route with a probability of 0.5. An example is shown in <ref type="figure" target="#fig_2">Figure 3</ref>(e), where the background is chosen from BG-20k and blurred using the averaging filter. As can be seen, it has a similar style to the original image in (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Noise Discrepancy</head><p>Since the foregrounds and backgrounds come from different image sources, they may contain different noise distributions. This is another type of discrepancy, which will mislead the model to search noise cues during training, resulting in overfitting. To address this discrepancy, we adopt BM3D <ref type="bibr" target="#b7">(Dabov et al., 2009)</ref> to remove noise in both foreground and background images in RSSN. Furthermore, we add Gaussian noise with a standard deviation of 10 to the composite image such that the noise distributions in both foreground and background areas are the same. We find that it is effective in improving the generalization ability of trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The RSSN Composition Route</head><p>We summarize the proposed composition route RSSN in Pipeline 1. The input of the pipeline is the matting dataset, e.g., AM-2k and PM-10k as will be introduced in Section 5.1, DIM <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>, or DAPM <ref type="bibr" target="#b25">(Shen et al., 2016)</ref>, and the proposed background image set BG-20k. If the matting dataset provides original images, e.g., AM-2K and PM-10k, we compute the foreground from the original image given the alpha matte by referring to <ref type="bibr" target="#b13">(Levin et al., 2007)</ref>. We random sample K background candidates from BG-20k for each foreground for data augmentation. We set K = 5 in our experiments. For each foreground image and background image, we carried out the denoising step with a probability of 0.5. To simulate the effect of large-aperture, we carried out the blur step on the background image with a probability of 0.5, where the blur kernel size was randomly sampled from {20, 30, 40, 50, 60}. We then generated the composite image according to the alpha-blending equation Eq. (1). Finally, with a probability of 0.5, we added Gaussian noise to the composite image to ensure the foreground and background areas have the same noise distribution. To this end, we generate a composite image set that has reduced many kinds of discrepancies, thereby narrowing the domain gap with natural images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20:</head><p>Alpha blending:</p><formula xml:id="formula_7">C ik = F i ? ? i + B ik ? (1 ? ? i ) 21:</formula><p>if random() &lt; 0.5 then 22:</p><p>C ik = AddGaussianN oise(C ik ) 23: Due to the tedious process for generating manually labeled high-quality alpha mattes, the amount of realworld matting datasets is very limited, most previous methods adopted composite datasets such as Comp-1k <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>, HATT-646 <ref type="bibr" target="#b21">(Qiao et al., 2020)</ref> and LF <ref type="bibr" target="#b36">(Zhang et al., 2019b)</ref> for data augmentation. However, as discussed in Section. 4.4, the composition artifacts caused by such convention would result in a large domain gap when adapting to real-world images. To fill this gap, we propose two large-scale high-resolution real-world image matting datasets AM-2k and PM-10k, consists of 2,000 animal images and 10,000 portrait images respectively, along with the high-quality manually labeled alpha mattes to serve as the appropriate training and testing bed for real-world image matting. We also set up two evaluation tracks for different purposes. Details are presented as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">AM-2k</head><p>AM-2k (Animal Matting 2,000 Dataset) consists of 2,000 high-resolution images collected and carefully selected from websites with open licenses. AM-2k contains 20 categories of animals including alpaca, antelope, bear, camel, cat, cattle, deer, dog, elephant, giraffe, horse, kangaroo, leopard, lion, monkey, rabbit, rhinoceros, sheep, tiger, zebra, each with 100 real-world images of various appearance and diverse backgrounds. We ensure the shorter side of each image is more than 1080 pixels. We then manually annotate the alpha mattes using open-source image editing softwares, e.g., Adobe Photoshop, GIMP, etc. We randomly select 1,800 out of 2,000 to form the training set and the rest 200 as the validation set. Some examples and their ground truth are shown in <ref type="figure" target="#fig_5">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">PM-10k</head><p>PM-10k (Portrait Matting 10,000 Dataset) consists of 10,000 high-resolution images collected and carefully selected from websites with open licenses. We ensure PM-10k includes images with multiple postures and diverse backgrounds. We process the images by a human keypoint detection method  to ensure each image contains clear and salient human. We then generate the ground truth alpha mattes as in AM-2k. Finally, we split 9,500 of 10,000 to serve as the training set and 500 as the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Benchmark Tracks</head><p>To benchmark the performance of matting models that 1) trained and tested both on real-world images; and 2) trained on composite images and tested on real-world images, we set up the following two evaluation tracks.</p><p>ORI-Track (Original Images Based Track) is set to perform end-to-end matting tasks on the original real-world images. The ORI-Track is the primary benchmark track.</p><p>COMP-Track (Composite Images Based Track) is set to investigate the influence of domain gap in image matting. As discussed before, the composite images have a large domain gap with natural images due to the composition artifacts. If we can reduce the domain gap and learn a domain-invariant feature representation, we can obtain a model with better generalization. To this end, we set up this track by making the first attempt towards this research direction. Specifically, we construct the composite training set by alpha-blending each foreground with five background images from the COCO dataset <ref type="bibr" target="#b17">(Lin et al., 2014)</ref> (denoted as COMP-COCO) and our BG-20k dataset (denoted as COMP-BG20K), or adopting the composition route RSSN proposed in Section 4.4 based on our BG-20K (denoted as COMP-RSSN). Moreover, unlike previous benchmarks that evaluate matting methods on composite images <ref type="bibr" target="#b31">(Xu et al., 2017;</ref><ref type="bibr" target="#b36">Zhang et al., 2019b;</ref><ref type="bibr" target="#b21">Qiao et al., 2020)</ref>, we evaluate matting methods on real-world images in the validation set same as the ORI-Track to validate their generalization ability.</p><p>Experiments were carried out on two tracks of the AM-2k and PM-10k datasets: 1) to compare the proposed GFM with SOTA methods, where we trained and evaluated them on the ORI-Track; and 2) to evaluate the side effect of domain gap caused by previous composition method and our proposed composition route, we trained and evaluated GFM and SOTA methods on the COMP-Track, i.e., COMP-COCO, COMP-BG20K, and COMP-RSSN, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics and Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Evaluation Metrics</head><p>Following the common practice in <ref type="bibr" target="#b22">(Rhemann et al., 2009;</ref><ref type="bibr" target="#b36">Zhang et al., 2019b;</ref><ref type="bibr" target="#b31">Xu et al., 2017)</ref>, we used the mean squared error (MSE), the sum of absolute differences (SAD), gradient (Grad.), and connectivity (Conn.) as the major metrics to evaluate the quality of alpha matte predictions. Note that the MSE and SAD metrics evaluate the pixel-wise differences between the prediction and the ground truth alpha matte, while the gradient and connectivity metrics favor clear details. Besides, we also use some auxiliary metrics such as Mean Absolute Difference (MAD), SAD-TRAN (SAD in the transition areas), SAD-FG (SAD in the foreground areas), and SAD-BG (SAD in the background   areas) to comprehensively evaluate the quality of the alpha matte predictions. While MAD evaluates the average quantitative difference regardless of the image size, SAD-TRAN, SAD-FG, and SAD-BG evaluate SAD in different semantic areas, respectively. In addition, we compared the model complexity of different methods in terms of the number of parameters, computational complexity, and inference time.</p><formula xml:id="formula_8">COMP-COCO COMP-BG20K COMP-RSSN Method SHM GFM-TT(d) GFM-TT(r) GFM-TT(r ? ) SHM GFM-TT(d) GFM-TT(r) GFM-TT(r ? ) SHM GFM-TT(d) GFM-FT(d) GFM-BT(d) GFM-TT(r) GFM-TT(r ? )<label>SAD</label></formula><formula xml:id="formula_9">-COCO COMP-BG20K COMP-RSSN Method SHM GFM-TT(d) GFM-TT(r) GFM-TT(r ? ) SHM GFM-TT(d) GFM-TT(r) GFM-TT(r ? ) SHM GFM-TT(d) GFM-FT(d) GFM-BT(d) GFM-TT(r) GFM-TT(r ? )<label>SAD</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Implementation Details</head><p>During training, we used multi-scale augmentation similar to <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>. Specifically, we cropped each of the selected images with size from {640 ? 640, 960 ? 960, 1280 ? 1280} randomly, resized the cropped image to 320 ? 320, and randomly flipped it with a probability of 0.5. The encoder of GFM was initialized with DenseNet-121 , <ref type="bibr">ResNet-34, or ResNet-101 (He et al., 2016)</ref> pre-trained on the ImageNet dataset. GFM was trained on two NVIDIA Tesla V100 GPUs. The batch size was 4 for DenseNet-121 , 32 for ResNet-34, and 8 for ResNet-101 <ref type="bibr" target="#b8">(He et al., 2016)</ref>. For the COMP-Track, we composite five training images by using five different backgrounds for each foreground on-the-fly during training. It took about two days to train GFM for 500 epochs on ORI-Track and 100 epochs on COMP-Track. The learning rate was fixed to 1 ? 10 ?5 for both tracks.</p><p>For baseline end-to-end matting methods LF <ref type="bibr" target="#b36">(Zhang et al., 2019b)</ref> and SSS <ref type="bibr" target="#b0">(Aksoy et al., 2018)</ref>, we used the official codes released by authors. For SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, HATT <ref type="bibr" target="#b21">(Qiao et al., 2020)</ref> and SHMC  with no public codes, we re-implemented them according to the papers. For SHMC  which does not specify the backbone network, we used ResNet-34 <ref type="bibr" target="#b8">(He et al., 2016)</ref> for a fair comparison. These models were trained using the training set on the ORI-Track or COMP-Track.</p><p>Furthermore, we also evaluated the performance of several representative trimap-based matting methods, including DIM <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>, GCA <ref type="bibr" target="#b16">(Li and Lu, 2020)</ref>, and IndexNet <ref type="bibr" target="#b20">(Lu et al., 2019b)</ref> on both ORI-Track and COMP-Track of AM-2k and PM-10k. For DIM <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>, we implement the code following the papers since the original code is not published. For GCA <ref type="bibr" target="#b16">(Li and Lu, 2020)</ref> and IndexNet <ref type="bibr" target="#b20">(Lu et al., 2019b)</ref>, we used the official code released by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Quantitative and Subjective Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Results on the ORI-Track</head><p>We benchmarked several SOTA methods <ref type="bibr" target="#b4">(Chen et al., 2018;</ref><ref type="bibr" target="#b36">Zhang et al., 2019b;</ref><ref type="bibr" target="#b0">Aksoy et al., 2018;</ref><ref type="bibr" target="#b21">Qiao et al., 2020;</ref><ref type="bibr" target="#b18">Liu et al., 2020)</ref> on the ORI-Track of AM-2k and PM-10k. The results are summarized in the top rows of <ref type="table" target="#tab_0">Table 1</ref>. GFM-TT, GFM-FT, and GFM-BT denote the proposed GFM model with different RoSTa as described in Section 3.4. (d), (r) and (r ?) stand for using DenseNet-121 , ResNet-34 <ref type="bibr" target="#b8">(He et al., 2016)</ref>, and ResNet-101 <ref type="bibr" target="#b8">(He et al., 2016)</ref> as the backbone encoder, respectively. There are several empirical findings from <ref type="table" target="#tab_0">Table 1.</ref> SSS <ref type="bibr" target="#b0">(Aksoy et al., 2018)</ref> has a larger foreground and background SAD errors than other methods because it aims to extract all the semantic regions in the image rather than extracting the salient animal or portrait foregrounds like other methods. SHMC, which adopts global guidance  , and the stage-wise method LF <ref type="bibr" target="#b36">(Zhang et al., 2019b)</ref> perform better than SSS but have large SAD errors in the transition area. Because they have not explicitly defined the transition area, the matting networks have limited abilities to distinguish the details in the transition area. HATT <ref type="bibr" target="#b21">(Qiao et al., 2020)</ref> obtains a smaller SAD error in the transition area and foreground area, owing to the attention module which can provide better global appearance filtration. SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref> performs better than the above methods, e.g., obtaining a smaller SAD error in the transition area and the background area than HATT <ref type="bibr" target="#b21">(Qiao et al., 2020)</ref>. We believe the improvement credits to the explicit definition of RoSTa (i.e., the trimap) and the PSPNet <ref type="bibr" target="#b37">(Zhao et al., 2017)</ref> used in the first stage which has a good semantic segmentation capability. Nevertheless, SHM still has a large error in the background area due to its stage-wise pipeline, which will accumulate the segmentation error into the matting network.</p><p>Compared with all the SOTA methods, our GFM achieves the best performance by simultaneously segmenting the foregrounds, backgrounds, and matting on the transition areas, regardless of which RoSTa and encoder backbone it adopts. For example, it achieves the lowest SAD error in different areas, i.e. 8.24 v.s. 10.26 for AM-2k, 7.70 v.s. 8.53 for PM-10k in the transition area, 0.42 v.s. 0.60 for AM-2k, 0.69 v.s. 0.74 for PM-10k in the foreground area, and 0.58 v.s. 6.95 for AM-2k, 1.44 v.s. 7.37 for PM-10k in the background area compared with the previous best method SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>. The results of using different RoSTa are comparable, especially for FT and BT, since they both define two classes in the image for segmentation by the GD. GFM using TT as the RoSTa performs the best due to its explicit definition of the transition area as well as the foreground and background areas. We also tried three different backbone networks, DenseNet-121 , <ref type="bibr">ResNet-34, and ResNet-101 (He et al., 2016)</ref>. All of them achieve the best performance compared with other SOTA methods. The superiority of GFM over other methods can be explained as follows.</p><p>First, compared with stage-wise methods, GFM can be trained in a single stage and the collaboration module acts as an effective gateway to back-propagate matting errors to the responsible branch adaptively. Second, compared with those methods that adopt global guidance, GFM explicitly models the end-to-end matting task into two separate but collaborative sub-tasks by two distinct decoders. Moreover, it uses a collaboration module to merge the predictions according to the definition of RoSTa, which explicitly defines the role of each decoder.</p><p>From <ref type="figure" target="#fig_6">Figure 6</ref> and <ref type="figure" target="#fig_7">Figure 7</ref>, we can find similar observations. SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, LF <ref type="bibr" target="#b36">(Zhang et al., 2019b)</ref>, and SSS <ref type="bibr" target="#b0">(Aksoy et al., 2018)</ref> fail to segment some foreground parts, implying inferiority of its stagewise network structure. HATT <ref type="bibr" target="#b21">(Qiao et al., 2020)</ref> and SHMC  struggle to obtain clear details in the transition areas since the global guidance is helpful for recognizing the semantic areas while being less useful for matting of details. Compared to them, our GFM achieves the best results owing to the advantage of a unified model, which deals with the foreground/background and transition areas using separate decoders and optimizes them in a collaborative manner. More results of GFM can be found in the demo video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Results on the COMP-Track</head><p>We evaluated SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, the best performed SOTA method, and our GFM with three different backbones on the COMP-Track of AM-2k and PM-10k including COMP-COCO, COMP-BG20K, and COMP-RSSN. The results are summarized in the bottom rows of <ref type="table" target="#tab_0">Table 1</ref>, from which we have several empirical findings. First, when training matting models using images from MS COCO dataset <ref type="bibr" target="#b17">(Lin et al., 2014)</ref> as backgrounds, GFM performs much better than SHM <ref type="bibr">(Chen et al., 2018), i.e, 46.16, 30.05, and 33.79 v.s. 182.70 for AM-2k, 61.69, 34.58, and 33.90 v.s. 168</ref>.75 for PM-10k in terms of whole image SAD, confirming the superiority of the proposed model over the two-stage one for generalization. Second, GFM using ResNet-34 or ResNet-101 <ref type="bibr" target="#b8">(He et al., 2016)</ref> performs better than using DenseNet-121 , implying that the residual structure in ResNet has better representation ability in extracting more accurate semantic representations and generalization ability in dealing with the domain gap between composite and real data, especially at the settings of COMP-BG20K Track and COMP-RSSN Track. Third, when training matting models using background images from the proposed BG-20k dataset, the errors of all the methods are significantly reduced, especially for SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, i.e.,   5.55 6.16 5.75 6.32 6.99 6.79 6.11 6.71 6.47 6.08 5.78 6.24 from 182.70 to 52.36 for AM-2k, or 168.75 to 34.06 for PM-10k, which mainly attributes to the reduction of SAD error in the background area, i.e., from 134.43 to 33.52 for AM-2k and 123.62 to 11.55 for PM-10k. There is the same trend for GFM(d), GFM(r) and GFM(r ?). These results confirm the value of our BG-20k, which helps to reduce resolution discrepancy and eliminate semantic ambiguity in the background area. Fourth, when using the proposed RSSN for training, the errors can be reduced further for SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, i.e., from 52.36 to 23.94 for AM-2k and 34.06 to 22.02 for PM-10k, from 25.19 to 19.19 and 21.54 to 18.15 for GFM(d), from 16.44 to 15.88 and 20.29 to 13.84 for GFM(r), and from 15.88 to 14.78, 18.11 to 12.51 for GFM(r ?) The improvement is attributed to the composition techniques in RSSN: 1) we simulate the large-aperture effect to reduce sharpness discrepancy; and 2) we remove the noise of foregrounds/backgrounds and add noise to the composite images to reduce noise discrepancy. Note that the SAD error of SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref> has dramatically reduced about 87% from 182.70 to 23.93 for AM-2k or 168.75 to 22.02 for PM-10k when using RSSN compared with the traditional composition method based on MS COCO dataset, which is even comparable with the one obtained by training using original images, i.e., 17.81 for AM-2k and 16.64 for PM-10k. It demonstrates that the proposed composition route RSSN can significantly narrow the domain gap and help to learn down-invariant features. Last, We also conducted experiments by using different RoSTa in GFM(d) on the track COMP-RSSN, their results have a similar trend to those on the ORI-TRACK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Results of Trimap-based Matting Methods</head><p>We also benchmarked several SOTA trimap-based matting methods <ref type="bibr" target="#b31">(Xu et al., 2017;</ref><ref type="bibr" target="#b16">Li and Lu, 2020;</ref><ref type="bibr" target="#b20">Lu et al., 2019b)</ref> on the ORI-Track and COMP-Track of AM-2k and PM-10k. The results are summarized in the <ref type="table" target="#tab_4">Table 2</ref>. As can be seen, the performance of trimapbased methods follows the same trends as the trimapfree matting methods on both tracks. For all the three methods, there are performance gaps between training them using the MS COCO <ref type="bibr" target="#b17">(Lin et al., 2014)</ref> background and the original images from our dataset as background. The gaps are different for each method, i.e. 7.59 to 6.82, 6.78 to 6.00 for DIM on the two datasets, 8.97 to 7.28, 7.45 to 6.16 for <ref type="bibr">GCA,</ref><ref type="bibr">9.59 to 7.40,</ref><ref type="bibr">7</ref>.36 to 6.44 for IndexNet, respectively. As always, after employing BG-20k and RSSN, the error has decreased for all methods on two datasets. Besides, there are two points to note, 1) even with RSSN, the performance gaps still exist for trimap-based matting methods, which deserves further studies; and 2) trimap-based matting methods perform better than our GFM in terms of SAD-TRAN, i.e., 6.82 to 8.24 on AM-2k, 6.00 to 7.70 on PM-10k, implying that GFM can be further improved on the transition area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">GFM with Trimap-based Matting Method</head><p>To further investigate the effectiveness of GFM, we provide the evaluation results of several variants on the ORI-Track of AM-2k in <ref type="table">Table 3</ref>, including 1) DIM: the trimap-based matting method DIM <ref type="bibr" target="#b31">(Xu et al., 2017)</ref>, which is trained and tested both on the ground truth trimap; 2) GFM(d): our proposed GFM with DesNet-121 as the backbone; 3) GFM(d)+DIM: we replace the predicted alpha matte in the transition area with the result from DIM, which is trained on the ground truth trimap while tested on the predicted trimap generated by GFM GD; and 4) SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>: a twostage "trimap prediction + trimap-based matting method" network, where its trimap-based matting method is very similar to DIM. As always, SAD, MSE, MAD, Grad., and Conn. are calculated on the whole image. SAD-TRAN, SAD-FG, and SAD-BG calculated in transition, foreground, and background area respectively. <ref type="table">Table 3</ref> Comparison of GFM and its variant with DIM <ref type="bibr" target="#b31">(Xu et al., 2017)</ref> and SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>  There are several conclusions we can draw from <ref type="table">Table 3</ref>. First, GFM still performs the best among all the trimap-free matting methods. Specifically, compared with the "GFM+DIM" variant, which uses the trimap-based matting method DIM <ref type="bibr" target="#b31">(Xu et al., 2017)</ref> to obtain the alpha matte in the transition area, our end-to-end GFM model still performs better, especially in the transition area, i.e. 8.24 to 9.54 for GFM(d). These results validate the effectiveness of the FD in our GFM. Second, comparing DIM with "GFM+DIM" variant, we can find that DIM is very sensitive to the trimap, e.g., 6.82 to 9.54 for GFM <ref type="bibr">(d)</ref>. It can also explain the above performance gap between GFM and "GFM+DIM" variant. These results validate the effectiveness of our proposed one-stage "sharing encoder + multi-task decoder" structure since the FD can be adapted to the predicted trimap from the GD, owing to the collaboration matting module. This can be further proven by comparing GFM(d) with SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, which is a typical two-stage structure. Third, compared with trimapbased method DIM, which is trained and tested on the ground truth trimap, there are still rooms to improve GFM in the transition area (SAD 6.82 to 8.24), which can be the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Results on the ORI-Track</head><p>To further verify the benefit of the designed structure of GFM, we conducted ablation studies on several variants of GFM on the ORI-Track of AM-2k, including 1) motivated by Qin et.al <ref type="bibr">(Qin et al., 2019)</ref>, in GFM encoder when using ResNet-34 <ref type="bibr" target="#b8">(He et al., 2016)</ref> as the backbone, we modified the convolution kernel of E 0 from 7 ? 7 with stride 2 to 3 ? 3 with stride 1, removed the first max pooling layer in E 0 , and added two more encoder layers E 5 and E 6 after E 4 , each of which had a max pooling layer with stride 2 and three basic resblocks with 512 filters, denoting "r2b"; 2) using a single decoder to replace both FD and GD in GFM, denoting "SINGLE"; 3) excluding the pyramid pooling module (PPM) in GD, and 4) excluding the bridge block (BB) in FD. The results are summarized in the top rows of <ref type="table" target="#tab_6">Table 4</ref>. First, when using r2b structure, all the metrics have been improved compared with GFM-TT(r), which is attributed to the larger feature maps at the early stage of the encoder part. However, it has more parameters and computations than GFM-TT(r), which will be discussed later. Second, using a single decoder results in worse performance, i.e., SAD increases from 10.26 to 13.79 for GFM-TT(d) and 10.89 to 15.50 for GFM-TT(r), which confirms the value of decomposing the end-to-end image matting task into two collaborative sub-tasks. Third, without PPM, SAD increases from 10.26 to 10.86 for GFM-TT(d) and 10.89 to 11.90 for GFM-TT(r), demonstrating that the global context features by PPM due to its larger receptive field are beneficial for semantic segmentation in GD. Moreover, compared with excluding PPM, using one PPM block leads to better performance but still falls behind the default setting, i.e., using five PPM blocks. Fourth, without BB, SAD increases from 10.26 to 11.27 for GFM-TT(d) and 10.89 to 11.29 for GFM-TT(r), demonstrating that the learned local structural features from BB are beneficial for matting in FD due to its dilated convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Results on the COMP-Track</head><p>To verify the different techniques in the proposed composition route RSSN, we conducted ablation studies on several variants of RSSN, including 1) only using the simulation of large-aperture effect, denoting "w/ blur"; 2) only removing foreground and background noise, denoting "w/ denoise"; 3) only adding noise on the composite images, denoting "w/ noise"; and 4) using all the techniques in RSSN, denoting "w/ RSSN". We used the BG-20k for sampling background images in these experiments. The results are summarized in the bottom rows of <ref type="table" target="#tab_6">Table 4</ref>. First, compared with the baseline model listed in the first row, which was trained using the composite images by alpha-blending, each technique in the proposed composition route is helpful to improve the matting performance in terms of all the metrics. Second, simulation of large-aperture effect and adding noise on the composite images are more effective than denoising. Third, different techniques are complementary to each other in that they contribute to the best performance achieved by RSSN collaboratively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">RoSTa Integration</head><p>Since TT, FT, and BT have their own advantages, how to design the network to make benefit from them remains a challenge. In this paper, we explored two RoSTa integration techniques on AM-2k, and compared them with individual RoSTa. Please note that for a fair comparison, we set the testing resolution ratio as 1/3 for all the methods. The results are shown in <ref type="table" target="#tab_7">Table 5</ref>. A straightforward solution for RoSTa integration is to employ ensemble by taking the median value of alpha mattes obtained by the models of all RoSTa, denoted as EN-median in <ref type="table" target="#tab_7">Table 5</ref>. The results are better than each individual RoSTa on all evaluation metrics, i.e. SAD 10.62 to 11.28, 11.76 and 12.26. However, the test speed of EN-median is quite slow comparing with others. To address this issue, we designed a new variant of GFM with a RoSTa Integration Module (RIM) to make use of all three representations in a learnable manner. Specifically, we modified the last layer of GD and FD to three RoSTa-specific layers, where each pair of outputs from a specific RoSTa layer goes through a Collaboration Matting(CM) module to obtain the alpha matte. The three alpha mattes are then concatenated together and fed into RIM. RIM consists of a concatenation layer, an 1?1 convolutional layer to transform the feature channel from 3 to 16, and a Squeeze-and-Excitation (SE) attention module <ref type="bibr" target="#b10">(Hu et al., 2018)</ref> to help re-calibrate the features and select the most informative ones to predict a better alpha matte via another 1?1 convolutional layer. We adopted the same training loss as the original GFM, except that there are three groups of losses corresponding to the outputs of three RoSTa-specific layers and a CM loss on the final alpha matte after RIM. In this way, the network is trained to learn a better RoSTa integration.</p><p>As shown in <ref type="table" target="#tab_7">Table 5</ref>, the result of GFM-RIM is better than each individual RoSTa on four evaluation metrics and it achieves the best performance on MSE, even outperforming EN-median, i.e. 0.0025 to 0.0026. It is also worth noting that GFM-RIM has comparable test speed with each individual RoSTa, while runs much faster than EN-median. In summary, GFM-RIM makes a good trade-off between performance and speed. The study of more effective RoSTa integration techniques deserves more effort. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Hybrid-resolution Test</head><p>To investigate the influence of balancing GD and FD with different down-sampling ratios during the test, we conducted several experiments and reported the results in the <ref type="table" target="#tab_8">Table 6</ref>. For simplicity, we denote the downsampling ratio at each step as 1/d 1 and 1/d 2 , which are subject to d 1 ? {2, 3, 4}, d 2 ? {2, 3, 4}, and d 1 ? d 2 . A larger d 1 increases the receptive field and benefits the GD, while a smaller d 2 benefits the FD with clear details in high-resolution images. We then find out that when using a hybrid-resolution test strategy, the per-formance achieves the best when d 1 = 3 and d 2 = 2, but with the cost of slower inference time.</p><p>To address this issue, we searched the optimal hyperparameter setting of the down-sampling ratio to make a better balance between the two decoders. As can be observed from <ref type="table" target="#tab_8">Table 6</ref>, GFM achieves the best when the ratios are 1/2 for FD and 1/3 for GD. Accordingly, we tried to search for the best hyper-parameter that is the same for both decoders, i.e., from 1/2 to 1/3 with a step of 0.2. As can be seen, GFM achieves the best performance when d 1 = d 2 and equals to 2.6, i.e., resizing the test image to 5/13 of its original size. The results on all metrics are comparable to the previously best setting, i.e., a hybrid-resolution ratio of 1/2 and 1/3. It is also noting that there is only a single forward pass when using the same ratio for both decoders, which is much more computationally efficient than the hybridresolution test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Model Complexity Analysis</head><p>We compared the number of model parameters (million, denoting "M"), computational complexity (denoting "GMac"), and inference time (seconds, denoting "s") of each method on an image resized to 800 ? 800. All methods are performed on a server with an Intel Xeon CPU (2.30GHz) and an NIVDIA Tesla V100 GPU (16GB memory). As shown in <ref type="table">Table 7</ref>, GFM using either DenseNet-121  or <ref type="bibr">ResNet-34, or ResNet-101 (He et al., 2016)</ref> as the backbone surpasses SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref>, LF <ref type="bibr" target="#b36">(Zhang et al., 2019b)</ref>, HATT <ref type="bibr" target="#b21">(Qiao et al., 2020)</ref>, and SHMC  in running speed, i.e., taking about 0.2085s and 0.1734s to process an image. In terms of parameters, GFM has fewer parameters than all the SOTA methods except for LF <ref type="bibr" target="#b36">(Zhang et al., 2019b)</ref>. For computational complexity, GFM has fewer computations than all the SOTA methods when adopting ResNet-34 <ref type="bibr" target="#b8">(He et al., 2016)</ref> as the backbone, i.e., 132.28 GMacs. When adopting DenseNet-121 , it only has more computations than SHMC  while being smaller. As for GFM(r2b) and GFM(r ?), they have more parameters and computations, but similar inference time. Although it can achieve better results, a trade-off between performance and complexity should be made for practical applications. Generally, GFM is light-weight and computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>In this paper, we propose a novel deep matting model for end-to-end natural image matting. It addresses two challenges in the matting task: 1) recognizing various foregrounds with diverse shapes, sizes, and textures from different categories; and 2) extracting details from ambiguous context background. Specifically, a Glance Decoder is devised for the first task and a Focus Decoder is devised for the latter one, while they share an encoder and are trained jointly. Therefore, they collaboratively accomplish the matting task and achieve superior performance than state-of-the-art matting methods. Besides, we also investigate the domain discrepancy issue between composite images and natural ones, which suggests that the common practice for data augmentation may not be suitable for training end-to-end matting models. To remedy this issue, we establish two large-scale real-world image matting datasets AM-2k and PM-10k, which contains 2,000 high-resolution animal images from 20 categories and 10,000 high-resolution portrait images along with the manually labeled alpha mattes. Furthermore, we systematically analyze the factors affecting composition quality including resolution, sharpness, semantic, and noise, and propose a novel composition route together with a large-scale background dataset BG-20k containing 20,000 high-resolution images without salient objects, which can effectively address the domain discrepancy issue. Extensive experiments validate the superiority of the proposed methods over state-of-the-art methods. We believe the proposed matting method and composition route will benefit the research for both trimap-based and end-to-end image matting. Moreover, the proposed datasets can provide a test bed to study the matting problem regarding the domain discrepancy issue.</p><p>Although GFM outperforms state-of-the-art methods in terms of both objective metrics and subjective evaluation, there are some limitations to be addressed in future work. First, after taking a detailed analysis of the error source as evidenced by SAD-TRAN, SAD-FG, and SAD-BG, the error in the transition areas is larger than SAD in the foreground and background areas, i.e., 8.24 v.s. 2.01 or 7.80 v.s. 4.09, even if the size of transition areas is usually much smaller than that of foreground and background areas. It tells that the performance could be further enhanced by devising a more effective Focus Decoder as well as leveraging some structure-aware and perceptual losses. Second, there is still room to improve the composite-based models to match those trained using original images by domain adaptation methods <ref type="bibr" target="#b35">(Zhang et al., 2019a)</ref>, since the cost required to generate a composite dataset is much easier than constructing a natural images based one. Besides, given that alpha matting and alpha blending are inverse problems, it is interesting to see whether or not these two tasks benefit each other if we model them in a sin- <ref type="table">Table 7</ref> Comparison of model parameters, computational complexity, and inference time. (d), (r) and (r ?) stand for DenseNet-121  and <ref type="bibr">ResNet-34, and ResNet-101 (He et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Parameters (M) Complexity (GMac) Inference time (s) SHM <ref type="bibr" target="#b4">(Chen et al., 2018)</ref> 79.27 870.16 0.3346 LF <ref type="bibr" target="#b36">(Zhang et al., 2019b)</ref> 37.91 2821.14 0.3623 HATT <ref type="bibr" target="#b21">(Qiao et al., 2020)</ref> 106.96 1502.46 0.5176 SHMC  78 gle framework. Third, current available natural image matting datasets contain only salient foreground images with small transition areas, e.g., human or animal, how to expand the dataset as well as extending the matting method to handle semi-transparent or long-range transition areas like a plastic bag or raindrops, remains as an open challenge. To this end, the two decoders in our GFM may be modified to adapt to different matting image types, i.e., containing salient objects or semitransparent objects. For example, a new representation like the trimap for the GD can be investigated to help the FD focus on explicit transition areas in those different types of images. Fourth, investigation of the interaction between the GD and FD to enable a more effective collaborative learning scheme also deserves more effort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>(a) The comparison between representative end-to-end-matting methods in (i) and (ii) and our GFM in (iii). (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Diagram of the proposed Glance and Focus Matting (GFM) network, which consists of a shared encoder and two separate decoders responsible for rough segmentation of the whole image and details matting in the transition area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Comparison of different image composition methods. (a) Original natural image. (b) Composite with a background from MS COCO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>Some examples from our BG-20k dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>for Real-world Image Matting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>Some examples from our AM-2k dataset. The alpha matte is displayed on the right of the original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6</head><label>6</label><figDesc>Subjective comparisons and the close views on AM-2k ORI-Track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7</head><label>7</label><figDesc>Subjective comparisons and the close views on PM-10k ORI-Track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Pipeline 1 :</head><label>1</label><figDesc>The Proposed Composition Route: RSSN Input: The matting dataset M containing |M | images and the background image set BG-20k Output: The composite image set C 1: for each i ? [1, |M |] do 2:</figDesc><table><row><cell></cell><cell>AM-2k,</cell></row><row><cell></cell><cell>PM-10k then</cell></row><row><cell>3:</cell><cell>Sample an original image I i ? M</cell></row><row><cell>4:</cell><cell>Sample the paired alpha matte ? i ? M</cell></row><row><cell>5:</cell><cell>Compute the foreground F i given (I i , ? i ) (Levin</cell></row><row><cell></cell><cell>et al., 2007)</cell></row><row><cell>6:</cell><cell>else</cell></row><row><cell>7:</cell><cell>Sample a foreground image F i ? M</cell></row><row><cell>8:</cell><cell>Sample the paired alpha matte ? i ? M</cell></row><row><cell>9:</cell><cell>end if</cell></row><row><cell>10:</cell><cell>for each k ? [1, K] do</cell></row><row><cell>11:</cell><cell>Sample a background candidate B ik ? BG-20k</cell></row><row><cell>12:</cell><cell>if random() &lt; 0.5 then</cell></row><row><cell>13:</cell><cell>F (Dabov</cell></row><row><cell></cell><cell>et al., 2009)</cell></row><row><cell>14:</cell><cell>B ik = Denoise(B ik )</cell></row><row><cell>15:</cell><cell>end if</cell></row><row><cell>16:</cell><cell>if random() &lt; 0.5 then</cell></row><row><cell>17:</cell><cell>Sample a blur kernel size r ? {20, 30, 40, 50, 60}</cell></row><row><cell>18:</cell><cell></cell></row></table><note>if there are original images in M , e.g.i = Denoise(F i ) //denoising by BM3DB ik = Blur(B ik , r) // the averaging filter 19: end if</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Results on the ORI-Track and COMP-Track of AM-2k and PM-10k. (d) stands for DenseNet-121 backbone, (r) stands for ResNet-34<ref type="bibr" target="#b8">(He et al., 2016)</ref> backbone and (r ?) stands for ResNet-101<ref type="bibr" target="#b8">(He et al., 2016)</ref> backbone. Representations of T T , F T and BT can refer to Section 3.4.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AM-2k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Track</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ORI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>SHM</cell><cell>LF</cell><cell>SSS</cell><cell>HATT</cell><cell cols="3">SHMC GFM-TT(d) GFM-FT(d)</cell><cell>GFM-BT(d)</cell><cell>GFM-TT(r)</cell><cell>GFM-FT(r)</cell><cell cols="4">GFM-BT(r) GFM-TT(r  ? ) GFM-FT(r  ? ) GFM-BT(r  ? )</cell></row><row><cell>SAD</cell><cell>17.81</cell><cell>36.12</cell><cell>552.88</cell><cell>28.01</cell><cell>61.50</cell><cell>10.26</cell><cell>12.74</cell><cell>12.74</cell><cell>10.89</cell><cell>12.58</cell><cell>12.61</cell><cell>9.66</cell><cell>11.54</cell><cell>12.80</cell></row><row><cell>MSE</cell><cell>0.0068</cell><cell>0.0116</cell><cell>0.2742</cell><cell>0.0055</cell><cell>0.0270</cell><cell>0.0029</cell><cell>0.0038</cell><cell>0.0030</cell><cell>0.0029</cell><cell>0.0037</cell><cell>0.0028</cell><cell>0.0024</cell><cell>0.0032</cell><cell>0.0036</cell></row><row><cell>MAD</cell><cell>0.0102</cell><cell>0.0210</cell><cell>0.3225</cell><cell>0.0161</cell><cell>0.0356</cell><cell>0.0059</cell><cell>0.0075</cell><cell>0.0075</cell><cell>0.0064</cell><cell>0.0073</cell><cell>0.0074</cell><cell>0.0056</cell><cell>0.0067</cell><cell>0.0075</cell></row><row><cell>Grad.</cell><cell>12.54</cell><cell>21.06</cell><cell>60.81</cell><cell>18.29</cell><cell>37.00</cell><cell>8.82</cell><cell>9.98</cell><cell>9.13</cell><cell>10.00</cell><cell>10.33</cell><cell>9.27</cell><cell>9.37</cell><cell>10.32</cell><cell>19.25</cell></row><row><cell>Conn.</cell><cell>17.02</cell><cell>33.62</cell><cell>555.97</cell><cell>17.76</cell><cell>60.94</cell><cell>9.57</cell><cell>11.78</cell><cell>10.07</cell><cell>9.99</cell><cell>11.65</cell><cell>9.77</cell><cell>8.98</cell><cell>10.89</cell><cell>12.06</cell></row><row><cell>SAD-TRAN</cell><cell>10.26</cell><cell>19.68</cell><cell>88.23</cell><cell>13.36</cell><cell>35.23</cell><cell>8.24</cell><cell>9.66</cell><cell>8.67</cell><cell>9.15</cell><cell>9.34</cell><cell>8.77</cell><cell>8.55</cell><cell>8.92</cell><cell>9.97</cell></row><row><cell>SAD-FG</cell><cell>0.60</cell><cell>3.79</cell><cell>401.66</cell><cell>1.36</cell><cell>10.93</cell><cell>0.42</cell><cell>1.47</cell><cell>3.07</cell><cell>0.77</cell><cell>1.31</cell><cell>2.84</cell><cell>0.53</cell><cell>0.92</cell><cell>0.93</cell></row><row><cell>SAD-BG</cell><cell>6.95</cell><cell>12.55</cell><cell>62.99</cell><cell>13.29</cell><cell>15.34</cell><cell>1.59</cell><cell>1.61</cell><cell>1.00</cell><cell>0.96</cell><cell>1.93</cell><cell>1.00</cell><cell>0.58</cell><cell>1.70</cell><cell>1.90</cell></row><row><cell>Track</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Results of trimap-based methods on the ORI-Track and COMP-Track of AM-2k and PM-10k.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AM-2k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Track</cell><cell></cell><cell>ORI</cell><cell></cell><cell></cell><cell cols="2">COMP-COCO</cell><cell></cell><cell cols="2">COMP-BG20K</cell><cell></cell><cell cols="2">COMP-RSSN</cell></row><row><cell>Method</cell><cell>DIM</cell><cell>GCA</cell><cell>IndexNet</cell><cell>DIM</cell><cell>GCA</cell><cell>IndexNet</cell><cell>DIM</cell><cell>GCA</cell><cell>IndexNet</cell><cell>DIM</cell><cell>GCA</cell><cell>IndexNet</cell></row><row><cell>SAD</cell><cell>6.82</cell><cell>7.28</cell><cell>7.40</cell><cell>7.59</cell><cell>8.97</cell><cell>9.59</cell><cell>7.45</cell><cell>8.61</cell><cell>9.23</cell><cell>7.35</cell><cell>8.56</cell><cell>8.16</cell></row><row><cell>MSE</cell><cell cols="2">0.0010 0.0011</cell><cell>0.0010</cell><cell cols="2">0.0012 0.0016</cell><cell>0.0015</cell><cell cols="2">0.0011 0.0015</cell><cell>0.0015</cell><cell cols="2">0.0011 0.0015</cell><cell>0.0012</cell></row><row><cell>MAD</cell><cell cols="2">0.0040 0.0042</cell><cell>0.0043</cell><cell cols="2">0.0044 0.0052</cell><cell>0.0056</cell><cell cols="2">0.0044 0.0050</cell><cell>0.0054</cell><cell cols="2">0.0043 0.0050</cell><cell>0.0047</cell></row><row><cell>Grad.</cell><cell>7.63</cell><cell>7.76</cell><cell>8.81</cell><cell>8.54</cell><cell>11.54</cell><cell>12.93</cell><cell>8.48</cell><cell>10.92</cell><cell>12.71</cell><cell>8.46</cell><cell>10.69</cell><cell>9.86</cell></row><row><cell>Conn.</cell><cell>6.17</cell><cell>6.50</cell><cell>6.41</cell><cell>6.89</cell><cell>8.40</cell><cell>8.84</cell><cell>6.75</cell><cell>8.02</cell><cell>8.52</cell><cell>6.68</cell><cell>7.92</cell><cell>7.42</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PM-10k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Track</cell><cell></cell><cell>ORI</cell><cell></cell><cell></cell><cell cols="2">COMP-COCO</cell><cell></cell><cell cols="2">COMP-BG20K</cell><cell></cell><cell cols="2">COMP-RSSN</cell></row><row><cell>Method</cell><cell>DIM</cell><cell>GCA</cell><cell>IndexNet</cell><cell>DIM</cell><cell>GCA</cell><cell>IndexNet</cell><cell>DIM</cell><cell>GCA</cell><cell>IndexNet</cell><cell>DIM</cell><cell>GCA</cell><cell>IndexNet</cell></row><row><cell>SAD</cell><cell>6.00</cell><cell>6.16</cell><cell>6.44</cell><cell>6.78</cell><cell>7.45</cell><cell>7.36</cell><cell>6.55</cell><cell>7.18</cell><cell>6.97</cell><cell>6.52</cell><cell>6.30</cell><cell>6.83</cell></row><row><cell>MSE</cell><cell cols="2">0.0008 0.0008</cell><cell>0.0008</cell><cell cols="2">0.0010 0.0012</cell><cell>0.0011</cell><cell cols="2">0.0010 0.0011</cell><cell>0.0010</cell><cell cols="2">0.0010 0.0090</cell><cell>0.0010</cell></row><row><cell>MAD</cell><cell cols="2">0.0035 0.0036</cell><cell>0.0038</cell><cell cols="2">0.0039 0.0043</cell><cell>0.0043</cell><cell cols="2">0.0038 0.0042</cell><cell>0.0041</cell><cell cols="2">0.0038 0.0037</cell><cell>0.0040</cell></row><row><cell>Grad.</cell><cell>8.59</cell><cell>8.32</cell><cell>9.87</cell><cell>10.58</cell><cell>10.45</cell><cell>10.98</cell><cell>9.96</cell><cell>10.07</cell><cell>10.06</cell><cell>10.05</cell><cell>7.43</cell><cell>9.90</cell></row><row><cell>Conn.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Ablation study of GFM on AM-2k.</figDesc><table><row><cell>Track</cell><cell></cell><cell></cell><cell>ORI</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD</cell><cell>Grad</cell><cell>Conn</cell></row><row><cell>GFM-TT(d)</cell><cell>10.26</cell><cell>0.0029</cell><cell>0.0059</cell><cell>8.82</cell><cell>9.57</cell></row><row><cell>GFM-TT(r)</cell><cell>10.89</cell><cell>0.0029</cell><cell>0.0064</cell><cell>10.00</cell><cell>9.99</cell></row><row><cell>GFM-TT(r2b)</cell><cell cols="2">10.24 0.0028</cell><cell>0.0060</cell><cell>8.65</cell><cell>9.33</cell></row><row><cell>GFM-TT-SINGLE(d)</cell><cell>13.79</cell><cell>0.0040</cell><cell>0.0081</cell><cell>13.45</cell><cell>13.04</cell></row><row><cell>GFM-TT-SINGLE(r)</cell><cell>15.50</cell><cell>0.0040</cell><cell>0.0091</cell><cell>14.21</cell><cell>13.15</cell></row><row><cell>GFM-TT(d) w/ one PPM</cell><cell>10.71</cell><cell>0.0029</cell><cell>0.0063</cell><cell>10.23</cell><cell>9.88</cell></row><row><cell>GFM-TT(d) excl. PPM</cell><cell>10.86</cell><cell>0.0030</cell><cell>0.0064</cell><cell>9.91</cell><cell>9.92</cell></row><row><cell>GFM-TT(d) excl. BB</cell><cell>11.27</cell><cell>0.0035</cell><cell>0.0067</cell><cell>9.33</cell><cell>10.40</cell></row><row><cell>GFM-TT(r) excl. PPM</cell><cell>11.90</cell><cell>0.0035</cell><cell>0.0070</cell><cell>10.50</cell><cell>11.07</cell></row><row><cell>GFM-TT(r) excl. BB</cell><cell>11.29</cell><cell>0.0032</cell><cell>0.0066</cell><cell>9.59</cell><cell>10.43</cell></row><row><cell>Track</cell><cell></cell><cell cols="2">COMP-RSSN</cell><cell></cell><cell></cell></row><row><cell>GFM-TT(d)</cell><cell>25.19</cell><cell>0.0104</cell><cell>0.0146</cell><cell>15.04</cell><cell>24.31</cell></row><row><cell>GFM-TT(d) w/ blur</cell><cell>21.37</cell><cell>0.0081</cell><cell>0.0124</cell><cell>14.31</cell><cell>20.50</cell></row><row><cell>GFM-TT(d) w/ denoise</cell><cell>22.95</cell><cell>0.0090</cell><cell>0.0134</cell><cell>14.37</cell><cell>22.10</cell></row><row><cell>GFM-TT(d) w/ noise</cell><cell>19.87</cell><cell>0.0075</cell><cell>0.0116</cell><cell>13.22</cell><cell>18.97</cell></row><row><cell>GFM-TT(d) w/ RSSN</cell><cell cols="3">19.19 0.0069 0.0112</cell><cell>13.37</cell><cell>18.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Results of GFM using different RoSTa and twoRoSTa integration techniques on AM-2k.</figDesc><table><row><cell>RoSTa</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD</cell><cell cols="3">Grad. Conn. Speed (s)</cell></row><row><cell>TT</cell><cell>11.28</cell><cell>0.0030</cell><cell>0.0065</cell><cell>10.89</cell><cell>10.63</cell><cell>0.1536</cell></row><row><cell>BT</cell><cell>11.76</cell><cell>0.0033</cell><cell>0.0070</cell><cell>11.56</cell><cell>11.15</cell><cell>0.1416</cell></row><row><cell>FT</cell><cell>12.26</cell><cell>0.0034</cell><cell>0.0072</cell><cell>12.52</cell><cell>11.68</cell><cell>0.1326</cell></row><row><cell cols="2">EN-median 10.62</cell><cell>0.0026</cell><cell>0.0062</cell><cell>11.19</cell><cell>9.85</cell><cell>0.4644</cell></row><row><cell>GFM-RIM</cell><cell>10.79</cell><cell>0.0025</cell><cell>0.0063</cell><cell>11.61</cell><cell>10.11</cell><cell>0.1859</cell></row><row><cell cols="7">5.5 RoSTa Integration and Hybrid-resolution Test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>Grid search results of the down-sampling ratio during test in GFM on AM-2k.</figDesc><table><row><cell>d 1</cell><cell>d 2</cell><cell>SAD</cell><cell>MSE</cell><cell>MAD</cell><cell cols="2">Grad. Conn.</cell></row><row><cell>2</cell><cell>2</cell><cell>12.26</cell><cell>0.0041</cell><cell>0.0072</cell><cell>9.18</cell><cell>11.59</cell></row><row><cell cols="2">2.2 2.2</cell><cell>11.49</cell><cell>0.0036</cell><cell>0.0067</cell><cell>9.22</cell><cell>10.81</cell></row><row><cell cols="2">2.4 2.4</cell><cell>11.66</cell><cell>0.0036</cell><cell>0.0068</cell><cell>9.34</cell><cell>10.99</cell></row><row><cell cols="2">2.6 2.6</cell><cell>10.77</cell><cell>0.0029</cell><cell>0.0062</cell><cell>9.93</cell><cell>10.10</cell></row><row><cell cols="2">2.8 2.8</cell><cell>10.96</cell><cell>0.0030</cell><cell>0.0064</cell><cell>10.06</cell><cell>10.28</cell></row><row><cell>3</cell><cell>3</cell><cell>11.28</cell><cell>0.0030</cell><cell>0.0065</cell><cell>10.89</cell><cell>10.63</cell></row><row><cell>4</cell><cell>4</cell><cell>15.99</cell><cell>0.0054</cell><cell>0.0095</cell><cell>14.75</cell><cell>15.35</cell></row><row><cell>3</cell><cell>2</cell><cell>10.26</cell><cell cols="2">0.0029 0.0059</cell><cell>8.82</cell><cell>9.57</cell></row><row><cell>4</cell><cell>2</cell><cell>12.96</cell><cell>0.0046</cell><cell>0.0077</cell><cell>9.58</cell><cell>12.27</cell></row><row><cell>4</cell><cell>3</cell><cell>13.97</cell><cell>0.0048</cell><cell>0.0083</cell><cell>11.61</cell><cell>13.32</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code, datasets, models, and a video demo will be made publicly available at https://github.com/JizhiziLi/ GFM.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic soft segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Disentangled image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8819" to="8828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward realistic image compositing with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8415" to="8424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knn matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recursive context routing for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="160" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dovenet: Deep image harmonization via domain verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8394" to="8403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bm3d image denoising with shape-adaptive principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K ;</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPARS&apos;09-Signal Processing with Adaptive Sparse Structured Representations Everingham M</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
		</imprint>
	</monogr>
	<note>The pascal visual object classes (voc) challenge</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context-aware image matting for simultaneous foreground and alpha estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4130" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spectral matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1699" to="1712" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Patch alignment manifold matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3214" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural image matting via guided contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11450" to="11457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Boosting semantic human matting with coarse annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8563" to="8572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J ;</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3266" to="3275" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Indices matter: Learning to index for deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention-guided hierarchical structure aggregation for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X ;</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Qin X</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Qin X</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">180402767</biblScope>
		</imprint>
	</monogr>
	<note>Basnet: Boundary-aware salient object detection. Yolov3: An incremental improvement. arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A perceptually motivated online benchmark for image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1826" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Alpha estimation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ruzon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="92" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Poisson matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning-based sampling for natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Aydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3055" to="3063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep image harmonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3789" to="3797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An iterative optimization approach for unified image segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="936" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding and improving the realism of image composites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dorsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rushmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mask guided matting via progressive refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition Zhang J, Tao D (2020) Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Zhang J, Tao D (2020) Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="7789" to="7817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards high performance human keypoint detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2639" to="2662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7469" to="7478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fuzzymatte: A computationally efficient scheme for interactive matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kambhamettu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
