<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mutual Suppression Network for Video Prediction using Disentangled Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungbeom</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungmin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
							<email>sryoon@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<postCode>08826</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Corresponding Author</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mutual Suppression Network for Video Prediction using Disentangled Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>LEE ET AL.: VIDEO PREDICTION USING DISENTANGLED FEATURES 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video prediction has been considered a difficult problem because the video contains not only high-dimensional spatial information but also complex temporal information. Video prediction can be performed by finding features in recent frames, and using them to generate approximations to upcoming frames. We approach this problem by disentangling spatial and temporal features in videos. We introduce a mutual suppression network (MSnet) which are trained in an adversarial manner and then produces spatial features which are free of motion information, and motion features with no spatial information. MSnet then uses motion-guided connection within an encoder-decoder-based architecture to transform spatial features from a previous frame to the time of an upcoming frame. We show how MSnet can be used for video prediction using disentangled representations. We also carry out experiments to assess the effectiveness of our method to disentangle features. MSnet obtains better results than other recent video prediction methods even though it has simpler encoders.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given a sequence of frames from a video, the process of video prediction attempts to generate one or more upcoming frames. Video prediction is important in real-time systems such as robots, closed-circuit television (CCTV), and self-driving cars, and also has a place in applications such as the unsupervised learning of image representations from videos <ref type="bibr" target="#b17">[18]</ref>.</p><p>The learning of representations from images has been studied extensively, and the results now surpass human ability <ref type="bibr" target="#b3">[4]</ref>. However, learning representations from videos remains a challenging task because of the temporal dimension, which brings a huge number of variations, and because it is not possible to annotate every frame in a video with labels. Some 'natural' labeling of videos is possible, for instance based on temporal coherence. However, the entangling of content and motion information in videos tends to make unsupervised learning challenging. In this regard, there have been previous works on decomposing videos into content and motion components <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. While the learning techniques used on images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> can be extended for the content representations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>, the learning of representations of motion has not been studied so extensively. Temporal information can be obtained from optical flow <ref type="bibr" target="#b11">[12]</ref>, with reasonable results. However, optical flow estimation arXiv:1804.04810v2 [cs.CV] 14 Jul 2019 involves a great deal of computation and depends on having a labeled dataset, which requires tremendous effort and cost to obtain.</p><p>We propose a technique in which a mutual suppression network (MSnet) is used to disentangle motion and content features. This approach is based on the following intuitive assertions: Separability of features: We train MSnet in such a way that information of one type is suppressed during the extraction of features of another type. This can be achieved by mutual adversarial learning. Content from several frames: The majority of methods that encode video into motion and content obtain content features from a single frame. We argue that content features, as well as motion features, should be obtained from several frames. A single frame is not sufficient to capture content information if two objects are occluded or cannot be distinguished. Reproducibility: Given three frames x (1) , x <ref type="bibr" target="#b1">(2)</ref> , and x <ref type="bibr" target="#b2">(3)</ref> , the content features from x (1) and x <ref type="bibr" target="#b1">(2)</ref> , together with the motion features from x <ref type="bibr" target="#b1">(2)</ref> and x <ref type="bibr" target="#b2">(3)</ref> , should allow us to reproduce x <ref type="bibr" target="#b2">(3)</ref> . This leads to motion and content features which contain semantic information. Time-reversibility of content: While previous methods have been based on the assumption that content features are mostly time-invariant, we propose that the content features should be time-reversible, so a content feature obtained from (x 1 , x 2 ) should be the same as that obtained from (x 2 , x 1 ). This time-reversible property is intended to ensure that motion information is not unwittingly included in content features because we extract content features from two frames, which may be related by temporal information.</p><p>The second step is the frame prediction task using the encoders and a generator trained in the first step. To generate frames from the features from the encoders, previous methods utilize the skip connections as used in UNet <ref type="bibr" target="#b13">[14]</ref>, that transfers information direct from a previous frame to a target frame. During frame prediction, however, it is better that the generator takes information related to the target frame, not the previous frame. Therefore we introduce a motion-guided connection which modifies the information from the previous frame to become the information needed for generating the target frame by considering the motion features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is no easy way of representing spatial and temporal information simultaneously in videos. Recent work in video representation learning has therefore focused on disentangling temporal and spatial information in natural videos. Simonyan and Zisserman used a twostream network for action recognition in videos, motivated by the way in which the human visual cortex decouples complementary information appearing in videos <ref type="bibr" target="#b16">[17]</ref>, which has subsequently been used for various fields of video processing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>The prediction of video frames requires the ability both to understand previous frames and to produce realistic new frames. These tasks can be facilitated by decomposing a video into motion and content components using techniques based on a two-stream network. VGAN <ref type="bibr" target="#b20">[21]</ref> predicted upcoming frames by modeling the foreground separately from the background. MCnet <ref type="bibr" target="#b19">[20]</ref> used an encoder-decoder technique to separate the motion and content information of a video: a content encoder extracts spatial features from the most recent frame of a video, and a motion encoder captures motion dynamics from pixel-wise differences between previous pairs of frames. However, few of these differences contain any semantic information about motions. DRnet <ref type="bibr" target="#b1">[2]</ref> used a content discriminator to separate the pose attributes from the content attributes in a frame. The content discriminator examines whether two pose features relate to the same content or not. The pose features acquired in this way are used to predict future pose features, from which upcoming frames can be generated. However, DRnet cannot catch pure content information because it only uses one-way suppression. For example, when predicting the frames using videos in the KTH dataset <ref type="bibr" target="#b14">[15]</ref>, DRnet sometimes changed the identities of human in the predicted frames. In addition, poses tend to be more ambiguous than motions in videos, so DRnet sometimes swapped the locations of two numerals when applied to the Moving MNIST dataset <ref type="bibr" target="#b17">[18]</ref>. DRnet is only concerned with a series of absolute locations (poses), and not with relative locations (motion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>The proposed method consists of two steps. The first step is frame reproduction, which obtains disentangled features from a consideration of semantics in the frames. The second step is video prediction using the disentangled features obtained during frame reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Frame Reproduction</head><p>Let x t denote the t th frame in video x. The frame reproduction is to reproduce x t+k from the known frames x t , x t+1 , and x t+k . Following the 'Reproducibility' assertion presented in the Introduction, we reproduce x t+k from the content of x t , x t+1 and the motion of x t+1 , x t+k , with the aim of obtaining disentangled motion and content features by considering semantics. We describe our network architecture in Section 3.1.1 and our training procedure in Section 3.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Network Architecture</head><p>The structure of MSnet is presented in <ref type="figure" target="#fig_0">Figure 1</ref>. One encoder extracts content features and another extracts motion features. From these features, a generator reproduces x t+k . Specifically, The content encoder E c obtains the content information E c (x t , x t+1 ) from two successive frames x t and x t+1 . The motion encoder E m extracts motion information E m (x t+1 , x t+k ) from frames x t+1 and x t+k , which do not have to be adjacent. The generator G reproduces the last frame of the input x t+k . The motivation for these settings is shown in the appendix.</p><p>Motion-guided Connection: The generator G is connected to the content encoder E c by blockwise motion-guided connections, which play a similar role to the skip connections in UNet <ref type="bibr" target="#b13">[14]</ref>, but each motion-guided connection performs an additional convolution operation guided by motion feature. This reduces ghosting in the reproduced frame: a standard skip connection tends to preserve information about previous frames x t and x t+1 (but not x t+k ), which causes a ghost of x t and x t+1 to remain in the reproduced x t+k . We concatenate the features of each convolutional block and a bi-linearly upscaled motion feature E m (x t+1 , x t+k ), and then pass the concatenated features into a 1 ? 1 convolutional layer to adjust the number of channels, and add residual connections. These motion-guided connections use motion information to modify the spatial information from previous frames, so that it can be effectively transferred to the target frames.</p><p>Discriminators for adversarial training: We apply adversarial learning to train MSnet, with three discriminators. For realistic and sharp results, we use a frame discriminator. We use two additional discriminators to disentangle motion and content features. More details of these are given in Section 3.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Training Procedure</head><p>Based on the intuitive assertions presented in the Introduction, we define the following objective terms: To train the encoders, we use</p><formula xml:id="formula_0">L 1 = L rec + ?L rev + ? (L advC + L advM + L advF ),<label>(1)</label></formula><p>where ? and ? are hyperparameters. To train the discriminators, we use</p><formula xml:id="formula_1">L 2 = L DF + L DC + L DM .<label>(2)</label></formula><p>We optimize L 1 and L 2 alternately. The loss terms in L 1 and L 2 are described below. In what follows,x t+k denotes the reproduced frame</p><formula xml:id="formula_2">G(E c (x t , x t+1 ), E m (x t+1 , x t+k )).</formula><p>Reconstruction and time-reversal Losses: Based on the 'Reproducibility' and 'Timereversibility' assertions presented in the Introduction, we define L rec and L rev as follows:</p><formula xml:id="formula_3">L rec = x t+k ? x t+k 2 2 ,<label>(3)</label></formula><formula xml:id="formula_4">L rev = E c (x t , x t+1 ) ? E c (x t+1 , x t ) 2 2 ,<label>(4)</label></formula><p>where k represents the temporal distance between the target frame and the reference frame. Frame adversarial loss: DRnet <ref type="bibr" target="#b1">[2]</ref> uses mean squared error loss alone, which tends to produce blurry results in image reproduction <ref type="bibr" target="#b9">[10]</ref>. We thus introduce an extra frame adversarial loss, using a technique similar to that employed in the pix2pix network <ref type="bibr" target="#b6">[7]</ref>. The frame discriminator D f is trained to determine whether its input is a real pair of frames or not, and D f is trained by L DF which is expressed as follows:</p><formula xml:id="formula_5">L DF = ? log D f (x t , x t+k ) ? log(1 ? D f (x t ,x t+k ))<label>(5)</label></formula><p>The adversarial loss L advF expresses the extent to which synthetic frames produced by the generator G manage to deceive the discriminator. The generator G is trained by L advF to synthesize realistic frames with the aim of deceiving the frame discriminator, and L advF is expressed as follows:</p><formula xml:id="formula_6">L advF = ? log D f (x t ,x t+k ).<label>(6)</label></formula><p>Disentangling adversarial loss: The notion of 'Separability of features' described in the Introduction is realized by the content discriminator D c and motion discriminator D m . The content discriminator is trained to determine whether two motion features come from the same video, which requires it to discover the content information in these features. Thus, to deceive the content discriminator, the motion encoder must generate motion features that contain as little content information as possible. We train the content discriminator to discover content information in motion features using the loss L DC , and the loss L advC is used to train the motion encoder in such a way that the motion discriminator cannot make a  decision, which means that the entropy becomes maximized. Note that Eq 8 can be simplified to L advC = ? log x ? log(1 ? x) if we set x = Dc(), and the function has the minimum value when x = 1/2. The result is that the motion encoder obtains a pure motion feature. These two losses are formulated as follows:</p><formula xml:id="formula_7">L DC = ? log D c (E m (x a , x a+1 ), E m (x b , x b+1 )) ? log(1 ? D c (E m (x a , x a+1 ), E m (y b , y b+1 )))<label>(7)</label></formula><formula xml:id="formula_8">L advC = ? log D c (E m (x a , x a+1 ), E m (x b , x b+1 )) ? log(1 ? D c (E m (x a , x a+1 ), E m (x b , x b+1 ))) (8)</formula><p>where a and b are different frame numbers, and x and y are different videos. In a similar way, the motion discriminator is trained to determine whether two content features are from sequential or non-sequential frames, which requires it to discover the motion information from the content feature. The content encoder can deceive the motion discriminator if it generates content features that do not contain motion information. We train the motion discriminator to discover motion information in the content feature by the loss L DM , and the content encoder is trained to deceive the motion discriminator by the loss L advM ; so that the content encoder can obtain a pure content feature. These two losses are formulated as follows:</p><formula xml:id="formula_9">L DM = ? log D m (E c (x a , x a+1 )) ? log(1 ? D m (E c (x a , x b ))),<label>(9)</label></formula><formula xml:id="formula_10">L advM = ? log D m (E c (x a , x a+1 )) ? log(1 ? D m (E c (x a , x a+1 ))),<label>(10)</label></formula><p>where x a and x a+1 are sequential frames, and x a and x b are non-sequential frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Video Frame Prediction</head><p>We apply the motion and content encoders trained during frame reproduction to video prediction. MSnet is given k frames (x 1 , ? ? ?, x k ) and trained to predict the following T frames (x k+1 , ? ? ?, x k+T ), using the network illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. The motion encoder extracts motion features from the pairs (x k , x 1 ), (x k , x 2 ), ? ? ?, (x k , x k ), and the content encoder extracts content features from (x k?1 , x k ). Note that the first frame in each pair is always x k during motion extraction. A convolutional LSTM network (cLSTM) <ref type="bibr" target="#b24">[25]</ref> takes the motion features E m (x k , x t )(1 ? t ? k) extracted from each given pairs of frames and predicts the motion features of the subsequent frames? m (x k , x t+1 ) until the k th frame.</p><formula xml:id="formula_11">cLSTM(E m (x k , x t )) =? m (x k , x t+1 ) (1 ? t ? k).<label>(11)</label></formula><p>For subsequent unknown frames, the predicted motion features are fed back into the cLSTM and the motion features of the next upcoming frames are predicted. By repeating this step, we can predict the motion features of the following T frames.</p><formula xml:id="formula_12">cLSTM(? m (x k , x t )) =? m (x k , x t+1 ) (k &lt; t &lt; T )<label>(12)</label></formula><p>The cLSTM is trained using the following objective function:</p><formula xml:id="formula_13">L lstm = cLSTM(E m (x k , x k )) ? E m (x k , x k+1 ) 2 + T ?1 ? t=k+1 cLSTM(? m (x k , x t )) ? E m (x k , x t+1 ) 2 (13)</formula><p>Finally, the generator producesx t from the t th (t &gt; k) predicted motion features? m (x k , x t ), together with the content features E c (x k?1 , x k ). By repeating this step, we can generate the required number of upcoming frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We performed experiments using the Moving MNIST and KTH datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. First, we performed frame reproduction using the Moving MNIST to compare MSnet with DRnet <ref type="bibr" target="#b1">[2]</ref>. Then, we present frame reproduction, frame prediction and disentangling experiments (feature-based nearest retrieval and t-SNE visualization) on the KTH dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Moving MNIST</head><p>The Moving MNIST dataset <ref type="bibr" target="#b17">[18]</ref> contains 10,000 video sequences, each consisting of 20 frames. In each video sequence, two digits move independently around the frame, which has a spatial rsolution of 64 ? 64 pixels. The digits frequently intersect with each other and bounce off the edges of the frame. We used 8,000 sequences for training and 2,000 for testing. We used motion features with a 4 ? 4 spatial map and 4 channels, and content features with a 4 ? 4 spatial map and 8 channels. We use more channels for the content features because the motions occurring in the Moving MNIST videos are not as complicated as those in natural videos. We used values of the temporal distance k between 0 and 5 in the frame reproduction process, and we set ? = 1.0 and ? = 3.3 ? 10 ?5 in Eq. (1). Qualitative results from this experiment are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Note that Denton and Birodkar (2017) used self-generated colored digits to train DRnet, thus we re-trained it with the publicly available Moving MNIST data to make a fair comparison with MSnet. MSnet obtains content features from the given frames and motion features from the last given and target frames. DRnet obtains content features from the given frames and pose features from the target frames.</p><p>In the first example of <ref type="figure" target="#fig_3">Figure 3</ref>, DRnet generates digits in the wrong places. We attribute this to the way in which DRnet encodes temporal attributes into pose features, and not into motion features, like those used by MSnet. This suggests that motion is a more natural attribute of video than pose. In the second example, DRnet produces blurry results where the two digits overlap in target frames. In the third example, DRnet cannot identify two digits which overlap in a given frame. MSnet can identify these overlapping digits correctly because it obtains content features from two frames. More results with Moving MNIST are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">KTH Dataset</head><p>The KTH dataset <ref type="bibr" target="#b14">[15]</ref> contains videos of 25 people performing six actions. For our experiments, we resized the frames in the videos to 128 ? 128 pixels. We used person 1-16 for training and person 17-25 for testing, following the widely used baseline method MCnet <ref type="bibr" target="#b19">[20]</ref>. We used SSIM, PSNR, and inception score as evaluation metrics. We used motion features with a 8 ? 8 spatial map and 8 channels and content features with a 8 ? 8 spatial map and 8 channels. We used values of the temporal distance k between 0 and 10 in the frame reproduction process. We set ? = 1.0 and ? = 4 ? 10 ?5 in Eq. (1). For the following figures, we denote the motion and content discriminators as MD and CD, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Video frame prediction</head><p>We used the same experimental settings used in MCnet for frame prediction experiments. All baseline networks were trained by taking 10 frames from the KTH dataset and using them to predict the following 10 frames. During testing, 3,559 sequences of 30 frames were used: 10 given frames and 20  <ref type="table">t=12  t=15  t=18  t=21  t=24  t=27  t=30  t=12  t=15  t=18  t=21  t=24  t=27  t=30  t=12  t=15  t=18  t=21  t=24  t=27  t=30   t=12  t=15  t=18  t=21  t=24  t=27  t=30  t=12  t=15  t=18  t=21  t=24  t=27  t=30  t=12  t=15  t=18  t=21  t=24</ref> t=27 t=30 <ref type="figure">Figure 5</ref>: Qualitative results of frame prediction on the KTH dataset. Given 10 frames, the following 20 frames are predicted. We show every 3 frames.</p><p>frames to be predicted. The published DRnet model was trained on person 1-20, so we re-trained DRnet on person 1-16 for fair comparison with other baseline methods. Quantitative results are presented in <ref type="figure" target="#fig_4">Figure 4</ref>(a) and (b). <ref type="figure" target="#fig_4">Figure 4</ref>(a) shows that MSnet obtained better results than other state-of-the-art methods on three evaluation metrics even though it has simpler motion and content encoders. Note that we do not report the number of parameters for ConvLSTM <ref type="bibr" target="#b24">[25]</ref>, TrajGRU <ref type="bibr" target="#b15">[16]</ref>, and fRNN <ref type="bibr" target="#b12">[13]</ref>, because it does not decompose videos into motion and content streams. Qualitative results are shown in <ref type="figure">Figure 5</ref>. DRnet produces the wrong motion in the boxing video, and changes the identity of the person in the handwaving video. We attribute these problems to DRnet's use of a basic UNet and one-directional suppression. MCnet produces a person with an unrealistic shape, which we attribute to its poor disentangling of features and a lack of semantic information in its motion features. fRNN has difficulty when the person in the frame makes a large motion, and we attribute this to the way in which it considers motion and content information simultaneously. These results suggest that meaningful features are obtained by mutual suppression and motion-guided connection. More results are presented in the supplementary material.</p><p>In ablation experiments, we removed each discriminator in turn to show the effects of mutual suppression on the disentangled features. <ref type="figure" target="#fig_4">Figure 4(b)</ref> shows that the results from MSnet are worse when either the motion or content discriminator is removed. Without the content discriminator (blue and gray lines), prediction performance drops significantly across subsequent frames because the motion encoder generates impoverished motion feature. With only the content discriminator (green line), the content encoder cannot extract meaningful content features, so it performs poorly on the first predicted frame. However, its performance does not drop significantly across subsequent frames as meaningful motion features can be extracted with the content discriminator. These results demonstrate how mutual suppression disentangles motion and content features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Disentangling Experiments</head><p>We present t-SNE visualization <ref type="bibr" target="#b8">[9]</ref> and feature-based nearest retrieval results to show the disentangling effect.</p><p>Feature-based frame retrieval: The aim of feature-based frame retrieval is to fetch the frame   which is closest to a query frame in terms of motion and content features. More formally,</p><formula xml:id="formula_14">x r = argmin x E(x q ) ? E(x) , E ? {E c , E m },<label>(14)</label></formula><p>where x q is a query frame, x is a frame other than the query frame, and x r is the retrieved frame. <ref type="figure" target="#fig_7">Figure 6</ref> shows the results of nearest motion and content retrieval. Ideally, nearest motion retrieval should retrieve the most similar motion regardless of the content (the identity of the person and the background), and content retrieval should retrieve the most similar content regardless of any motion. As shown in <ref type="figure" target="#fig_7">Figure 6</ref>(a), MSnets with the content discriminator (the second and third rows) retrieve the most similar motions regardless of the identity the person and the background, because the content discriminator helps the motion encoder to extract pure motion features. In <ref type="figure" target="#fig_7">Figure 6</ref>(b), MSnets with the motion discriminator (the second and fourth rows) retrieve the most similar content regardless of the motions, because the motion discriminator helps the content encoder to extract pure content features. t-SNE visualization: As MSnet is trained in an unsupervised manner, it cannot separate similar actions, such as running and jogging. For visualization by t-SNE dimensional reduction, we thus classify walking, jogging, and running as dynamic actions, and boxing, handclapping, and handwaving as static actions. If motion and content are disentangled as intended, motion features in the same actions should be clustered, and motion features in different actions should be separated. Conversely, points corresponding to content features which do not contain motion information should not be clustered.</p><p>The results of t-SNE visualization are shown in <ref type="figure" target="#fig_8">Figure 7</ref>. MSnet with both discriminators produces the most clustered motion features, and the most random distribution of content features. Using the content discriminator alone, motion features are reasonably well clustered, as pure motion features can still be captured effectively. However, content features are now more clustered because the omission of the motion discriminator means that content features contain unwanted temporal information. Without the content discriminator, and without both discriminators, the results are far from what we intend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have proposed a new method for video frame prediction. We have introduced mutual suppression adversarial training to acquire disentangled motion and content representations, and applied motionguided connection to refine the content information from previous frames for use in the prediction of upcoming frames. MSnet has been shown to obtain well-disentangled features. This lead to better results in terms of frame prediction than other state-of-the-art methods. We have also shed light on the way in which mutual suppression disentangles features by ablation studies in the domains of t-SNE visualization and feature-based nearest frame retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of MSnet, showing multi-scale motion-guided connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The frame prediction network. Given k frames, the network predicts upcoming T frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results of frame reproduction task on the Moving MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Quantitative results of the frame prediction on the KTH dataset. (a) Comparison with state-of-the-art methods. (b) Comparison with ablation settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Feature-based nearest frame retrieval results. content MSnet w/o MD MSnet w/o CD MSnet w/o both MSnet motion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Visualization by t-SNE dimensional reduction of dynamic actions (walking, jogging, and running) (the red points), and static actions (boxing, handclapping, and handwaving) (the blue points), with and without the content and motion discriminators. The first row shows the distribution of motion features, and the second row shows the distribution of content features.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A two stream siamese convolutional neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahjung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Tahboub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4417" to="4426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Suyog Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05384</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Yann LeCun, and s s. Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Folded recurrent neural networks for future video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Selva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Pattern Recognition</title>
		<meeting>the IEEE International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for precipitation nowcasting: A benchmark and a new model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5617" to="5627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04993</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Antonio Torralba, and s s. Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00687</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Actions?transformations</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2658" to="2667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Spatio-temporal action detection with cascade proposal and location anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00042</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
