<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A New Journey from SDRTV to HDRTV</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwen</forename><surname>Zhang</surname></persName>
							<email>zhengwen.zhang02@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
							<email>jimmy.sj.ren@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Qing Yuan Research Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynhoo</forename><surname>Tian</surname></persName>
							<email>lynhoo.tian@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<email>chao.dong@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution" key="instit1">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A New Journey from SDRTV to HDRTV</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays modern displays are capable to render video content with high dynamic range (HDR) and wide color gamut (WCG). However, most available resources are still in standard dynamic range (SDR). Therefore, there is an urgent demand to transform existing SDR-TV contents into their HDR-TV versions. In this paper, we conduct an analysis of SDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV content. Base on the analysis, we propose a three-step solution pipeline including adaptive global color mapping, local enhancement and highlight generation. Moreover, the above analysis inspires us to present a lightweight network that utilizes global statistics as guidance to conduct image-adaptive color mapping. In addition, we construct a dataset using HDR videos in HDR10 standard, named HDRTV1K, and select five metrics to evaluate the results of SDRTV-to-HDRTV algorithms. Furthermore, our final results achieve state-of-the-art performance in quantitative comparisons and visual quality. The code and dataset are available at https://github.com/ chxy95/HDRTVNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The resolution of television (TV) content has increased from standard definition (SD) to high definition (HD) and most recently to ultra-high definition (UHD). High dynamic range (HDR) is one of the biggest features of the latest TV generation. HDRTV 1 content has much wider color space and higher dynamic range than SDR content, and HDRTV standard allows us to create images and videos that are closer to what we see in real life. While HDR display devices have become prevalent in daily life, most accessible resources are still in SDR format. Therefore, we need al-* indicates contribute equally. ? Corresponding author. <ref type="bibr">1</ref> We add a suffix TV after HDR/SDR to indicate content in HDR-TV/SDR-TV format and standard. gorithms to convert SDRTV content to their HDRTV version. The task, denoted as SDRTV-to-HDRTV, is of great practical value, but received relatively less attention in the research community. The reason is mainly two-fold. First, existing HDRTV standards (e.g., HDR10 and HLG) have not been well defined until recent years. Second, there is a lack of large-scale datasets for training and testing. This work aims at promoting the development of this emerging area, by presenting the analysis of this problem, basic methods, evaluation metrics and a new dataset.</p><p>SDRTV-to-HDRTV is a highly ill-posed problem. In actual production, contents of SDRTV and HDRTV are derived from the same Raw file but are processed under different standards. Thus, they have different dynamic ranges, color gamuts and bit-depths. To some extent, SDRTV-to-HDRTV is similar to image-to-image translation such as Pixel2Pixel <ref type="bibr" target="#b11">[12]</ref> and CycleGAN <ref type="bibr" target="#b39">[39]</ref>. On the contrary, the task of LDR-to-HDR, which is similar in terms of name, has completely different connotations. LDR-to-HDR methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr">6]</ref> aim to predict the HDR scene luminance in the linear domain, which is closer to Raw file in essence. SDRTV-to-HDRTV has recently been touched in Deep SR-ITM <ref type="bibr" target="#b19">[20]</ref> and JSI-GAN <ref type="bibr" target="#b20">[21]</ref>, which try to solve the problem of joint super-resolution and SDRTV-to-HDRTV. Although the above-mentioned works are all related to SDRTV-to-HDRTV, they are not dedicated to this problem. We will detail these comments in Sec. 2 and Sec. <ref type="bibr">3.</ref> This paper aims to address SDRTV-to-HDRTV based on deep understanding of this problem. We first provide a simplified formation pipeline for SDRTV/HDRTV content, which consists of tone mapping, gamut mapping, transfer function and quantization, as in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Based on the formation pipeline, we propose a solution pipeline, including adaptive global color mapping (AGCM), local enhancement (LE) and highlight generation (HG). For AGCM, we propose a novel color condition block to extract global image prior and adapt different images. With only 1 ? 1 filters, the network achieves the best performance with less parameters compared with other photo retouching methods such as CSRNet <ref type="bibr">[9]</ref>, HDRNet <ref type="bibr">[7]</ref> and Ada-3DLUT <ref type="bibr" target="#b37">[37]</ref>. Besides, we adopt a commonly used ResNet-based network and a GAN-based model for LE and HG, respectively.</p><p>To promote the progress of this new research area, we collect a new large-scale dataset, named HDRTV1K. We also select five evaluation metrics -PSNR, SSIM, SR-SIM <ref type="bibr" target="#b38">[38]</ref>, ?E IT P <ref type="bibr" target="#b17">[18]</ref> and HDR-VDP3 <ref type="bibr" target="#b26">[27]</ref>, to evaluate the mapping accuracy, structural similarity (SSIM and SR-SIM), color difference and visual quality, respectively.</p><p>Our contributions are four-fold: <ref type="bibr">(1)</ref> We conduct a detailed analysis for SDRTV-to-HDRTV task by modeling the formation of SDRTV/HDRTV content. <ref type="bibr">(2)</ref> We propose a three-step SDRTV-to-HDRTV solution pipeline and a corresponding method, which performs best in quantitative and qualitative comparisons. <ref type="bibr">(3)</ref> We present a novel global color mapping network based on color condition blocks. With about only 35K parameters, it can still achieve state-of-theart performance. (4) We provide a HDRTV dataset and select five metrics to evaluate SDRTV-to-HDRTV algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminary</head><p>Background. In this paper, we use SDRTV/HDRTV to represent the content (including image and video) under SDR-TV/HDR-TV standard. The two standards are specified in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref> and <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, respectively. The basic elements of the HDR-TV standard generally include wide color gamut <ref type="bibr" target="#b15">[16]</ref>, PQ or HLG OETF <ref type="bibr" target="#b16">[17]</ref> and 10-16 bits depth. In terms of name, "LDR" is often used in academia, and "SDR" is generally used in industry. Essentially, both of them represent content with low dynamic range but generated from TV production and camera ISP, respectively. For the convenience of distinction and reference, we uniformly use "LDR-to-HDR" and "SDRTV-to-HDRTV" to represent the conventional image HDR reconstruction and conversion of content from SDR-TV to HDR-TV standard.</p><p>Explanation. Before introducing our method, we first explain that SDRTV-to-HDRTV is functionally different from the previous LDR-to-HDR (i.e., inverse tone mapping) problem. Although the concept of "HDR" is involved in these issues, it is undeniable that the connotations of HDR are not the same. It is non-trivial to explain the concepts and differences exhaustively due to the overwhelm of data-level details. In general, the previous LDR-to-HDR methods aim to predict the luminance of images in the linear domain, which is the physical brightness of the scene. In contrast, our goal is to predict HDR images with the HDR display format in the pixel domain, which are encoded in HDR-TV standards, such as HDR10, HLG and Dolby Vision. Essentially, content in HDR-TV standard can also be generated from HDR scene radiance. However, the process is an engineering problem and still requires a lot of operations, such as tone mapping and gamut mapping. Therefore, the methods of these two different tasks are not generalizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Analysis</head><p>In this section, we first present a simplified SDRTV/ HDRTV formation pipeline which contains the most critical steps in actual production. Then, based on the analysis of the formation pipeline, we propose a novel three-step solution based on the idea of "divide-and-conquer". Finally, we compare different solution pipelines of previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SDRTV/HDRTV Formation Pipeline</head><p>To further understand the task of SDRTV-to-HDRTV, we introduce a simplified formation pipeline of SDRTV and HDRTV based on camera ISP and HDR-TV content production <ref type="bibr" target="#b18">[19]</ref> as depicted in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Although there are some operations we do not mention here, such as denoising and white balance in camera pipeline and color grading in HDR content production, we retain the four key operations that lead to the difference between SDRTV and HDRTV, which are tone mapping, gamut mapping, opto-electronic transfer function and quantization. In the following equations, we use the subscript "S" to represent SDRTV and "H" to represent HDRTV. More details about the pipelines can be found in the supplementary material.</p><p>Tone mapping. Tone mapping is used to transform the high dynamic range signals to low dynamic range signals for adapting different display devices. The process includes global tone mapping <ref type="bibr">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">35]</ref> and local tone mapping <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Global tone mapping processes all pixels equally with the same function, while parameters are generally related to global image statistics (e.g., average luminance). Local tone mapping can be adaptive to local content and human preference but often brings high computational cost and artifacts. Thus, global tone mapping is mainly used in the HDRTV/SDRTV image formation. The function of global tone mapping can be denoted as:</p><formula xml:id="formula_0">I tS = T S (I|? S ), I tH = T H (I|? H ),<label>(1)</label></formula><p>where T S and T H represent the specific tone mapping functions, ? S and ? H are coefficients related to image statistics. It is noteworthy that S-shape curves are commonly used for global tone mapping and clipping operations often exist in actual process of tone mapping. Gamut mapping. Gamut mapping is to convert colors from source gamut to target gamut while still preserving the overall look of the scene. According to the standards of ITU-R <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref>, the transformations from the original XYZ space to SDRTV (rec.709) and HDRTV (rec.2020) can be represented as :</p><formula xml:id="formula_1">? ? R 709 G 709 B 709 ? ? = M S ? ? X Y Z ? ? , ? ? R 2020 G 2020 B 2020 ? ? = M H ? ? X Y Z ? ? ,<label>(2)</label></formula><p>where M S and M H are 3 ? 3 constant matrices.</p><p>Opto-electronic transfer function. Opto-electronic transfer function abbreviates as OETF. It is used to convert linear optical signals to non-linear electronic signals in the image formation pipeline. For SDRTV, it approximates a gamma exponential function as I f S = f S (I) = I 1/2.2 . For HDRTV, there are several kinds of OETFs for different standards such as PQ-OETF <ref type="bibr" target="#b13">[14]</ref> for HDR10 standard and HLG-OETF <ref type="bibr" target="#b16">[17]</ref> for HLG standard (HLG stands for Hybrid Log-Gamma). We take the PQ-OETF as an example:</p><formula xml:id="formula_2">I f H = f H (I) = ( a 1 + a 2 I b1 1 + a 3 I b1 ) b2 ,<label>(3)</label></formula><p>where a 1 , a 2 , a 3 , b 1 , b 2 are constants. Quantization. After the above operations, the encoded pixel values are quantized with the function:</p><formula xml:id="formula_3">I q = Q(I, n) = ?(2 n ? 1) ? I + 0.5? 2 n ? 1 ,<label>(4)</label></formula><p>where n is 8 for SDRTV and 10-16 for HDRTV. In summary, the HDRTV and SDRTV content formation pipelines are given by:</p><formula xml:id="formula_4">I S = Q S ? f S ? M S ? T S (I raw ),<label>(5)</label></formula><formula xml:id="formula_5">I H = Q H ? f H ? M H ? T H (I raw ),<label>(6)</label></formula><p>where ? represents the connection between two operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Solution Pipeline</head><p>According to the above formation pipeline, the process of SDRTV-to-HDRTV can be formulated as:</p><formula xml:id="formula_6">I H = Q H ?f H ?M H ?T H ?T ?1 S ?M ?1 S ?f ?1 S ?Q ?1 S (I S ), (7) where T ?1 S , M ?1 S , f ?1 S , Q ?1</formula><p>S denote the inversions of the corresponding operations.</p><p>We obtain the following observations and insights based on the formation pipeline. Firstly, many critical operations in the formation pipeline are global operations, such as global tone mapping, OETF and gamut mapping. Moreover, the inversions of these operations are also global operations or can be approximately equal to global operations. These operations can be processed using global operators. Secondly, some operations such as local tone mapping and dequantization rely on local spatial information, which can be processed by local operators. Thirdly, there is severe information compression/loss. For example, highlight areas are generally processed through a shoulder operation or simply clipped by tone mapping.</p><p>Inspired by the observations, we propose a new SDRTVto-HDRTV solution pipeline using the idea of "divide and conquer". Our method includes three steps, as shown in <ref type="figure" target="#fig_0">Fig.  1(d)</ref>. The first step is adaptive global color mapping, which aims at dealing with global operations. This step roughly and adaptively converts the input from SDRTV domain to HDRTV domain. Then, we perform local enhancement, which utilizes local information to enhance the result of the first step. Finally, the highlight generation step is to restore the lost information in the overexposed regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison with Existing Solutions</head><p>As an actual industrial problem, SDRTV-to-HDRTV is rarely discussed in academia. In this part, we elaborate on the two groups of existing solutions.</p><p>End-to-end solution. Image-to-Image translation methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">39]</ref> and joint SDRTV-to-HDRTV and superresolution methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> learn a direct mapping with an end-to-end model to solve the problem, as shown in <ref type="figure" target="#fig_0">Fig.  1(b)</ref>. However, their methods do not consider the imaging mechanism of SDRTV and HDRTV, thus the results contain some obvious local artifacts and unnatural colors.</p><p>LDR-to-HDR based solution. LDR-to-HDR is discussed a lot in academia <ref type="bibr" target="#b30">[31,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b28">29]</ref>. Although these methods are dedicated to predicting HDR scene radiance, it is still necessary to compare them with ours. As in <ref type="figure" target="#fig_0">Fig.1(c)</ref>, the HDR radiance map generated by LDR-to-HDR methods needs to go through color gamut mapping to rec.2020, applying PQ or HLG OETF and quantization. For HDR10 standard, the results are obtained by setting the maximum brightness to 1000 cd/m 2 . Since these steps need to be adjusted according to different data in actual processing, it is hard to get a fair comparison. In this paper, we use the same processing pipeline as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> to compare with LDRto-HDR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>We propose HDRTVNet, a cascaded method consisting of three deep networks for SDRTV-to-HDRTV. Each network corresponds to each step of the solution pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Adaptive Global Color Mapping</head><p>As the first step, adaptive global color mapping (AGCM) aims to achieve image-adaptive color mapping from SDR-TV to HDRTV domain. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the proposed model consists of a base network and a condition network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Base Network</head><p>The base network is designed to handle global operations, which works on each pixel independently. This global mapping can be denoted as:</p><formula xml:id="formula_7">I B (x, y) = f (I S (x, y)), ?(x, y) ? I S ,<label>(8)</label></formula><p>where (x, y) represents the coordinate of the pixel in the image I and I B represents the output of base network. As demonstrated in CSRNet <ref type="bibr" target="#b24">[25]</ref>, a fully convolution network with only 1 ? 1 convolutions and activation functions can achieve this kind of global mapping. Thus, our base network is composed of N l convolutional layers with 1 ? 1 filters and N l -1 ReLU activations, which can be denoted as:</p><formula xml:id="formula_8">I B = Conv 1?1 ? (ReLU ? Conv 1?1 ) N l ?1 (I S ). (9)</formula><p>The proposed base network takes an 8-bits SDRTV image as input and generates an HDRTV image encoded with 10-16 bits. Although the base network can only learn a oneto-one color mapping, it also achieves considerable performances, as shown in Tab. 1. It is noteworthy that the base network can perform like a 3D lookup table (3D LUT) with fewer parameters rather than learning 3D LUT directly, and please refer to supplementary material for more results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Condition Network</head><p>The global priors are indispensable for adaptive global color mapping. For example, global tone mapping requires global image statistics. To achieve image-adaptive mapping, we add a condition network to modulate the base network. Previous works <ref type="bibr" target="#b36">[36,</ref><ref type="bibr">9,</ref><ref type="bibr">3]</ref> usually adopt the prior of image content, where the condition network can extract spatial and local information from the input image by N k ? N k (N k &gt; 1) filters. However, for SDRTV-to-HDRTV problem, we find that global mapping is conditioned on global image statistics or color distribution. This kind of color condition is independent of spatial information, thus it is inappropriate to adopt previous structures of condition in this problem. Our proposed condition network focuses on extracting information related to color to realize adjustable mapping. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, the proposed condition network consists of several color condition blocks (CCB), convolution layers, feature dropout and global average pooling.</p><p>Color condition block. A color condition block contains a convolution layer with 1 ? 1 filters, an average pooling layer, a LeakyReLU activation and an instance normalization layer <ref type="bibr">[5]</ref>, which can be written as follows:</p><formula xml:id="formula_9">CCB(?) = IN ? LReLU ? avgpool ? Conv 1?1 (?),<label>(10)</label></formula><p>where ? denotes the input of CCB. The condition network takes a down-sampled SDRTV image as input and outputs a condition vector V . Our condition network is denoted by:</p><formula xml:id="formula_10">V = GAP ? Conv 1?1 ? Dropout ? CCB Nc (I S ). (11)</formula><p>Since the convolutional layers only contain 1 ? 1 filters, the condition network can not extract local features. With the help of pooling layers, the network can extract global priors based on image statistics. To avoid overfitting, we add a dropout before the last convolutional layer and global average pooling. It performs like adding a multiplicative Bernoulli noise on features which has an effect similar to data augmentation. It is worth noting that even if we take the image by shuffling the pixels randomly as input, we can also obtain comparable performance with the correct arrangement of pixels. It suggests that the effective prior is not related to spatial information in global color mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Global Feature Modulation</head><p>To utilize the extracted global priors, we introduce the global feature modulation (GFM) <ref type="bibr">[9]</ref> strategy to modulate the base network, which has been successfully applied in photo retouching tasks. Through GFM, the intermediate features of the base network can be modulated by scaling and shifting operations. It can be described as:</p><formula xml:id="formula_11">GF M (x i ) = ? 1 * x i + ? 2 ,<label>(12)</label></formula><p>where x i denotes the feature map. ? 1 and ? 1 represent the scale and shift factor, respectively. Overall, AGCM network can be formulated as:</p><formula xml:id="formula_12">I AGCM = GF M ? Conv 1?1 ? (ReLU ? GF M ? Conv 1?1 ) N l ?1 (I S ),<label>(13)</label></formula><p>where I AGCM denotes the output of adaptive global color mapping. To optimize adaptive global color mapping, we minimize the distance of the output and the ground truth HDRTV image using L2 loss function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Local Enhancement</head><p>Local enhancement (LE) is performed followed by AGCM. Although AGCM can obtain considerable performance, local enhancement is indispensable for SDRTV-to-HDRTV. It is a remarkable fact that if we directly use local operations to learn end-to-end mapping before adaptive global color mapping, the output results often have obvious artifacts. Details can be founded in the supplementary material.</p><p>To achieve local enhancement, we directly adopt a classical ResNet structure <ref type="bibr">[10]</ref>. More advanced architectures can be used here, but this is not the focus of this work. Details of the enhancement network are described as follows. This step takes the output of adaptive color mapping as input. During this process, the input is down-sampled by the first convolutional layer, then goes through several residual blocks (RB), and finally is up-sampled to the original size as output. The overall operation can be represented as:</p><formula xml:id="formula_13">I LE = Conv ? ReLU ? Conv ? ReLU ? P S ? RB N b ? ReLU ? Conv(I AGCM ),<label>(14)</label></formula><p>where M is the number of residual blocks. L LE is the HDR image generated by the proposed local enhancement network. L2 loss function is adopted for local enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Highlight Generation</head><p>The third step of our solution pipeline is highlight generation (HG), which aims at hallucinating some details that were lost due to extreme compression. To achieve this goal, we adopt a generative adversarial network (GAN) <ref type="bibr">[8]</ref> that has the potential to generate details. This network consists of a generator and a discriminator. We adopt an encoderdecoder architecture with skip connections <ref type="bibr" target="#b31">[32]</ref> as generator and an commonly used VGG architecture <ref type="bibr" target="#b34">[34]</ref> as discriminator. The formulation of this step can be represented as:</p><formula xml:id="formula_14">I o = I mask ? G(I i ) + I i ,<label>(15)</label></formula><p>where G denotes the generator and ? denotes elementwise multiplication. The mask I mask can be computed by p = max(I i ? ?, 0)/1 ? ? as <ref type="bibr" target="#b25">[26]</ref>. For learning highlight generation network, we joint optimize three types of losses, including L1 loss, perceptual loss and GAN loss, which can be formulated as:</p><formula xml:id="formula_15">L HG = ?L 1 + ?L P + ?L GAN ,<label>(16)</label></formula><p>where ?, ?, ? are loss weights. The perceptual loss is to measures the similarity in feature space, as L P = ??(I HG ) ? ?(I LE )? 2 2 , where ?(I) represents the feature maps of image I. The GAN loss is denoted as L G = ? log D(G(I LE )), where D denotes the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup.</head><p>Dataset. We collect 22 videos under HDR10 standard and their counterpart SDR version from YouTube as <ref type="bibr" target="#b19">[20]</ref>. All of these HDR videos are encoded by PQ-OETF and rec.2020 gamut. 18 video pairs are used for training and the left for testing. To avoid high coherence among extracted frames, we sample a frame every two seconds of each video and generate a training set with 1235 images. Besides, 117 images without repeated scenes are selected as the test set.</p><p>Training details. For the proposed AGCM, the base network consists of 3 convolution layers with 1 ? 1 kernel size and 64 channels, and the condition network contains 4 CCBs. For LE, all convolution filters are of size 3 ? 3 with 64 output channels, except for the convolution layer in the pixel shuffle module <ref type="bibr" target="#b33">[33]</ref> with 256 channels, and the output layer with 3 channels. The number of the RBs is 16. Strides of all layers are set to 1 except for the first convolution layer with a stride of 2. For the part of HG, there are five convolution layers followed by max-pooling operation and also five convolution layers with pixel shuffle. Each filter has kernel size of 3 ? 3. The number of channels is increased from 64 to 1024 in the downsampling process and reverses in the process of upsampling. More implementation details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation of SDRTV-to-HDRTV</head><p>Evaluation metrics. We employ five evaluation metrics for comprehensive comparisons, including PSNR, SSIM, SR-SIM <ref type="bibr" target="#b38">[38]</ref>, HDR-VDP3 <ref type="bibr" target="#b26">[27]</ref> and ?E IT P <ref type="bibr" target="#b17">[18]</ref>. SSIM and SR-SIM are commonly used to measure image similarity. Although they are designed to evaluate SDR image, <ref type="bibr">[1]</ref> shows that SR-SIM has a favorable performance of evaluation for HDR standard. Besides, we introduce ?E IT P to measure the color difference, which is designed for HDRTV. HDR-VDP3 is a new improved version of HDR-VDP2 that supports the rec.2020 gamut. To employ HDR-VDP3, results are compared by setting "side-by-side" task, "rgb-bt.2020" color encoding, 50 pixel per degree and option of "rgb-display" with "led-lcd-wcg".</p><p>Visualization. We directly show the HDRTV images encoded in 16-bits "PNG" format without extra processing. Since HDRTV images are decoded by gamma EOTF on SDR screens, they may look relatively darker than on HDR screens. Previous work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> shows HDRTV images by software for visualization. However, it introduces an unfair comparison since the video player may reduce the unnatural artifacts of original HDRTV images. In contrast, our visualization method preserves the details in highlight areas and compares all methods in the same conditions. Tone mapped results can be founded in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with Other Methods</head><p>Compared methods. We compare our results with four types of methods including joint SR with SDRTV-to-HDRTV, image-to-image translation, photo retouching and LDR-to-HDR. Since these methods are not completely designed for this task, we have done the necessary processing of these methods. For joint SR with SDRTV-to-HDRTV methods, we change the stride of the first convolutional layer to 2 for downsampling to match the size of input and output 1 . For LDR-to-HDR methods, we process the results as illustrated in Sec. 3.3. Note that we adopt the same processing steps as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. All data-driven methods are retrained on the proposed dataset.</p><p>Quantitative comparison. As shown in Tab. 1, our method HDRTVNet outperforms other methods by a large margin on all evaluation metrics. It is worth noticing that our first step AGCM could already achieve comparable performance to Ada-3DLUT, but with only 1/17 of its parameters. The LDR-to-HDR based solutions have poor results as their pipeline is different from ours. It is hard to compare with them on a completely fair platform.</p><p>Qualitative comparison. The results of qualitative comparisons are shown in <ref type="figure" target="#fig_2">Fig.3</ref>. LDR-to-HDR based methods and image-to-image translation methods tend to generate low-contrast images. All approaches of LDRto-HDR based, image-to-image translation and SDRTVto-HDRTV produce unnatural colors and obvious artifacts except for HuoPhyEO <ref type="bibr">[11]</ref>. Outputs generated by photo retouching methods are relatively better but suffer from the color cast. Our method HDRTVNet could produce natural colors and high contrast as referred ground truth and do not introduce any artifacts. Further, the visual quality improves with more processing steps, i.e., AGCM&lt;AGCM+LE&lt;AGCM+LE+HG. More results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Color Transition Test</head><p>We observe that many previous methods perform poorly in the highlight regions, especially where the color changes. To reveal this phenomenon, we conduct a color transition test using a man-made color card as input image shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. We obtain the following three observations. First, the unnatural transition and color blending problem can be easily observed in the outputs of several methods, which rely on learning local information (e.g., Deep SR-ITM, JSI-GAN, Pixel2Pixel, CycleGAN) or based on local conditions (e.g., 3D-LUT). Second, our method performs smooth transition even learning local information (e.g., AGCM+LE, AGCM+LE+HG), which shows the superiority of our cascaded solution pipeline. Third, blue regions suffer the most server unnatural color transition. A reasonable explanation is that blue colors are harder to recover than other colors, since more information is missing in the blue area in the process of extreme compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Study</head><p>Adaptive global color mapping. The process of adaptive color mapping can be observed by LUT cubes shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. The color of each point in the cube corresponds to the input SDRTV color, and its coordinates in the cube correspond to values of HDRTV pixels after the current mapping. Note that if an SDRTV color is mapped to multiple HDRTV colors, the color will also appear in multiple positions of the cube. Since the basic network can only learn a one-to-one color mapping throughout the dataset, the color transition of the LUT manifold in the highlight areas is not smooth. It can be easily seen that the output of the base network suffers severe posterization artifacts. In contrast, the color condition network helps the base network learn image-adaptive color mapping. Then the artifacts disappear, and the LUT becomes concentrated. In <ref type="figure" target="#fig_4">Fig. 5</ref>, our method also performs better in the color transition test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Params? PSNR? SSIM? SR-SIM? ?E IT P ? HDR-VDP3? HuoPhyEO <ref type="bibr">[</ref>  Local enhancement. This part takes the adaptiveglobal-color-mapped image as input, aiming to handle the one-to-many color mapping and some local operations in the SDRTV-to-HDRTV process. The same color in SDRTV domain is mapped to multiple HDRTV colors, and the color distribution becomes more compact and smooth as in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>Noting that adding local enhancement after adaptive global color mapping achieves the best performance in quantitative comparison (in Tab. 1) without introducing artifacts. If we apply the local enhancement as the first step, it will generate apparent local artifacts, similar to other end-to-end mapping methods. Please refer to the supplementary material.  Highlight generation. Highlight generation aims to hallucinate details in the over-exposed regions. We declare that this part is not designed for numerical improvement, but functional increase. Due to the inevitable loss of information in the production (e.g., dynamic range compression), we believe that it is necessary to use some generative methods to deal with these parts. Although our HG method has certain limitations for numerical evaluation, we can still observe that it makes colors in the LUT cube denser and makes the highlight regions look more natural.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">User Study</head><p>We conduct a user study with 20 participants for subjective evaluation. Four methods with the best performance in each category are selected to compare with our method and ground truth. Participants are asked to rank them according to the visual quality. 25 images are randomly selected from the testing set and shown to participants on HDR-TV (Sony X9500G with a peak brightness of 1300 nits) in darkroom. More details of how we conduct experiment can be founded in the supplementary material. As suggested in <ref type="figure" target="#fig_10">Fig. 6</ref>, our method achieves a better visual ranking than other competitors except for the ground truth. <ref type="figure" target="#fig_10">Figure 6</ref>. Ranking results of user study. Rank1 means the best subjective feeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have provided a novel SDRTV-to-HDRTV solution pipeline based on the HDRTV/SDRTV formation pipeline using divide-and-conquer. Moreover, we have introduced a new method, HDRTVNet, for the problem. According to the three types of operations in HDRTV/SDRTV formation pipeline including global operation, local operation and shoulder operation, the whole method is divided into adaptive global color mapping, local enhancement and highlight generation correspondingly. Besides, a novel color condition network has been proposed with fewer parameters and better performance than other photo retouching approaches. Comprehensive experiments show the superiority of our solution in quantitative comparison and visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">More details about the formation pipeline</head><p>In this section, we aim at providing more details about the formation pipeline in two aspects: (1) The principle to simplify the formation pipeline. (2) Specific explanations of the diagrams and curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The principle to simplify the formation pipeline</head><p>In <ref type="bibr">[10]</ref>, the LDR image formation pipeline is simplified as in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. The pipeline contains three main parts in the camera pipeline for HDR-to-LDR, which are dynamic range clipping, non-linear mapping (i.e., applying transfer function in the paper) and quantization. In this pipeline, "HDR" represents the scene radiance and color mapping is not involved. Similar to this pipeline, for SDRTV/HDRTV formation, the process also contains dynamic range compression, applying transfer function and quantization. However, there are two essential differences between the LDR formation pipeline and SDRTV/HDRTV pipeline. First, for LDR-to-HDR, the LDR image is generated from HDR scene radiance, thus LDR-to-HDR is a kind of reverse process to some extent. In contrast, SDRTV and HDRTV are generated from the same raw data (or resource in Log format) by different operations for different standards as in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. From this perspective, SDRTVto-HDRTV is more like an image-to-image translation task. Second, color conversion is generally not involved in LDR-to-HDR, as the "HDR" has been converted from camera RGB to sRGB, which is generally encoded in the same color gamut as LDR by default. However, gamut expansion is a crucial step in SDRTV-to-HDRTV. Therefore, we add gamut mapping in SDRTV/HDRTV formation pipeline. Overall, we provide the formation pipeline to illustrate the key steps in production leading to the difference between SDRTV and HDRTV content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Specific explanations of the diagrams and curves</head><p>In the main paper, we provide sketch maps of four operations for leading viewers to focus on the formation pipeline and corresponding solution pipeline. In this subsection, we take specific explanations of the diagrams and curves.</p><p>Tone mapping. We draw the diagram and curve of the tone mapping operation by using Hable tone mapping <ref type="bibr">[2]</ref> shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, which can be formulated as: Gamut mapping. Gamut mapping is to convert colors of content from source gamut to target gamut. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, we utilize the CIE chromaticity diagram to distinguish different target gamuts for the color gamut mapping of SDRTV and HDRTV, which correspond to rec.709 and rec.2020, respectively. Opto-electronic transfer function. Opto-electronic transfer function abbreviates as OETF. It is used to convert linear optical signals to non-linear electronic signals in the image formation pipeline. We provide the gamma curve for SDRTV and PQ curve for HDRTV, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation details</head><p>For the proposed AGCM, we set the dimension of the condition vector to 6. Before training, we crop images by 480 ? 480 with the step of 240. When training model, patches with 480 ? 480 are input to the base network while full images downsampled by the scale of 4 are the inputs of the condition network. The mini-batch size is set to 4 and we adopt the L2 loss function and Adam <ref type="bibr">[8]</ref> optimizer for training, of total 1 ? 10 6 iterations. The initial learning rate is set to 5 ? 10 ?4 and decayed by a factor of 2 after every 2 ? 10 5 iterations. For the part of LE, we use the outputs of AGCM as the inputs. The size of the mini-batch is set to 16 and the patch size is 160 ? 160. We set the initial learning rate to 1 ? 10 ?4 and then decayed by a factor of 2 after every 1 ? 10 5 iterations, of total 5 ? 10 5 iterations. L2 loss function, Adam optimizer and Kaiming-initialization <ref type="bibr">[4]</ref> are utilized for training. For training HG model, the generator is first pre-trained based on L2 loss function and Adam optimizer with a batch size of 16 and patch size of 160. The initial learning rate is set to 2 ? 10 ?4 and decayed by 2 after every 2 ? 10 5 iterations, of total 5 ? 10 5 iterations. Then, we train the GAN model with batch size of 16 and patch size of 128. The initial learning rate is set to 1 ? 10 ?4 and the total number of iterations is set to 4 ? 10 5 . The learning rate is decayed by a factor of 2 after every 1 ? 10 5 iterations. Adam optimizer and Kaiming-initialization are still adopted and the L1 loss function is used for training. The weighting parameters for the losses are set to ? = 0.01, ? = 1 and ? = 0.005. All models are built on the PyTorch framework and trained with NVIDIA 2080Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Additional quantitative and qualitative analyses 4.1. Conversion from base network to 3D LUT</head><p>In many practical applications, 3D LUTs are widely used for manipulating the color and tone of images. Besides, 3D LUTs are used extensively in the movie production chain, as part of the digital intermediate process. Thus, there are many software and tools to edit images by modifying 3D LUTs. However, 3D LUTs are not flexible to interpolate and modulate as our base network for training. In previous work <ref type="bibr">[11]</ref>, to realize the image-adaptive function, multiple 3D LUTs are trained with a CNN weight predictor, which brings high computational cost. On the other hand, as proven in <ref type="bibr">[3]</ref>, a network consisting of multiple layers with only 1 ? 1 filters can deal with many global operations well and has the flexibility to modulate the feature maps. In this part, we prove that it is easy to convert our base network to a 3D LUT with almost no loss of accuracy.</p><p>We take a 3D lattice composed of SDR colors as input and obtain a 3D LUT for SDRTV-to-HDRTV. Then, we use lookup and trilinear interpolation operations to achieve the color transformation as <ref type="bibr">[11]</ref>. As in Tab. 1, 3D LUTs converted from the base network obtain comparable performance with fewer parameters. Note that the operation can also apply to our AGCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Params?  <ref type="table">Table 1</ref>. Quantitative comparisons between base network and its converted 3D LUTs. s33 or s64 represents the size of LUT. LUTs of these two sizes are commonly used in the process of production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of color mapping via LUT manifold</head><p>We provide more detailed explanations of the LUT manifold and its visualization. In the 3D LUT cube, each point has four basic attributes, which are color and three coordinate values. These three coordinate values determine the position of the color in the current domain. In <ref type="figure" target="#fig_4">Fig. 5(a)</ref>, the left shows the 3D LUT cube of identity mapping. In this cube, the coordinate values of each point correspond to the three-channel values of its color in the SDR domain. We also show the LUT manifold of SDRTV-to-HDRTV color mapping using the base network in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>. In this cube, the coordinate values of each point represent its corresponding HDR color. We can see that SDR colors in the range of 0 to 255 are mapped to HDR colors with the range of 0 to 1023.  We can observe the effect of our condition network in AGCM by LUT manifold. As shown in <ref type="figure" target="#fig_10">Fig. 6</ref>, LUT manifold changes obviously by taking different images as inputs of condition network. It suggests that our AGCM achieves imageadaptive function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis on the necessity of AGCM</head><p>We conducted some experiments to illustrate the necessity of AGCM. We make quantitative and qualitative comparisons between methods with AGCM and without AGCM. Basic3x3 represents the same network as our base network but replaces all 1 ? 1 filters with 3 ? 3 filters. As shown in Tab. 2, the methods performing AGCM before LE obtain higher performance than the methods learning end-to-end mapping directly. In <ref type="figure" target="#fig_11">Fig. 7</ref>, we can see that outputs of methods without AGCM suffer obvious artifacts in some over-exposed and oversaturated regions. Besides, methods without AGCM perform badly in the color transition test. Therefore, it is necessary to perform AGCM before local enhancement. In other words, it is a better choice to perform color mapping before learning end-to-end mapping using local information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Params?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with data-driven methods of LDR-to-HDR</head><p>As illustrated in the main paper Sec. 2 and Sec. 5.3, previous LDR-to-HDR methods are functionally different from our method. For some non-data-driven methods, we can set some variables of these algorithms to obtain considerable results for comparison. However, it is not applicable to data-driven methods, such as HDRCNN <ref type="bibr">[1]</ref> and SingleHDR <ref type="bibr">[10]</ref>. To illustrate this problem, we make comparisons with these two methods. In <ref type="figure" target="#fig_12">Fig. 8</ref>, the first and third columns show the outputs of these two methods processed the same as described in Sec. 5.3. Since the networks are not dedicated to the recovery of the entire range, pixel values processed by PQ-OETF are compressed to a small range. Thus, there is a lack of contrast in these results. To compare these methods as fair as possible, we manually changed the contrast of these images by multiplying the pixel value by 5. In this case, we can still observe that these methods introduce some obvious artifacts and produce</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">More comparisons with existing methods</head><p>In <ref type="figure" target="#fig_0">Fig. 10</ref>, We provide more visual comparisons of our methods against existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Additional explanations about visualization</head><p>First of all, we must declare that the HDR results can only be displayed correctly on HDR display devices. To show the results in the paper, we visualize HDRTV images by directly showing the pixel values encoded by PQ-OETF in this work. We do this out of two considerations: (1) Displaying the PQ value directly is equivalent to linearly decoding the result. Although the images may look relatively darker than them on HDR screens, details are better preserved and can be seen more easily, especially in the highlight and saturated regions. (2) Visualized results by tone mapping as <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> are closer to the intuitive feeling of HDR, but the artifacts, which can be easily seen on HDR display devices, may be reduced and not easily visible in paper, as shown in <ref type="figure" target="#fig_0">Fig. 11</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Additional explanations about the user study</head><p>In this section, we provide more explanations about how we conduct the user study. As described in the main paper Sec. 5.6, we randomly select 25 images from the testing set and show results of the images produced by different methods to participants on HDR-TV in dark conditions. The HDR-TV is Sony X9500G with a peak brightness of 1300 cd/m 2 . Before experimenting, we first instruct the participants to take the following three factors as the main evaluation considerations: <ref type="bibr">(1)</ref> whether there are obvious artifacts and unnatural colors, (2) whether the overall color, brightness and contrast of the images are natural and comfortable, (3) whether the contrast between light and dark levels and highlight details can be perceived. Based on these three principles, participants are asked to rank the results of different methods in each scenario. When displaying the results, the TV is set to rec.2020 color gamut and HDR10 standard. We compared five methods including Ada-3DLUT <ref type="bibr">[11]</ref>, Deep SR-ITM <ref type="bibr">[6]</ref>, Pixel2pixel <ref type="bibr">[5]</ref>, KovaleskiEO <ref type="bibr">[9]</ref> and our method with Ground Truth. When ranking the images for a certain scene, participants can observe six images of different methods at the same time or compare any two pictures at will until they decide the order. We show the counts of different results in the top three ranks, as shown in the main paper <ref type="figure" target="#fig_10">Fig. 6</ref>. There are 208 (41.6%) and 86 counts (17.2%) for GT and our method, respectively. It means that among the results considered to have the best visual quality, GT and our method account for 41.6% and 17.2%, respectively. Similarly, the results of our method account for 35.4% among the results considered to be the second-best visual quality. In conclusion, the results obtained by our method are second only inferior to GT in terms of visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Realistic SDRTV-to-HDRTV</head><p>We test our HDRTVNet on native 4K videos to demonstrate the performance of conversion in realistic scenes. To revive landscape viewing as far as possible, we employ an HDR camera * fixed by a tripod to shot the same frame of HDRTV/SDRTV respectively and directly show the JPG images processed by the camera. In <ref type="figure">Fig. 9</ref>, we can observe that the generated results have more vivid color and higher contrast than their SDRTV versions. <ref type="figure">Figure 9</ref>. Predictions for realistic conditions. * The camera we use is Sony ILCE-7M2, whose light measuring range is EV-1?EV20. <ref type="figure" target="#fig_0">Figure 10</ref>. Qualitative comparisons. The top row describes the categories of algorithms. <ref type="figure" target="#fig_0">Figure 11</ref>. Comparison of visualization methods. The first two rows of every four rows are tone mapped results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) SDRTV/HDRTV formation pipeline (b) end-to-end solution (c) LDR-to-HDR based solution (d) proposed solution pipeline Analysis of SDRTV-to-HDRTV. (a) Simplified SDRTV /HDRTV content formation pipeline. (b) Existing end-to-end solution pipeline. (c) Solution pipeline based on LDR-to-HDR methods. (d) Proposed solution pipeline. Please refer to supplementary file for detailed explanations of the above diagrams and curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the proposed three-step SDRTV-to-HDRTV method. Each step has a corresponding network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative comparisons. The top row describes the categories of algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Color test visualization. The top row describes the categories of algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of LUTs. The LUT cube shows the position of SDR colors in the HDRTV domain at the coordinates determined by the pixel values of their corresponding HDRTV pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>TFigure 2 .</head><label>2</label><figDesc>Hable = f Hable (I) = h(1.6 * k * I)/h(w) (1) * indicates contribute equally. ? Corresponding author.(a) LDR imgae formation pipeline in SingleHDR [10] (b) SDRTV/HDRTV formation pipelineFigure 1. (a) LDR image formation pipeline provided in SingleHDR [10]. (b) SDRTV/HDRTV formation pipeline. Raw data here does not strictly refer to Raw file, which represents content that has not been processed by the subsequent operations. where h(x) = ((x * (a * x + c * b) + d * e)/(x * (a * x + b) + d * f )) ? e/f , and a, b, c, d, e, f, k, w are specific parameters. We set a = 0.15, b = 0.50, c = 0.10, d = 0.20, e = 0.02, f = 0.30, k = 0.2 and w = 11.2. I denotes radiance map. Owing to SDRTV and HDRTV have different dynamic range, we set the range of I to 0 ? 100cd/m 2 for SDRTV and 0 ? 10000cd/m 2 for HDRTV. Note that we use Hable tone mapping for an example and it can actually be any tone mapping algorithm here. (a) SDRTV tone mapping curve; (b) HDRTV tone mapping curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>(a) SDRTV target gamut (rec.709). (b) HDRTV target gamut (rec.2020). It can be seen that rec.2020 for HDRTV is wider than rec.709 for SDRTV in CIE chromaticity diagram</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>(a) gamma-OETF for SDRTV. (b) PQ-OETF for HDRTV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Identity mapping of SDRTV colors.(b) SDRTV-to-HDRTV color mapping of base network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of 3D LUTs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>3D LUTs generated by taking various images as inputs of condition network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparisons between methods w/ and w/o AGCM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative comparisons. The top row describes the categories of algorithms.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We have also conducted experiments to remove the pixel shuffle layer instead of downsampling at the beginning, but the results show that it could not bring improvements but increase the computational cost significantly.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Qing Yuan Research Institute, Shanghai Jiao Tong University 4 Shanghai AI Laboratory, Shanghai {chxy95, zhengwen.zhang02, jimmy.sj.ren, lynhoo.tian}@gmail.com {yu.qiao, chao.dong}@siat.ac.cn1. OverviewIn this supplementary material, we present additional explanations and analyses to complement the main manuscript. First, we provide more details about the formation pipeline include the principle to simplify the pipeline and the specific explanations of diagrams and curves in the pipeline. Second, we introduce the implementation details for training our networks. Third, we present additional quantitative and qualitative analyses of our method. Fourth, we provide more additional explanations about visualization and the user study. Finally, we demonstrate the performance of our method in realistic scenes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of uhd-hdr-wcg videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahrukh</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilan</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1740" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th international conference on Computer graphics and interactive techniques in Australasia and Southeast Asia</title>
		<meeting>the 4th international conference on Computer graphics and interactive techniques in Australasia and Southeast Asia</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hdrunet: Single image hdr reconstruction with denoising and dequantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="354" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive logarithmic mapping for displaying high contrast scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Drago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Myszkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Annen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norishige</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07629</idno>
		<title level="m">Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hdr image reconstruction from a single exposure using deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyorgy</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rafa?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep bilateral learning for realtime image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Conditional sequential modulation for efficient global image retouching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10390</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Physiological inverse tone mapping based on retina response. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqing</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Brost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reference electro-optical transfer function for flat panel displays used in hdtv studio production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-R</surname></persName>
		</author>
		<idno>BT.1886</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>ITU-R Rec</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High dynamic range electro-optical transfer function of mastering reference displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-R</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SMPTE ST2084</title>
		<imprint>
			<publisher>SMPTE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Parameter values for the hdtv standards for production and international programme exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-R</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="709" to="715" />
		</imprint>
		<respStmt>
			<orgName>ITU-R Rec</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parameter values for ultra-high definition television systems for production and international programme exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-R</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ITU-R Rec</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image parameter values for high dynamic range television for use in production and international programme exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-R</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>ITU-R Rec</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Objective metric for the assessment of the potential visibility of colour differences in television</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-R</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>ITU-R Rec, BT</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">High dynamic range television for production and international programme exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Itu-R</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>ITU-R Rec, BT</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep sr-itm: Joint learning of super-resolution and inverse tone-mapping for 4k uhd hdr applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Soo Ye Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Jsi-gan: Ganbased joint super-resolution and inverse tone-mapping with pixel-wise task-specific filters for uhd hdr video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Soo Ye Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11287" to="11295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High-quality reverse tone mapping for a wide range of exposures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Kovaleski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A visibility matching tone reproduction operator for high dynamic range scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">Ward</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holly</forename><surname>Rushmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="306" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interactive local adjustment of tonal values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeev</forename><surname>Farbman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="646" to="653" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very lightweight photo retouching network with conditional sequential modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06279,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single-image hdr reconstruction by learning to reverse the camera pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Lung</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hdr-vdp-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kil Joong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">G</forename><surname>Rempel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluation of reverse tone mapping through varying exposure conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belen</forename><surname>Masia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Agustin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Roland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gutierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH Asia 2009 papers</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic range expansion based on image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belen</forename><surname>Masia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Guti?rrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parameter estimation for photographic tone reproduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Reinhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of graphics tools</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="51" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ldr2hdr: on-the-fly reverse tone mapping of legacy video and photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Rempel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seetzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorne</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tone reproduction for realistic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holly</forename><surname>Rushmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="42" to="48" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning image-adaptive 3d lookup tables for high performance photo enhancement in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sr-sim: A fast and high performance iqa index based on spectral residual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 19th IEEE international conference on image processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hdr image reconstruction from a single exposure using deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyorgy</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rafa?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Uncharted 2: Hdr lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hable</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Game Developers Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">56</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Conditional sequential modulation for efficient global image retouching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10390,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep sr-itm: Joint learning of super-resolution and inverse tone-mapping for 4k uhd hdr applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Soo Ye Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Jsi-gan: Gan-based joint super-resolution and inverse tone-mapping with pixel-wise task-specific filters for uhd hdr video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Soo Ye Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11287" to="11295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">High-quality reverse tone mapping for a wide range of exposures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Kovaleski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Single-image hdr reconstruction by learning to reverse the camera pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Lung</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning image-adaptive 3d lookup tables for high performance photo enhancement in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
