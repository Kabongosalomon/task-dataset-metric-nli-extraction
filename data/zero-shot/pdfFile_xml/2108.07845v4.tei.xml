<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ARCH++: Animation-Ready Clothed Human Reconstruction Revisited</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Facebook Reality Labs Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Facebook Reality Labs Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
							<email>shunsuke.saito16@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Facebook Reality Labs Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<email>soatto@cs.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
							<email>tony.tung@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Facebook Reality Labs Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ARCH++: Animation-Ready Clothed Human Reconstruction Revisited</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present ARCH++, an image-based method to reconstruct 3D avatars with arbitrary clothing styles. Our reconstructed avatars are animation-ready and highly realistic, in both the visible regions from input views and the unseen regions. While prior work shows great promise of reconstructing animatable clothed humans with various topologies, we observe that there exist fundamental limitations resulting in sub-optimal reconstruction quality. In this paper, we revisit the major steps of image-based avatar reconstruction and address the limitations with ARCH++. First, we introduce an end-to-end point based geometry encoder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features. Second, in order to address the occupancy ambiguity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to jointly estimate the occupancy in both the posed and canonical spaces. Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints. In the experiments, we demonstrate improvements over the state of the art on both public benchmarks and user studies in reconstruction quality and realism.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Digital humans have become an increasingly important building block for numerous AR/VR applications, such as video games, social telepresence <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b42">43]</ref> and virtual try-on. Towards truly immersive experiences, it is crucial for these avatars to obtain higher level of realism that goes beyond the uncanny valley <ref type="bibr" target="#b49">[51]</ref>. Building a photorealistic avatar involves many manual works by artists or expensive capture systems under controlled environments <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b54">55]</ref>, limiting access and increasing cost. Therefore, it is vital to revolutionize reconstruction techniques with minimal prerequisite (e.g., a selfie) for future digital human applications.</p><p>Recent human models reconstructed from a single im- age combine category-specific data prior with image observations <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b72">73]</ref>. Among which, template-based approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref> nevertheless suffer from lack of fidelity and difficulty supporting clothing variations; while non-parametric reconstruction methods <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b24">25]</ref>, e.g., using implicit surface functions, do not provide intuitive ways to animate the reconstructed avatar despite impressive fidelity. In the recent work ARCH <ref type="bibr" target="#b27">[28]</ref>, the authors propose reconstructing non-parametric human model using pixel-aligned implicit functions <ref type="bibr" target="#b61">[62]</ref> in a canonical space, where all reconstructed avatars are transformed to a common pose. To do so, a parametric human body model is exploited to determine the transformations. By transferring skinning weights, which encode how much each vertex is influenced by the transformation of each body joint, from the underling body model, the reconstruction results are ready to animate. However, we observe that the advantages of a parametric body model and pixel-aligned implicit functions are not fully exploited.</p><p>In this paper we introduce ARCH++, which revisits the major steps of animatable avatar reconstruction from images and addresses the limitations in the formulation and representation of the prior work. First, current implicit function based methods mainly use hand-crafted features as the 3D space representation, which suffers from depth ambiguity and lacks human body semantic information. To address this, we propose an end-to-end geometry encoder based on PointNet++ <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref>, which expressively describes the underlying 3D human body. Second, we find the unposing process to obtain the canonical space supervision causes topology change (e.g., removing self-intersecting regions) and consequently the articulated reconstruction fails to obtain the same level of accuracy in the original posed space. Therefore, we present a co-supervising framework where occupancy is jointly predicted in both the posed and canonical spaces, with additional constraints on the cross-space consistency. This way, we benefit from both: supervision in the posed space allows the prediction to retain all the details of the original scans; while canonical space reconstruction can ensure the completeness of a reconstructed avatar. Last, image-based avatar reconstruction often suffers from degraded geometry and texture in the occluded regions. To make the problem more tractable, we first infer surface normals and texture of the occluded regions in the image domain using image translation networks, and then refine the reconstructed surface with a moulding-inpainting scheme.</p><p>In the experiments, we evaluate ARCH++ on photorealistically rendered synthetic images as well as in-the-wild images, outperforming prior works based on implicit functions and other design choices on public benchmarks.</p><p>The contributions of ARCH++ include: 1) a point-based geometry encoder for implicit functions to directly extract human shape and pose priors, which is efficient and free from quantization errors; 2) we are the first to point out and study the fundamental issue of determining target occupancy space: posed-space fidelity vs. canonical-space completeness. Albeit ignored before, we outline the pros and cons of different spaces, and propose a co-supervising framework of occupancy fields in joint spaces; 3) we discover image-based surface attribute estimation could address the open problem of view-inconsistent reconstruction quality. Our moulding-inpainting surface refinement strategy generates 360 ? photorealistic 3D avatars. 4) our method demonstrates enhanced performance on the brand new task of image-based animatable avatar reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Template-based reconstruction utilizes parametric human body models, e.g., SCAPE <ref type="bibr" target="#b3">[4]</ref> and SMPL <ref type="bibr" target="#b43">[44]</ref> to provide strong prior on body shape and pose to address ill-posed problems including body estimation under clothing <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b79">80]</ref> and image-based human shape reconstruction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b73">74]</ref>. While these works primarily focus on underling body shapes without clothing, the template-based representations are later extended to modeling clothed humans with displacements from the minimal body <ref type="bibr" target="#b56">[57]</ref>, or external clothing templates <ref type="bibr" target="#b9">[10]</ref>, from 3D scans <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b56">57]</ref>, videos <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>, and a single image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref>. As these approaches build clothing shapes on a body template mesh, the reconstructed models can be easily driven by pose parameters of the parametric body model. To ad-dress the lack of details with limited mesh resolutions, recent works propose to utilize 2D UV maps <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b2">3]</ref>. However, as a clothing topology can significantly deviate from the underling body mesh and its variation is immense, these template-based solutions fail to capture clothing variations in the real world.</p><p>Non-parametric capture is widely used to capture highly detailed 3D shapes with an arbitrary topology from multi-view systems under controlled environments <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b46">47]</ref>. Recent advances of deep learning further push the envelope by supporting sparse view inputs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>, and even monocular input <ref type="bibr" target="#b40">[41]</ref>. For single-view clothed human reconstruction, direct regression methods demonstrate promising results, supporting various clothing types with a wide range of shape representations including voxels <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b29">30]</ref>, two-way depth maps <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b63">64]</ref>, visual hull <ref type="bibr" target="#b51">[52]</ref>, and implicit functions <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b24">25]</ref>. In particular, pixel-aligned implicit functions (PIFu) <ref type="bibr" target="#b61">[62]</ref> and its follow-up works <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b24">25]</ref> demonstrate impressive reconstruction results by leveraging neural implicit functions <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b55">56]</ref> and fully convolutional image features. Unfortunately, despite its high-fidelity results, non-parametric reconstructions are not animation-ready due to missing body part separation and articulation. Recently, IF-Net <ref type="bibr" target="#b13">[14]</ref> exploits partial point cloud inputs and learns implicit functions using latent voxel features. Compared with image-based avatar reconstruction, completion from points can leverage directly provided strong shape and pose cues, and thus skip learning them from complex images.</p><p>Hybrid approaches combine template-based and nonparametric methods and allow us to leverage the best of both worlds, namely structural prior and support of arbitrary topology. Recent work <ref type="bibr" target="#b7">[8]</ref> shows that using SMPL model as guidance significantly improves robustness of non-rigid fusion from RGB-D inputs. For single-view human reconstruction, Zheng et al. first introduce a hybrid approach of a template-model (SMPL) and a non-parametric shape representation (voxel <ref type="bibr" target="#b81">[82]</ref> and implicit surface <ref type="bibr" target="#b80">[81]</ref>). These approaches, however, choose an input view space for shape modeling with reconstructed body parts potentially glued together, making the reconstruction difficult to animate as in the aforementioned non-parametric methods. The most relevant work to ours is ARCH <ref type="bibr" target="#b27">[28]</ref>, where the reconstructed clothed humans are ready for animation as pixel-aligned implicit functions are modeled in an unposed canonical space. However, such framework fundamentally leads to sub-optimal reconstruction quality. We achieve significant improvement on accuracy and photorealism by addressing the hand-crafted spatial encoding for implicit functions, the lack of supervision in the original posed space, and the limited fidelity of occluded regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methods</head><p>Our proposed framework, ARCH++, uses a coarse-tofine scheme, i.e., initial reconstruction by learning jointspace implicit surface functions (see <ref type="figure">Fig. 2</ref>  <ref type="figure">Figure 2</ref>. Overview of the initial joint-space implicit surface reconstruction. This procedure includes three components: i) semantic-aware geometry encoder, ii) pixel-aligned appearance encoder and iii) joint-space occupancy estimator. See text for detailed explanation. refinement in both spaces (see <ref type="figure" target="#fig_1">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint-Space Implicit Surface Reconstruction</head><p>Semantic-Aware Geometry Encoder. The spatial feature representation of a query point is critical for deep implicit function. While the pixel-aligned appearance feature via Stack Hourglass Network <ref type="bibr" target="#b52">[53]</ref> has already demonstrated its effectiveness in detailed clothed human reconstruction by prior works <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25]</ref>, an effective design of pointwise spatial encoding has not yet been well studied. The extracted geometry features should be informed of the semantics of the underlying 3D human body, which provide strong priors to regularize the overall dressed people shape.</p><p>The spatial encoding methods used previously include hand-crafted features (e.g., RBF <ref type="bibr" target="#b27">[28]</ref>) and latent voxel features <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b80">81]</ref>. The former is constructed based on Euclidean distances between a query point and the body joints, ignoring the shapes. The voxel-based features capture both shape and pose priors of a parametric body mesh. Compared with the hand-crafted features, the end-to-end learned voxel features are better informed of the underlying body structures but often constrained by GPU memory sizes and suffer from quantization errors due to low spatial resolution. To effectively encode the shape and pose priors without losing any precision, we propose a novel semantic-aware geometry encoder that extracts point-wise spatial encodings. Essentially a parametric body mesh can be sampled into a point cloud and fed into PointNet++ <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref> to learn pointbased spatial features, which have several advantages over both hand-crafted RBF features and voxel-based ones. Our method encodes both shape and pose priors from parametric shapes without computation overhead and quantization errors caused by the mesh voxelization process. Additional detailed statistical comparisons on points v.s. voxels in representing 3D shapes are reported in <ref type="bibr" target="#b18">[19]</ref>.</p><p>Given a parametric body mesh estimated and deformed by <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b27">28]</ref>, we use a PointNet++ <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref> based semanticaware geometry encoder to learn the underlying 3D human body prior. We sample N 0 (e.g., 7324) points from the body mesh surfaces and feed them into the geometry encoder for spatial feature learning, that is,</p><formula xml:id="formula_0">fpn : {x i 0 } N 0 i=1 ? {x j 1 , h j 1 } N 1 j=1 , {x k 2 , h k 2 } N 2 k=1 , {x l 3 , h l 3 } N 3 l=1 ,<label>(1)</label></formula><p>where x i 0 ? R 3 is a point sampled from the parametric body mesh. The PointNet++ based encoder utilizes fullyconnected layers and neighborhood Max-Pooling to extract semantic-aware geometry features h ? R 32 of a point. It also applies Furthest Point Sampling to progressively down sample the points N 1 = 2048, N 2 = 512, N 3 = 128 to extract latent features with increasing receptive fields. For example {x j 1 } is a down sampled point set with size N 1 , and h j 1 ? R 32 is the learned feature w.r.t. each point. As illustrated in <ref type="figure">Fig. 2</ref>, for any query point p a ? R 3 in the canonical space we obtain its point-wise spatial encoding f g ? R 96 via inverse L2-norm kernel based feature interpolation, followed by query coordinates concatenated Multi-layer Perceptrons (MLP). Particularly, we extract these features from different point set densities-j, k, l to construct concatenated features f g = (f j g ? f k g ? f l g ) that are informed of multi-scale structures. For example, f j g ? R 32 is defined as:</p><formula xml:id="formula_1">f j g (pa, {x j 1 , h j 1 }) = MLP(pa ? m ?pa ? x m 1 ? ?2 S(pa, {x j 1 , h j 1 }) h m 1 ), S(pa, {x j 1 , h j 1 }) = m ?pa ? x m 1 ? ?2 ,<label>(2)</label></formula><p>where the index m is determined by finding the K nearest neighbors among the point set {x j 1 } w.r.t. the query point. Empirically we found setting K = 3 obtains fair performance. The features extracted at other point set densities f k g , f l g ? R 32 are obtained similarly leveraging {x k 2 , h k 2 } and {x l 3 , h l 3 }, respectively. Pixel-Aligned Appearance Encoder. We share the same architecture design as <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25]</ref> to map an input image I ? R 512?512?3 into the latent feature maps ? ? (I) ? R 128?128?256 via a Stacked Hourglass Network <ref type="bibr" target="#b52">[53]</ref> with weights ?. To obtain appearance encoding f a ? R 256 of any query point p b ? R 3 in the posed space, we project it back to the image plane based on a camera model of weak perspective projection, and bilinearly interpolate the latent image features:</p><formula xml:id="formula_2">fa(p b , I) = B(??(I), ?(p b )),<label>(3)</label></formula><p>where B(?) indicates the differentiable bilinear sampling operation, and ?(?) means weak perspective camera projection from the query point p b to the image plane of I. Joint-Space Occupancy Estimator. While most nonparametric and hybrid methods use the posed space as the learning and inference target space, ARCH instead reconstructs the clothed human mesh directly in a canonical space where humans are in a normalized A-shape pose. Different choices of the target space have pros and cons. The posed space is naturally aligned with the input pixel evidence and therefore the reconstructions have high data fidelity leveraging the direct image feature correspondences. Thus, many works choose to reconstruct a clothed human mesh in its original posed space (e.g., PIFu(HD) <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63]</ref>, Geo-PIFu <ref type="bibr" target="#b24">[25]</ref>, PaMIR <ref type="bibr" target="#b80">[81]</ref>). However, in many situations the human can demonstrate complex poses with selfintersection (e.g., hands in the pocket, crossed arms) and cause a "glued" mesh that is difficult to articulate. Meanwhile, canonical pose reconstruction offers us a rigged mesh that is animation ready (via its registered A-shape parametric mesh <ref type="bibr" target="#b27">[28]</ref>). The problem of using the canonical space as the target space is that when we warp the mesh into its posed space there could be artifacts like intersecting surfaces and distorted body parts (see <ref type="figure">Fig. 6</ref>). Thus, the reconstruction fidelity of the warping obtained canonical-toposed space mesh will degenerate. To maintain both input image fidelity and reconstruction surface completeness, we propose to learn the joint-space occupancy distributions.</p><p>We use a joint-space defined occupancy map O to implicitly represent the 3D clothed human under both its original posed space and a rigged canonical space: <ref type="bibr" target="#b3">(4)</ref> where o a , o b denote the occupancy for points p a and p b . A point in the posed space is p b and its mapped counterpart in the canonical space is p a = SemDF(p b ). The semantic deformation mapping (SemDF) between the original posed and the rigged canonical spaces is enabled by nearest neighbor-based skinning weights matching between p b and the estimated underlying parametric body mesh <ref type="bibr" target="#b27">[28]</ref>.</p><formula xml:id="formula_3">O = {(pa, p b , oa, o b ) : pa, p b ? R 3 , ?1 ? oa, o b ? 1},</formula><p>To enable mesh reconstruction in joint spaces, we use both point-wise spatial features f g ? R 96 that are informed of semantic full-body structures, and pixel-aligned features f a ? R 256 that encode human front-view appearances:</p><formula xml:id="formula_4">oa = F ? (fg ? fa), o b = F ? (fg ? fa),<label>(5)</label></formula><p>where ?, ? are network weights of the MLP-based deep implicit surface functions. To reconstruct avatars from the dense occupancy estimations in two spaces, we use Marching Cube <ref type="bibr" target="#b44">[45]</ref> to extract the isosurface at o a = ? and o b = ? (i.e., ? = 0), respectively. The network outputs o a , o b are supervised by the ground truth joint-space occupancy? a ,? b , depending on whether a Namely, the SemDF defines a dense correspondence mapping between the two spaces but their occupancy values are not necessarily the same. Therefore, naively learning the distribution in one space and then warping the reconstruction into another pose can cause mesh artifacts (see <ref type="figure">Fig. 6</ref>). This motivates us to model two space occupancy distributions jointly in order to maintain both canonical space mesh completeness and posed space reconstruction fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mesh Refinement</head><p>We further refine the reconstructed meshes in joint spaces by adding geometric surface details and photorealistic textures. As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, we propose a mouldinginpainting scheme to utilize the front and back side normals and textures estimated in the image space. This is based on the observation that direct learning and inference of dense normal/color fields using deep implicit functions as <ref type="bibr" target="#b27">[28]</ref> usually leads to over-smooth blur patterns and block artifacts (see <ref type="figure">Fig. 5</ref>). In contrast, image space estimation of normal and texture maps produces sharp results with fine-scale details, and is robust to human pose and shape changes. These benefits are from well-designed 2D convolutional deep networks (e.g., Pix2Pix <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b69">70]</ref>) and advanced (adversarial) image generation training schemes like GAN, with perceptual losses. The image-space estimated normal (and texture) maps could be used in two different ways. They can be used as either direct inputs into the Stack Hourglass as additional channels of the single-view image, or moulding-based front and back side mesh refinement sampling sources. In the experiments, we conduct ablation studies on these two schemes (i.e., early direct input, late surface refinement) and demonstrate that our mouldingbased refinement is better at maintaining fine-scale surface details across different views (see <ref type="figure" target="#fig_4">Fig. 8</ref>).</p><p>Posed Space. For the clothed human mesh obtained by Marching Cube in the original posed space, we conduct visibility tracing to determine if a vertex V ? R 3 should be projected onto the front or the back side to bilinearly sample the normal/texture maps. Essentially, this is a mouldingbased mesh refinement process for surface details and textures enhancement. We first conduct normal refinement. Note that for vertices whose unrefined normals n ? R 3 are near parallel (i.e., within ? degrees) to the input image plane, we project them onto both the front and the back side normal maps I f n , I b n ? R 512?512?3 . We could then compute the refined surface normals n ? ? R 3 via a linear blend fusion:</p><formula xml:id="formula_5">n ? = ?(1 ? ? ? ) B(I f n , ?(V )) + ?(? ? ) B(I b n , ?(V )), ? ? = (90 ? + ? ? ?)/(2?),<label>(6)</label></formula><p>where ? is the angle between the unrefined normal and the forward camera raycast, and ? ? is the normalized value of ?. Again, B(?) indicates the bilinear sampling operation. The indicator function ?(?) determines the blending weights of sampled normals from the front and the back sides:</p><formula xml:id="formula_6">?(? ? ) = min(max(? ? , 0), 1)<label>(7)</label></formula><p>This simple yet effective fusion scheme creates a normalrefined mesh with negligible blending boundary artifacts. With the refined surface normals we can further apply Poisson Surface Reconstruction <ref type="bibr" target="#b34">[35]</ref> to update the mesh topology but in practice we find this unnecessary since the moulding-refined avatar can already satisfy various AR/VR and novel-view rendering applications. This bump rendering idea is also used in DeepHuman <ref type="bibr" target="#b81">[82]</ref> but they only refine meshes using the front views. We further conduct the texture refinement in a similar manner but use the refined normals to help determine the linear blending weights of boundary vertices. Our moulding-based front/back normal and texture refinement method yields clothed human meshes that look photorealistic at different viewpoints with full-body surface details (e.g., clothes wrinkles, hairs).</p><p>Canonical Space. The reconstructed canonical space avatar is rigged and thus can be warped back to its posed space and then refined via the same pipeline described above. However, a unique challenge for canonical avatar refinement is that mesh reconstructions in this space might contain unseen surfaces under the posed space. For example, in the third row of <ref type="figure">Fig. 5</ref>, the folded arm is in contact with the chest in the posed space but unfolded in the canonical space. Therefore, we do not have direct normal/texture correspondences of the chest regions of the canonical mesh. To address this problem, we render the front and the back side images of the canonical mesh with incomplete normal and texture, and treat it as an inpainting task. This problem has been well studied using deep neural networks <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b77">78]</ref> and patch matching based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref>. We use Patch-Match <ref type="bibr" target="#b5">[6]</ref> for its robustness. As demonstrated in the last two columns of <ref type="figure">Fig. 5</ref>, compared to directly regressing pointwise normal and texture, our inpainting-based results obtain sharper details and fewer artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training Losses</head><p>The training process involves learning deep networks for two goals: joint-space occupancy estimation with L o , and normal/texture estimation with L n and L t . Specifically, L o is the occupancy regression loss of our joint-space deep implicit functions, and L n , L t are image translation losses of the normal, texture estimation networks.</p><p>Joint-space Occupancy Estimation. The deep implicit function training is based on query point sampling and supervised occupancy regression with Tanh output layers. We randomly sample mesh points p a , p b in two spaces and then add diagonal Gaussian perturbation with a standard deviation of 5 cm to increase the sample coverage of closeto-surface regions in space. In each training iteration we sample 20480 pairs of query points (p a , p b ), with predicted occupancy (o a , o b ). The joint-space occupancy regression loss contains three terms:</p><formula xml:id="formula_7">L o (o a , o b ) = L occ o (o a ) + L occ o (o b ) + L con o (o a , o b ), (8) where L occ o (o a ), L occ o (o b ) denote the Smooth L1</formula><p>-Loss between the estimated occupancy values and their ground truth in the canonical and the posed spaces, respectively.</p><formula xml:id="formula_8">L con o (o a , o b )</formula><p>is a contrastive loss regularizing the occupancy consistency between the two spaces, that is,</p><formula xml:id="formula_9">L con o (oa, o b ) = |oa ? o b |, if?a =? b , ?1 max(?2 ? |oa ? o b |, 0), otherwise,<label>(9)</label></formula><p>where ? 1 and ? 2 are two parameters to adjust the penalty of inconsistent joint-space groundtruth pairs. Those pairs usually exist around the self-intersecting regions and need to be down-weighted due to the errors in canonical space supervision. Empirically, we set ? 1 = 0.1 and ? 2 = 0.3. Mesh Refinement. We consider the image-space normal and texture estimation as an image-to-image translation task. Given an input image I, our task is to learn the front normal map I f n , the back normal map I b n and the back side texture map I b t ? R 512?512?3 . Note we assume the input image can be directly used as the front texture map. Inspired by the demonstrated superior results of Pix2Pix <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b69">70]</ref>, we define the training losses as:</p><formula xml:id="formula_10">Ln(I f n , I b n ) = L rec n (I f n )+L rec n (I b n )+L vgg n (I f n ) + L vgg n (I b n ), Lt(I b t ) = L rec t (I b t ) + L vgg t (I b t ) + L adv t (I b t ),<label>(10)</label></formula><p>where L rec (?) denotes the L1 distance reconstruction loss, L adv (?) means the generative adversarial loss and L vgg (?) is the VGG-perceptual loss proposed by <ref type="bibr" target="#b31">[32]</ref>. In the experiments, we found that the generative adversarial loss L adv (?) counteracts to performance in the normal map estimation task and thus we only enforce this loss term upon the back side texture map. One explanation is that the normal map space is more constrained and has fewer variations than  <ref type="table">Table 1</ref>. Ablation studies on the effectiveness of ARCH++ proposed components in both spaces: posed vs. canonical. Best scores are in bold. Rows are target reconstruction spaces, columns are evaluation spaces. The first row means using the posed space as the target space (e.g., PIFu, PIFuHD, Geo-PIFu, PaMIR), whose reconstruction can be warped into the canonical space via a registered parametric body to compute evaluation metrics in both spaces. The second row means direct supervision and reconstruction in the canonical space, followed by warping into the posed space (e.g., ARCH). The rest rows are based on our joint-space co-supervision and reconstruction scheme.  the texture map, and therefore adversarial training does not fully show its effectiveness in this case.</p><formula xml:id="formula_11">Methods RenderPeople BUFF Normal ? P2S ? Chamfer ? Normal ? P2S ? Chamfer ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we present the experimental settings, result comparisons and ablation studies of ARCH++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>We implement our framework using PyTorch and conduct the training with one NVIDIA Tesla V100 GPU. The proposed deep neural networks are trained with RMSprop optimizer with a learning rate starting from 1e-4. We use an exponential learning rate scheduler to update it every 3 epochs by multiplying with the factor 0.1 and terminate the training after 12 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets</head><p>We adopt the dataset setting from <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">63]</ref>. Our training dataset consists of 375 3D scans from RenderPeople dataset <ref type="bibr" target="#b59">[60]</ref> and 205 3D scans from AXYZ dataset <ref type="bibr" target="#b16">[17]</ref>. These watertight human meshes have various clothes styles as well as body shapes and poses. Our testing set includes 37 scans from RenderPeople dataset <ref type="bibr" target="#b59">[60]</ref>, 192 scans from AXYZ dataset, 26 scans from BUFF dataset <ref type="bibr" target="#b79">[80]</ref>, and 2D images from Internet public domains, representing clothed people with a large variety of complex clothes. The subjects in the training dataset are mostly in standing pose, while the subjects in the test dataset contain various poses including sitting, twisted and standing, as well as self-glued and separated limbs. We use Blender and 38 environment maps to render each scan under different natural lighting conditions. For each 3D scan, we generate 360 images by rotating a camera around the mesh with a step size of 1 degree. These RenderPeople images are used to train both the occupancy estimation and the image translation networks.</p><p>We generate ground truth clothed human meshes in the canonical pose using the method introduced in <ref type="bibr" target="#b27">[28]</ref>. Note that the warping process between the posed and the canonical spaces inevitably contain model noises (e.g., selfcontact region artifacts, skinning weights nearest neighbor discontinuities), which motivates our joint-space cosupervision and reconstruction scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results and Comparisons</head><p>We use the same metrics as <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b27">28]</ref> for quantitative evaluation of the reconstructed meshes. We report the average point-to-surface Euclidean distance (P2S) and the Chamfer distance in centimeters, as well as the L2 normal re-projection errors. The two state of the art methods for our main comparisons are PIFuHD <ref type="bibr" target="#b62">[63]</ref> and ARCH <ref type="bibr" target="#b27">[28]</ref>, both are built upon PIFu <ref type="bibr" target="#b61">[62]</ref> with improvements in different aspects. PIFuHD ingests high-resolution images in a sliding window manner to achieve rich surface reconstruction details. ARCH leverages nearest neighbor-based linear blend skinning weights and hand-crafted RBF features to reconstruct animatable avatars in a canonical space. In addition to these two most related methods, we also include multiple prior methods <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b61">62]</ref> and report the benchmark results on the RenderPeople and the BUFF datasets in Tab. 2. ARCH++ [Ours] results outperform the second best method ARCH by large gaps.</p><p>The visual comparisons in <ref type="figure">Fig. 5</ref> and <ref type="figure" target="#fig_2">Fig. 4</ref> further explain the advantages of our improvement. PIFuHD suffers from shape distortions due to lacking shape and pose priors provided by the end-to-end geometry encoder. Note that Input PIFu ARCH Ours PIFuHD ARCH Ours ARCH Ours <ref type="figure">Figure 5</ref>. Qualitative comparisons against the state-of-the-art methods <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b27">28]</ref>. The first column is input. Column 2-4, 5-7 are color and shape reconstruction results, respectively, in the posed space. The last two columns are canonical space avatar reconstructions. Our method handles arbitrary poses with self-contact and occlusions robustly, and reconstructs a higher level of details than existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variants</head><p>Normal ? P2S ? Chamfer ? Depth <ref type="bibr" target="#b61">[62]</ref> 0.047 0.78 0.93 RBF <ref type="bibr" target="#b27">[28]</ref> 0.042 0.74 0.85 End-to-end Voxel <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b80">81]</ref> 0.034 0.52 0.63 End-to-end Point 0.033 0.50 0.61 <ref type="table">Table 3</ref>. Ablation studies on different types of geometry encoders.</p><p>PIFuHD is incapable of reconstruct canonical space avatars and lacks texture estimation. ARCH reconstructions tend to be over smooth and blurry. Its recovered mesh normal and texture also have several block artifacts. Additionally, both methods fail to hallucinate plausible back-side surface details like clothes wrinkles, hairs, etc. In comparison, our approach achieves photorealistic and animatable reconstructions in joint spaces and across different viewpoints. We further show our results on Internet images in <ref type="figure" target="#fig_5">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Studies</head><p>Joint Space Reconstruction. To further understand the impact of the proposed methods, we present ablation studies in Tab. 1. The first three rows demonstrate the effectiveness of joint-space co-supervision, achieving balanced</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variants</head><p>Posed ? Canonical ? Mean ? Baseline 0.033 0.040 0.037 Object-space Regression <ref type="bibr" target="#b27">[28]</ref> 0.032 0.041 0.037 Image-space Input <ref type="bibr" target="#b62">[63]</ref> 0.032 0.038 0.035 Image-space Regression 0.031 0.039 0.035 <ref type="table">Table 4</ref>. Ablation studies on different ways of normal refinement.</p><p>performances on both the posed and the canonical space mesh reconstructions. Choosing the posed space as the reconstruction target space (e.g., PIFu, PIFuHD, Geo-PIFu, PaMIR) can cause missing surfaces and topology distortions in the posed-to-canonical space warped meshes (see <ref type="figure">Fig. 6</ref>). Meanwhile, choosing the canonical space as the target space (e.g., ARCH) can cause self-intersecting meshes with broken manifold as well as body part un-natural deformations in the canonical-to-posed space warped meshes. In contrast, our co-supervision and joint-space inference methods achieve both reconstruction fidelity in the posed space and body mesh completeness in the canonical space. Geometry Encoding. As shown in Tab. 1, we observe further error reduction leveraging the end-to-end learned point-wise spatial encodings. The prior method ARCH uses  <ref type="figure">Figure 6</ref>. Ablation studies on the reconstruction space. Singlespace reconstruction shows artifacts of either mesh surface overstretching or intersecting surfaces when warping from one space to another. Our joint-space reconstruction obtains balanced performance of both high reconstruction completeness under the canonical space and high input image fidelity under the posed space. <ref type="figure">Figure 7</ref>. Ablation studies on geometry encoding. Learned spatial features capture both pose and shape priors of the underlying parametric models and thus enable mesh reconstruction with more surface details than the handcrafted RBF features. Meanwhile, results of the voxel-based features are noisier than the point-based ones due to mesh quantization (i.e., voxelization) errors.</p><formula xml:id="formula_12">Input RBF E2E Point E2E Voxel</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OS Reg. Input</head><p>Baseline IS Input IS Reg.  the front and the back side remains an open question, some empirical observations and prior works indicate that normal estimation is a relatively easier task and can help refine the reconstructions. In Tab. 4 and <ref type="figure" target="#fig_4">Fig. 8</ref> we experiment on three principle ways of leveraging the estimated normals for mesh reconstructions with refined surface details. Among these normal refinement methods, our front/back-side image space normal regression and moulding-based surface refinement approach outperforms other variants. Objectspace normal regression is adopted in ARCH and is based on learning deep implicit functions of spatial normal fields. It fails to generate rich back side details and sometimes causes block artifacts as shown in the fourth row of <ref type="figure">Fig. 5</ref>. Image-space input is used in PIFuHD. It concatenates the color image input with estimated image-space normal maps and feeds them into Stack Hourglass for feature extraction. While this method achieves the same level of quantitative performance as our mesh refinement approach, its visual results are not as sharp as ours at both the front and the back sides. A degenerated case of our mesh refinement method is studied before in DeepHuman where they only estimate front-view normal maps and therefore lack reconstruction details at the back side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we revisit the major components in existing deep implicit function based 3D avatar reconstruction. Our method ARCH++ produces results which have high-level fidelity and are animation-ready for many AR/VR applications. We conduct a series of comparisons with and analysis on the state of the art to validate our findings. For future works, we plan to incorporate environment information (e.g., lighting, affordance) to further understand the body pose and appearance, and address current limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Input Preprocessing</head><p>During both training and test time, the input images to the network are normalized with regard to the human body scale. In particular, we re-scale the image based on the 3D skeleton estimation of the subject. The image is resized then centered, such that the pelvis of the person is aligned with the center of the image. Each pixel represents 1cm length using an orthographic scene projection. In this way we ensure proper scaling of the body parts, which allows us to capture the variations of different heights of people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Network Architectures</head><p>Semantic-Aware Geometry Encoder is based on Point-Net++ <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b57">58]</ref>, which consists of 3 Set Abstraction (SA) layers.  <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">128]</ref>). The explanation of each argument is (furthest point sampling size, point neighborhood radius, point neighborhood size limit, input feature channel, MLP output channels list). Namely, the multiscale point set sizes of Eq. (1) in the main paper are: N 1 = 2048, N 2 = 512, N 3 = 128. When extracting spatially-aligned features for any given query point, we leverage the point Feature Propagation (FP) layers defined at the aforementioned 3 different point set scales: FP(32, <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32]</ref>), FP(64, <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32]</ref>), FP(128, <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b31">32]</ref>). The explanation of each argument is (input feature channel, MLP output channels list). Therefore, the dimensions of our spatiallyaligned geometry features f g in Eq. (3) of the main paper are 96 = 32 * 3. Please refer to <ref type="bibr" target="#b58">[59]</ref> for further details.</p><p>Pixel-Aligned Appearance Encoder adopts the architecture from Stack Hourglass Network <ref type="bibr" target="#b52">[53]</ref>. The layer configuration is the same as PIFu(HD) and ARCH <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b27">28]</ref>, which is composed of a 4-stack model and each stack uses 2 residual blocks. The output latent image feature length is 256. Therefore, the dimensions of our pixel-aligned appearance features f a in Eq. (3) of the main paper are 256. Please refer to <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b27">28]</ref> for further details.</p><p>Joint-Space Occupancy Estimator is a two-branch multilayer perceptron (MLP). Each branch takes the spatially-aligned geometry features f g and pixel-aligned appearance features f a described above, and estimates onedimension occupancy o a or o b using Tanh activation. o a is the occupancy probability in the canonical space and o b is the one in the posed space. Similar to <ref type="bibr" target="#b61">[62]</ref>, we design the MLP with four fully-connected layers and the numbers of hidden neuron sizes are (1024, 512, 256, 128). Each layer of MLP has skip connections from the input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initial 3D Human Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization with OpenPose</head><p>Optimized 3D Human <ref type="figure" target="#fig_0">Figure 10</ref>. Postprocessing of 3D human body estimations. We observe the initially estimated 3D human body does not perfectly align with the 2D body landmarks (e.g., face, hands, feet) and we thus implement an additional optimization script to further minimize the distance between the re-projected 2D landmarks (red) and OpenPose detected 2D landmarks (blue), leading to a better 3D human body estimation.</p><p>Normal and Texture Image Translation both use a network architecture designed by <ref type="bibr" target="#b31">[32]</ref> using 9 residual blocks with 4 downsampling layers. The same network is also used in PIFuHD <ref type="bibr" target="#b62">[63]</ref> for normal maps estimation. In our work we extend this architecture to back side texture inference by adding GAN losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Hyper-Parameters</head><p>When training the joint-space occupancy losses L o , we use 0.5, 0.5, 0.05 to weight the canonical/posed space occupancy estimation losses L occ o as well as the contrastive regularizer L con o . When supervising the normal and texture image translation losses L n , L t , we set the weights of the L1 reconstruction losses L rec and the perceptual losses L vgg to 5.0 and 1.0, respectively. Particularly, the GAN losses (i.e. generator, discriminator) used in back-side texture hallucination is weighted by 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Computation Cost</head><p>Empirically, when using a single Tesla V100 GPU for training and one batch of 4 images (each image with 20480 pairs of query points), the forward pass takes around 1.4s and the backward propagation takes around 0.6s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inference on Images in the Wild</head><p>To perform inference on in-the-wild images, our staged pipeline involves person instance segmentation, parametric 3D human body estimation and the proposed 3D avatar reconstruction. The total pipeline takes around 5 seconds to reconstruct a fully colored animation-ready avatar from an unconstrained photo (RGB image) using one Tesla V100 GPU. In comparison, PaMIR <ref type="bibr" target="#b80">[81]</ref> takes over 40 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PIFuHD Ours</head><p>Input PIFuHD Ours <ref type="figure" target="#fig_0">Figure 11</ref>. 3D human body prior is crucial for our task, without which the reconstructions might look like squeezed relief sculpture due to wrong poses and shapes.</p><p>Note our current implementation lets all modules run sequentially and the intermediate results (e.g., masks, 3D human body parameters) are mostly exchanged through CPU memory and file IO, which leaves room for optimization. For our proposed avatar reconstruction framework alone, we could run 1) Semantic-Aware Geometry Encoder, 2) Pixel-Aligned Appearance Encoder and 3) Normal and Texture Refinement Networks in parallel to greatly boost the efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Person Instance Segmentation</head><p>Similar to most existing implicit surface function based methods, our occupancy estimation module also requires the person segmentation mask. Such mask is used to remove redundant and erroneous estimated occupancy in the background regions and serves as a visual hull prior similar to multiple view stereo. In this paper, we utilize one state of the art semantic instance segmentation <ref type="bibr" target="#b35">[36]</ref>, which is able to generate per-person segmentation mask. Note we are able to handle multiple people in the same image with such a detection-and-segmentation method. We set a minimum detection score 0.5 and minimum bounding box size 100 ? 100 to filter out people instances with too small resolution and guarantees the proper scale used for our proposed avatar reconstruction approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Parametric 3D Human Body Estimation</head><p>Underlying 3D human body serves as an important semantic cue for our approach. In this paper, we adopt a similar way as ARCH <ref type="bibr" target="#b27">[28]</ref> to estimate the parametric 3D human body from the input image. We first run DenseRaC <ref type="bibr" target="#b73">[74]</ref> to obtain the initial estimation of 3D human pose and shape parameters. Furthermore, we observe that when reprojecting such estimated 3D human body back to the input image, the body landmarks (e.g., joints, face, hands, feet) do not align with the input image well. We thus implement an additional optimization script using pytorch to compute the offsets between the re-projected body landmarks and detected body landmarks from OpenPose <ref type="bibr" target="#b11">[12]</ref> and backpropagate to the estimated 3D human pose and shape parameters (see <ref type="figure" target="#fig_0">Fig.10</ref>). The optimization is run over 200 iterations and we obtain better-aligned 3D human body in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. 3D Avatar Reconstruction</head><p>Given the intermediate results obtained from the modules above, we are able to run our proposed approach and obtain the jointly reconstructed avatars in both the original posed space and the canonical space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extended Experiments</head><p>In this section, we show some interesting conclusions we obtained along the way and extended experiments as well as comparisons (e.g., user studies, applications of avatar animations and video-based fusion, failure cases). Note we remove the background for all images for better visualization of the inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. 3D Human Body Prior is Crucial for Our Task</head><p>In <ref type="figure" target="#fig_0">Fig. 11</ref> we show that reconstruction results of PI-FuHD <ref type="bibr" target="#b62">[63]</ref> fail to capture the underlying correct body shapes and poses. PIFuHD and our method are trained using the same set of RenderPeople clothed human meshes, which consist of mostly upstanding poses. While obtaining large-scale ground truth clothed avatars with various poses and shapes is still an open problem, we can leverage parametric body shape estimation networks (e.g. DenseRaC <ref type="bibr" target="#b73">[74]</ref> and HMR <ref type="bibr" target="#b33">[34]</ref>) whose training data is easier to obtain. This motivates our design of learning both PIFuHD Ours Input <ref type="figure" target="#fig_0">Figure 12</ref>. Moulding-based Surface Refinement Obtains Better Consistency across Views. Our reconstruction results contain more full-body surface details (e.g., belt around the waist and clothing wrinkles on the back/legs) than the competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With Normal Refinement</head><p>Without Normal Refinement </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Moulding-based Surface Refinement Obtains Better Consistency across Views</head><p>For previous methods like PIFu <ref type="bibr" target="#b61">[62]</ref>, ARCH <ref type="bibr" target="#b27">[28]</ref> and PI-FuHD <ref type="bibr" target="#b62">[63]</ref>, we often observe the reconstructed surface de-tails only look plausible from the input camera view. Once we change the camera view to preview the reconstructed avatar from other view points, the rendered results contain fewer surface details and are less realistic. Such quality inconsistency limits the applicability of the prior works to AR/VR applications that require free viewpoint rendering.</p><p>Based on the aforementioned observations, we conclude that such phenomenon is caused by the suppression of occluded region hallucination. Although PIFuHD generates detailed back-side surface compared to PIFu and ARCH by leveraging inferred normal maps, its final rendering quality remains less sharp than our moulding-based refinement approach. Besides the results shown in the main paper, we provide more results with zoom-in in <ref type="figure" target="#fig_0">Fig. 12</ref> to further demonstrate the improvement on reconstruction details. Moreover, the normal refinement step enhances photorealistic rendering results by enabling fine-grained shading effects. In <ref type="figure" target="#fig_0">Fig. 13</ref>, we show the rendered images with and without the refined normals using the same mesh textures. With the normal refinement, the rendered images show more plausible clothing wrinkles at different views than the ones without normal refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. GAN Improves Back-Side Texture Estimation</head><p>In our experiments, we found that GAN losses help to enhance the realism of back-side texture maps. In <ref type="figure" target="#fig_0">Fig. 14,</ref> we demonstrate that the estimated textures with GAN training contain more plausible texture details and better lighting effect than those without GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Estimating Shaded Textures Preserves More Details</head><p>Notably PIFu and ARCH all choose the albedo color space to predict. This requires the neural network to implicitly learn to compensate light/shading from the given shaded input images. However our current data scale seems insufficient to capture such a complicated space and might produce erroneous reconstructed textures. We believe our best strategy is to "reconstruct the texture as similar/compatible as the input image". We conduct ablation studies on learning different color spaces and the results are shown in <ref type="figure" target="#fig_0">Fig. 15</ref>. It can be observed that estimating back-side shaded textures, in comparison with albedos, preserves more details and overall generates similar and consistent color space to the input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Inpainting Is a Needed Step for Avatar Reconstruction</head><p>Compared with PIFu and ARCH, we propose to utilize the image space characteristics to better refine the reconstructed geometry. Our insight is that such features are naturally encoded in the image and there are already lots of powerful generative models in the literature which can solve similar tasks. However, one issue when trying to estimate the surface normals and textures lies in the missing surfaces</p><formula xml:id="formula_13">With GAN Without GAN Input With GAN Without GAN Input</formula><p>With GAN Without GAN Input <ref type="figure" target="#fig_0">Figure 14</ref>. Texture Map Estimation With and Without GAN. The hallucinated back-side texture maps with GAN contain more surface details (e.g., clothing patterns/wrinkles, hairs) and more realistic (directional) lighting effect than those without GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shaded Texture</head><p>Albedo Input Shaded Texture Albedo Input Shaded Texture Albedo Input <ref type="figure" target="#fig_0">Figure 15</ref>. Texture Map Estimation With and Without Shading (Albedo). We study whether the shaded texture or the albedo space are easier to learn. Results show when trying to predict albedos, we lose lots of clothing details and textured patterns. cannot be ray traced from either the front or the back side, e.g., the occluded clothes by the arms in <ref type="figure" target="#fig_0">Fig. 16</ref>. As a result, we believe such missing surfaces could be formulated and solved as an inpainting task. As shown in the figure, we are able to inpaint those missing surfaces (marked as gray regions) using the context. We could also observe that the final reconstructed avatar in the canonical space looks more complete and realistic than the one obtained by ARCH via implicit color field interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6. More Normal Refinement Results</head><p>We show more qualitative results on the testing sets (e.g., DeepHuman, AXYZ, RenderPeople and Unsplash) for people in different camera views, poses and clothes in <ref type="figure" target="#fig_0">Fig. 18</ref>. It can be observe that our normal refinement results achieve high fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7. User Study on Reconstruction Photorealism Shows Superior Quality of Our Method</head><p>We set up a user study to further evaluate the photorealism of our method against other state of the art. In our study, we randomly pick 30 examples from our testing set (i.e., RenderPeople, AXYZ, Unsplash) and run the compar-ison methods <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b62">63</ref>] to obtain the results. For each example, we showed the input image and side-by-side rendered reconstruction results from two approaches in both front and back views to the participants. We numerate all pairs of approaches following a "similarity judgment" design <ref type="bibr" target="#b45">[46]</ref>. For each pair, the participants were asked to choose the more realistic result ("Which reconstructed avatar looks more real given the input image?"). They could choose either results or indicate that they were equally real. To avoid biases and learning effects, we randomized the order of pairs as well as the position of the results while ensuring that for each participant the same number of techniques were shown on the left and the right sides. To evaluate the results, we attribute the choice of one technique with +1 and the other with ?1. Averaging over all results in a quality score for each pair of techniques. Using a t-test, we determine the probability of the drawn sample to come from a zero-mean distribution-zero-mean would indicate both techniques to be of equal quality.</p><p>We recruited 22 participants from universities and research institutes. Most participants are with medium to high experiences for computer vision and graphics. The results of the user study are summarized in <ref type="figure" target="#fig_0">Fig. 17</ref>. We conduct</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARCH Without Inpainting</head><p>Ours With Inpainting ARCH Without Inpainting Ours With Inpainting <ref type="figure" target="#fig_0">Figure 16</ref>. Inpainting Is a Needed Step for Avatar Reconstruction. Canonical space reconstruction inevitably contains unseen surfaces from the input image. After applying a ray tracing and moulding, we identify those gray regions can be seen in neither front side nor back side as occluded region and fill in those missing details using an off-the-shelf image inpainting algorithm. Compared with ARCH which interpolates the per-point normal/texture using deep implicit functions, our moulding-inpainting approach obtains higher fidelity and completeness. two sessions with one studying the normal reconstruction quality and the other one studying the texture reconstruction quality. We compute the p-value to indicates statistical significant difference between the methods according to a ttest (all significant results achieved at least p value &lt; .001).</p><p>All three pair-wise comparisons showed significant results:</p><p>For normal reconstruction, Ours vs. PIFuHD (mean = 0.39697, std = 0.34625, t(21) = 4.02, p &lt; .001), Ours vs. ARCH (mean = 0.83636, std = 0.15535, t(21) = 25.25, p &lt; .001), and PIFuHD vs. ARCH (mean = 0.32121, std = 0.27826, t(21) = 5.41, p &lt; .001), with ours being significantly better than other state of the art and PIFuHD being significantly better than ARCH.</p><p>For texture reconstruction, Ours vs. PIFu (mean = 0.49091, std = 0.21513, t(21) = 10.70, p &lt; .001), Ours vs. ARCH (mean = 0.74545, std = 0.21615, t(21) = 16.18, p &lt; .001), and ARCH vs. PIFu (mean = 0.22424, std = 0.21134, t(21) = 4.98, p &lt; .001), with ours being significantly better than other state of the art and ARCH being significantly better than PIFu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8. Applications</head><p>When multi-view inputs are available (e.g., monocular videos), our method naturally supports canonical space normal and texture fusion to recover a photorealistic and animatable avatar thanks to the shared canonical space among different poses. For mesh vertices that are co-visible under multiple viewpoints we apply a simple yet effective normal-DeepHuman AXYZ RenderPeople Unsplash <ref type="figure" target="#fig_0">Figure 18</ref>. More normal refinement results from our approach on DeepHuman, AXYZ, RenderPeople and Unsplash datasets, covering people with different camera views, poses and clothes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Frames</head><p>Animation Fused 3D Avatar <ref type="figure" target="#fig_0">Figure 19</ref>. An application of social telepresence. Given an input video, our method can generate a fused 3D human avatar and further animate it with pre-defined Mixamo motions [50]. based linear blending scheme. For each vertex, the normal/texture fusion weights w.r.t. one visible view is determined by the angle between the (unrefined)normal of that vertex and the camera direction. This is similar to Eq. <ref type="formula" target="#formula_5">(6)</ref> and <ref type="formula" target="#formula_6">(7)</ref> of the main paper. Here we further extend them to multiple views. Namely, one image that is facing towards the surface is weighted higher than another image of a large viewing angle. As show in <ref type="figure" target="#fig_0">Fig. 19</ref>, we fuse the normal and texture from multiple frames, and further generate an animation sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.9. Failure cases</head><p>As shown in <ref type="figure" target="#fig_8">Fig. 20</ref>, there are some typical failure cases due to strong directional lighting and challenging poses. To tackle these issues we plan to add more lighting augmentation for the back-side texture estimation Pix2Pix module and also increase the size of our training scan set. For example, compared with the widely used image classification and detection datasets like ImageNet <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b60">61]</ref> and COCO <ref type="bibr" target="#b41">[42]</ref>, our training dataset is relatively small consisting of only hundreds of 3D scans. Building a large-scale and highquality clothed human mesh dataset with sufficient clothes types and human pose/shape variations is critical for pushing research works in image-based photorealistic avatar reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Given an image of a subject in arbitrary pose (left), our method could generate photorealistic avatars in both the posed input space (middle) as well as auto-rigged canonical space (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Overview of the mesh refinement steps. Our approach refines the initially estimated joint-space meshes fromFig. 2 using estimated normals and textures. posed space query point p b and its corresponding canonical space point p a are inside the clothed human meshes or not. Though p a , p b are a pair of mapped points their ground-truth occupancy values are not the same in all cases. For example, a point outside and close to the hand of a parametric body could has? b &gt; 0 and? a &lt; 0 if the original mesh in posed space has self-contact (e.g., hands in the pocket).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>We intentionally hide the method names for you to have a fair comparison on your own (please zoom in). The answers are 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Ablation studies on normal refinement: Object-space Regression (OS Reg.), Image-space Input (IS Input) and Image-space Regression (IS Reg.). Our method IS Reg. leads to rich reconstruction details (e.g., clothes wrinkles) in all views.handcrafted RBF features that only model the pose prior of parametric body mesh skeletons, ignoring the mesh shape. In comparison, our point-based features are informed of both pose and shape priors of the underlying parametric body model w.r.t. a clothed human mesh, and thus improve the surface reconstruction quality. We further implement the learned volumetric spatial feature encodings used in Geo-PIFu and PaMIR as an alternative encoder and inject into our framework for direct comparisons. The results are shown in Tab. 3 andFig. 7. While both types of end-to-end spatial features outperform the hand crafted RBF features, our point-based feature extraction method does not suffer from computation overhead and mesh quantization errors of the voxel-based approach.Normal Refinement. While single-image based direct inference of human meshes with rich surface details at both</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>An application of digital human capture from photos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 13 .</head><label>13</label><figDesc>Normal refinement can enhance photorealistic rendering at different viewpoints. These images use the same mesh textures for rendering in order to demonstrate the effect of refined normals. The input image is the same as the bottom row inFig. 12.semantic-aware geometry features and pixel-aligned appearance features. The geometry features encode shape and pose priors of the underlying parametric body mesh, while the appearance features provide image evidence for finescale clothing wrinkles and surface details reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>11Figure 17 .</head><label>17</label><figDesc>Ours v.s. PIFuHD Ours v.s. ARCH PIFuHD v.s. ARCH Ours v.s. PIFu Ours v.s. ARCH ARCH v.s. PIFuTexture Reconstruction Fidelity Results of our user study. Values &gt; 0 show a preference of the first method over the second, i.e., Ours over PIFuHD, Ours over ARCH and PIFuHD over ARCH. Note PIFuHD doesn't reconstruct texture so we use PIFu instead for the texture reconstruction study. Error bars show standard error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 20 .</head><label>20</label><figDesc>Failure cases. Reconstruction with strong directional lights and rare poses could be further improved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>Quantitative results and comparisons of normal, P2S and Chamfer errors between posed reconstruction and ground truth on RenderPeople and BUFF datasets. Best scores are in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The configurations of each layer are SA(2048, 0.1, 16, 3, [16,16,32]), SA(512, 0.2, 32, 32, [32,32,64]), SA(128, 0.4, 64, 64,</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our results (green boxes) have fewer reconstruction artifacts (e.g., incorrect normal directions, mesh distortion) than ARCH (red boxes).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to thank Minh Vo and Nikolaos Sarafianos for the discussions and synthetic data creation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>In this section, we provide the implementation details of our proposed method.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to reconstruct people in clothing from a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><forename type="middle">Lal</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video based reconstruction of 3d people models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tex2shape: Detailed full human body geometry from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scape: Shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detailed human shape and pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><forename type="middle">W</forename><surname>Haussecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coloma</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Combining implicit function learning and parametric models for 3d human reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bharat Lal Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-garment net: Learning to dress 3d people from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garvita</forename><surname>Bharat Lal Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-garment net: Learning to dress 3d people from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garvita</forename><surname>Bharat Lal Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Implicit functions in feature space for 3d shape reconstruction and completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Chibane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-quality streamable free-viewpoint video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Gillett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Evseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R L</forename><surname>Axyz Design</surname></persName>
		</author>
		<ptr target="https://secure.axyz-design.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1362" to="1376" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moulding humans: Non-parametric 3d human shape estimation from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-S?bastien</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion capture using joint skeleton tracking and surface estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Edilson De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Volumetric performance capture from minimal camera viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The relightables: Volumetric performance capture of humans with realistic relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dourgarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepcap: Monocular human performance capture using weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geo-pifu: Geometry and pixel aligned implicit functions for single-view human reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image completion using planar structure guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep volumetric video from very sparse multi-view performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Legendre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ARCH: Animatable reconstruction of clothed humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<title level="m">Chris Manafas, and Georgios Tzimiropoulos. 3d human body reconstruction from a single image via volumetric regression. European Conference of Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bcnet: Learning body and cloth shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3d human pose fitting towards in-the-wild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03686</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Screened poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepwrinkles: Accurate and realistic clothing modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zorah</forename><surname>Laehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Monocular real-time volumetric performance capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep appearance models for face rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Differentiable monte carlo ray tracing through edge sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvey</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIG-GRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="163" to="169" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Comparison of four subjective methods for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rafal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radoslaw</forename><surname>Tomaszewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mantiuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">3D Video and Its Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Takai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image-based visual hulls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Gortler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The uncanny valley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">F</forename><surname>Macdorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norri</forename><surname>Kageki</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>from the field</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics &amp; Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="98" to="100" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Siclope: Silhouette-based clothed people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Holoportation: Virtual 3d teleportation in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Degtyarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsong</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium on User Interface Software and Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">UNOC: Understanding occlusion for embodied presence in virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Parger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengcheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Twigg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Steinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Clothcap: seamless 4d clothing capture and retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonny</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<ptr target="http://renderpeople.com/.6" />
		<title level="m">RENDERPEOPLE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">PIFu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">PIFuHD: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Facsimile: Fast and accurate scans from an image in less than a second</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Mavroidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Simultaneous super-resolution and 3d video using graph-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsuyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Complete multi-view reconstruction of dynamic scenes from probabilistic fusion of narrow and wide baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsuyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">BodyNet: Volumetric inference of 3D human body shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gul</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Articulated mesh animation from multi-view silhouettes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovan</forename><surname>Popovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dynamic shape capture using multi-view photometric stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Peers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovan</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Full body performance capture under uncontrolled and varying illumination: A shading-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation via pose grammar and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">DenseRaC: Joint 3D pose and shape estimation by dense render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Analyzing clothing layer deformation statistics of 3d human motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-S?bastien</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>H?troy-Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Wuhrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Estimation of human body shape in motion with wide clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-S?bastien</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>H?troy-Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Wuhrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Contextual residual aggregation for ultra high-resolution image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daesik</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Weakly supervised 3d human pose and shape reconstruction with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Detailed, accurate, human shape estimation from clothed 3D scan sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">PaMIR: Parametric model-conditioned implicit representation for image-based human reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deephuman: 3d human reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
