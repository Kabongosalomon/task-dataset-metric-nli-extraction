<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DP-SSL: Towards Robust Semi-supervised Learning with A Few Labeled Samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiandong</forename><surname>Ding</surname></persName>
							<email>jdding@fudan.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
							<email>l_zhang19@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
							<email>sgzhou@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Shanghai Key Lab of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DP-SSL: Towards Robust Semi-supervised Learning with A Few Labeled Samples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The scarcity of labeled data is a critical obstacle to deep learning. Semi-supervised learning (SSL) provides a promising way to leverage unlabeled data by pseudo labels. However, when the size of labeled data is very small (say a few labeled samples per class), SSL performs poorly and unstably, possibly due to the low quality of learned pseudo labels. In this paper, we propose a new SSL method called DP-SSL that adopts an innovative data programming (DP) scheme to generate probabilistic labels for unlabeled data. Different from existing DP methods that rely on human experts to provide initial labeling functions (LFs), we develop a multiple-choice learning (MCL) based approach to automatically generate LFs from scratch in SSL style. With the noisy labels produced by the LFs, we design a label model to resolve the conflict and overlap among the noisy labels, and finally infer probabilistic labels for unlabeled samples. Extensive experiments on four standard SSL benchmarks show that DP-SSL can provide reliable labels for unlabeled data and achieve better classification performance on test sets than existing SSL methods, especially when only a small number of labeled samples are available. Concretely, for CIFAR-10 with only 40 labeled samples, DP-SSL achieves 93.82% annotation accuracy on unlabeled data and 93.46% classification accuracy on test data, which are higher than the SOTA results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The de-facto approaches to deep learning achieve phenomenal success with the release of huge labeled datasets. However, large manually-labeled datasets are time-consuming and expensive to acquire, especially when expert labelers are required. Nowadays, many techniques are proposed to alleviate the burden of manual labeling and help to train models from scratch, such as active learning <ref type="bibr" target="#b0">[1]</ref>, crowd-labeling <ref type="bibr" target="#b1">[2]</ref>, distant supervision <ref type="bibr" target="#b2">[3]</ref>, semi <ref type="bibr" target="#b3">[4]</ref>/weak <ref type="bibr" target="#b4">[5]</ref>/self-supervision <ref type="bibr" target="#b5">[6]</ref>. Among them, semi-supervised learning (SSL) is one of the most popular techniques to cope with the scarcity of labeled data. Two major strategies of SSL are pseudo labels <ref type="bibr" target="#b6">[7]</ref> and consistency regularization <ref type="bibr" target="#b7">[8]</ref>. Pseudo labels (also called self-training <ref type="bibr" target="#b8">[9]</ref>) utilize a model's predictions as the labels to train the model again, while consistency of regularization forces a model to make the same prediction under different transformations. However, when the size of labeled data is small, SSL performance degrades drastically in both accuracy and robustness. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the change of prediction error rate with the number of labeled samples of CIFAR-10. When the number of labeled samples reduces from 250 to 40, error rates of major existing SSL methods increase from 4.74% (USADTM) to <ref type="bibr" target="#b35">36</ref>.49% (MixMatch). One possible reason of performance deterioration is the quality degradation of learnt pseudo labels when labeled data size is small. Therefore, in this paper we address this problem by developing sophisticated labeling techniques for unlabeled data to boost SSL even when the number of labeled samples is very small (e.g. a few labeled samples per class). <ref type="bibr" target="#b39">40</ref> 250 Recently, data programming (DP) was proposed as a new paradigm of weak supervision <ref type="bibr" target="#b9">[10]</ref>. In DP, human experts are required to transform the decision-making process into a series of small functions (called labeling functions, abbreviated as LFs), thus data can be labeled programmatically.</p><p>Besides, a label model is applied to determining the correct labels based on consensus from the noisy and conflicting labels assigned by the LFs. Such a paradigm achieves considerable success in NLP tasks <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. In addition, DP has also been applied to computer vision tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. However, current DP methods require human experts to provide initial LFs, which is time-consuming and expensive, and it is not easy to guarantee the quality of LFs. Furthermore, LFs specifically defined for one task usually cannot be re-used for other tasks.</p><p>In this paper, we propose a new SSL method called DP-SSL that is effective and robust even when the number of labeled samples is very small. In DP-SSL, an innovative data programming (DP) scheme is developed to generate probabilistic labels for unlabeled data. Different from existing DP methods, we develop a multiple-choice learning (MCL) based approach to automatically generate LFs from scratch in SSL style. To remedy the over-confidence problem with existing MCL methods, we assign an additional option as abstention for each LF. After that, we design a label model to resolve the conflict and overlap among the noisy labels generated by LFs, and infer a probabilistic label for each unlabeled sample. Finally, the probabilistic labels are used to train the end model for classifying unlabeled data. Our experiments validate the effectiveness and advantage of DP-SSL. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, DP-SSL performs best, and only 1.76% increase of error rate when the number of labeled samples decreases from 250 to 40 in CIFAR-10.</p><p>Note that the pseudo labels used in existing SSL methods are quite different from the probabilistic labels in DP-SSL, which may explain the advantage of DP-SSL over existing SSL methods. On the one hand, pseudo labels are "hard" labels that indicate an unlabeled sample belonging to a certain class or not, while probabilistic labels are "soft" labels that indicate the class distributions of unlabeled samples. Obviously, the latter should be more flexible and robust. On the other hand, pseudo labels are actually generated by a single model for all unlabeled samples, while probabilistic labels are generated from a number of diverse and specialized LFs (due to the MCL mechanism), which makes the latter more powerful in generalization as a whole.</p><p>In summary, the contributions of this paper are as follows: 1) We propose a new SSL method DP-SSL that employs an innovative data programming method to generate probabilistic labels for unlabeled data, which makes DP-SSL effective and robust even when there are only a few labeled samples per class. 2) We develop a multiple choice learning based approach to automatically generate diverse and specialized LFs from scratch for unlabeled data in SSL manner. 3) We design a label model with a novel potential and an unsupervised quality guidance regularizer to infer probabilistic labels from the noisy labels generated by LFs. 4) We conduct extensive experiments on four standard benchmarks, which show that DP-SSL outperforms the state-of-the-art methods, especially when only a small number of labeled samples are available, DP-SSL is still effective and robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multiple Choice Learning</head><p>Multiple choice learning (MCL) <ref type="bibr" target="#b21">[22]</ref> was proposed to overcome the low diversity problem of models trained independently in ensemble learning. For example, stochastic multiple choice learning <ref type="bibr" target="#b22">[23]</ref> is for training diverse deep ensemble models. However, a crucial problem with MCL is that each model tends to be overconfident, which results in poor final prediction. To solve this problem, <ref type="bibr" target="#b23">[24]</ref> forces the predictions of non-specialized models to meet a uniform distribution, so that the final decision is summed over diverse outputs. <ref type="bibr" target="#b24">[25]</ref> proposes an additional network to estimate the weight of each specialist's output. In this paper, we develop an improved MCL based scheme to automatically generate diverse and specialized labeling functions (LFs) from scratch in an SSL manner. These LFs are used to generate preliminary (usually noisy) labels for unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-supervised Learning</head><p>Semi-supervised learning (SSL) has been extensively studied in image classification <ref type="bibr" target="#b25">[26]</ref>, object detection <ref type="bibr" target="#b26">[27]</ref>, and semantic segmentation <ref type="bibr" target="#b27">[28]</ref>. Two popular SSL strategies for image classification are pseudo labels <ref type="bibr" target="#b6">[7]</ref> and consistency regularization <ref type="bibr" target="#b7">[8]</ref>. Pseudo-label methods generate artificial labels for some unlabeled images and then train the model with these artificial labels, while consistency regularization tries to obtain an artificial distribution/label and applied it as a supervision signal with other augmentations/views. These two strategies have been adopted by a number of recent SSL works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref>. For example, FixMatch <ref type="bibr" target="#b3">[4]</ref> proposes a simple combination of pseudo labels and consistency regularization. <ref type="bibr" target="#b35">[36]</ref> employs unsupervised learning and clustering to determine the pseudo labels. In this paper, we propose a new SSL method that is effective and robust even when the size of labeled data is very small. Our method employs an innovative data programming alike method to automatically generate probabilistic labels for unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Programming</head><p>Data programming <ref type="bibr" target="#b9">[10]</ref> is a weak supervision paradigm proposed to infer correct labels based on the consensus among noisy labels from labeling functions (LFs), which are modules embedded with decision-making processes for generating labels programmatically. Following the DP paradigm, Snorkel <ref type="bibr" target="#b11">[12]</ref> and Snuba <ref type="bibr" target="#b39">[40]</ref> were proposed as rapid training data creation systems. Their LFs are built with various weak supervision sources, like pattern regexes, heuristics, and external knowledge base etc. Recently, more works are reported in the literature <ref type="bibr">[11, 13-16, 21, 41-48]</ref>. Among them, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref> focus on the adaption of label model in DP. For example, <ref type="bibr" target="#b20">[21]</ref> aims to reduce the computational cost and proposes a closed-formed solution for training the label model. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref> apply DP to computer vision. Concretely, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> heavily rely on the pretrained models. <ref type="bibr" target="#b40">[41]</ref> combines crowdsourcing, data augmentation, and DP to create weak labels for image classification. <ref type="bibr" target="#b14">[15]</ref> presents a novel view for resolving infrequent data in scene graph prediction training datasets via image-agnostic features in LFs. However, all these methods cannot be directly applied to training models from scratch with a small number of labeled samples. Thus, in this paper we extend DP by exploring both MCL and SSL to generate arbitrary labeling functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>For a C-class SSL classification problem, assume that all training data X are divided into labeled data X l and unlabeled X u , and test data are denoted as X t . Following the notation in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>, {x l , x w l } ? X l are the paired labeled samples with labels y l ? {1, . . . , C}, and {x u , x w u , x s u } ? X u are the triple unlabeled samples. Here, x l and x u represent the raw images without any transformations. x w l , x w u , and x s u are the images based on the weak and strong augmentation strategies, respectively. In this paper, weak augmentation uses a standard flip-and-shift strategy, and strong augmentation uses the RandAugment <ref type="bibr" target="#b48">[49]</ref> strategy with Cutout [50] augmentation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>FixMatch. In the FixMatch <ref type="bibr" target="#b3">[4]</ref> algorithm, apart from basic cross-entropy on labeled samples, consistency regularization with pseudo labels on unlabeled samples is represented as: where H is the cross-entropy, ? is the pre-defined threshold, p represents the output probability of the model, and? w u := arg max(p(y|x w u )) is the pseudo label from the weakly augmented predictions. Multiple Choice Learning. Stochastic Multiple Choice Learning (sMCL) <ref type="bibr" target="#b22">[23]</ref> aims to specialize each individual model on a subset of data, by minimizing the loss as follows:</p><formula xml:id="formula_0">L F M (x w u , x s u ) = 1(max(p(y|x w u )) ? ) ? H(? w u , p(y|x s u )),<label>(1)</label></formula><formula xml:id="formula_1">L sM CL (x l , y l ) = min k?{1,??? ,K} H(y l , p k (y|x l )),<label>(2)</label></formula><p>where p k is the output probability of the k-th model. For a training sample (x l , y l ), sMCL feeds the data to all K models but only chooses the most accurate model to do back-propagation. Consequently, each model performs better on some classes than the other models, i.e., each model becomes a specialist on some particular classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework</head><p>Fig <ref type="figure" target="#fig_1">. 2</ref> shows the framework of our DP-SSL method, which works in three major steps as follows:</p><p>? Step 1. We employ an MCL based approach to automatically generate K LFs from scratch in an SSL style. Here, each LF is trained on a subset of C classes in the training set based on MCL. As shown in <ref type="figure" target="#fig_1">Fig 2,</ref> the 2nd LF is trained with samples of classes "horse" and "dog", and abstains from predicting when facing monkey images.</p><p>? Step 2. A graphical model is developed as the label model to aggregate the noisy labels and produce probabilistic labels for unlabeled training data. The label model is learned in an SSL manner with an additional regularizer.</p><p>? Step 3. The end model is trained with both provided labels and probabilistic labels generated from Step 2. Finally, we verify the performance of the end model on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Labeling Function</head><p>In Step 1 of our method, LFs are exploited to generate noisy labels for each unlabeled image. In previous DP works for computer vision, LFs are built via external image-agnostic knowledge <ref type="bibr" target="#b14">[15]</ref> or pretrained models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>. However, it is difficult to explicitly describe the rules of image classification. Instead, here we innovatively explore MCL and SSL for automatic LF generation.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we share the same backbone (Wide ResNet <ref type="bibr" target="#b50">[51]</ref> in this paper) to extract features of images for multiple prediction heads (called LFs in this paper). To promote the diversity of LFs, we transform the features and feed each LF with different transformed features as follows:</p><formula xml:id="formula_2">f k = HW j=1 e ?? k dis(f [j],c k ) K k =1 e ?? k dis(f [j],c k ) (f [j] ? c k ).<label>(3)</label></formula><p>In this paper, K is the number of LFs, f ? R HW ?D denotes the feature map of input image x before global average pooling of backbone, f [j] ? R D is the feature vector at the spatial position j of f . c k ? R D is the learnable clustering center of the k-th LF, ? k is the learnable variable of the k-th cluster, dis(A, B) represents the distance between A and B. Thus, f k corresponds to the feature fed to the k-th LF, and describes the k-th aggregated pattern of f among the K centers, it can also be considered as a learnable weighted spatial pooling for feature f . Then, supposing F k is the classifier in the k-th LF, which would output the probability F k (f k ) as prediction. For clarity, in the following we denote p k (y|x) := F k (f k ) of the k-th LF with the input image x.</p><p>As depicted in <ref type="bibr" target="#b22">[23]</ref>, the classifiers lack diversity of prediction even trained with different protocols. Therefore, we adopt MCL to assign a subset of labeled data for each classifier automatically to improve diversity. However, it is intuitive to observe that in Eq. (2) each category can only be assigned to one LF, and no consensus can be exploited. Therefore, we increase the proportion of selected models in Eq.</p><p>(2) to do back-propagation, which is formulated as</p><formula xml:id="formula_3">L M CL l (x w l , y l ) = min M?{H(y l ,p k (y|x w l )} K k=1 |M|=??K 1 ? ? K ??K k =1 M k ,<label>(4)</label></formula><p>where M k indicates the k -th element in the set M, and ? ? [1/K, 1] is a designed parameter to represent the ratio of specialist LFs. When ? ? K is equal to 1, Eq. (4) becomes the traditional MCL in Eq. <ref type="bibr" target="#b1">(2)</ref>. In contrast, if ? ? K is equal to K, it deteriorates to the basic ensemble learning, where all K classifiers are trained with the same data.</p><p>Based on MCL, each LF is a specialist for some classes, so it can get high accuracy for samples in these classes. While for samples from other classes not specialized by the LF, it fails to predict due to over-confidence. Thus, we take only the probabilities of specialized categories as predictions, and allow each LF to abstain from some samples in the dataset. Formally, we denote '0' as the abstention label, and the specialized category set of the k-th LF as</p><formula xml:id="formula_4">? k = {? 1 k , . . . , ? |? k | k }.</formula><p>Then, the output label of the k-th LF? k satisfies? k ? {0} ? ? k , e.g., the output of the 1st LF in <ref type="figure" target="#fig_1">Fig. 2</ref> is among "monkey", "deer" and "abstention" because its specialized category set ? 1 = {monkey, deer}. Then, we denote the probability over the specialized category set ? k and "abstention" option asp k (y|x), wherep k (y|x) ? R |? k |+1 . The objective function over labeled samples with abstention option is</p><formula xml:id="formula_5">L l (x w l , y l ) = K k=1 1(y l ? ? k )H(y l ,p k (y|x w l )) + 1(y l / ? ? k )H(0,p k (y|x w l )) ,<label>(5)</label></formula><p>Then, for the unlabeled training data, we follow the settings in FixMatch <ref type="bibr" target="#b3">[4]</ref>, where unlabeled data are supervised by the pseudo labels? w,k u of weak augmentation data x w u . Thus,</p><formula xml:id="formula_6">L u (x w u , x s u ) = K k=1 1(max(p k (y|x w u )) ? ) 1(? w,k u ? ? k )H(? w,k u ,p k (y|x s u ))+ 1(? w,k u / ? ? k )H(0,p k (y|x w u )) ,<label>(6)</label></formula><p>where? w,k u := arg max(p k (y|x w u )). Specifically, we only keep samples whose largest probability (including the abstention option) is above the predefined threshold (0.95 in our paper), and train the model on the kept data with pseudo label? w,k u . Accordingly, the training in this step is to minimize the objective function as follows:</p><formula xml:id="formula_7">L(x w l , y l , x w u , x s u ) = ? M CL l L M CL l (x w l , y l ) + ? l L l (x w l , y l ) + ? u L u (x w u , x s u ),<label>(7)</label></formula><p>where ? M CL l , ? l and ? u are hyper-parameters. In our implementation, we first set ? M CL l = 1 and ? l = ? u = 0, then adjust ? M CL l to 0 and ? l = ? u = 1 after the convergence of L M CL l . Generally, in Step 1, MCL is expected to generate specialized class sets ? for LFs, with which samples are more easily discriminated by SSL classifiers even there are a few labeled samples. Besides, the abstention option is for addressing the over-confidence problem of samples from non-specialized sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Label Model</head><p>In Step 2 of our method, we utilize a graphical model to specify a single prediction by integrating noisy labels provided by K LFs. For simplification, we assume that the K LFs are independent (as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>). Then, suppose that? = (? 1 , ? ? ? ,? K ) ? R K is the vectorized form of the predictions from K LFs, the joint distribution of the label model can be described as:</p><formula xml:id="formula_8">P (y,?) = 1 Z K k=1 ?(y,? k )<label>(8)</label></formula><p>where Z is the normalizer of the joint distribution, ? is the potential that couples the target y and noisy label? k . In this paper, we extend the dimension of parameters ? in label model to K ? C to support multi-class classification. Set e ky := exp(? ky ), which is the exponent of parameters ? ky . Now we are to construct the potential function ?. Due to the specialized LFs, the potential ? should benefit the final prediction when a noisy label agrees with the target. That is, we should have ?(y,? k ) &gt; 1.</p><p>Thus, we set ? as 1 + e ky for this case. On the contrary, the potential ? should negatively impact the final prediction when a noisy label conflicts with the target label in the specialized category set, i.e., we should have ?(y,? k ) &lt; 1. Therefore, for this case we set ? to 1/(1 + e ky ). For the other cases, we follow the design in <ref type="bibr" target="#b51">[52]</ref>. In summary, the potential ? is defined as follows:</p><formula xml:id="formula_9">?(y,? k ) = ? ? ? ? ? ? ? ? ? 1 + e ky , if y ? ? k ,? k ? ? k ,? k = y 1/(1 + e ky ), if y ? ? k ,? k ? ? k ,? k = y e ky , if y / ? ? k ,? k ? ? k ,? k = y 1. otherwise<label>(9)</label></formula><p>With the potential above, the normalizer Z of the joint distribution in Eq. <ref type="formula" target="#formula_8">(8)</ref> can be obtained by summarizing over y and? k :</p><formula xml:id="formula_10">Z = y?Y K k=1 ? k ?{0}?? k ?(y,? k ) = y?Y K k=1 1(y ? ? k )(2 + e ky + |? k | ? 1 1 + e ky ) + 1(y / ? ? k )(1 + |? k |e ky ) .<label>(10)</label></formula><p>Then, the objective function of the label model can be expressed in an SSL manner as follows:</p><p>L(? l , y l ,? u ) = </p><p>where the first part is the cross-entropy loss, the second is the negative log marginal likelihood on the observed noisy labels? u , and the third is a regularizer. In our method, the regularizer is utilized to guide the label model with statistical information (the accuracy of each LF). However, the accuracy of each LF on noisy labels is unavailable, while the accuracy on labeled training is almost 100% due to over-fitting. Thus, we have to estimate the accuracy of each LF with the observable noisy labels?, which will be presented in Sec. 3.5. After training, the label model produces probabilistic labels ? by computing the joint distribution in Eq. (8) with the noisy labels?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Accuracy Estimation</head><p>Now, we formally describe our method for estimating the accuracy of LFs. We transform the multiclass problem into C one-versus-all tasks. For the i-th (i ? [1, ? ? ? , C]) one-versus-all task, we denote the unobserved ground-truth labels as z i ? {?1} (z i = +1 means y = i, and z i = ?1 represents y = i), noisy labels of the k-th LF as? k i ? {?1, 0},</p><formula xml:id="formula_12">z k i = ? ? ? 1 if? k = i, 0 if? k = 0, ?1 otherwise.<label>(12)</label></formula><p>Then, we can write E[? k i z i ] as</p><formula xml:id="formula_13">E[? k i z i ] = P (? k i z i = 1) ? P (? k i z i = ?1) = P (? k i z i = 1) ? (1 ? P (? k i z i = 1) ? P (? k i z i = 0)) = 2P (? k i = z i ) + P (? k i = 0) ? 1.<label>(13)</label></formula><p>Assume that? j i ? ?? k i |z i for distinct j and k, then</p><formula xml:id="formula_14">E[? j i? k i ] = E[? j i z 2 i? k i ] = E[? j i z i ]E[? k i z i ]<label>(14)</label></formula><p>with the fact that z 2 i = 1. In Eq. <ref type="formula" target="#formula_0">(14)</ref> </p><formula xml:id="formula_15">|?[? j i z i ]| = |?[? j i? k i ] ??[? j i? l i ]/?[? k i? l i ]|, |?[? k i z i ]| = |?[? j i? k i ] ??[? k i? l i ]/?[? j i? l i ]|, |?[? l i z i ]| = |?[? j i? l i ] ??[? k i? l i ]/?[? j i? k i ]|.<label>(15)</label></formula><p>We can obtain the estimated accuracy of each LF by resolving the sign of E[? k i z i ] <ref type="bibr" target="#b20">[21]</ref>. Let? k i := P (? k i = z i |? k i = 0) be the estimated accuracy of the k-th LF on the i-th category. Therefore, the regularizer of R(?,? u ) can be formulated as</p><formula xml:id="formula_16">R(?,? u ) = C i=1 K k? k i log P ? (? k i = z i |? k i = 0) + (1 ?? k i ) log(1 ? P ? (? k i = z i |? k i = 0)) (16)</formula><p>where P ? (? k i = z i |? k i = 0) can be computed in closed form by marginalizing over all the other variables in the model in Eq. <ref type="bibr" target="#b7">(8)</ref>. Details of P ? can be referred to Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">End Model</head><p>In Step 3, probabilistic labels are used to train an end model under any network architecture. We utilize noise-aware empirical risk expectation as the objective function to take annotation errors into account. Accordingly, the final objective function is as follows:</p><p>L(x l , y l , x u , ?) =  <ref type="bibr" target="#b16">(17)</ref> where p(y|x l ) and p(y|x u ) are the predicted distributions of x l and x u , ? is the distribution produced by the label model in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>In the training phase, we follow the settings of previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, augment data in weak (a standard flip-and-shift strategy) and strong forms (RandAugment <ref type="bibr" target="#b48">[49]</ref> followed by Cutout <ref type="bibr" target="#b49">[50]</ref> operation), and utilize a Wide ResNet as the end model for a fair comparison. In our framework, the batch size for labeled data and unlabeled data is set to 64 and 448, respectively. Besides, we use the same hyperparameters (K = 50, ? = 0.2, = 0.95) for all datasets. We compare DP-SSL with major existing methods on CIFAR-10 <ref type="bibr" target="#b52">[53]</ref>, CIFAR-100 <ref type="bibr" target="#b52">[53]</ref>, SVHN <ref type="bibr" target="#b53">[54]</ref> and STL-10 <ref type="bibr" target="#b54">[55]</ref>. We also analyze the effect of annotation and conduct ablation study in Sec    <ref type="bibr" target="#b28">[29]</ref>, Pseudo-Labeling <ref type="bibr" target="#b6">[7]</ref>, Mean Teacher <ref type="bibr" target="#b31">[32]</ref>, MixMatch <ref type="bibr" target="#b30">[31]</ref>, UDA <ref type="bibr" target="#b33">[34]</ref>, ReMixMatch <ref type="bibr" target="#b34">[35]</ref>, FixMatch <ref type="bibr" target="#b3">[4]</ref> and USADTM <ref type="bibr" target="#b35">[36]</ref>) and our DP-SSL method. <ref type="bibr">CIFAR</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Existing SSL Methods</head><p>For a fair comparison, we conduct experiments with the codebase of FixMatch and cite the results on CIFAR-10, CIFAR-100, SVHN and STL-10 from <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>. We utilize the same network architecture (a Wide ResNet-28-2 for CIFAR-10 and SVHN, WRN-28-8 for CIFAR-100, and WRN-37-2 for STL-10) and training protocol of FixMatch, such as optimizer and learning rate schedule. Unlabeled data are generated by the scripts in FixMatch. Results of DP-SSL and existing methods in Tab. 1 and Tab. 2 are presented with the mean and standard deviation (STD) of accuracy on 5 pre-defined folds.</p><p>As shown in Tab. 1, our method achieves the best performance in most cases, especially when there are only 4 labeled samples per class. Specifically, our method achieves a 93.46% accuracy on CIFAR-10 with 4 labeled samples per category, which is 3.3% higher than that of USADTM -the state-of-the-art method. Again on STL-10, our method surpasses USADTM and achieves the best performance when there are 4 and 25 labeled samples per class.</p><p>On CIFAR-100, our method performs the best for 400 labels case and the 2nd for 2500 and 10,000 labels cases. We also notice that DP-SSL has relatively large STDs for 2500 and 10,000 labels cases, which is due to the coarse accuracy estimation. In fact, even if triplet mean is adopted in estimation, the triplet selection in Eq. (15) still impacts accuracy estimation and regularizer a lot, especially when E[? k i z i ] is close to 0 or sign recovery of E[? k i z i ] is wrong. Actually, there are some advanced approaches to unsupervised accuracy estimation <ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref> that can replace the naive triplet mean estimation. Ideally, if we can obtain the exact accuracy of each classb k i :=P (? k i = z i |? k i = 1) and regularize it as</p><formula xml:id="formula_17">R(?,? u ) = C i=1 K kb k i log P ? (? k i = z i |? k i = 1) + (1 ?b k i ) log(1 ? P ? (? k i = z i |? k i = 1)</formula><p>), we will get an end model with (27.92 ? 0.23)% error rate for 2500 labeled samples. Comparing with USADTM, our method does not perform well enough when more labeled data available. For USADTM, apart from the proxy label generator, unsupervised representation learning contributes a lot for its performance. As shown in the ablation study of <ref type="bibr" target="#b35">[36]</ref>, USADTM without unsupervised representation learning achieves around 5.73% and 4.99% error rate for 250 and 4000 labeled samples in CIFAR-10, while our method DP-SSL obtains 4.78% and 4.23% error rate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>Annotation performance. Intuitively, the holistic performance of the end model in our method highly depends on the quality of annotation results. Thus, we present the macro precision/recall/F1 score and coverage of the annotated labels of our method on CIFAR-10, CIFAR-100, and SVHN in Tab. 3. We can see that our method achieves over 99% coverage, which means that it produces probabilistic labels for almost all unlabeled data. Comparing to the results in <ref type="bibr" target="#b35">[36]</ref>, the label model with 40 labeled samples outperforms the proxy label generator, FixMatch and USADTM get 88.51% and 89.48% accuracy, respectively. Furthermore, our method achieves 97.36% accuracy for unlabeled data with the top-500 highest probabilities in each category. Meanwhile, we also present results of Majority Voting and FlyingSquid <ref type="bibr" target="#b20">[21]</ref> in Tab. 3 based on the noisy labels from Step 1 of our method for comparison. Majority Voting gets bad performance because the number of LFs triggered for different categories is not equal. For FlyingSquid, we implement it with C one-versus-all models to support multi-class tasks, and the large C in CIFAR-100 results in the worst performance.</p><p>Barely supervised learning. We conduct experiments to test the performance (accuracy and STD) of our method on CIFAR-10 for some extreme cases (10, 20 and 30 labeled samples) to verify the effectiveness of our method. Here, we select the labeled data through the scripts of FixMatch with 5 different random seeds. As claimed in FixMatch, it reaches between 48.58% and 85.32% test accuracy with a median of 64.28% for 10 labeled samples, while our method obtains accuracy from 61.32% to 83.7%. As for 20 and 30 labeled samples, our method gets (85.29 ? 3.14)% and (89.81 ? 1.59)% accuracy respectively, which have much smaller STDs than that reported in <ref type="bibr" target="#b36">[37]</ref>. In DP-SSL, LFs and the label model are the core components to assign probabilistic labels for training the end model. Here, we check the effects of the following factors in the process of producing probabilistic labels by taking CIFAR-10 as the example. For ease of exposition, only the accuracy of predicted labels is presented in Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCL.</head><p>Feature transformation (FT) described in Eq. <ref type="formula" target="#formula_2">(3)</ref> can be regarded as a weighted spatial pooling for extracted features. It is proposed to boost the diversity of generated LFs. We conduct comparative experiments for three configurations: 1) Exp1: w.o. MCL, 2) Exp2: MCL w.o. FT, 3) Exp3: MCL w. FT. The results are presented in Tab. 4. It is interesting to see that Exp1 is better than Exp2 but worse than Exp3. In fact, Exp1 is a simple ensemble model with a shared backbone, where each LF is trained independently and predicts the labels within C categories. In Exp2, we observe that some classifiers have never been optimized in the training phase and thus have an empty specialized set when only a few labeled samples per class are available. Moreover, the specialized sets of many LFs are duplicate, which incurs a negative impact on the performance. However, MCL with FT addresses the drawbacks and helpes our method obtain versatile LFs.</p><p>Hyperparameters. K and ? are the number of LFs and the ratio of specialists in Eq. (4). In our ablation study, we focus on the variance of performance for different K and ? with 40 labeled samples on CIFAR-10. In <ref type="figure" target="#fig_6">Fig. 3a</ref>, K=50 performs the best when 40 labeled samples are available. On the other hand, performance reaches the best when ?=0.2 in <ref type="figure" target="#fig_6">Fig. 3b</ref>. We present more results of ? and K in Appendix A.5.</p><p>Regularizer. The regularizer is proposed to impose a global guidance and improve the robustness of the label model. As shown in Tab. 4, the regularizer does boost the accuracy, especially when facing less labeled samples. Besides, as mentioned in Sec. 4.3, the high-quality guidance of the regularizer also reduces the label model's performance variance, thus improves its robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we explore the data programming idea to boost SSL when only a small number of labeled samples available by providing more accurate labels for unlabeled data. To this end, we propose a new SSL method DP-SSL that employs an innovative DP mechanism to automatically generate labeling functions. To make the labeling functions diverse and specialized, a multiple choice learning based approach is developed. Furthermore, we design an effective label model by incorporating a novel potential and a regularizer with estimated accuracy. With this model, probabilistic labels are inferred by resolving the conflict and overlap among noisy labels from the labeling functions. Finally, an end model is trained under the supervision of the probabilistic labels. Extensive experiments show that DP-SSL can produce high-quality probabilistic labels, and outperforms the existing methods to achieve a new SOTA, especially when only a small number of labeled samples are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations of This Work</head><p>In this work, we use coarse accuracy estimation as the statistic information to guide the label model for simplicity. As described in Sec. 3.5, we estimate the accuracy P ? (? k i = z i |? k i = 0), rather than class-wise accuracy P ? (? k i = z i |? k i = 1). Besides, we do not consider the dependency between LFs and directly assume they are independent. R |? k |+1 , output probability over specialized categories and "abstention" option y k = arg max(p k (y|x)), predicted label of the k-th LF y = (? 1 , ? ? ? ,? K ) ? R K , vectorized predicted laebls of K LFs ? ? ? R K?|Y| is the parameters of label model e ky e ky := exp(? ky ), exponent of parameters ? ky ?(y,? k )</p><p>Potential value with the target label y and predicted label? k P (y,?) Joint distribution between target label y and predicted label? in the label model Z Normalizer item of P (y,?). Z = y?Y ??? P (y,?)</p><formula xml:id="formula_18">R(?,? u )</formula><p>Regularizer of unlabeled data in the Label Model The glossary is given in <ref type="table" target="#tab_6">Table 5</ref>.</p><formula xml:id="formula_19">z i z i = +1 y u = i ?1 y u = i ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Label Model</head><p>Label Model in Snorkel <ref type="bibr" target="#b51">[52]</ref> is also called as "Generative Model", which models and integrates the noisy labels provided by K LFs. In this paper, we suppose that the K LFs are independent. Assuming that? = (? 1 , ? ? ? ,? K ) ? R K is the vectorized form of the predicted labels from K LFs, and? k is the predicted label of the k-th model. For clarity, we denote ? as the "abstention" option in the LFs. Then, following the definition of Snorkel, the label model in Snorkel can be represented as:</p><formula xml:id="formula_20">P (y,?) = 1 Z K k=1 ?(y,? k ),<label>(18)</label></formula><p>with the potential function ?(y,? k ):</p><formula xml:id="formula_21">?(y,? k ) = ? ? ? ? ? exp(? k1 + ? k2 ), if? k = y exp(? k1 ), if? k = y,? = ? 1. if? k = ?<label>(19)</label></formula><p>where ? ? R 2K . It can be observed that the potential function in Eq. <ref type="bibr" target="#b18">(19)</ref> provides the same values for all target categories y. However, each LF in our method specializes in multiple categories with different performance. Thus, we extend the parameters ? to K ?C to support multi-class classification. In Eq. <ref type="formula" target="#formula_0">(19)</ref>, we set exp(? k1 + ? k2 ) to guarantee that the potential with? k = y is larger than that wit? y k = y. Similarly, we set 1 + exp(? ky ) and 1/(1 + exp(? ky )). Then, we have the potential function: </p><p>However, the conclusion from <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> tells us that MCL tends to be overconfident for the samples whose ground-truth labels are out of the specialized set. In other words, when one LF is fed with a sample whose ground-truth label is out of the specialized set, it may still produce a label in the specialized set with high confidence. Although the cases of overconfidence decrease a lot due to the introduction of the "abstention" option in Step 1, these cases still exist in our framework. Therefore, we introduce the item exp(? yk ) to represent the relationship between the predicted label? k and the target label y, even when the target labels conflict with the predicted labels. Based on these considerations, we define the four-part potential function in Eq. (9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Regularizer</head><p>We give the complementary formulation of P ? (? k i = z i |? k i = 0). Set ? k (y,?) := ? k ?? ?(y,? k ) for ease of exposition. We write P ? (? k i = z i |? k i = 0) as follows: P ? (? k i = z i |? k i = 0) = P ? (? k i = z i = 1) + P ? (? k i = z i = ?1) P ? (? k i = 0) = P ? (y = i,? k = y,? k = 0) + P ? (y = i,? k = i,? k = 0) P ? (? k = 0)</p><formula xml:id="formula_23">= ?(i, i) k =k ? k (i, {0} ? ? k ) + y =i (? k (y, ? k ? {i}) k =k ? k (y, {0} ? ? k )) y?Y ? k (y, ? k ) k =k ? k (y, {0} ? ? k ) .<label>(21)</label></formula><p>As mentioned in our paper, the class-wise accuracy P ? (? k i = z i |? k i = 1) without negative samples can be written as:</p><formula xml:id="formula_24">P ? (? k i = z i |? k i = 1) == ?(i, i) k =k ? k (i, {0} ? ? k ) y?Y ?(y, i) k =k ? k (y, {0} ? ? k ) .<label>(22)</label></formula><p>However, we can estimateP (? k i = z i |? k i = 0) by Eq. (15) directly, but it is more difficult to estimate the class-wise accuracyP (? k i = z i |? k i = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Model Analysis</head><p>We give an example with 40 labeled samples in <ref type="figure" target="#fig_9">Fig. 4</ref> to illustrate why Majority Vote falls with K = 50 and ? = 0.2. In the extreme case, category "automobile" and category "ship" are only specialized by 4 LFs, while 15 LFs specialize in "dog". If an image with category "automobile" triggers all (4) true specialized LFs but triggers 40% (5) specialized LFs with class "dog", Majority Vote would misclassify it into "dog". With our label model, we can achieve 95.22% annotation accuracy in this case.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Hyperparameter ? and K</head><p>We also present the complete experimental results with different ? and K on CIFAR-10 with 40 labels in Tab. 6.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Error rate vs. #labeled samples (CIFAR-10). Results of existing methods are from the original papers. When only 40 labeled samples are given, all existing SSL methods are substantially degraded and more unstably, while our method is still effective and robust.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Framework of the DP-SSL method with four LFs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>l , p(y|x l )) labeled samples + xu E y?? H(y, p(y|x u )) unlabeled samples with probabilistic label</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. 4.4 and Sec. 4.5 respectively. All experiments are implemented in Pytorch v1.7 and conducted on 16 NVIDIA RTX3090s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 2</head><label>2</label><figDesc>Datasets CIFAR-10 and CIFAR-100 [53] contain 50,000 training examples and 10,000 validation examples. All images are of 32x32 pixel size and fall in 10 or 100 classes, respectively. SVHN [54] is a digital image dataset that consists of 73,257, 26,032 and 531,131 samples in the train, test, and extra folders. It has the same image resolution and category number as CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Plots of ablation study on CIFAR-10 with 40 labels. (a) Varying the number of LFs K. (b) Varying the specialized ratio ?. Here, the red dashed line indicates the error rate of DP-SSL with default hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>latent variable of ground-truth label in the i-th one-versus-all task of? k in the i-th one-versus-all taskE[? k i z i ] Expectation of? k i z ? E[? k i z i ]Estimated expectation of? k i z i over all unlabeled data without ground-truch z i .P (?)Probability estimated with the observable data. a k i Precision of the k-th LF in the i-th one-versus-all classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 +</head><label>1</label><figDesc>exp(? yk ), if? ? ? k ,? k = y 1/(1 + exp(? yk )), if? ? ? k ,? k = y 1.otherwise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>The number of specialized LFs for each category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>is observable, which can be derived from the noisy labels of the j-th and k-th LFs, while E[? j i z i ] and E[? k i z i ] remain to be solved due to true label z i is unavailable. Next, we introduce a third labeling result from the l-th LF as? l i</figDesc><table><row><cell></cell><cell>,?[? j i? i ] = 1 k |xu|</cell><cell>j i? xu? i , such that k</cell></row><row><cell>E[? j i? i ] and?[? k l i?</cell><cell cols="2">l i ] are observable. Then, |?[? j i z i ]|, |?[? k i z i ]|, |?[? l i z i ]| can be solved by a triplet</cell></row><row><cell cols="2">method as follows:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of error rate on CIFAR-10, CIFAR-100 and SVHN for different existing SSL methods (?-Model</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>54?11.50 11.05?0.86 6.42?0.10 67.61?1.32 39.94?0.<ref type="bibr" target="#b36">37</ref> 28.31?0.<ref type="bibr" target="#b32">33</ref> 42.55?14.53 3.98?0.23 3.50?0.28 UDA 29.05?5.93 8.82?1.08 4.88?0.18 59.28?0.88 33.13?0.22 24.50?0.25 52.63?20.51 5.69?2.76 2.46?0.24 ReMixMatch 19.10?9.64 5.44?0.05 4.72?0.13 44.28?2.06 27.43?0.31 23.03?0.56 3.34?0.20 2.92?0.48 2.65?0.08 USADTM 9.54?1.04 4.80?0.32 4.40?0.15 43.36?1.89 28.11?0.21 21.35?0.17 3.01?1.97 2.11?0.65 1.96?0.05 FixMatch (RA) 13.81?3.37 5.07?0.65 4.26?0.05 48.85?1.75 28.29?0.11 22.60?0.12 3.96?2.17 2.48?0.38 2.28?0.11 FixMatch (CTA) 11.39?3.35 5.07?0.33 4.31?0.15 49.95?3.01 28.64?0.24 23.18?0.11 7.65?7.65 2.64?0.64 2.36?0.19 DP-SSL (ours) 6.54?0.98 4.78?0.26 4.23?0.20 43.17?1.29 28.00?0.79 22.24?0.31 2.98?0.86 2.16?0.36 1.99?0.18</figDesc><table><row><cell></cell><cell>-10</cell><cell>CIFAR-100</cell><cell>SVHN</cell></row><row><cell>Method</cell><cell cols="2">40 labels 250 labels 4000 labels 400 labels 2500 labels 10000 labels</cell><cell>40 labels 250 labels 1000 labels</cell></row><row><cell>? -Model</cell><cell>-54.26?3.97 14.01?0.38</cell><cell>-57.25?0.48 37.88?0.11</cell><cell>-18.96?1.92 7.54?0.36</cell></row><row><cell>Pseudo-Labeling</cell><cell>-49.78?0.43 16.09?0.28</cell><cell>-57.38?0.46 36.21?0.19</cell><cell>-20.21?1.09 9.94?0.61</cell></row><row><cell>Mean Teacher</cell><cell>-32.32?2.30 9.19?0.19</cell><cell>-53.91?0.57 35.83?0.24</cell><cell>-3.57?0.11 3.42?0.07</cell></row><row><cell>MixMatch 47.Fully Supervised</cell><cell>2.74</cell><cell>16.84</cell><cell>1.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of error rate on STL-10.</figDesc><table><row><cell>STL-10</cell></row></table><note>STL-10 [55] is a dataset for evaluating unsupervised and semi-supervised learning. It consists of 5000 labeled images and 8000 validation samples of 96x96 size from 10 classes. Besides, there are 100,000 unlabeled images available, including odd samples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The macro Precision/Recall/F1 Score/Coverage of the annotated labels on CIFAR-10, CIFAR-100, and SVHN for our method and two typical existing label models.</figDesc><table><row><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell><cell></cell><cell>SVHN</cell><cell></cell></row><row><cell>Method</cell><cell cols="9">Metrics 40 labels 250 labels 4000 labels 400 labels 2500 labels 10000 labels 40 labels 250 labels 1000 labels</cell></row><row><cell>Majority Vote</cell><cell>F1 Score 85.96</cell><cell>94.23</cell><cell>95.77</cell><cell>49.97</cell><cell>69.81</cell><cell>76.03</cell><cell>90.86</cell><cell>95.38</cell><cell>96.14</cell></row><row><cell cols="2">FlyingSquid[21] F1 Score 90.25</cell><cell>94.99</cell><cell>95.85</cell><cell>48.90</cell><cell>69.73</cell><cell>74.12</cell><cell>93.92</cell><cell>97.24</cell><cell>97.70</cell></row><row><cell></cell><cell>Precision 93.47</cell><cell>95.30</cell><cell>95.89</cell><cell>55.62</cell><cell>71.91</cell><cell>75.12</cell><cell>95.20</cell><cell>97.65</cell><cell>97.79</cell></row><row><cell>DP-SSL (ours)</cell><cell>Recall F1 Score 93.61 93.82</cell><cell>95.33 95.19</cell><cell>95.91 95.90</cell><cell>56.86 54.42</cell><cell>72.01 71.89</cell><cell>78.35 76.36</cell><cell>96.78 95.95</cell><cell>97.64 97.59</cell><cell>97.94 97.81</cell></row><row><cell></cell><cell>Coverage 99.35</cell><cell>99.79</cell><cell>99.91</cell><cell>99.33</cell><cell>99.87</cell><cell>99.94</cell><cell>99.15</cell><cell>99.67</cell><cell>99.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">Annotation performance for</cell></row><row><cell cols="3">different configurations on CIFAR-10</cell></row><row><cell cols="3">with 40 and 250 labels. K and ? are set</cell></row><row><cell cols="2">to 50 and 0.2 by default.</cell><cell></cell></row><row><cell>Experiments</cell><cell cols="2">40 labels 250 labels</cell></row><row><cell>Exp1: w.o. MCL</cell><cell>92.46</cell><cell>95.02</cell></row><row><cell>Exp2: MCL w.o. FT</cell><cell>91.61</cell><cell>94.98</cell></row><row><cell>Exp3: MCL w. FT</cell><cell>93.82</cell><cell>95.33</cell></row><row><cell cols="2">Exp4: w.o. Regularizer 93.19</cell><cell>94.94</cell></row><row><cell>Exp5: Regularizer</cell><cell>93.82</cell><cell>95.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Glossary of variables and symbols used in this paper.</figDesc><table><row><cell>Symbol</cell><cell>Used for</cell></row><row><cell>x l , y l</cell><cell>Labeled samples and ground-truth labels</cell></row><row><cell>x u x w ? x s ?</cell><cell>Unlabeled samples Weakly augmented (labeled / unlabeled) samples Strongly augmented (labeled / unlabeled) samples</cell></row><row><cell>Y</cell><cell>Label space. e.g., in CIFAR-10, Y = {1, 2, ? ? ? , 10}</cell></row><row><cell>C</cell><cell>Number of categories. C = |Y|</cell></row><row><cell>f</cell><cell>R HW ?D , feature map before global average pooling of backbone</cell></row><row><cell>f (j)</cell><cell>R D , feature vector at spatial position j of f</cell></row><row><cell cols="2">dis(A, B) Distance between A and B</cell></row><row><cell>c k</cell><cell>R D , learnable clustering center of the k-th LF</cell></row><row><cell>f k</cell><cell>R D , input feature of classifier in the k-th LF</cell></row><row><cell>F k</cell><cell>The classifier head in the k-th LF</cell></row><row><cell>p k (y|x)</cell><cell>R C , output probability of the k-th LF</cell></row><row><cell>? k</cell><cell>Specialized category set of the k-th LF</cell></row><row><cell>p k (y|x)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Annotation performance for different ? and K on CIFAR-10 with 40 labels .25 86.42 88.99 90.62 91.28 92.39 20 89.16 91.27 91.83 92.31 92.52 92.48 30 91.39 93.46 93.54 93.16 92.91 92.41 40 92.53 93.28 93.45 93.24 92.80 92.47 50 92.95 93.82 93.55 93.19 93.07 92.46 60 93.01 93.54 93.11 92.97 92.63 92.43</figDesc><table><row><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>1.0</cell></row><row><cell>10 83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised active learning: Towards minimizing labeling cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">?</forename><surname>Ar?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="510" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficiently scaling up crowdsourced video annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="204" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Visual distant supervision for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15365</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1163" to="1171" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets, quickly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3567" to="3575" />
			<date type="published" when="2016" />
			<publisher>NIH Public Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning from rules generalizing labeled exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Snorkel: Fast training set generation for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1683" to="1686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training complex models with multi-task weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4763" to="4771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Data programming using continuous and quality-guided labeling functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scene graph prediction with limited labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2580" to="2590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cut out the annotator, keep the cutout: better segmentation with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wornow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kellman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter Andxue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distillation multiple choice learning for multimodal action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ablavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>WACV</publisher>
			<biblScope unit="page" from="2754" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Van Engelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="373" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A survey on deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00550</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Train and you&apos;ll miss it: Interactive model iteration with weak supervision and pre-trained embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Mullapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15168</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and three-rious: Speeding up weak supervision with triplet methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3280" to="3291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficiently enforcing diversity in multi-output structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rutenbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics. PMLR</title>
		<imprint>
			<biblScope unit="page" from="284" to="292" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic multiple choice learning for training diverse deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2119" to="2127" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Confident multiple choice learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Versatile multiple choice learning and its application to vision computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6349" to="6357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3546" to="3554" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5050" to="5060" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1195" to="1204" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised semantic aggregation and deformable template matching for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Comatch: Semi-supervised learning with contrastive graph regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9475" to="9484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simple: Similar pseudo label exploitation for semi-supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="99" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shinozaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08263</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Snuba: automating weak supervision to label training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">223</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Inspector gadget: A data programming-based labeling system for industrial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Whang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="36" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial data programming: Using gans to relax the bottleneck of curated labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1556" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Goggles: Automatic image labeling with affinity coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD, 2020</title>
		<imprint>
			<biblScope unit="page" from="1717" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Interactive weak supervision: Learning useful heuristics for data labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubrawski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Inferring generative model structure with static analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Khandwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="240" to="250" />
			<date type="published" when="2017" />
			<publisher>NIH Public Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning the structure of generative models without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning dependency structures for weak supervision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6418" to="6427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Weakly-supervised text classification based on keyword graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02591</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC. BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Snorkel: Rapid training data creation with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="709" to="730" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Tront</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Estimating the accuracies of multiple classifiers without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics. PMLR</title>
		<imprint>
			<biblScope unit="page" from="407" to="415" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Estimating accuracy from unlabeled data: A probabilistic logic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4361" to="4370" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Blind multiclass ensemble classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Traganitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pages-Zamora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="4737" to="4752" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
