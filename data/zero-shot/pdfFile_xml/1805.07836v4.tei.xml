<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">Meinig School of Biomedical Engineering Cornell University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
							<email>msabuncu@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">Meinig School of Biomedical Engineering Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The resurrection of neural networks in recent years, together with the recent emergence of large scale datasets, has enabled super-human performance on many classification tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. However, supervised DNNs often require a large number of training samples to achieve a high level of performance. For instance, the ImageNet dataset <ref type="bibr" target="#b5">[6]</ref> has 3.2 million hand-annotated images. Although crowdsourcing platforms like Amazon Mechanical Turk have made large-scale annotation possible, some error during the labeling process is often inevitable, and mislabeled samples can impair the performance of models trained on these data. Indeed, the sheer capacity of DNNs to memorize massive data with completely randomly assigned labels <ref type="bibr" target="#b41">[42]</ref> proves their susceptibility to overfitting when trained with noisy labels. Hence, an algorithm that is robust against noisy labels for DNNs is needed to resolve the potential problem. Furthermore, when examples are cheap and accurate annotations are expensive, it can be more beneficial to have datasets with more but noisier labels than less but more accurate labels <ref type="bibr" target="#b17">[18]</ref>.</p><p>Classification with noisy labels is a widely studied topic <ref type="bibr" target="#b7">[8]</ref>. Yet, relatively little attention is given to directly formulating a noise-robust loss function in the context of DNNs. Our work is motivated by Ghosh et al. <ref type="bibr" target="#b8">[9]</ref> who theoretically showed that mean absolute error (MAE) can be robust against noisy labels under certain assumptions. However, as we demonstrate below, the robustness of MAE can concurrently cause increased difficulty in training, and lead to performance drop. This limitation is particularly evident when using DNNs on complicated datasets. To combat this drawback, we advocate the use of a more general class of noise-robust loss functions, which encompass both MAE and CCE. Compared to previous methods for DNNs, which often involve extra steps and algorithmic modifications, changing only the loss function requires minimal intervention to existing architectures 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr?al, Canada. and algorithms, and thus can be promptly applied. Furthermore, unlike most existing methods, the proposed loss functions work for both closed-set and open-set noisy labels <ref type="bibr" target="#b39">[40]</ref>. Open-set refers to the situation where samples associated with erroneous labels do not always belong to a ground truth class contained within the set of known classes in the training data. Conversely, closed-set means that all labels (erroneous and correct) come from a known set of labels present in the dataset.</p><p>The main contributions of this paper are two-fold. First, we propose a novel generalization of CCE and present a theoretical analysis of proposed loss functions in the context of noisy labels. And second, we report a thorough empirical evaluation of the proposed loss functions using CIFAR-10, CIFAR-100 and FASHION-MNIST datasets, and demonstrate significant improvement in terms of classification accuracy over the baselines of MAE and CCE, under both closed-set and open-set noisy labels.</p><p>The rest of the paper is organized as follows. Section 2 discusses existing approaches to the problem. Section 3 introduces our noise-robust loss functions. Section 4 presents and analyzes the experiments and result. Finally, section 5 concludes our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Numerous methods have been proposed for learning with noisy labels with DNNs in recent years. Here, we briefly review the relevant literature. Firstly, Sukhbaatar and Fergus <ref type="bibr" target="#b34">[35]</ref> proposed accounting for noisy labels with a confusion matrix so that the cross entropy loss becomes</p><formula xml:id="formula_0">L(?) = 1 N N n=1 ? log p( y = y n |x n , ?) = 1 N N n=1 ? log( c i p( y = y n |y = i)p(y = i|x n , ?)),<label>(1)</label></formula><p>where c represents number of classes, y represents noisy labels, y represents the latent true labels and p( y = y n |y = i) is the ( y n , i)'th component of the confusion matrix. Usually, the real confusion matrix is unknown. Several methods have been proposed to estimate it <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b11">12]</ref>. Yet, accurate estimations can be hard to obtain. Even with the real confusion matrix, training with the above loss function might be suboptimal for DNNs. Assuming (1) a DNN with enough capacity to memorize the training set, and (2) a confusion matrix that is diagonally dominant, minimizing the cross entropy with confusion matrix is equivalent to minimizing the original CCE loss. This is because the right hand side of Eq. 1 is minimized when p(y = i|x n , ?) = 1 for i = y n and 0 otherwise, ? n.</p><p>In the context of support vector machines, several theoretically motivated noise-robust loss functions like the ramp loss, the unhinged loss and the savage loss have been introduced <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b26">27]</ref>. More generally, Natarajan et al. <ref type="bibr" target="#b28">[29]</ref> presented a way to modify any given surrogate loss function for binary classification to achieve noise-robustness. However, little attention is given to alternative noise robust loss functions for DNNs. Ghosh et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> proved and empirically demonstrated that MAE is robust against noisy labels. This paper can be seen as an extension and generalization of their work.</p><p>Another popular approach attempts at cleaning up noisy labels. Veit et al. <ref type="bibr" target="#b38">[39]</ref> suggested using a label cleaning network in parallel with a classification network to achieve more noise-robust prediction. However, their method requires a small set of clean labels. Alternatively, one could gradually replace noisy labels by neural network predictions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36]</ref>. Rather than using predictions for training, Northcutt et al. <ref type="bibr" target="#b30">[31]</ref> offered to prune the correct samples based on softmax outputs. As we demonstrate below, this is similar to one of our approaches. Instead of pruning the dataset once, our algorithm iteratively prunes the dataset while training until convergence.</p><p>Other approaches include treating the true labels as a latent variable and the noisy labels as an observed variable so that EM-like algorithms can be used to learn true label distribution of the dataset <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref>. Techniques to re-weight confident samples have also been proposed. Jiang et al. <ref type="bibr" target="#b15">[16]</ref> used a LSTM network on top of a classification model to learn the optimal weights on each sample, while Ren, et al. <ref type="bibr" target="#b33">[34]</ref> used a small clean dataset and put more weights on noisy samples which have gradients closer to that of the clean dataset. In the context of binary classification, Liu et al. <ref type="bibr" target="#b23">[24]</ref> derived an optimal importance weighting scheme for noise-robust classification. Our method can also be viewed as re-weighting individual samples; instead of explicitly obtaining weights, we use the softmax outputs at each iteration as the weightings. Lastly, Azadi et al. <ref type="bibr" target="#b1">[2]</ref> proposed a regularizer that encourages the model to select reliable samples for noise-robustness. Another method that uses knowledge distillation for noisy labels has also been proposed <ref type="bibr" target="#b22">[23]</ref>. Both of these methods also require a smaller clean dataset to work.</p><p>3 Generalized Cross Entropy Loss for Noise-Robust Classifications</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>We consider the problem of k-class classification. Let X ? R d be the feature space and Y = {1, ? ? ? , c} be the label space. In an ideal scenario, we are given a clean dataset</p><formula xml:id="formula_1">D = {(x i , y i )} n i=1 , where each (x i , y i ) ? (X ? Y).</formula><p>A classifier is a function that maps input feature space to the label space f : X ? R c . In this paper, we consider the common case where the function is a DNN with the softmax output layer. For any loss function L, the (empirical) risk of the classifier f is defined</p><formula xml:id="formula_2">as R L (f ) = E D [L(f (x), y x )]</formula><p>, where the expectation is over the empirical distribution. The most commonly used loss for classification is cross entropy. In this case, the risk becomes:</p><formula xml:id="formula_3">R L (f ) = E D [L(f (x; ?), y x )] = ? 1 n n i=1 c j=1 y ij log f j (x i ; ?),<label>(2)</label></formula><p>where ? is the set of parameters of the classifier, y ij corresponds to the j'th element of one-hot encoded label of the sample x i , y i = e yi ? {0, 1} c such that 1 y i = 1 ? i, and f j denotes the j'th element of f . Note that, n j=1 f j (x i ; ?) = 1, and f j (x i ; ?) ? 0, ?j, i, ?, since the output layer is a softmax. The parameters of DNN can be optimized with empirical risk minimization.</p><p>We denote a dataset with label noise by</p><formula xml:id="formula_4">D ? = {(x i , y i )} n i=1</formula><p>where y i 's are the noisy labels with respect to each sample such that p( y i = k|y i = j, x i ) = ? (xi) jk . In this paper, we make the common assumption that noise is conditionally independent of inputs given the true labels so that</p><formula xml:id="formula_5">p( y i = k|y i = j, x i ) = p( y i = k|y i = j) = ? jk .</formula><p>In general, this noise is defined to be class dependent. Noise is uniform with noise rate ?, if ? jk = 1 ? ? for j = k, and ? jk = ? c?1 for j = k. The risk of classifier with respect to noisy dataset is then defined as R ?</p><formula xml:id="formula_6">L (f ) = E D? [L(f (x), y x )</formula><p>]. Let f * be the global minimizer of the risk R L (f ). Then, the empirical risk minimization under loss function L is defined to be noise tolerant <ref type="bibr" target="#b25">[26]</ref> if f * is a global minimum of the noisy risk R ?</p><formula xml:id="formula_7">L (f ). A loss function is called symmetric if, for some constant C, c j=1 L(f (x), j) = C, ?x ? X , ?f.<label>(3)</label></formula><p>The main contribution of Ghosh et al. <ref type="bibr" target="#b9">[10]</ref> is they proved that if loss function is symmetric and ? &lt; c?1 c , then under uniform label noise, for any f ,</p><formula xml:id="formula_8">R ? L (f * ) ? R ? L (f ) ? 0.</formula><p>Hence, f * is also the global minimizer for R ? L and L is noise tolerant. Moreover, if R L (f * ) = 0, then L is also noise tolerant under class dependent noise.</p><p>Being a nonsymmetric and unbounded loss function, CCE is sensitive to label noise. On the contrary, MAE, as a symmetric loss function, is noise robust. For DNNs with a softmax output layer, MAE can be computed as:</p><formula xml:id="formula_9">L M AE (f (x), e j ) = ||e j ? f (x)|| 1 = 2 ? 2f j (x).<label>(4)</label></formula><p>With this particular configuration of DNN, the proposed MAE loss is, up to a constant of proportionality, the same as the unhinged loss L unh (f (x), e j ) = 1 ? f j (x) <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">L q Loss for Classification</head><p>In this section, we will argue that MAE has some drawbacks as a classification loss function for DNNs, which are normally trained on large scale datasets using stochastic gradient based techniques. Let's look at the gradient of the loss functions: Thus, in CCE, samples with softmax outputs that are less congruent with provided labels, and hence smaller f yi (x i ; ?) or larger 1/f yi (x i ; ?), are implicitly weighed more than samples with predictions that agree more with provided labels in the gradient update. This means that, when training with CCE, more emphasis is put on difficult samples. This implicit weighting scheme is desirable for training with clean data, but can cause overfitting to noisy labels. Conversely, since the 1/f yi (x i ; ?) term is absent in its gradient, MAE treats every sample equally, which makes it more robust to noisy labels. However, as we demonstrate empirically, this can lead to significantly longer training time before convergence. Moreover, without the implicit weighting scheme to focus on challenging samples, the stochasticity involved in the training process can make learning difficult. As a result, classification accuracy might suffer.</p><formula xml:id="formula_10">n i=1 ?L(f (x i ; ?), y i ) ?? = n i=1 ? 1 fy i (xi;?) ? ? f yi (x i ; ?) for CCE n i=1 ?? ? f yi (x i ; ?) for MAE/unhinged loss.<label>(5)</label></formula><p>To demonstrate this, we conducted a simple experiment using ResNet <ref type="bibr" target="#b12">[13]</ref> optimized with the default setting of Adam <ref type="bibr" target="#b18">[19]</ref> on the CIFAR datasets <ref type="bibr" target="#b19">[20]</ref>. <ref type="figure" target="#fig_0">Fig. 1(a)</ref> shows the test accuracy curve when trained with CCE and MAE respectively on CIFAR-10. As illustrated clearly, it took significantly longer to converge when trained with MAE. In agreement with our analysis, there was also a compromise in classification accuracy due to the increased difficulty of learning useful features. These adverse effects become much more severe when using a more difficult dataset, such as CIFAR-100 (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). Not only do we observe significantly slower convergence, but also a substantial drop in test accuracy when using MAE. In fact, the maximum test accuracy achieved after 2000 epochs, a long time after training using CCE has converged, was 38.29%, while CCE achieved an higher accuracy of 39.92% after merely 7 epochs! Despite its theoretical noise-robustness, due to the shortcoming during training induced by its noise-robustness, we conclude that MAE is not suitable for DNNs with challenging datasets like ImageNet.</p><p>To exploit the benefits of both the noise-robustness provided by MAE and the implicit weighting scheme of CCE, we propose using the the negative Box-Cox transformation <ref type="bibr" target="#b3">[4]</ref> as a loss function:</p><formula xml:id="formula_11">L q (f (x), e j ) = (1 ? f j (x) q ) q ,<label>(6)</label></formula><p>where q ? (0, 1]. Using L'H?pital's rule, it can be shown that the proposed loss function is equivalent to CCE for lim q?0 L q (f (x), e j ), and becomes MAE/unhinged loss when q = 1. Hence, this loss is a generalization of CCE and MAE. Relatedly, Ferrari and Yang <ref type="bibr" target="#b6">[7]</ref> viewed the maximization of Eq. 6 as a generalization of maximum likelihood and termed the loss function L q , which we also adopt.</p><p>Theoretically, for any input x, the sum of L q loss with respect to all classes is bounded by:</p><formula xml:id="formula_12">c ? c (1?q) q ? c j=1 (1 ? f j (x) q ) q ? c ? 1 q .<label>(7)</label></formula><p>Using this bound and under uniform noise with ? ? 1 ? 1 c , we can show (see Appendix)</p><formula xml:id="formula_13">A ? (R Lq (f * ) ? R Lq (f )) ? 0,<label>(8)</label></formula><p>where</p><formula xml:id="formula_14">A = ?[1?c (1?q) ] q(c?1??c) &lt; 0, f * is the global minimizer of R Lq (f ), andf is the global minimizer of R ? Lq (f ).</formula><p>The larger the q, the larger the constant A, and the tighter the bound of Eq. 8. In the extreme case of q = 1 (i.e., for MAE), A = 0 and R Lq (f ) = R Lq (f * ). In other words, for q values approaching 1, the optimum of the noisy risk will yield a risk value (on the clean data) that is close to f * , which implies noise tolerance. It can also be shown that the difference</p><formula xml:id="formula_15">(R ? Lq (f * ) ? R ? Lq (f )) is bounded under class dependent noise, provided R Lq (f * ) = 0 and q ij &lt; q ii ?i = j (see Thm 2 in Appendix).</formula><p>The compromise on noise-robustness when using L q over MAE prompts an easier learning process. Let's look at the gradients of L q loss to see this:</p><formula xml:id="formula_16">?L q (f (x i ; ?), y i ) ?? = f yi (x i ; ?) q (? 1 f yi (x i ; ?) ? ? f yi (x i ; ?)) = ?f yi (x i ; ?) q?1 ? ? f yi (x i ; ?),</formula><p>where f yi (x i ; ?) ? [0, 1] ? i and q ? (0, 1). Thus, relative to CCE, L q loss weighs each sample by an additional f yi (x i ; ?) q so that less emphasis is put on samples with weak agreement between softmax outputs and the labels, which should improve robustness against noise. Relative to MAE, a weighting of f yi (x i ; ?) q?1 on each sample can facilitate learning by giving more attention to challenging datapoints with labels that do not agree with the softmax outputs. On one hand, larger q leads to a more noise-robust loss function. On the other hand, too large of a q can make optimization strenuous. Hence, as we will demonstrate empirically below, it is practically useful to set q between 0 and 1, where a tradeoff equilibrium is achieved between noise-robustness and better learning dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Truncated L q Loss</head><p>Since a tighter bound in c j=1 L(f (x, j)) would imply stronger noise tolerance, we propose the truncated L q loss:</p><formula xml:id="formula_17">L trunc (f (x), e j ) = L q (k) if f j (x) ? k L q (f (x), e j ) if f j (x) &gt; k<label>(9)</label></formula><p>where 0 &lt; k &lt; 1, and L q (k) = (1 ? k q )/q. Note that, when k ? 0, the truncated L q loss becomes the normal L q loss. Assuming k ? 1/c, the sum of truncated L q loss with respect to all classes is bounded by (see Appendix):</p><formula xml:id="formula_18">dL q ( 1 d ) + (c ? d)L q (k) ? c j=1 L trunc (f (x), e j ) ? cL q (k),<label>(10)</label></formula><p>where d = max(1, (1?q) 1/q k ). It can be verified that the difference between upper and lower bounds for the truncated L q loss, L q (k), is smaller than that for the L q loss of Eq. 7, if</p><formula xml:id="formula_19">d[L q (k) ? L q ( 1 d )] &lt; c (1?q) ? 1 q .<label>(11)</label></formula><p>As an example, when k ? 0.3, the above inequality is satisfied for all q and c. When k ? 0.2, the inequality is satisfied for all q and c ? 10. Since the derived bounds in Eq. 7 and Eq. 10 are tight, introducing the threshold k can thus lead to a more noise tolerant loss function.</p><p>If the softmax output for the provided label is below a threshold, truncated L q loss becomes a constant. Thus, the loss gradient is zero for that sample, and it does not contribute to learning dynamics. While Eq. 10 suggests that a larger threshold k leads to tighter bounds and hence more noise-robustness, too large of a threshold would precipitate too many discarded samples for training. Ideally, we would want the algorithm to train with all available clean data and ignore noisy labels. Thus the optimal choice of k would depend on the noise in the labels. Hence, k can be treated as a (bounded) hyper-parameter and optimized. In our experiments, we set k = 0.5 that yields a tighter bound for truncated L q loss, and which we observed to work well empirically.</p><p>A potential problem arises when training directly with this loss function. When the threshold is relatively large (e.g., k = 0.5 in a 10-class classification problem), at the beginning of the training phase, most of the softmax outputs can be significantly smaller than k, resulting in a dramatic drop in the number of effective samples. Moreover, it is suboptimal to prune samples based on softmax values at the beginning of training. To circumvent the problem, observe that, by definition of the truncated L q loss:</p><formula xml:id="formula_20">argmin ? n i=1 L trunc (f (x i ; ?), y i ) = argmin ? n i=1 v i L q (f (x i ; ?), y i ) + (1 ? v i )L q (k),<label>(12)</label></formula><p>where v i = 0 if f yi (x i ) ? k and v i = 1 otherwise, and ? represents the parameters of the classifier.</p><p>Optimizing the above loss is the same as optimizing the following:</p><formula xml:id="formula_21">argmin ? n i=1 v i L q (f (x i ; ?), y i ) ? v i L q (k) = argmin ?,w?[0,1] n n i=1 w i L q (f (x i ; ?), y i ) ? L q (k) n i=1 w i ,<label>(13)</label></formula><p>because for any ?, the optimal w i is</p><formula xml:id="formula_22">1 if L q (f (x i ; ?), y i ) ? L q (k) and 0 if L q (f (x i ; ?), y i ) &gt; L q (k).</formula><p>Hence, we can optimize the truncated L q loss by optimizing the right hand side of Eq. 13. If L q is convex with respect to the parameters ?, optimizing Eq. 13 is a biconvex optimization problem, and the alternative convex search (ACS) algorithm <ref type="bibr" target="#b2">[3]</ref> can be used to find the global minimum. ACS iteratively optimizes ? and w while keeping the other set of parameters fixed. Despite the high non-convexity of DNNs, we can apply ACS to find a local minimum. We refer to the update of w as "pruning". At every step of iteration, pruning can be carried out easily by computing f (x i ; ? (t) ) for all training samples. Only samples with f yi (x i ; ? (t) ) ? k and L q (f (x i ; ?), y i ) ? L q (k) are kept for updating ? during that iteration (and hence w i = 1 ). The additional computational complexity from the pruning steps is negligible. Interestingly, the resulting algorithm is similar to that of self-paced learning <ref type="bibr" target="#b21">[22]</ref>.</p><formula xml:id="formula_23">Algorithm 1 ACS for Training with L q Loss Input Noisy dataset D ? , total iterations T , threshold k Initialize w (0) i = 1 ? i Update ? (0) = argmin ? n i=1 w (0) i L q (f (x i ; ?), y i ) ? L q (k) n i=1 w (0) i while t &lt; T do Update w (t) = argmin w n i=1 w i L q (f (x i ; ? (t?1) ), y i ) ? L q (k) n i=1 w i [Pruning Step] Update ? (t) = argmin ? n i=1 w (t) i L q (f (x i ; ?), y i ) ? L q (k) n i=1 w (t) i Output ? (T )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The following setup applies to all of the experiments conducted. Noisy datasets were produced by artificially corrupting true labels. 10% of the training data was retained for validation. To realistically mimic a noisy dataset while justifiably analyzing the performance of the proposed loss function, only the training and validation data were contaminated, and test accuracies were computed with respect to true labels. A mini-batch size of 128 was used. All networks used ReLUs in the hidden layers and softmax layers at the output. All reported experiments were repeated five times with random initialization of neural network parameters and randomly generated noisy labels each time. We compared the proposed functions with CCE, MAE and also the confusion matrix-corrected CCE, as shown in Eq. 1. Following <ref type="bibr" target="#b31">[32]</ref>, we term this "forward correction". All experiments were conducted with identical optimization procedures and architectures, changing only the loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Toward a Better Understanding of L q Loss</head><p>To better grasp the behavior of L q loss, we implemented different values of q and uniform noise at different noise levels, and trained ResNet-34 with the default setting of Adam on CIFAR-10. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, when trained on clean dataset, increasing q not only slowed down the rate of convergence, but also lowered the classification accuracy. More interesting phenomena appeared when trained on noisy data. When CCE (q = 0) was used, the classifier first learned predictive patterns, presumably from the noise-free labels, before overfitting strongly to the noisy labels, in agreement with Arpit et al.'s observations <ref type="bibr" target="#b0">[1]</ref>. Training with increased q values delayed overfitting and attained higher classification accuracies. One interpretation of this behavior is that the classifier could learn more about predictive features before overfitting. This interpretation is supported by our plot of the average softmax values with respect to the correctly and wrongly labeled samples on the training set for CCE and L q (q = 0.7) loss, and with 40% uniform noise ( <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). For CCE, the average softmax for wrongly labeled samples remained small at the beginning, but grew quickly when the model started overfitting. L q loss, on the other hand, resulted in significantly smaller softmax values for wrongly labeled data. This observation further serves as an empirical justification for the use of truncated L q loss as described in section 3.3.</p><p>We also observed that there was a threshold of q beyond which overfitting never kicked in before convergence. When ? = 0.2 for instance, training with L q loss with q = 0.8 produced an overfittingfree training process. Empirically, we noted that, the noisier the data, the larger this threshold is. However, too large of a q hampers the classification accuracy, and thus a larger q is not always preferred. In general, q can be treated as a hyper-parameter that can be optimized, say via monitoring validation accuracy. In remaining experiments, we used q = 0.7, which yielded a good compromise between fast convergence and noise robustness (no overfitting was observed for ? ? 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>CIFAR-10/CIFAR-100: ResNet-34 was used as the classifier optimized with the loss functions mentioned above. Per-pixel mean subtraction, horizontal random flip and 32 ? 32 random crops after padding with 4 pixels on each side was performed as data preprocessing and augmentation. Following <ref type="bibr" target="#b14">[15]</ref>, we used stochastic gradient descent (SGD) with 0.9 momentum, a weight decay of 10 ?4 and learning rate of 0.01, and divided it by 10 after 40 and 80 epochs (120 in total) for CIFAR-10, and after 80 and 120 (150 in total) for CIFAR-100. To ensure a fair comparison, the identical optimization scheme was used for truncated L q loss. We trained with the entire dataset for the first 40 epochs for CIFAR-10 and 80 for CIFAR-100, and started pruning and training with the pruned dataset afterwards. Pruning was done every 10 epochs. To prevent overfitting, we used the model at the optimal epoch <ref type="table">Table 1</ref>: Average test accuracy and standard deviation (5 runs) on experiments with closed-set noise. We report accuracies of the epoch where validation accuracy is maximum. Forward T andT represent forward correction with the true and estimated confusion matrices, respectively <ref type="bibr" target="#b31">[32]</ref>. q = 0.7 was used for all experiments with L q loss and truncated L q loss. Best 2 accuracies are bold faced.  based on maximum validation accuracy for pruning. Uniform noise was generated by mapping a true label to a random label through uniform sampling. Following Patrini, et al. <ref type="bibr" target="#b31">[32]</ref> class dependent noise was generated by mapping TRUCK ? AUTOMOBILE, BIRD ? AIRPLANE, DEER ? HORSE, and CAT ? DOG with probability ? for CIFAR-10. For CIFAR-100, we simulated class-dependent noise by flipping each class into the next circularly with probability ?.</p><p>We also tested noise-robustness of our loss function on open-set noise using CIFAR-10. For a direct comparison, we followed the identical setup as described in <ref type="bibr" target="#b39">[40]</ref>. For this experiment, the classifier was trained for only 100 epochs. We observed validation loss plateaued after about 10 epochs, and hence started pruning the data afterwards at 10-epoch intervals. The open-set noise was generated by using images from the CIFAR-100 dataset. A random CIFAR-10 label was assigned to these images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FASHION-MNIST:</head><p>ResNet-18 was used. The identical data preprocessing, augmentation, and optimization procedure as in CIFAR-10 was deployed for training. To generate a realistic class dependent noise, we used the t-SNE <ref type="bibr" target="#b24">[25]</ref> plot of the dataset to associated classes with similar embeddings, and mapped BOOT ? SNEAKER , SNEAKER ? SANDALS, PULLOVER ? SHIRT, COAT ? DRESS with probability ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussion</head><p>Experimental results with closed-set noise is summarized in <ref type="table">Table 1</ref>. For uniform noise, proposed loss functions outperformed the baselines significantly, including forward correction with the ground truth confusion matrices. In agreement with our theoretical expectations, truncating the L q loss enhanced results. For class dependent noise, in general Forward T offered the best performance, as it relied on the knowledge of the ground truth confusion matrix. Truncated L q loss produced similar accuracies as ForwardT for FASHION-MNIST and better results for CIFAR datasets, and outperformed the other baselines at most noise levels for all datasets. While using L q loss improved over baselines for CIFAR-100, no improvements were observed for FASHION-MNIST and CIFAR-10 datasets. We believe this is in part because very similar classes were grouped together for the confusion matrices and consquently the DNNs might falsely put high confidence on wrongly labeled samples.</p><p>In general, classification accuracy for both uniform and class dependent noise would be further improved relative to baselines with optimized q and k values and more number of epochs. Based on the experimental results, we believe the proposed approach would work well when correctly labeled data can be differentiated from wrongly labeled data based on softmax outputs, which is often the case with large-scale data and expressive models. We also observed that MAE performed poorly for all datasets at all noise levels, presumably because DNNs like ResNet struggled to optimize with MAE loss, especially on challenging datasets such as CIFAR-100.  <ref type="bibr" target="#b39">[40]</ref>, we reported the last-epoch test accuracy after training for 100 epochs. We also repeated the closed-set noise experiment with their setup. Using L q loss noticeably prevented overfitting, and using truncated L q loss achieved better results than the state-of-the-art method for open-set noise reported in <ref type="bibr" target="#b39">[40]</ref>. Moreover, our method is significantly easier to implement. Lastly, note that the poor performance of L q loss compared to MAE is due to the fact that test accuracy reported here is long after the model started overfitting, since a shallow CNN without data augmentation was deployed for this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In conclusion, we proposed theoretically grounded and easy-to-use classes of noise-robust loss functions, the L q loss and the truncated L q loss, for classification with noisy labels that can be employed with any existing DNN algorithm. We empirically verified noise robustness on various datasets with both closed-and open-set noise scenarios.</p><formula xml:id="formula_24">R ? Lq (f ) = E D [L q (f (x), y x )] = E x, yx [L q (f (x), y x )] = E x E yx|x E yx|yx,x [L q (f (x), y x )] = E x E yx|x [(1 ? ?)L q (f (x), y x ) + ? c ? 1 i =yx L q (f (x), i)] = E x E yx|x [(1 ? ?)L q (f (x), y x ) + ? c ? 1 ( c i=1 L q (f (x), i) ? L q (f (x), y x ))] = (1 ? ?)R Lq (f ) ? ? c ? 1 R Lq (f ) + ? c ? 1 E x E yx|x [ c i=1 L q (f (x), i)] = (1 ? ?c c ? 1 )R Lq (f ) + ? c ? 1 E x E yx|x [ c i=1 L q (f (x), i)]</formula><p>Now, from Lemma 2, we have:</p><formula xml:id="formula_25">(1 ? ?c c ? 1 )R Lq (f ) + ?[c ? c (1?q) ] q(c ? 1) ? R ? Lq (f ) ? (1 ? ?c c ? 1 )R Lq (f ) + ? q .</formula><p>We can also write the inequality in terms of R Lq (f ):</p><formula xml:id="formula_26">(R ? Lq (f ) ? ? q )/(1 ? ?c c ? 1 ) ? R Lq (f )) ? (R ? Lq (f ) ? ?[c ? c (1?q) ] q(c ? 1) )/(1 ? ?c c ? 1 )</formula><p>Thus, forf ,</p><formula xml:id="formula_27">R ? Lq (f * ) ? R ? Lq (f ) ? A + (1 ? ?c c ? 1 )(R Lq (f * ) ? R Lq (f )) ? A,</formula><p>or equivalently,</p><formula xml:id="formula_28">R Lq (f * ) ? R Lq (f ) ? A + (R ? Lq (f * ) ? R ? Lq (f ))/(1 ? ?c c ? 1 ) ? A where A = ?[c (1?q) ?1] q(c?1) ? 0 and A = ?[1?c (1?q) ] q(c?1??c) , since ? ? c?1 c , and f * is a minimizer of R Lq (f ). Lastly, sincef is the minimizer of R ? Lq (f ), we have that R ? Lq (f * ) ? R ? Lq (f ) ? 0, or R Lq (f * ) ? R Lq (f ) ? 0 . This completes the proof.</formula><p>Remark. Note that, when q = 1, A = 0, and f * is also minimizer of risk under uniform noise.</p><formula xml:id="formula_29">Theorem 2. Under class dependent noise when ? ij &lt; (1 ? ? i ), ?j = i, ?i, j ? [c], where ? ij = p( y = j|y = i), ?j = i, and (1 ? ? i ) = p( y = i|y = i), if R Lq (f * ) = 0, then 0 ? (R ? Lq (f * ) ? R ? Lq (f )) ? B,<label>(17)</label></formula><formula xml:id="formula_30">where B = c 1?q ?1 q E D (1 ? ? yx ) ? 0, f * is the global minimizer of R Lq (f ), andf is the global minimizer of R ? Lq (f ).</formula><p>Proof. For class dependent noise, from Lemma 2, for any soft-max output function f we have</p><formula xml:id="formula_31">R ? Lq (f ) = E D [(1 ? ? yx )L q (f (x), y x )] + E D [ i =yx ? yxi L q (f (x), i)] ? E D [(1 ? ? yx )( c ? 1 q ? i =yx L q (f (x), i))] + E D [ i =yx ? yxi L q (f (x), i)] = c ? 1 q E D (1 ? ? yx ) ? E D [ i =yx (1 ? ? yx ? ? yxi )L q (f (x), i)], and R ? Lq (f ) ? c ? c 1?q q E D (1 ? ? yx ) ? E D [ i =yx (1 ? ? yx ? ? yxi )L q (f (x), i)].</formula><p>Hence,</p><formula xml:id="formula_32">(R ? Lq (f * ) ? R ? Lq (f )) ? c 1?q ? 1 q E D (1 ? ? yx )+ E D i =yx (1 ? ? yx ? ? yxi )[L q (f (x), i) ? L q (f * (x), i)].</formula><p>Now, from our assumption that R Lq (f * ) = 0, we have L q (f * (x), y x ) = 0. This is only satisfied iff f * i (x) = 1 when i = y x , and f * i (x) = 0 if i = y x . Hence, L q (f * (x), i) = 1/q ?i = y x . Moreover, by our assumption, we have (1 ? ? yx ? ? yxi ) &gt; 0. As a result, to derive a upper bound for the expression above, we need to maximize the second term. Note that by definition of the L q loss, L q (f (x), i) ? 1/q ?i ? [c], and hence the second term is maximized iff L q (f (x), i) = 1/q ?i = y x . This implies that the maximum of the second term is non-positive, so we have</p><formula xml:id="formula_33">(R ? Lq (f * ) ? R ? Lq (f )) ? c 1?q ? 1 q E D (1 ? ? yx ).</formula><p>Lastly, sincef is the minimizer of R ? Lq (f ), we have that R ? Lq (f * ) ? R ? Lq (f ) ? 0. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3.</head><p>For any x and q ? (0, 1), assuming 1/c ? k &lt; 1 where c represents the number of classes, the sum of truncated L q loss with respect to all classes is bounded by: wheref (x) = (p, ? ? ? , p, 0, ? ? ? , 0), with p = 1/d ? k and d is the number of elements in f (x) with a value ? k. Note that since p &gt; k, 1 ? d ? 1/k: We can get a universal lower bound (that does not depend on f ) by minimizing the above function with respect to d. To do so, we treat d to be continuous. By definition of L q loss, and recall that 0 &lt; q &lt; 1, min</p><formula xml:id="formula_34">dkL q ( 1 d ) + (c ?d)L q (k) ? c j=1 L trunc (f (x), e j ) ? cL q (k),<label>(18)</label></formula><formula xml:id="formula_35">d?[1,1/k] dL q ( 1 d ) + (c ? d)L q (k) = min d?[1,1/k] d[(1 ? ( 1 d ) q )/q ? (1 ? k q )/q] = min d?[1,1/k] d[(k q ? ( 1 d ) q )].</formula><p>We can verify using the second derivative test that the above objective function is convex. As a result, we can find the minimum by taking its derivative. Doing so, we find that d = (1?q) 1/q k minimizes the above objective function. Hence, the lower bound is Remark. Using Lemma 3, we can prove that the proposed truncated loss leads to more noise robust training following the same arguments as in Theorem 1 and 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a), (b) Test accuracy against number of epochs for training with CCE (orange) and MAE (blue) loss on clean data with (a) CIFAR-10 and (b) CIFAR-100 datasets. (c) Average softmax prediction for correctly (solid) and wrongly (dashed) labeled training samples, for CCE (orange) and L q (q = 0.7, blue) loss on CIFAR-10 with uniform noise (? = 0.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The test accuracy and validation loss against number of epochs for training with L q loss at different values of q. (a) and (d): ? = 0.0; (b) and (e): ? = 0.2; (c) and (f): ? = 0.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>whered = max( 1 ,</head><label>1</label><figDesc>For the upper bound, by definition of truncated L q , L trunc (f (x), e j ) ? L q (k) for any x and j. Hence, c j=1 L trunc (f (x), e j ) ? cL q (k). For the lower bound, it can be verified that, c j=1 L trunc (f (x), e j ) ? c j=1 L trunc (f (x), e j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>L</head><label></label><figDesc>trunc (f (x), e j ) = dL q (p) + (c ? d)L q (k) = dL q ( 1 d ) + (c ? d)L q (k).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>c ?d)L q (k) ? c j=1 L trunc (f (x), e j ), whered = max(1, (1?q) 1/q k ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? 0.18 62.64 ? 0.33 54.04 ? 0.56 29.60 ? 0.51 68.86 ? 0.14 66.59 ? 0.23 61.87 ? 0.39 47.66 ? 0.69</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Uniform Noise</cell><cell></cell><cell></cell><cell cols="2">Class Dependent Noise</cell></row><row><cell>Datasets</cell><cell>Loss Functions</cell><cell></cell><cell cols="2">Noise Rate ?</cell><cell></cell><cell></cell><cell cols="2">Noise Rate ?</cell></row><row><cell></cell><cell></cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell></row><row><cell></cell><cell>CCE</cell><cell>93.24 ? 0.12</cell><cell>92.09 ? 0.18</cell><cell>90.29 ? 0.35</cell><cell>86.20 ? 0.68</cell><cell>94.06 ? 0.05</cell><cell>93.72 ? 0.14</cell><cell>92.72 ? 0.21</cell><cell>89.82 ? 0.31</cell></row><row><cell></cell><cell>MAE</cell><cell>80.39 ? 4.68</cell><cell>79.30 ? 6.20</cell><cell>82.41 ? 5.29</cell><cell>74.73 ? 5.26</cell><cell>74.03 ? 6.32</cell><cell>63.03 ? 3.91</cell><cell>58.14 ? 0.14</cell><cell>56.04 ? 3.76</cell></row><row><cell>FASHION</cell><cell>Forward T</cell><cell cols="3">93.64 ? 0.12 92.69 ? 0.20 91.16 ? 0.16</cell><cell>87.59 ? 0.35</cell><cell cols="4">94.33 ? 0.10 94.03 ? 0.11 93.91 ? 0.14 93.65 ? 0.11</cell></row><row><cell>MNIST</cell><cell>ForwardT</cell><cell>93.26 ? 0.10</cell><cell>92.24 ? 0.15</cell><cell>90.54 ? 0.10</cell><cell>85.57 ? 0.86</cell><cell cols="3">94.09 ? 0.10 93.66 ? 0.09 93.52 ? 0.16</cell><cell>88.53 ? 4.81</cell></row><row><cell></cell><cell>Lq</cell><cell>93.35 ? 0.09</cell><cell>92.58 ? 0.11</cell><cell>91.30 ? 0.20</cell><cell>88.01 ? 0.22</cell><cell>93.51 ? 0.17</cell><cell>93.24 ? 0.14</cell><cell>92.21 ? 0.27</cell><cell>89.53 ? 0.53</cell></row><row><cell></cell><cell>Trunc Lq</cell><cell>93.21 ? 0.05</cell><cell cols="3">92.60 ? 0.17 91.56 ? 0.16 88.33 ? 0.38</cell><cell>93.53 ? 0.11</cell><cell>93.36 ? 0.07</cell><cell>92.76 ? 0.14</cell><cell>91.62 ? 0.34</cell></row><row><cell></cell><cell>CCE</cell><cell>86.98 ? 0.44</cell><cell>81.88 ? 0.29</cell><cell>74.14 ? 0.56</cell><cell>53.82 ? 1.04</cell><cell>90.69 ? 0.17</cell><cell>88.59 ? 0.34</cell><cell>86.14 ? 0.40</cell><cell>80.11 ?1.44</cell></row><row><cell></cell><cell>MAE</cell><cell>83.72 ? 3.84</cell><cell>67.00 ? 4.45</cell><cell>64.21 ? 5.28</cell><cell>38.63 ? 2.62</cell><cell>82.61 ? 4.81</cell><cell>52.93 ? 3.60</cell><cell>50.36 ? 5.55</cell><cell>45.52 ? 0.13</cell></row><row><cell>CIFAR-10</cell><cell>Forward T ForwardT</cell><cell>88.63 ? 0.14 87.99 ? 0.36</cell><cell>85.07 ? 0.29 83.25 ? 0.38</cell><cell>79.12 ? 0.64 74.96 ? 0.65</cell><cell cols="5">64.30 ? 0.70 91.32 ? 0.21 90.35 ? 0.26 89.25 ? 0.43 88.12 ? 0.32 54.64 ? 0.44 90.52 ? 0.26 89.09 ? 0.47 86.79 ? 0.36 83.55 ? 0.58</cell></row><row><cell></cell><cell>Lq</cell><cell cols="3">89.83 ? 0.20 87.13 ? 0.22 82.54 ? 0.23</cell><cell>64.07 ? 1.38</cell><cell>90.91 ? 0.22</cell><cell>89.33 ? 0.17</cell><cell>85.45 ? 0.74</cell><cell>76.74? 0.61</cell></row><row><cell></cell><cell>Trunc Lq</cell><cell>89.7 ? 0.11</cell><cell cols="3">87.62 ? 0.26 82.70 ? 0.23 67.92 ? 0.60</cell><cell>90.43 ? 0.25</cell><cell cols="2">89.45 ? 0.29 87.10 ? 0.22</cell><cell>82.28 ? 0.67</cell></row><row><cell></cell><cell>CCE</cell><cell>58.72 ? 0.26</cell><cell>48.20 ? 0.65</cell><cell>37.41 ? 0.94</cell><cell>18.10 ? 0.82</cell><cell>66.54 ? 0.42</cell><cell>59.20 ? 0.18</cell><cell>51.40 ? 0.16</cell><cell>42.74 ? 0.61</cell></row><row><cell></cell><cell>MAE</cell><cell>15.80 ? 1.38</cell><cell>9.03 ? 1.54</cell><cell>7.74 ? 1.48</cell><cell>3.76 ? 0.27</cell><cell>13.38 ? 1.84</cell><cell>11.50 ? 1.16</cell><cell>8.91 ? 0.89</cell><cell>8.20 ? 1.04</cell></row><row><cell>CIFAR-100</cell><cell>Forward T ForwardT</cell><cell>63.16 ? 0.37 39.19 ? 2.61</cell><cell>54.65 ? 0.88 31.05 ? 1.44</cell><cell>44.62 ? 0.82 19.12 ? 1.95</cell><cell>24.83 ? 0.71 8.99 ? 0.58</cell><cell cols="4">71.05 ? 0.30 71.08 ? 0.22 70.76 ? 0.26 70.82 ? 0.45 45.96 ? 1.21 42.46 ? 2.16 38.13 ? 2.97 34.44 ? 1.93</cell></row><row><cell></cell><cell>Lq</cell><cell cols="4">66.81 ? 0.42 61.77 ? 0.24 53.16 ? 0.78 29.16 ? 0.74</cell><cell>68.36 ? 0.42</cell><cell>66.59 ? 0.22</cell><cell>61.45 ? 0.26</cell><cell>47.22 ? 1.15</cell></row><row><cell></cell><cell>Trunc Lq</cell><cell>67.61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average test accuracy on experiments with CIFAR-10. We replicated the exact experimental setup as in<ref type="bibr" target="#b39">[40]</ref>. The reported accuracies are the average last epoch accuracies after training for 100 epochs. ? = 40%. CCE, Forward and method by Wang et al. are adapted for direct comparison.</figDesc><table><row><cell>Noise type</cell><cell cols="4">CCE [40] Forward [40] Wang, et al. [40] MAE L q</cell><cell>Trunc L q</cell></row><row><cell>CIFAR-10 + CIFAR-100 (open-set noise)</cell><cell>62.92</cell><cell>64.18</cell><cell>79.28</cell><cell>75.06 71.10</cell><cell>79.55</cell></row><row><cell>CIFAR-10 (closed-set noise)</cell><cell>62.38</cell><cell>77.81</cell><cell>78.15</cell><cell>74.31 64.79</cell><cell>79.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>summarizes the results for open-set noise with CIFAR-10. Following Wang et al.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by NIH R01 grants (R01LM012719 and R01AG053949), the NSF NeuroNex grant 1707312, and NSF CAREER grant (1748377).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head> <ref type="bibr">Lemma 1</ref><p>. lim q?0 L q (f (x), e j ) = L C (f (x), e j ), where L q represents the L q loss, and L C represents the categorical cross entropy loss.</p><p>Proof. from equation 6, and using L'H?pital's rule,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2.</head><p>For any x and q ? (0, 1], the sum of L q loss with respect to all classes is bounded by:</p><p>Proof. Observe that, since we have a softmax layer at the end, f j (x) ? 1 for all j, and</p><p>Moreover, since (</p><p>and</p><p>where</p><p>Proof. Recall that for any softmax output f ,</p><p>and since for uniform noise with noise rate ?, ? jk = 1 ? ? for j = k, and ? jk = ? c?1 for j = k, we have</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05394</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Auxiliary image regularization for deep cnns with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07069</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Nonlinear programming: theory and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mokhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bazaraa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hanif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitharanjan M</forename><surname>Sherali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shetty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David R</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support vector machines with the ramp loss and the hard margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Paul</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum lq-likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="753" to="783" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Fr?nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="845" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1919" to="1925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making risk minimization tolerant to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="page" from="93" to="107" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08193</idno>
		<title level="m">Masking: A New Perspective of Noisy Supervision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05300</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05055</idno>
		<title level="m">Mentornet: Regularizing very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep networks from noisy labels with dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Nokleby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="967" to="972" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning from noisy singly-labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04577</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02391</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Noise tolerance under risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the design of loss functions for classification: theory, robustness to outliers, and savageboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Masnadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1049" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning with confident examples: Rank pruning for robust classification with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tailin</forename><surname>Curtis G Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01936</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: a loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09050</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11364</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5601" to="5610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00092</idno>
		<title level="m">Iterative learning with open-set noisy labels</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
