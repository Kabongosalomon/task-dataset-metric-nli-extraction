<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VIDT: AN EFFICIENT AND EFFECTIVE FULLY TRANSFORMER-BASED OBJECT DETECTOR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
							<email>hwanjun.song@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
							<email>sanghyuk.c@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
							<email>varunjampani@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
							<email>dongyoon.han@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
							<email>bh.heo@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
							<email>wonjae.kim@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>mhyang@ucmerced.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of California at Merced</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VIDT: AN EFFICIENT AND EFFECTIVE FULLY TRANSFORMER-BASED OBJECT DETECTOR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We will release the code and trained models at https://github.com/naver-ai/vidt. 1  We refer to each model based on the combinations of its body and neck. For example, DETR (DeiT) indicates that DeiT (vision transformers) is integrated with DETR (detection transformers).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Object detection is the task of predicting both bounding boxes and object classes for each object of interest in an image. Modern deep object detectors heavily rely on meticulously designed components, such as anchor generation and non-maximum suppression <ref type="bibr" target="#b13">(Papageorgiou &amp; Poggio, 2000;</ref><ref type="bibr" target="#b10">Liu et al., 2020)</ref>. As a result, the performance of these object detectors depend on specific postprocessing steps, which involve complex pipelines and make fully end-to-end training difficult. Motivated by the recent success of Transformers <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref> in NLP, numerous studies introduce Transformers into computer vision tasks. <ref type="bibr" target="#b0">Carion et al. (2020)</ref> proposed Detection Transformers (DETR) to eliminate the meticulously designed components by employing a simple transformer encoder and decoder architecture, which serves as a neck component to bridge a CNN body for feature extraction and a detector head for prediction. Thus, DETR enables end-to-end training of deep object detectors. By contrast, <ref type="bibr">Dosovitskiy et al. (2021)</ref> showed that a fully-transformer  <ref type="table">Table 2</ref>. The text in the plot indicates the backbone model size.</p><p>backbone without any convolutional layers, Vision Transformer (ViT), achieves the state-of-theart results in image classification benchmarks. Approaches like ViT have been shown to learn effective representation models without strong human inductive biases, e.g., meticulously designed components in object detection (DETR), locality-aware designs such as convolutional layers and pooling mechanisms. However, there is a lack of effort to synergize DETR and ViT for a better object detection architecture. In this paper, we integrate both approaches to build a fully transformer-based, end-to-end object detector that achieves state-of-the-art performance without increasing computational load.</p><p>A straightforward integration of DETR and ViT can be achieved by replacing the ResNet backbone  (body) of DETR with ViT - <ref type="figure" target="#fig_2">Figure 2</ref>(a). This naive integration, DETR (ViT) 1 , has two limitations. First, the canonical ViT suffers from the quadratic increase in complexity w.r.t. image size, resulting in the lack of scalability. Furthermore, the attention operation at the transformer encoder and decoder (i.e., the "neck" component) adds significant computational overhead to the detector. Therefore, the naive integration of DETR and ViT show very high latency -the blue lines of <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Recently, <ref type="bibr" target="#b5">Fang et al. (2021)</ref> propose an extension of ViT to object detection, named YOLOS, by appending the detection tokens [DET] to the patch tokens [PATCH] <ref type="figure" target="#fig_2">(Figure 2(b)</ref>), where [DET] tokens are learnable embeddings to specify different objects to detect. YOLOS is a neck-free architecture and removes the additional computational costs from the neck encoder. However, YOLOS shows limited performance because it cannot use additional optimization techniques on the neck architecture, e.g., multi-scale features and auxiliary loss. In addition, YOLOS can only accommodate the canonical transformer due to its architectural limitation, resulting in a quadratic complexity w.r.t. the input size.</p><p>In this paper, we propose a novel integration of Vision and Detection Transformers (ViDT) ( <ref type="figure" target="#fig_2">Figure  2(c)</ref>). Our contributions are three-folds. First, ViDT introduces a modified attention mechanism, named Reconfigured Attention Module (RAM), that facilitates any ViT variant to handle the appended [DET] and [PATCH] tokens for object detection. Thus, we can modify the latest Swin Transformer  backbone with RAM to be an object detector and obtain high scalability using its local attention mechanism with linear complexity. Second, ViDT adopts a lightweight encoder-free neck architecture to reduce the computational overhead while still enabling the additional optimization techniques on the neck module. Note that the neck encoder is unnecessary because RAM directly extracts fine-grained representation for object detection, i.e., <ref type="bibr">[DET]</ref> tokens. As a result, ViDT obtains better performance than neck-free counterparts. Finally, we introduce a new concept of token matching for knowledge distillation, which brings additional performance gains from a large model to a small model without compromising detection efficiency.</p><p>ViDT has two architectural advantages over existing approaches. First, similar to YOLOS, ViDT takes [DET] tokens as the additional input, maintaining a fixed scale for object detection, but constructs hierarchical representations starting with small-sized image patches for [PATCH] tokens. Second, ViDT can use the hierarchical (multi-scale) features and additional techniques without a significant computation overhead. Therefore, as a fully transformer-based object detector, ViDT facilitates better integration of vision and detection transformers. Extensive experiments on Microsoft COCO benchmark <ref type="bibr" target="#b8">(Lin et al., 2014)</ref> show that ViDT is highly scalable even for large ViT models, such as Swin-base with 0.1 billion parameters, and achieves the best AP and latency trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Vision transformers process an image as a sequence of small-sized image patches, thereby allowing all the positions in the image to interact in attention operations (i.e., global attention). However, the canonical ViT <ref type="bibr">(Dosovitskiy et al., 2021)</ref> is not compatible with a broad range of vision tasks due to its high computational complexity, which increases quadratically with respect to image size. The Swin Transformer  resolves the complexity issue by introducing the notion of shifted windows that support local attention and patch reduction operations, thereby improving compatibility for dense prediction task such as object detection. A few approaches use vision transformers as detector backbones but achieve limited success <ref type="bibr">(Heo et al., 2021;</ref><ref type="bibr" target="#b5">Fang et al., 2021)</ref>.</p><p>Detection transformers eliminate the meticulously designed components (e.g., anchor generation and non-maximum suppression) by combining convolutional network backbones and Transformer encoder-decoders. While the canonical DETR <ref type="bibr" target="#b0">(Carion et al., 2020)</ref> achieves high detection performance, it suffers from very slow convergence compared to previous detectors. For example, DETR requires 500 epochs while the conventional Faster R- <ref type="bibr">CNN (Ren et al., 2015)</ref> training needs only 37 epochs <ref type="bibr">(Wu et al., 2019)</ref>. To mitigate the issue, <ref type="bibr">Zhu et al. (2021)</ref> propose Deformable DETR which introduces deformable attention for utilizing multi-scale features as well as expediting the slow training convergence of DETR. In this paper, we use the Deformable DETR as our base detection transformer framework and integrate it with the recent vision transformers. DETR (ViT) is a straightforward integration of DETR and ViT, which uses ViT as a feature extractor, followed by the transformer encoder-decoder in DETR. As illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(a), it is a body-neck-head structure; the representation of input [PATCH] tokens are extracted by the ViT backbone and then directly fed to the transformer-based encoding and decoding pipeline. To predict multiple objects, a fixed number of learnable [DET] tokens are provided as additional input to the decoder. Subsequently, output embeddings by the decoder produce final predictions through the detection heads for classification and box regression. Since DETR (ViT) does not modify the backbone at all, it can be flexibly changed to any latest ViT model, e.g., Swin Transformer. Additionally, its neck decoder facilitates the aggregation of multi-scale features and the use of additional techniques, which help detect objects of different sizes and speed up training <ref type="bibr">(Zhu et al., 2021)</ref>. However, the attention operation at the neck encoder adds significant computational overhead to the detector. In contrast, ViDT resolves this issue by directly extracting fine-grained [DET] features from Swin Transformer with RAM without maintaining the transformer encoder in the neck architecture.</p><p>YOLOS <ref type="bibr" target="#b5">(Fang et al., 2021</ref>) is a canonical ViT architecture for object detection with minimal modifications. As illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>(b), YOLOS achieves a neck-free structure by appending randomly initialized learnable [DET] tokens to the sequence of input [PATCH] tokens. Since all the embeddings for [PATCH] and [DET] tokens interact via global attention, the final [DET] tokens are generated by the fine-tuned ViT backbone and then directly generate predictions through the detection heads without requiring any neck layer. While the naive DETR (ViT) suffers from the computational overhead from the neck layer, YOLOS enjoys efficient computations by treating the [DET] tokens as additional input for ViT. YOLOS shows that 2D object detection can be accomplished in a pure sequence-to-sequence manner, but this solution entails two inherent limitations: 1) YOLOS inherits the drawback of the canonical ViT; the high computational complexity attributed to the global attention operation. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, YOLOS shows very poor latency compared with other fully transformer-based detectors, especially when its model size becomes larger, i.e., small ? base. Thus, YOLOS is not scalable for the large model.</p><p>2) YOLOS cannot benefit from using additional techniques essential for better performance, e.g., multi-scale features, due to the absence of the neck layer. Although YOLOS used the same DeiT backbone with Deformable DETR (DeiT), its AP was lower than the straightforward integration.</p><p>In contrast, the encoder-free neck architecture of ViDT enjoys the additional optimization techniques from <ref type="bibr">Zhu et al. (2021)</ref>, resulting in the faster convergence and the better performance. Further, our RAM enables to combine Swin Transformer and the sequence-to-sequence paradigm for detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIDT: VISION AND DETECTION TRANSFORMERS</head><p>ViDT first reconfigures the attention model of Swin Transformer to support standalone object detection while fully reusing the parameters of Swin Transformer. Next, it incorporates an encoder-free neck layer to exploit multi-scale features and two essential techniques: auxiliary decoding loss and iterative box refinement. We further introduce knowledge distillation with token matching to benefit from large ViDT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RECONFIGURED ATTENTION MODULE</head><p>Applying patch reduction and local attention scheme of Swin Transformer to the sequence-tosequence paradigm is challenging because (1) the number of [DET] tokens must be maintained at a fixed-scale and (2) the lack of locality between [DET] tokens. To address this challenge, we introduce a reconfigured attention module (RAM) 2 that decomposes a single global attention associated with  [ tokens are performed for the remaining stages except the last one. In Section 4.2.2 we show that this design choice helps achieve the highest FPS, while achieving similar detection performance as when cross-attention is enabled at every stage. We provide details on RAM including its complexity analysis and algorithmic design in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ENCODER-FREE NECK STRUCTURE</head><p>To exploit multi-scale feature maps, ViDT incorporates a decoder of multi-layer deformable transformers <ref type="bibr">(Zhu et al., 2021)</ref>. In the DETR family <ref type="figure" target="#fig_2">(Figure 2(a)</ref>), a transformer encoder is required at the neck to transform features extracted from the backbone for image classification into the ones suitable for object detection; the encoder is generally computationally expensive since it involves [PATCH] ? [PATCH] attention. However, ViDT maintains only a transformer decoder as its neck, in that Swin Transformer with RAM directly extracts fine-grained features suitable for object detection as a standalone object detector. Thus, the neck structure of ViDT is computationally efficient.</p><p>The decoder receives two inputs from Swin Transformer with RAM: (1) [PATCH] tokens generated from each stage (i.e., four multi-scale feature maps, {x l } L l=1 where L = 4) and <ref type="formula" target="#formula_1">(2)</ref>  </p><formula xml:id="formula_0">l } L l=1 , MSDeformAttn([DET], {x l } L l=1 ) = M m=1 Wm L l=1 K k=1 A mlk ? W m x l ? l (p) + ?p mlk ,<label>(1)</label></formula><p>where m indices the attention head and K is the total number of sampled keys for content aggregation. In addition, ? l (p) is the reference point of the [DET] token re-scaled for the l-th level feature map, while ?p mlk is the sampling offset for deformable attention; and A mlk is the attention weights of the K sampled contents. W m and W m are the projection matrices for multi-head attention.</p><p>Auxiliary Techniques for Additional Improvements. The decoder of ViDT follows the standard structure of multi-layer transformers, generating refined [DET] tokens at each layer. Hence, ViDT leverages the two auxiliary techniques used in (Deformable) DETR for additional improvements:</p><p>? Auxiliary Decoding Loss: Detection heads consisting of two feedforward networks (FNNs) for box regression and classification are attached to every decoding layer. All the training losses from detection heads at different scales are added to train the model. This helps the model output the correct number of objects without non-maximum suppression <ref type="bibr" target="#b0">(Carion et al., 2020</ref>). ? Iterative Box Refinement: Each decoding layer refines the bounding boxes based on predictions from the detection head in the previous layer. Therefore, the box regression process progressively improves through the decoding layers (Zhu et al., 2021). These two techniques are essential for transformer-based object detectors because they significantly enhance detection performance without compromising detection efficiency. We provide an ablation study of their effectiveness for object detection in Section 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">KNOWLEDGE DISTILLATION WITH TOKEN MATCHING FOR OBJECT DETECTION</head><p>While a large model has a high capacity to achieve high performance, it can be computationally expensive for practical use. As such, we additionally present a simple knowledge distillation approach that can transfer knowledge from the large ViDT model by token matching. Based on the fact that all ViDT models has exactly the same number of [PATCH] and [DET] tokens regardless of their scale, a small ViDT model (a student model) can easily benefit from a pre-trained large ViDT (a teacher model) by matching its tokens with those of the large one, thereby bringing out higher detection performance at a lower computational cost.</p><p>Matching all the tokens at every layer is very inefficient in training. Thus, we only match the tokens contributing the most to prediction. The two sets of tokens are directly related: (1) P: the set of [PATCH] tokens used as multi-scale feature maps, which are generated from each stage in the body, and (2) D: the set of [DET] tokens, which are generated from each decoding layer in the neck. Accordingly, the distillation loss based on token matching is formulated by  <ref type="table">Table 1</ref>. Summary on the ViT backbone. "C" is the distillation strategy for classification <ref type="bibr" target="#b16">(Touvron et al., 2021)</ref>.</p><formula xml:id="formula_1">dis (Ps, Ds, Pt, Dt) = ? dis 1 |Ps| |Ps| i=1 Ps[i] ? Pt[i] 2 + 1 |Ds| |Ds| i=1 Ds[i] ? Dt[i] 2 ,<label>(2)</label></formula><p>where the subscripts s and t refer to the student and teacher model.</p><formula xml:id="formula_2">P[i] and D[i] return the i-th [PATCH]</formula><p>and [DET] tokens, n-dimensional vectors, belonging to P and D, respectively. ? dis is the coefficient to determine the strength of dis , which is added to the detection loss if activated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In this section, we show that ViDT achieves the best trade-off between accuracy and speed (Section 4.1). Then, we conduct detailed ablation study of the reconfigured attention module (Section 4.2) and additional techniques to boost detection performance (Section 4.3). Finally, we provide a complete analysis of all components available for ViDT (Section 4.4).</p><p>Dataset. We carry out object detection experiments on the Microsoft COCO 2017 benchmark dataset <ref type="bibr" target="#b8">(Lin et al., 2014)</ref>. All the fully transformer-based object detectors are trained on 118K training images and tested on 5K validation images following the literature <ref type="bibr" target="#b0">(Carion et al., 2020)</ref>. Implementation Details. All the algorithms are implemented using PyTorch and executed using eight NVIDIA Tesla V100 GPUs. We train ViDT using AdamW <ref type="bibr" target="#b12">(Loshchilov &amp; Hutter, 2019)</ref> with the same initial learning rate of 10 ?4 for its body, neck and head. In contrast, following the (Deformable) DETR setting, DETR (ViT) is trained with the initial learning rate of 10 ?5 for its pretrained body (ViT backbone) and 10 ?4 for its neck and head. YOLOS and ViDT (w.o. Neck) are trained with the same initial learning rate of 5?10 ?5 , which is the original setting of YOLOS for the neck-free detector. We do not change any hyperparameters used in transformer encoder and decoder for (Deformable) DETR; thus, the neck decoder of ViDT also consists of six deformable transformer layers using exactly the same hyperparameters. The only new hyperparameter introduced, the distillation coefficient ? dis in Eq. <ref type="formula" target="#formula_1">(2)</ref>, is set to be 4. For fair comparison, knowledge distillation is not applied for ViDT in the main experiment in Section 4.1. The efficacy of knowledge distillation with token matching is verified independently in Section 4.3.2. Auxiliary decoding loss and iterative box refinement are applied to the compared methods if applicable.</p><p>Regarding the resolution of input images, we use scale augmentation that resizes them such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333 <ref type="bibr">(Wu et al., 2019)</ref>. More details of the experiment configuration can be found in Appendix B.3-B.5. All the source code and trained models will be made available to the public. One might argue that better integration could be also achieved by (1) Deformable DETR without its neck encoder because its neck decoder also has [DET] ? [PATCH] cross-attention, or (2) YOLOS with VIDT's neck decoder because of the use of multiple auxiliary techniques. Such integration is actually not effective; the former significantly drops AP, while the latter has a much greater drop in FPS than an increase in AP. The detailed analysis can be found in Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MAIN EXPERIMENTS WITH MICROSOFT COCO BENCHMARK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ABLATION STUDY ON RECONFIGURED ATTENTION MODULE (RAM)</head><p>We extend Swin Transformer with RAM to extract fine-grained features for object detection without maintaining an additional transformer encoder in the neck. We provide an ablation study on the two main considerations for RAM, which leads to high accuracy and speed. To reduce the influence of secondary factors, we mainly use our neck-free version, ViDT (w.o. Neck), for the ablation study. Spatial positional encoding is essential for [DET] ? [PATCH] attention in RAM. Typically, the spatial encoding can be added to the [PATCH] tokens before or after the projection layer in <ref type="figure" target="#fig_3">Figure 3</ref>. We call the former "pre-addition" and the latter "postaddition". For each one, we can design the encoding in a sinusoidal or learnable manner <ref type="bibr" target="#b0">(Carion et al., 2020)</ref>. <ref type="table">Table 3</ref> contrasts the results with different spatial positional encodings with ViDT (w.o. Neck). Overall, pre-addition results in performance improvement higher than post-addition, and specifically, the sinusoidal encoding is better than the learnable one; thus, the 2D inductive bias of the sinusoidal spatial encoding is more helpful in object detection. In particular, pre-addition with the sinusoidal encoding increases AP by 5.0 compared to not using any encoding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION STUDY ON ADDITIONAL TECHNIQUES</head><p>We analyze the performance improvement of two additional techniques, namely auxiliary decoding loss and iterative box refinement, and the proposed distillation approach in Section 3.3. Furthermore, we introduce a simple technique that can expedite the inference speed of ViDT by dropping unnecessary decoding layers at inference time. To thoroughly verify the efficacy of auxiliary decoding loss and iterative box refinement, we extend them even for the neck-free detector like YOLOS; the principle of them is applied to the encoding layers in the body, as opposed to the conventional way of using the decoding layers in the neck. <ref type="table" target="#tab_7">Table 5</ref> shows the performance of the two neck-free detectors, YOLOS and ViDT (w.o. Neck), decreases considerably with the two techniques. The use of them in the encoding layers is likely to negatively affect feature extraction of the transformer encoder. In contrast, an opposite trend is observed with the neck component. Since the neck decoder is decoupled with the feature extraction in the body, the two techniques make a synergistic effect and thus show significant improvement in AP. These results justify the use of the neck decoder in ViDT to boost object detection performance.  <ref type="table">Table 6</ref>. AP comparison of student models associated with different teacher models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">AUXILIARY DECODING LOSS AND ITERATIVE BOX REFINEMENT</head><p>We show that a small ViDT model can benefit from a large ViDT model via knowledge distillation. The proposed token matching is a new concept of knowledge distillation for object detection, especially for a fully transformer-based object detector. Compared to very complex distillation methods that rely on heuristic rules with multiple hyperparameters <ref type="bibr" target="#b2">(Chen et al., 2017;</ref><ref type="bibr" target="#b3">Dai et al., 2021)</ref>, it simply matches some tokens with a single hyperparameter, the distillation coefficient ? dis . <ref type="table">Table 6</ref> summarizes the AP improvement via knowledge distillation with token matching with varying distillation coefficients. Overall, the larger the size of the teacher model, the greater gain to the student model. Regarding coefficients, in general, larger values achieve better performance. Distillation increases AP by 1.0-1.7 without affecting the inference speed of the student model. ViDT has six layers of transformers as its neck decoder. We emphasize that not all layers of the decoder are required at inference time for high performance. <ref type="table">Table 7</ref> show the performance of ViDT when dropping its decoding layer one by one from the top in the inference step. Although there is a trade-off relationship between accuracy and speed as the layers are detached from the model, there is no significant AP drop even when the two layers are removed. This technique is not designed for performance evaluation in <ref type="table">Table 2</ref> with other methods, but we can accelerate the inference speed of a trained ViDT model to over 10% by dropping its two decoding layers without a much decrease in AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">DECODING LAYER DROP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">COMPLETE COMPONENT ANALYSIS</head><p>In this section, we combine all the proposed components (even with distillation and decoding layer drop) to achieve high accuracy and speed for object detection. As summarized in <ref type="table">Table 8</ref>, there are four components: (1) RAM to extend Swin Transformer as a standalone object detector, (2) the neck decoder to exploit multi-scale features with two auxiliary techniques, (3) knowledge distillation to benefit from a large model, and <ref type="formula" target="#formula_6">(4)</ref>   </p><formula xml:id="formula_3">O(d 2 T1 + dT 2 1 ) O(d 2 (T1 + T2) + dT1T2) O(d 2 T1 + dk 2 T1)</formula><formula xml:id="formula_4">(d 2 T1 + dT 2 1 ), where O(d 2 T1)</formula><p>is the cost of computing the query, key, and value embeddings and O(dT 2 1 ) is the cost of computing the attention weights. The complexity of global cross-attention is O(d 2 (T1 + T2) + dT1T2), which is the interaction between the two different tokens T1 and T2. In contrast, Swin Transformer achieves much lower attention complexity of O(d 2 T1 + dk 2 T1) with window partitioning, where k is the width and height of the window (k &lt;&lt; T1, T2).</p><p>Let P and D be the number of [PATCH] and [DET] tokens (D &lt;&lt; P in practice, e.g., P = 66, 650 and D = 100 at the first stage of ViDT). Then, the computational complexity of the attention module for YOLOS and ViDT (RAM) is derived as below, also summarized in <ref type="table" target="#tab_12">Table 10</ref>: Consequently, the complexity of RAM is much lower than the attention module used in YOLOS since D &lt;&lt; P. Note that only RAM achieves the linear complexity to the patch tokens. In addition, one might argue that YOLOS can be efficient if the cross-attention is selectively removed similar to RAM. Even if we remove the complexity O(dPD) for the global cross-attention, the computational complexity is O(d 2 (P + D) + dP 2 + dD 2 ), which is still quadratic to the number of [PATCH] tokens.</p><formula xml:id="formula_5">?</formula><p>Attention Type YOLOS ViDT  </p><formula xml:id="formula_6">[PATCH] ? [PATCH] O(d 2 P + dP 2 ) O(d 2 P + dk 2 P) [DET] ? [DET] O(d 2 D + dD 2 ) O(d 2 D + dD 2 ) [DET] ? [PATCH] O<label>(</label></formula><formula xml:id="formula_7">[DET] new = Softmax [DET] Q [DET] K , [PATCH] K ? d ) [DET] V , [PATCH] V .<label>(3)</label></formula><p>This approach is commonly used in the recent Transformer-based architectures, such as YOLOS. Due to the absence of Swin models comparable to Deit-tiny, we configure Swin-nano, which is a 0.25? model of Swin-tiny such that it has 6M training parameters comparable to Deit-tiny. <ref type="table">Table 11</ref> summarizes the configuration of Swin Transformer models available, including the newly introduced Swinnano; S1-S4 indicates the four stages in Swin Transformer. The performance of all the pre-trained Swin Transformer models are summarized in <ref type="table">Table 1</ref> in the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 DETECTION PIPELINES OF ALL COMPARED DETECTORS</head><p>All the compared fully transformer-based detectors are composed of either (1) body-neck-head or (2) body-head structure, as summarized in We believe that our proposed RAM can be combined with even other latest efficient vision transformer architectures, such as PiT <ref type="bibr">(Heo et al., 2021)</ref>, <ref type="bibr">PVT (Wang et al., 2021)</ref> and Cross-ViT <ref type="bibr" target="#b1">(Chen et al., 2021)</ref>. We leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 HYPERPARAMETERS OF NECK TRANSFORMERS</head><p>The transformer decoder at the neck in ViDT introduces multiple hyperparameters. We follow exactly the same setting used in Deformable DETR. Specifically, we use six layers of deformable </p><formula xml:id="formula_8">B = FFN 3-layer [DET] andP = Linear [DET] .<label>(4)</label></formula><p>For box regression, the FFNs produce the bounding box coordinates for d objects,B ? [0, 1] d?4 , that encodes the normalized box center coordinates along with its width and height. For classification, the linear projection uses a softmax function to produce the classification probabilities for all possible classes including the background class,P ? [0, 1] d?(c+1) , where c is the number of object classes. When deformable attention is used on the neck in <ref type="table" target="#tab_14">Table 12</ref>, only c classes are considered without the background class for classification. This is the original setting used in DETR, YOLOS <ref type="bibr" target="#b0">(Carion et al., 2020;</ref><ref type="bibr" target="#b5">Fang et al., 2021)</ref> and <ref type="bibr">Deformable DETR (Zhu et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.2 LOSS FUNCTION FOR TRAINING</head><p>All the methods adopts the loss function of (Deformable) DETR. Since the detection head return a fixed-size set of d bounding boxes, where d is usually larger than the number of actual objects in an image, Hungarian matching is used to find a bipartite matching between the predicted boxB and the ground-truth box B. In total, there are three types of training loss: a classification loss cl 5 , a box distance l1 , and a GIoU loss iou <ref type="bibr" target="#b15">(Rezatofighi et al., 2019)</ref>,</p><formula xml:id="formula_9">cl (i) = ?logP ?(i),ci , 1 (i) = ||B i ?B ?(i) || 1 , and iou (i) = 1? |B i ?B ?(i) | |B i ?B ?(i) | ? |B(B i ,B ?(i) )\B i ?B ?(i) | |B(B i ,B ?(i) )| ,<label>(5)</label></formula><p>where c i and ?(i) are the target class label and bipartite assignment of the i-th ground-truth box, and B returns the largest box containing two given boxes. Thus, the final loss of object detection is a linear combination of the three types of training loss, = ? cl cl + ? 1 l 1 + ? iou iou . The coefficient for each training loss is set to be ? cl = 1, ? 1 = 5, and ? iou = 2. If we leverage auxiliary decoding loss, the final loss is computed for every detection head separately and merged with equal importance. Additionally, ViDT adds the distillation loss in Eq.</p><p>(2) to the final loss if the distillation approach in Section 3.3 is enabled for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 TRAINING CONFIGURATION</head><p>We train ViDT for 50 epochs using AdamW <ref type="bibr" target="#b12">(Loshchilov &amp; Hutter, 2019)</ref> with the same initial learning rate of 10 ?4 for its body, neck and head. The learning rate is decayed by cosine annealing with batch size of 16, weight decay of 1?10 ?4 , and gradient clipping of 0.1. In contrast, ViDT (w.o. Neck) is trained for 150 epochs using AdamW with the initial learning rate of 5 ? 10 ?5 by cosine annealing. The remaining configuration is the same as for ViDT.</p><p>Regarding DETR (ViT), we follow the setting of Deformable DETR. Thus, all the variants of this pipeline are trained for 50 epochs with the initial learning rate of 10 ?5 for its pre-trained body (ViT backbone) and 10 ?4 for its neck and head. Their learning rates are decayed at the 40-th epoch by a factor of 0.1. Meanwhile, the results of YOLOS are borrowed from the original paper <ref type="bibr" target="#b5">(Fang et al., 2021)</ref>    loss and iterative box refinement. Note that these modified versions follow exactly the same detection pipeline with ViDT, maintaining a encoder-free neck between their body and head. <ref type="table" target="#tab_6">Table 14</ref> summarizes the performance of all the variations in terms of AP, FPS, and the number of parameters.</p><p>Deformable DETR shows significant improvement in FPS (+14.4) but its AP drops sharply (?9.1) when its neck encoder is removed.</p><p>Thus, it is difficult to obtain fine-grained object detection representation directly from the raw ViT backbone without using an additional neck encoder. However, ViDT compensates for the effect of the neck encoder by adding [DET] tokens into the body (backbone), thus successfully removing the computational bottleneck without compromising AP; it maintains 6.4 higher AP compared with the neck encoder-free Deformable DETR (the second row) while achieving similar FPS. This can be attributed to that RAM has a great contribution to the performance w.r.t AP and FPS, especially for the trade-off between them.</p><p>YOLOS shows a significant gain in AP (+7.7) while losing FPS (?11.0) when the neck decoder is added. Unlike Deformable DETR, its AP significantly increases even without the neck encoder due to the use of a standalone object detector as its backbone (i.e., the modified DeiT in <ref type="figure" target="#fig_2">Figure  2(b)</ref>). However, its AP is lower than ViDT by 2.3AP. Even worse, it is not scalable for large models because of its quadratic computational cost for attention. Therefore, in the aspects of accuracy and speed, ViDT maintains its dominance compared with the two carefully tuned baselines.</p><p>For a complete analysis, we additionally add a neck encoder to ViDT. The inference speed of ViDT degrades drastically by 13.7 because of the self-attention for multi-scale features at the neck encoder. However, it is interesting to see the improvement of AP by 5.7 while adding only 3M parameters; it is 3.0 higher even than Deformable DETR. This indicates that lowering the computational complexity of the encoder and thus increasing its utilization could be another possible direction for a fully transformer-based object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 [DET] ? [PATCH] ATTENTION IN RAM</head><p>In Section 4.2.2, it turns out that the cross-attention in RAM is only necessary at the last stage of Swin Transformer; all the different selective strategies show similar AP as long as cross-attention is activated at the last stage. Hence, we analyze the attention map obtained by the cross-attention in RAM. <ref type="figure" target="#fig_8">Figure 4</ref> shows attention maps for the stages of Swin Transformer where cross-attention is utilized; it contrasts (a) ViDT with cross-attention at all stages and (b) ViDT with cross-attention at the last stage. Regardless of the use of cross-attention at the lower stage, it is noteworthy that the finally obtained attention map at the last stage is almost the same. In particular, the attention map at Stage 1-3 does not properly focus the features on the target object, which is framed by the bounding box. In addition, the attention weights (color intensity) at Stage 1-3 are much lower than those at Stage 4. Since features are extracted from a low level to a high level in a bottom-up manner as they go through the stages, it seems difficult to directly get information about the target object with such low-level features at the lower level of stages. Therefore, this analysis provides strong empirical evidence for the use of selective  <ref type="table" target="#tab_7">Table 15</ref>. When all the [DET] ? [DET] self-attention are removed, (5) the AP drops by 0.7, which is a meaningful performance degradation. On the other hand, as long as the selfattention is activated at the last two stages, (1) -(3) all the strategies exhibit similar AP. Therefore, only keeping [DET] ? [DET] self-attention at the last two stages can further increase FPS (+0.2) without degradation in AP. This observation could be used as another design choice for the AP and FPS trade-off. Therefore, we believe that [DET] ? [DET] self-attention is meaningful to use in RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PRELIMINARIES: TRANSFORMERS</head><p>A transformer is a deep model that entirely relies on the self-attention mechanism for machine translation <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref>. In this section, we briefly revisit the standard form of the transformer.</p><p>Single-head Attention. The basic building block of the transformer is a self-attention module, which generates a weighted sum of the values (contents), where the weight assigned to each value is the attention score computed by the scaled dot-product between its query and key. Let W Q , W K , and W V be the learned projection matrices of the attention module, and then the output is generated by</p><formula xml:id="formula_11">Attention(Z) = softmax (ZW Q )(ZW K ) ? d (ZW V ) ? R hw?d , where W Q , W K , W V ? R d?d .<label>(7)</label></formula><p>Multi-head Attention. It is beneficial to maintain multiple heads such that they repeat the linear projection process k times with different learned projection matrices. Let W Qi , W Ki , and W Vi be the learned projection matrices of the i-th attention head. Then, the output is generated by the concatenation of the results from all heads, Multi-Head(Z) = [Attention 1 (Z), Attention 2 (Z), . . . , Attention k (Z)] ? R hw?d ,</p><p>where ? i W Qi , W Ki , W Vi ? R d?(d/k) .</p><p>Typically, the dimension of each head is divided by the total number of heads.</p><p>Feed-Forward Networks (FFNs). The output of the multi-head attention is fed to the point-wise FFNs, which performs the linear transformation for each position separately and identically to allow the model focusing on the contents of different representation subspaces. Here, the residual connec-tion and layer normalization are applied before and after the FFNs. The final output is generated by</p><formula xml:id="formula_13">H = LayerNorm(Dropout(H ) + H ),</formula><p>where H = FFN(H ) and H = LayerNorm(Dropout(Multi-Head(Z)) + Z).</p><p>Multi-Layer Transformers. The output of a previous layer is fed directly to the input of the next layer. Regarding the positional encoding, the same value is added to the input of each attention module for all layers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>AP and latency (milliseconds) summarized in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Pipelines of fully transformer-based object detectors. DETR (ViT) means Detection Transformer that uses ViT as its body. The proposed ViDT synergizes DETR (ViT) and YOLOS and achieves best AP and latency trade-off among fully transformer-based object detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Reconfigured Attention Module (Q: query, K: key, V: value). The skip connection and feedforward networks following the attention operation is omitted just for ease of exposition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>[DET] tokens generated from the last stage. The overview is illustrated in "Neck" of Figure 2(c). In each deformable transformer layer, [DET] ? [DET] attention is performed first. For each [DET] token, multi-scale deformable attention is applied to produce a new [DET] token, aggregating a small set of key contents sampled from the multi-scale feature maps {x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>YOLOS Attention:[DET]  tokens are simply appended to[PATCH]  tokens to perform global selfattention on [PATCH, DET] tokens (i.e., T 1 = P + D). Thus, the computational complexity is O(d 2 (P + D) + d(P + D) 2 ), which is quadratic to the number of [PATCH] tokens. If breaking down the total complexity, we obtain O (d 2 P + dP 2 ) + (d 2 D + dD 2 ) + dPD , where the first and second terms are for the global self-attention for [PATCH] and [DET] tokens, respectively, and the last term is for the global cross-attention between them.? ViDT (RAM) Attention: RAM performs the three different attention operations: (1) [PATCH] ? [PATCH] local self-attention with window partition, O(d 2 P + dk 2 P); (2) [DET] ? [DET] global selfattention, O(d 2 D + dD 2 ); (3) [DET] ? [PATCH] global cross-attention, O(d 2 (D + P) + dDP). In total, the computational complexity of RAM is O(d 2 (D + P) + dk 2 P + dD 2 + dDP), which is linear to the number of [PATCH] tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>dPD) O(d 2 (D + P) + dDP) Total Complexity O(d 2 (P + D)+ d(P + D) 2 ) O(d 2 (D + P) + dk 2 P + dD 2 + dDP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(b) Cross-attention at the last stage {4}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of the attention map for cross-attention with ViDT (Swin-nano).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>In contrast, the learnable positional encoding is added for [DET] tokens for [DET]?[DET] attention because there is no particular order between [DET] tokens. However, for [DET] ? [PATCH] attention, it is crucial to inject spatial bias to the [PATCH] tokens due to the permutation-equivariant in transformers, ignoring spatial information of the feature map. Thus, ViDT adds the sinusoidalbased spatial positional encoding to the feature map, which is reconstructed from the [PATCH] tokens</figDesc><table><row><cell>PATCH] and [DET] tokens into the three different attention, namely [PATCH] ? [PATCH], [DET] ? [DET],</cell></row><row><cell>and [DET] ? [PATCH] attention. Based on the decomposition, the efficient schemes of Swin Trans-</cell></row><row><cell>former are applied only to [PATCH] ? [PATCH] attention, which is the heaviest part in computational</cell></row><row><cell>complexity, without breaking the two constraints on [DET] tokens. As illustrated in Figure 3, these</cell></row><row><cell>modifications fully reuse all the parameters of Swin Transformer by sharing projection layers for</cell></row><row><cell>[DET] and [PATCH] tokens, and perform the three different attention operations:</cell></row><row><cell>? [PATCH] ? [PATCH] Attention: The initial [PATCH] tokens are progressively calibrated across the</cell></row><row><cell>attention layers such that they aggregate the key contents in the global feature map (i.e., spatial</cell></row><row><cell>form of [PATCH] tokens) according to the attention weights, which are computed by query, key</cell></row><row><cell>pairs. For [PATCH]?[PATCH] attention, Swin Transformer performs local attention on each window</cell></row><row><cell>partition, but its shifted window partitioning in successive blocks bridges the windows of the</cell></row><row><cell>preceding layer, providing connections among partitions to capture global information. Without</cell></row><row><cell>modifying this concept, we use the same policy to generate hierarchical [PATCH] tokens. Thus, the</cell></row><row><cell>number of [PATCH] tokens is reduced by a factor of 4 at each stage; the resolution of feature maps</cell></row><row><cell>decreases from H/4 ? W/4 to H/32 ? W/32 over a total of four stages, where H and W denote</cell></row><row><cell>the width and height of the input image, respectively.</cell></row><row><cell>? [DET] ? [DET] Attention: Like YOLOS, we append one hundred learnable [DET] tokens as the ad-</cell></row><row><cell>ditional input to the [PATCH] tokens. As the number of [DET] tokens specifies the number of objects</cell></row><row><cell>to detect, their number must be maintained with a fixed-scale over the transformer layers. In ad-</cell></row><row><cell>dition, [DET] tokens do not have any locality unlike the [PATCH] tokens. Hence, for [DET] ? [DET]</cell></row><row><cell>attention, we perform global self-attention while maintaining the number of them; this attention</cell></row><row><cell>helps each [DET] token to localize a different object by capturing the relationship between them.</cell></row><row><cell>? [DET] ? [PATCH] Attention: This is cross-attention between [DET] and [PATCH] tokens, which pro-</cell></row><row><cell>duces an object embedding per [DET] token. For each [DET] token, the key contents in [PATCH]</cell></row><row><cell>tokens are aggregated to represent the target object. Since the [DET] tokens specify different ob-</cell></row><row><cell>jects, it produces different object embeddings for diverse objects in the image. Without the cross-</cell></row><row><cell>attention, it is infeasible to realize the standalone object detector. As shown in Figure 3, ViDT</cell></row><row><cell>binds [DET] ? [DET] and [DET] ? [PATCH] attention to process them at once to increase efficiency.</cell></row><row><cell>We replace all the attention modules in Swin Transformer with the proposed RAM, which receives</cell></row><row><cell>[PATCH] and [DET] tokens (as shown in "Body" of Figure 2(c)) and then outputs their calibrated new</cell></row><row><cell>tokens by performing the three different attention operations in parallel.</cell></row><row><cell>Positional Encoding. ViDT adopts different positional encodings for different types of attention. For</cell></row><row><cell>[PATCH]?[PATCH] attention, we use the relative position bias (Hu et al., 2019) originally used in Swin</cell></row><row><cell>Transformer.</cell></row></table><note>for [DET] ? [PATCH] attention, as can be seen from the left side of Figure 3. We present a thorough analysis of various spatial positional encodings in Section 4.2.1. Use of [DET] ? [PATCH] Attention. Applying cross-attention between [DET] and [PATCH] tokens adds additional computational overhead to Swin Transformer, especially when it is activated at the bottom layer due to the large number of [PATCH] tokens. To minimize such computational overhead, ViDT only activates the cross-attention at the last stage (the top level of the pyramid) of Swin Trans- former, which consists of two transformer layers that receives [PATCH] tokens of size H/32 ? W/32. Thus, only self-attention for [DET] and [PATCH]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithms. We compare ViDT with two existing fully transformer-based object detection pipelines, namely DETR (ViT) and YOLOS. Since DETR (ViT) follows the general pipeline of (Deformable) DETR by replacing its ResNet backbone with other ViT variants; hence, we use one canonical ViT and one latest ViT variant, DeiT and Swin Transformer, as its backbone without any modification. In contrast, YOLOS is the canonical ViT architecture, thus only DeiT is available. Table 1 summarizes all the ViT models pre-trained on ImageNet used for evaluation. Note that publicly available pretrained models are used except for Swin-nano. We newly configure Swin-nano 3 comparable to DeiTtiny, which is trained on ImageNet with the identical setting. Overall, with respect to the number of parameters, Deit-tiny, -small, and -base are comparable to Swin-nano, -tiny, and -base, respectively. Please see Appendix B.2 for the detailed pipeline of compared detectors.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 Table 2</head><label>22</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="3">Backbone Epochs AP</cell><cell cols="4">AP50 AP75 APS APM APL Param.</cell><cell>FPS</cell></row><row><cell></cell><cell>DeiT-tiny</cell><cell>50</cell><cell cols="2">30.0 49.2 30.5</cell><cell>9.9</cell><cell>30.8 50.6</cell><cell>24M 10.9 (13.1)</cell></row><row><cell></cell><cell>DeiT-small</cell><cell>50</cell><cell cols="4">32.4 52.5 33.2 11.3 33.5 53.7</cell><cell>39M</cell><cell>7.8 (8.8)</cell></row><row><cell></cell><cell>DeiT-base</cell><cell>50</cell><cell cols="4">37.1 59.2 38.4 14.7 39.4 52.9</cell><cell>0.1B</cell><cell>4.3 (4.9)</cell></row><row><cell>DETR</cell><cell>Swin-nano</cell><cell>50</cell><cell cols="2">27.8 47.5 27.4</cell><cell>9.0</cell><cell>29.2 44.9</cell><cell>24M 24.7 (46.1)</cell></row><row><cell></cell><cell>Swin-tiny</cell><cell>50</cell><cell cols="4">34.1 55.1 35.3 12.7 35.9 54.2</cell><cell>45M 19.3 (28.1)</cell></row><row><cell></cell><cell>Swin-small</cell><cell>50</cell><cell cols="4">37.6 59.0 39.0 15.9 40.1 58.9</cell><cell>66M 13.5 (17.7)</cell></row><row><cell></cell><cell>Swin-base</cell><cell>50</cell><cell cols="4">40.7 62.9 42.7 18.3 44.1 62.4</cell><cell>0.1B</cell><cell>9.7 (12.6)</cell></row><row><cell></cell><cell>DeiT-tiny</cell><cell>50</cell><cell cols="4">40.8 60.1 43.6 21.4 43.4 58.2</cell><cell>18M 12.4 (16.3)</cell></row><row><cell></cell><cell>DeiT-small</cell><cell>50</cell><cell cols="4">43.6 63.7 46.5 23.3 47.1 62.1</cell><cell>35M</cell><cell>8.5 (10.2)</cell></row><row><cell>Deformable DETR</cell><cell>DeiT-base Swin-nano Swin-tiny</cell><cell>50 50 50</cell><cell cols="4">46.4 67.3 49.4 26.7 50.1 65.4 43.1 61.4 46.3 25.9 45.2 59.4 47.0 66.8 50.8 28.1 49.8 63.9</cell><cell>0.1B 18M 39M</cell><cell>4.4 (5.3) 7.0 (7.8) 6.3 (7.0)</cell></row><row><cell></cell><cell>Swin-small</cell><cell>50</cell><cell cols="4">49.0 68.9 52.9 30.3 52.8 66.6</cell><cell>60M</cell><cell>5.5 (6.1)</cell></row><row><cell></cell><cell>Swin-base</cell><cell>50</cell><cell cols="4">51.4 71.7 56.2 34.5 55.1 67.5</cell><cell>0.1B</cell><cell>4.8 (5.4)</cell></row><row><cell></cell><cell>DeiT-tiny</cell><cell>150</cell><cell cols="4">30.4 48.6 31.1 12.4 31.8 48.2</cell><cell>6M</cell><cell>28.1 (31.3)</cell></row><row><cell>YOLOS</cell><cell>DeiT-small</cell><cell>150</cell><cell cols="4">36.1 55.7 37.6 15.6 38.4 55.3</cell><cell>30M</cell><cell>9.3 (11.8)</cell></row><row><cell></cell><cell>DeiT-base</cell><cell>150</cell><cell cols="4">42.0 62.2 44.5 19.5 45.3 62.1</cell><cell>0.1B</cell><cell>3.9 (5.4)</cell></row><row><cell></cell><cell>Swin-nano</cell><cell>150</cell><cell cols="4">28.7 48.6 28.5 12.3 30.7 44.1</cell><cell>7M</cell><cell>36.5 (64.4)</cell></row><row><cell>ViDT</cell><cell>Swin-tiny</cell><cell>150</cell><cell cols="4">36.3 56.3 37.8 16.4 39.0 54.3</cell><cell>29M 28.6 (32.1)</cell></row><row><cell>(w.o. Neck)</cell><cell>Swin-small</cell><cell>150</cell><cell cols="4">41.6 62.7 43.9 20.1 45.4 59.8</cell><cell>52M 16.8 (18.8)</cell></row><row><cell></cell><cell>Swin-base</cell><cell>150</cell><cell cols="4">43.2 64.2 45.9 21.9 46.9 63.2</cell><cell>91M 11.5 (12.5)</cell></row><row><cell></cell><cell>Swin-nano</cell><cell>50</cell><cell cols="4">40.4 59.6 43.3 23.2 42.5 55.8</cell><cell>16M 20.0 (45.8)</cell></row><row><cell>ViDT</cell><cell>Swin-tiny Swin-small</cell><cell>50 50</cell><cell cols="4">44.8 64.5 48.7 25.9 47.6 62.1 47.5 67.7 51.4 29.2 50.7 64.8</cell><cell>38M 17.2 (26.5) 61M 12.1 (16.5)</cell></row><row><cell></cell><cell>Swin-base</cell><cell>50</cell><cell cols="4">49.2 69.4 53.1 30.6 52.6 66.9</cell><cell>0.1B</cell><cell>9.0 (11.6)</cell></row></table><note>compares ViDT with DETR (ViT) and YOLOS w.r.t their AP, FPS, # parameters, where the two variants of DETR (ViT) are simply named DETR and Deformable DETR. We report the result of ViDT without using knowledge distillation for fair comparison. A summary plot is provided in Figure 1. The experimental comparisons with CNN backbones are provided in Appendix C.1.. Comparison of ViDT with other compared detectors on COCO2017 val set. Two neck-free detectors, YOLOS and ViDT (w.o. Neck) are trained for 150 epochs due to the slow convergence. FPS is measured with batch size 1 of 800 ? 1333 resolution on a single Tesla V100 GPU, where the value inside the parentheses is measured with batch size 4 of the same resolution to maximize GPU utilization.Highlights. ViDT achieves the best trade-off between AP and FPS. With its high scalability, it per- forms well even for Swin-base of 0.1 billion parameters, which is 2x faster than Deformable DETR with similar AP. Besides, ViDT shows 40.4AP only with 16M parameters; it is 6.3-12.6AP higher than those of DETR (swin-nano) and DETR (swin-tiny), which exhibit similar FPS of 19.3-24.7. ViDT vs. Deformable DETR. Thanks to the use of multi-scale features, Deformable DETR exhibits high detection performance in general. Nevertheless, its encoder and decoder structure in the neck becomes a critical bottleneck in computation. In particular, the encoder with multi-layer deformable transformers adds considerable overhead to transform multi-scale features by attention. Thus, it shows very low FPS although it achieves higher AP with a relatively small number of parameters. In contrast, ViDT removes the need for a transformer encoder in the neck by using Swin Transformer with RAM as its body, directly extracting multi-scale features suitable for object detection. ViDT (w.o. Neck) vs. YOLOS. For the comparison with YOLOS, we train ViDT without using its neck component. These two neck-free detectors show relatively low AP compared with other detectors in general. In terms of speed, YOLOS exhibits much lower FPS than ViDT (w.o. Neck) because of its quadratic computational complexity for attention. However, ViDT (w.o. Neck) extends Swin Transformers with RAM, thus requiring linear complexity for attention. Hence, it shows AP comparable to YOLOS for various backbone size, but its FPS is much higher.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>4.2.1 SPATIAL POSITIONAL ENCODINGMethod None Pre-addition Post-additionTypeNone Sin. Learn. Sin. Learn.</figDesc><table><row><cell>AP</cell><cell>23.7 28.7 27.4 28.0</cell><cell>24.1</cell></row><row><cell cols="3">Table 3. Results for different spatial encodings for</cell></row><row><cell cols="2">[DET] ? [PATCH] cross-attention.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="7">4.2.2 SELECTIVE [DET] ? [PATCH] CROSS-ATTENTION</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">The addition of cross-attention to Swin Transformer inevitably entails computational overhead, par-</cell></row><row><cell>Stage Ids</cell><cell cols="2">{1, 2, 3, 4}</cell><cell cols="2">{2, 3, 4}</cell><cell cols="2">{3, 4}</cell><cell>{4}</cell><cell></cell><cell>{ }</cell><cell></cell></row><row><cell>Metric</cell><cell>AP</cell><cell>FPS</cell><cell>AP</cell><cell>FPS</cell><cell>AP</cell><cell>FPS</cell><cell>AP</cell><cell>FPS</cell><cell>AP</cell><cell>FPS</cell></row><row><cell>w.o. Neck</cell><cell>29.0</cell><cell>21.8</cell><cell>28.8</cell><cell>29.1</cell><cell>28.5</cell><cell>34.3</cell><cell>28.7</cell><cell>36.5</cell><cell>FAIL</cell><cell>37.7</cell></row><row><cell>w. Neck</cell><cell>40.3</cell><cell>14.6</cell><cell>40.1</cell><cell>18.0</cell><cell>40.3</cell><cell>19.5</cell><cell>40.4</cell><cell>20.0</cell><cell>37.1</cell><cell>20.5</cell></row></table><note>ticularly when the number of [PATCH] is large. To alleviate such overhead, we selectively enable cross-attention in RAM at the last stage of Swin Transformer; this is shown to greatly improve FPS, but barely drop AP. Table 4 summarizes AP and FPS when used different selective strategies for the cross-attention, where Swin Transformer consists of four stages in total. It is interesting that all the strategies exhibit similar AP as long as cross-attention is activated at the last stage. Since features are extracted in a bottom-up manner as they go through the stages, it seems difficult to directly obtain useful information about the target object at the low level of stages. Thus, only using the last stage is the best design choice in terms of high AP and FPS due to the smallest number of [PATCH] tokens. Meanwhile, the detection fails completely or the performance significantly drops if all the stages are not involved due to the lack of interaction between [DET] and [PATCH] tokens that spatial positional encoding is associated with. A more detailed analysis of the [DET] ? [PATCH] cross-attention and [DET] ? [DET] self-attention is provided in appendices C.3 and C.4.. AP and FPS comparison with different selective cross-attention strategies.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>Aux. Box Ref. Neck</cell><cell>AP</cell><cell>?</cell></row><row><cell>YOLOS</cell><cell cols="2">30.4 29.2 ?1.2 20.1 ?10.3</cell></row><row><cell></cell><cell>28.7</cell><cell></cell></row><row><cell>ViDT</cell><cell cols="2">27.2 ?1.6 22.9 ?5.9 36.2 +7.4</cell></row><row><cell></cell><cell cols="2">40.4 +11.6</cell></row></table><note>. Effect of extra techniques with YOLOS (DeiT-tiny) and ViDT (Swin-nano).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>decoding layer drop to further accelerate inference speed. The performance of the final version is very outstanding; it achieves 41.7AP with reasonable FPS by only using 13M parameters when used Swin-nano as its backbone. Further, it only loses 2.7 FPS while exhibiting 46.4AP when used Swin-tiny. This indicates that a fully transformer-based object detector has the potential to be used as a generic object detector when further developed in the future. Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In ICCV, 2021. 13Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2.We analyze the computational complexity of the proposed RAM compared with the attention used in YOLOS. The analysis is based on the computational complexity of basic building blocks for Canonical and Swin Transformer, which is summarized inTable 94 , where T 1 and T 2 is the number of tokens for self-and cross-attention, and d is the embedding dimension.</figDesc><table><row><cell cols="5">An Efficient and Effective Fully Transformer-based Ob-ject Detector (Supplementary Material) Wenhai https://github.com/facebookresearch/detectron2, 2019. 3, 6 A RECONFIGURED ATTENTION MODULE</cell></row><row><cell cols="5">The proposed RAM in Figure 3 performs three attention operations, namely [PATCH] ? [PATCH], Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: [DET] ? [PATCH], and [DET] ? [DET] attention. This section provides (1) computational complexity Deformable transformers for end-to-end object detection. In ICLR, 2021. 3, 5, 14, 15 analysis and (2) further algorithmic design for [DET] tokens.</cell></row><row><cell cols="3">A.1 COMPUTATIONAL COMPLEXITY ANALYSIS</cell><cell></cell></row><row><cell>Transformer</cell><cell cols="3">Canonical Transformer</cell><cell>Swin Transformer</cell></row><row><cell>Attention</cell><cell>Global Self-attention</cell><cell cols="3">Global Cross-attention</cell><cell>Local Self-attention</cell></row><row><cell>Complexity</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Component</cell><cell cols="2">Swin-nano</cell><cell>Swin-tiny</cell></row><row><cell cols="5"># RAM Neck Distil Drop AP AP50 AP75 Param. FPS</cell><cell>AP AP50 AP75 Param. FPS</cell></row><row><cell>(1)</cell><cell cols="2">28.7 48.6 28.5</cell><cell>7M</cell><cell>36.5 36.3 56.3 37.8 29M 28.6</cell></row><row><cell>(2)</cell><cell cols="4">40.4 59.6 43.3 16M 20.0 44.8 64.5 48.7 38M 17.2</cell></row><row><cell>(3)</cell><cell cols="4">41.9 61.1 45.0 16M 20.0 46.5 66.3 50.2 38M 17.2</cell></row><row><cell>(4)</cell><cell cols="4">41.7 61.0 44.8 13M 22.3 46.4 66.3 50.2 35M 19.6</cell></row><row><cell></cell><cell cols="4">Table 8. Detailed component analysis with Swin-nano and Swin-tiny.</cell></row><row><cell cols="2">5 CONCLUSION</cell><cell></cell><cell></cell></row><row><cell cols="5">We have explored the integration of vision and detection transformers to build an effective and</cell></row></table><note>efficient object detector. The proposed ViDT significantly improves the scalability and flexibility of transformer models to achieve high accuracy and inference speed. The computational complexity of its attention modules is linear w.r.t. image size, and ViDT synergizes several essential techniques to boost the detection performance. On the Microsoft COCO benchmark, ViDT achieves 49.2AP with a large Swin-base backbone, and 41.7AP with the smallest Swin-nano backbone and only 13M parameters, suggesting the benefits of using transformers for complex computer vision tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc></figDesc><table /><note>Computational complexity of attention modules: In the canonical transformer, the complexity of global self-attention is O</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>.2 ALGORITHMIC DESIGN FOR [DET] TOKENS A.2.1 BINDING [DET] ? [DET] AND [DET] ? [PATCH] ATTENTION Binding the two attention modules is very simple in implementation. [DET] ? [DET] and [DET] ? [PATCH] attention is generating a new [DET] token, which aggregates relevant contents in [DET] and [PATCH] tokens, respectively. Since the two attention share exactly the same [DET] query embedding obtained after the projection as shown inFigure 3, they can be processed at once by performing the scaled-dot product between [DET] Q and [DET] K , [PATCH] K embeddings, where Q, K are the key and query, and [?] is the concatenation. Then, the obtained attention map is applied to the [DET] V , [PATCH] V embeddings, where V is the value and d is the embedding dimension,</figDesc><table /><note>Summary of computational complexity for different attention operations used in YOLOS and ViDT (RAM), where P and D are the number of [PATCH] and [DET] tokens, respectively (D &lt;&lt; P).A</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>A.2.2 EMBEDDING DIMENSION OF [DET] TOKENS[DET] ? [DET] attention is performed across all the stages, and the embedding dimension of [DET] tokens increases gradually like [PATCH] tokens. For the [PATCH] token, its embedding dimension is increased by concatenating nearby [PATCH] tokens in a grid. However, this mechanism is not applicable for [DET] tokens since we maintain the same number of [DET] tokens for detecting a fixed number of objects in a scene. Hence, we simply repeat a [DET] token multiple times along the embedding dimension to increase its size. This allows [DET] tokens to reuse all the projection and normalization layers in Swin Transformer without any modification.</figDesc><table><row><cell>B EXPERIMENTAL DETAILS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>B.1 SWIN-NANO ARCHITECTURE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Channel</cell><cell></cell><cell cols="2">Layer Numbers</cell><cell></cell></row><row><cell>Name</cell><cell>Dim.</cell><cell cols="4">S1 S2 S3 S4</cell></row><row><cell>Swin-nano</cell><cell>48</cell><cell>2</cell><cell>2</cell><cell>6</cell><cell>2</cell></row><row><cell>Swin-tiny</cell><cell>96</cell><cell>2</cell><cell>2</cell><cell>6</cell><cell>2</cell></row><row><cell>Swin-small</cell><cell>128</cell><cell>2</cell><cell>2</cell><cell>18</cell><cell>2</cell></row><row><cell>Swin-base</cell><cell>192</cell><cell>2</cell><cell>2</cell><cell>18</cell><cell>2</cell></row><row><cell cols="5">Table 11. Swin Transformer Architecture.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 .</head><label>12</label><figDesc>The main difference of ViDT is the use of reconfigured attention modules (RAM) for Swin Transformer, allowing the extraction of fine-grained detection features directly from the input image. Thus, Swin Transformer is extended to a standalone object detector called ViDT (w.o. Neck). Further, its extension to ViDT allows to use multi-scale features and multiple essential techniques for better detection, such as auxiliary decoding loss and iterative box refinement, by only maintaining a transformer decoder at the neck. Except for the two neck-free detector,YOLOS and ViDT (w.o. Neck), all the pipelines maintain multiple FFNs; that is, a single FFNs for each decoding layer at the neck for box regression and classification.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 .</head><label>13</label><figDesc>Evaluations of ViDT with other detectors using CNN backbones on COCO2017 val set. FPS is measured with batch size 1 of 800 ? 1333 resolution on a single Tesla V100 GPU, where the value inside the parentheses is measured with batch size 4 of the same resolution to maximize GPU utilization.Table 14. Variations of Deformable DETR, YOLOS, and ViDT with respect to their neck structure. They are trained for 50 epochs with the same configuration used in our main experimental results.</figDesc><table><row><cell>Method</cell><cell cols="8">Backbone Epochs AP AP50 AP75 APS APM</cell><cell cols="2">APL Param.</cell><cell>FPS</cell></row><row><cell>DETR</cell><cell cols="2">ResNet-50</cell><cell>500</cell><cell cols="6">42.0 62.4 44.2 20.5 45.8 61.1</cell><cell>41M 22.8 (38.6)</cell></row><row><cell>DETR-DC5</cell><cell cols="2">ResNet-50</cell><cell>500</cell><cell cols="6">43.3 63.1 45.9 22.5 47.3 61.1</cell><cell>41M 12.8 (14.2)</cell></row><row><cell>DETR-DC5</cell><cell cols="2">ResNet-50</cell><cell>50</cell><cell cols="6">35.3 55.7 36.8 15.2 37.5 53.6</cell><cell>41M 14.2 (14.2)</cell></row><row><cell cols="3">Deform. DETR ResNet-50</cell><cell>50</cell><cell cols="6">45.4 64.7 49.0 26.8 48.3 61.7</cell><cell>40M 13.7 (19.4)</cell></row><row><cell>ViDT</cell><cell cols="2">Swin-tiny</cell><cell>50</cell><cell cols="6">44.8 64.5 48.7 25.9 47.6 62.1</cell><cell>38M 17.2 (26.5)</cell></row><row><cell>ViDT</cell><cell cols="2">Swin-tiny</cell><cell>150</cell><cell cols="6">47.2 66.7 51.4 28.4 50.2 64.7</cell><cell>38M 17.2 (26.5)</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Backbone</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell><cell>APS</cell><cell>APM</cell><cell cols="2">APL Param.</cell><cell>FPS</cell></row><row><cell cols="2">Deformable DETR ? neck encoder</cell><cell cols="2">Swin-nano</cell><cell>43.1 34.0</cell><cell>61.4 52.8</cell><cell>46.3 35.6</cell><cell>25.9 18.0</cell><cell>45.2 36.3</cell><cell>59.4 48.4</cell><cell>17M 14M</cell><cell>7.0 22.4</cell></row><row><cell>YOLOS + neck decoder</cell><cell></cell><cell cols="2">DeiT-tiny</cell><cell>30.4 38.1</cell><cell>48.6 57.1</cell><cell>31.1 40.2</cell><cell>12.4 20.1</cell><cell>31.8 40.2</cell><cell>48.2 56.0</cell><cell>6M 14M</cell><cell>28.1 17.1</cell></row><row><cell>ViDT + neck encoder</cell><cell></cell><cell cols="2">Swin-nano</cell><cell>40.4 46.1</cell><cell>59.6 64.1</cell><cell>43.3 49.7</cell><cell>23.2 28.5</cell><cell>42.5 48.7</cell><cell>55.8 61.7</cell><cell>16M 19M</cell><cell>20.0 6.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>except YOLOS (DeiT-tiny); since the result of YOLOS (DeiT-tiny) for 800 ? 1333 is not reported in the paper, we train it by following the training configuration suggested by authors. COMPARISON WITH OBJECT DETECTOR USING CNN BACKBONE We compare ViDT with (Deformable) DETR using the ResNet-50 backbone, as summarized inTable 13, where all the results except ViDT are borrowed from<ref type="bibr" target="#b0">(Carion et al., 2020;</ref> Zhu et al.,  2021), and DETR-DC5 is a modification of DETR to use a dilated convolution at the last stage in ResNet. For a fair comparison, we compare ViDT (Swin-tiny) with similar parameter numbers. In general, ViDT shows a better trade-off between AP and FPS even compared with (Deformable) DETR with the ResNet-50. Specifically, ViDT achieves FPS much higher than DETR-DC5 and Deformable DETR with competitive AP. Particularly when training ViDT for 150 epochs, ViDT outperforms other compared methods using the ResNet-50 backbone in terms of both AP and FPS.C.2 VARIATIONS OF EXISTING PIPELINESWe study more variations of existing detection methods by modifying their original pipelines inTable 12. Thus, we remove the neck encoder of Deformable DETR to increase its efficiency, while adding a neck decoder to YOLOS to leverage multi-scale features along with auxiliary decoding Cross-attention at all stages {1, 2, 3, 4}.</figDesc><table><row><cell>C SUPPLEMENTARY EVALUATION Input Image C.1 Query Id: 82 (Cat) (a) Query Id: 44 (Cat) Input Image</cell><cell>Query Id: 94 (Cat) Query Id: 49 (Cat)</cell><cell>Query Id: 7 (Remote) Query Id: 25 (Remote)</cell><cell>Query Id: 53 (Remote) Query Id: 3 (Remote)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>[DET] ? [PATH] cross-attention. C.4 [DET] ? [DET] ATTENTION IN RAM Table 15. AP and FPS comparison with different [DET] ? [DET] self-attention strategies with ViDT. Another possible consideration for ViDT is the use of [DET] ? [DET] self-attention in RAM. We conduct an ablation study by removing the [DET] ? [DET] attention one by one from the bottom stage, and summarize the results in</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Stage Id</cell><cell></cell><cell cols="2">Swin-nano</cell></row><row><cell>#</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>AP</cell><cell>FPS</cell></row><row><cell>(1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.4</cell><cell>20.0</cell></row><row><cell>(2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.3</cell><cell>20.1</cell></row><row><cell>(3)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.4</cell><cell>20.2</cell></row><row><cell>(4)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.1</cell><cell>20.3</cell></row><row><cell>(5)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>39.7</cell><cell>20.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This reconfiguration scheme can be easily applied to other ViT variants with simple modification.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Swin-nano is designed such that its channel dimension is half that of Swin-tiny. Please see Appendix B.1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We used the computational complexity reported in the original paper<ref type="bibr" target="#b17">(Vaswani et al., 2017;</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Cross-entropy loss is used with standard transformer architectures, while focal loss<ref type="bibr" target="#b9">(Lin et al., 2017)</ref> is used with deformable transformer architecture.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CrossViT: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guobin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">General instance distillation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeren</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">You only look at one sequence: Rethinking transformer in vision through object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bencheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00666</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3464" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for generic object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A trainable system for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="33" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
