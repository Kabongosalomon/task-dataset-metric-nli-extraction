<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Glance and Gaze: Inferring Action-aware Points for One-Stage Human-Object Interaction Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
							<email>chxding@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Pazhou Lab</orgName>
								<address>
									<settlement>Guangzhou</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Glance and Gaze: Inferring Action-aware Points for One-Stage Human-Object Interaction Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern human-object interaction (HOI) detection approaches can be divided into one-stage methods and twostage ones. One-stage models are more efficient due to their straightforward architectures, but the two-stage models are still advantageous in accuracy. Existing one-stage models usually begin by detecting predefined interaction areas or points, and then attend to these areas only for interaction prediction; therefore, they lack reasoning steps that dynamically search for discriminative cues. In this paper, we propose a novel one-stage method, namely Glance and Gaze Network (GGNet), which adaptively models a set of actionaware points (ActPoints) via glance and gaze steps. The glance step quickly determines whether each pixel in the feature maps is an interaction point. The gaze step leverages feature maps produced by the glance step to adaptively infer ActPoints around each pixel in a progressive manner. Features of the refined ActPoints are aggregated for interaction prediction. Moreover, we design an actionaware approach that effectively matches each detected interaction with its associated human-object pair, along with a novel hard negative attentive loss to improve the optimization of GGNet. All the above operations are conducted simultaneously and efficiently for all pixels in the feature maps. Finally, GGNet outperforms state-of-the-art methods by significant margins on both V-COCO and HICO-DET benchmarks. Code of GGNet is available at https: //github.com/SherlockHolmes221/GGNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-Object Interaction (HOI) detection is one of the fundamental tasks in human-centric scene understanding. It involves not only detecting persons and objects in an image, but also the interactions (verbs) between each humanobject pair. The output of an HOI detection model can be represented as a set of triplets in the form of &lt;human  Green boxes or points represent the interaction area for "hold tennis racket", while the red ones stand for "hit sports ball". (a) InteractNet <ref type="bibr" target="#b2">[3]</ref> uses the same human bounding box to represent the interaction area for all interactions pertaining to the person. (b) UnionDet <ref type="bibr" target="#b0">[1]</ref> adopts the union box of one human-object pair to represent their interaction area. (c) PPDM <ref type="bibr" target="#b9">[10]</ref> leverages the middle point of one human-object pair to represent their interaction area. (d) GGNet employs a single set of dynamic points to adaptively capture informative areas for the interaction between each human-object pair.</p><p>interaction object&gt;, and each triplet is also referred to as a single HOI category. For example, there are two HOI categories in <ref type="figure" target="#fig_1">Figure 1</ref>, i.e. &lt;human hold tennis racket&gt; and &lt;human hit sports ball&gt;.</p><p>According to the order in which object detection and interaction detection is performed, modern HOI detection methods can be divided into one-stage and two-stage approaches. Two-stage methods must perform the object detection first and then identify the interactions between each possible human-object pair. However, because the two stages are separated in this approach, these methods are usually inefficient. By contrast, one-stage methods can per-form object detection and interaction detection in parallel by first defining interaction areas (e.g. a union box or a single point). Generally speaking, one-stage methods tend to be more efficient and structurally elegant, but two-stage methods are more accurate at present.</p><p>One of the key issues for one-stage methods is the way to represent the "interaction area" for each human-object pair <ref type="bibr" target="#b0">[1]</ref>. Existing approaches usually define this area artificially and the interaction area often face the semantic ambiguity problem. For example, as shown in <ref type="figure" target="#fig_1">Figure 1(a)</ref>, Interact-Net <ref type="bibr" target="#b2">[3]</ref> utilizes a human bounding box to represent the area of all interactions involving the person, meaning that objectspecific information is ignored. UnionDet <ref type="bibr" target="#b0">[1]</ref> addresses this problem by utilizing the union box for each human-object pair as their interaction region. However, union boxes may overlap significantly with each other <ref type="figure" target="#fig_1">(Figure 1(b)</ref>), which introduces ambiguity between pairs. For its part, PPDM <ref type="bibr" target="#b9">[10]</ref> utilizes the middle point of each human-object pair as the interaction point <ref type="figure" target="#fig_1">(Figure 1(c)</ref>). Although interaction points are less likely to overlap with each other under this approach, a single interaction point is often vague to represent complex interactions between a human-object pair.</p><p>With the predefined interaction areas discussed above, existing one-stage methods usually attend to the interaction area only once to predict the interaction categories. Recent works <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref> have revealed that the eyes of human beings usually move around an object to discover more cues regarding its location. Similarly, when it comes to HOI detection, people often first glance at the scene to identify possible human-object pairs with any interaction; they then search for cues around each pair, and finally gaze at discriminative areas to identify the interaction class. Accordingly, inspired by the above observation, we herein propose a novel model, named Glance and Gaze Network (GGNet), which adaptively infers a set of actionaware points (ActPoints) to represent the interaction area ( <ref type="figure" target="#fig_1">Figure 1(d)</ref>). GGNet mimics the two steps taken by humans to identify human-object interactions: Glance and Gaze. First, GGNet quickly determines whether each pixel in the feature maps is an interaction point; we call it the glance step. Based on the feature maps in the glance step, the subsequent gaze step searches for a set of ActPoints around each pixel. This step then progressively proceeds to refine the location of these ActPoints. In brief, this step comprises two sub-steps, in which the coarse location and location residuals of ActPoints are inferred, respectively. Finally, GGNet aggregates features of the refined ActPoints to predict interaction categories at the interaction points.</p><p>We further propose an action-aware point matching (APM) approach designed to match each interaction with its associated human-object pair. This matching process specifies the location of both the human and object instances for each interaction. Existing interaction point-based methods tend to employ a single location regressor shared by all interaction categories <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>; however, we observe that the interaction category affects the spatial layout of one humanobject pair. We accordingly propose to assign each interaction category a unique location regressor, which is proven in the experimentation section to be a more effective approach.</p><p>Finally, we propose a novel focal loss, namely Hard Negative Attentive (HNA) loss, to further promote the performance of GGNet. As there are massive numbers of negative samples for each interaction classifier of the interaction point-based methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, a serious imbalance problem exists between the positive and negative samples for each interaction category. We thus develop an efficient approach to address this problem by inferring and highlighting hard negative samples. Hard negatives are inferred between meaningful HOI categories containing the same object. For example, we can infer a hard negative sample "repair bicycle" according to the labeled positive sample "carry bicycle", unless "repair bicycle" is labeled as positive; in this way, the decision boundary between easily confused interaction categories can be clarified.</p><p>Both APM and HNA loss can be readily applied to other interaction point-based HOI detection methods. We conduct extensive experiments on the two most popular HOI detection databases, i.e. V-COCO <ref type="bibr" target="#b30">[31]</ref> and HICO-DET <ref type="bibr" target="#b29">[30]</ref>. Experimental results demonstrate that our proposed GGNet consistently outperforms start-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Two-Stage Methods. Most existing HOI detection methods belong to the two-stage category. These methods typically perform object detection first, then pair the human and object proposals for interaction recognition. Various types of features are utilized to promote the detection accuracy: for example, human-object spatial feature encoded using their bounding box locations are widely adopted in two-stage methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b11">[12]</ref>. In line with this, Liu et al. <ref type="bibr" target="#b7">[8]</ref> introduced a mechanism that encodes the fine-grained spatial layout of human-object pairs. The spatial layout is obtained using human parsing and object segmentation tools. Wan et al. <ref type="bibr" target="#b6">[7]</ref> and Zhou et al. <ref type="bibr" target="#b15">[16]</ref> utilized key points on the human body to crop human part features. Different model structures have also been developed to promote HOI detection performance. For example, several recent works have utilized graph convolutional networks to integrate human and object appearance features <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Moreover, Gao et al. <ref type="bibr" target="#b11">[12]</ref> and Wang et al. <ref type="bibr" target="#b12">[13]</ref> made use of attention mechanisms to capture context information. Other works have also investigated the semantic meaning of verbs and the relationships between them: for example, Zhong et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref>   <ref type="figure">Figure 2</ref>. Overview of GGNet in the training stage. GGNet includes three main tasks, namely interaction prediction, human-object pair matching, and object detection. The three tasks share the same backbone model. The interaction prediction task includes one Glance step and two Gaze steps. The two Gaze steps infer a set of ActPoints for each pixel in feature maps. By aggregating their features into the interaction point, the second Gaze step is able to predict interactions more robustly. The human-object pair matching task is realized by the Action-aware Point Matching (APM) module, which bridges the interaction prediction and object detection tasks. In the testing stage, the glance step and the first gaze step are only utilized to infer ActPoints, while the other layers of the two steps are removed. ? denotes the element-wise addition operation. Best viewed in color. ship between interaction categories by means of action cooccurrence matrices, which were then utilized to promote HOI detection performance.</p><p>Although two-stage methods are flexible to include diverse features, they divide object detection and interaction prediction into two sequential steps, which is typically very time-consuming.</p><p>One-Stage Methods. Some early HOI detection works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> devised end-to-end models based on the Faster R-CNN object detector <ref type="bibr" target="#b3">[4]</ref>. Although these methods are more efficient than most two-stage methods, they adopt the same human appearance features for predicting different interactions, meaning that object-specific information in different human-object pairs is ignored. Recent works, moreover, have proposed to perform object detection and interaction prediction in parallel branches. For example, Liao et al. <ref type="bibr" target="#b9">[10]</ref> and Wang et al. <ref type="bibr" target="#b19">[20]</ref> defined the middle point of each human-object pair as their interaction point, which is capable of roughly capturing both human and object appearance features. These authors then detect interaction points in the interaction prediction branch. Kim et al. <ref type="bibr" target="#b0">[1]</ref> represented the interaction location of one human-object pair using the union box, then detect union boxes using networks that are similar to anchor-based object detection models <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b31">[32]</ref>. These recent one-stage methods can substantially improve HOI detection efficiency; however, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, their definitions of the interaction areas or points remain relatively coarse and may thus introduce ambiguity into HOI detection.</p><p>In this paper, to handle the semantic ambiguity problem associated with interaction areas, we propose to represent these area using a set of dynamic ActPoints. These Act-Points are adaptively inferred around each interaction point, after which their features are aggregated to improve the interaction recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The architecture of GGNet is illustrated in <ref type="figure" target="#fig_7">Figure 5</ref>. Similar to PPDM <ref type="bibr" target="#b9">[10]</ref>, GGNet breaks HOI detection into three main tasks, namely interaction prediction, humanobject pair matching, and object detection. These three tasks all share the same backbone. The second task bridges the first and the third tasks via associating each detected interaction with a single human-object pair. The object detection task is realized according to <ref type="bibr" target="#b9">[10]</ref>. GGNet improves the interaction prediction task with a novel glance-and-gaze strategy. It also promotes the accuracy of the second task with an Action-aware Point Matching (APM) module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Glance and Gaze Network</head><p>As illustrated in <ref type="figure" target="#fig_7">Figure 5</ref>, given an input image I ? R H?W ?3 , the output feature maps of the backbone can be expressed as F ? R H d ? W d ?C , where d denotes the output stride of backbone and C denotes the number of channels.</p><p>In this paper, we adopt the same definition of interaction point as <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, which is also illustrated in Figure 1(c). GGNet handles the semantic ambiguity problem of the single interaction point by further inferring a set of action-aware points (ActPoints). ActPoints adaptively capture more contextual information, and their features are aggregated to predict the interaction category. The set of Act-Points around one interaction point can be represented as:</p><formula xml:id="formula_0">P = {(x k , y k )} n k=1 ,<label>(1)</label></formula><p>where n is the total number of ActPoints sampled for an interaction. We empirically set n as 25. The location of ActPoints are inferred progressively by mimicking human's visual system <ref type="bibr" target="#b23">[24]</ref> with one glance step and two gaze steps. Glance</p><p>Step. In this step, GGNet quickly determines whether each pixel in F is an interaction point. As shown in <ref type="figure" target="#fig_7">Figure 5</ref>, this step is realized using a 3 ? 3 convolutional (Conv) layer with ReLU, followed by a 1 ? 1 Conv layer and a sigmoid layer. The size of heatmaps produced by the sigmoid layer is</p><formula xml:id="formula_1">H d ? W d ? V ,</formula><p>where V denotes the number of interaction categories. We apply a V -dimensional element-wise focal loss <ref type="bibr" target="#b9">[10]</ref> to the heatmaps as supervision for the inference of interaction categories. Due to this supervision, the feature maps output by the 3 ? 3 Conv layer, i.e. F 0 in <ref type="figure" target="#fig_7">Figure 5</ref>, are action-aware. Gaze</p><p>Step. This step infers the location of ActPoints via two sub-steps, which are referred to as Gaze Step 1 and Gaze Step 2, respectively. In Gaze Step 1, F 0 is employed to predict the coarse location of n ActPoints for each pixel, since features in F 0 have been action-aware. Moreover, as the discriminative power of each ActPoint varies with respect to the target interaction, GGNet also predicts a weight for each ActPoint. Both location and weight prediction are achieved by a 5 ? 5 Conv layer. Next, we aggregate features of ActPoints as well as their weights using one deformable Conv layer <ref type="bibr" target="#b26">[27]</ref>. The 5?5 offset field is determined by the number of ActPoints, which are also verified in <ref type="table" target="#tab_2">Table 2</ref>. To ensure that the predicted ActPoints are reasonable, the feature maps produced by this sub-step are also used for interaction prediction with the V -dimensional element-wise focal loss as supervision.</p><p>On its own, the above step cannot always obtain precise locations of ActPoints; this is because the above 5 ? 5 Conv operation has a fixed field of view, while the location of the human and object instances in one pair can vary dramatically. To address this problem, Gaze Step 2 is introduced to refine the location of ActPoints. In more detail, we aggregate the features in F 1 of the coarse ActPoints using one deformable Conv layer, of which the output feature maps are denoted as G 1 . Now, each pixel in G 1 has a larger field of view; G 1 is then send to another 5 ? 5 Conv layer to predict the residual offsets of ActPoints locations, along with their new weights. As shown in <ref type="figure" target="#fig_7">Figure 5</ref>, the final position  of ActPoints are obtained by summing their coarse location and residual offsets. Finally, we aggregate the features of the refined ActPoints as well as their weights using another deformable Conv layer to predict interaction categories for each pixel in F 2 . Similar to Gaze Step 1, the V -dimensional element-wise focal loss is adopted as supervision.</p><formula xml:id="formula_2">(t) k , ?y (t)</formula><p>k ) denotes the predicted offset with respect to the k-th Act-Point's location in the last step. Moreover, we set the initial location of all n ActPoints as (0, 0) and set G 0 as F 0 . T (t) of f set is a Conv layer, whose kernel size is equal to the square root of n. More details of the glance and gaze network can be confirmed from our open-source project. Action-aware Point Matching. To compose one HOI instance, each detected interaction point is associated with one human-object pair. Existing works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref> adopt a single regressor shared by all interaction categories for this association process. However, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>, human-object pairs with different interactions also differ in terms of spatial characteristics. Therefore, we propose the action-aware point matching (APM) module that assigns a unique location regressor for each interaction category.</p><p>As illustrated in <ref type="figure" target="#fig_7">Figure 5</ref>, APM is attached to F 0 . It includes one 3 ? 3 and one 1 ? 1 Conv layers. The latter layer acts as regressors. Each regressor outputs a 2-dimensional offset to human point (object point) with respect to the interaction point. Therefore, the output dimension of this layer is H d ? W d ?4V . In line with <ref type="bibr" target="#b9">[10]</ref>, we utilize these predicted offsets to match the target human (object) proposal during inference. This process will be detailed in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hard Negative Attentive Loss</head><p>Recent one-stage methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref> adopt the elementwise focal loss <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b32">[33]</ref> on the output heatmaps as supervision to train their models. However, the heatmap size for each interaction category is H d ? W d , which is often a large number. Therefore, there are massive negative samples in each heatmap, which brings in the problem of imbalance between positive and negative samples. Moreover, because of the long-tailed distribution of interaction categories <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, some interaction categories have very limited positive samples, which further exacerbates the imbalance problem. Based on the above observation, we propose a novel hard negative attentive (HNA) loss to guide the model to focus more on hard negative samples for each respective interaction category.</p><p>Definition of Hard Negatives. We infer hard negatives between meaningful HOI categories that share the same object class. For example, we can infer a hard negative sample &lt;human repair bicycle&gt; for the "repair" category according to a labeled positive sample &lt;human carry bicycle&gt;, if &lt;human repair bicycle&gt; is indeed not labeled as positive. We do not infer hard negatives from meaningless HOI categories that have never appeared in the training set of each database, e.g. &lt;human eat bicycle&gt; and &lt;human drink bicycle&gt;. The inferred sample of &lt;human repair bicycle&gt; can be highlighted as a hard negative in the interaction heatmap for "repair" category. We repeat the above two operations for each ground truth sample. The value of remaining elements in M is set to 0. Finally, the HNA loss can be represented as follows:</p><formula xml:id="formula_3">L = ? 1 N xyv ? ? ? ? ? ? ? ? ? ? ? (1 ? P xyv ) ? log(P xyv ) if M xyv = 1, (1 ? M xyv ) ? (P xyv ) ? log(1 ? P xyv ) if M xyv = ?1, (1 ? M xyv ) ? (P xyv ) ? log(1 ? P xyv ) otherwise,<label>(4)</label></formula><p>where N is the number of ground truth interaction points in an image. P xyv is the predicted score for the interaction category v at point (x, y). ? is set as 7. ?, ?, and the parameters of the Gaussian distribution are set following <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and Inference</head><p>Training. The overall loss function for GGNet can be represented as follows:</p><formula xml:id="formula_4">L hoi = L gaze2 + ? 1 (L glance + L gaze1 + L m ) + L d ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">L m = L mh + L mo ,<label>(6)</label></formula><formula xml:id="formula_6">L d = L h + L o + ? 2 L wh + L of f .<label>(7)</label></formula><p>? 2 is set to 0.1 following <ref type="bibr" target="#b9">[10]</ref>; ? 1 is set to 0.1, which is analyzed in the supplementary file. L glance , L gaze1 and L gaze2 denote the HNA loss for the glance step, and each of two gaze steps, respectively. L mh and L mo stand for the L1 loss for matching human and object points in the APM module, respectively. L d stands for the object detection loss. L h and L o are the focal loss functions to predict human and object locations. L wh and L of f denote the L1 loss for object size and center offset predictions, respectively. L m and L d are realized in the same way as <ref type="bibr" target="#b9">[10]</ref>. Inference. We use the output of Gaze Step 2 to obtain interaction points; while the final score is the multiplication result of interaction prediction score from Gaze step 2 and object detection scores of the associated human-object pair. The following point matching process is the same as that in <ref type="bibr" target="#b9">[10]</ref>. First, a set of interaction? i , human? h and object points? o are respectively selected based on their prediction confidence scores. The number of points in? i ,? h , and? o is set to 100, respectively. Second, we associate each predicted interaction point (x i ,? i ) ?? i with a human point (x h opt ,? h opt ) ?? h and an object point (x o opt ,? o opt ) ?? o according to the predicted subject and object offsets by APM, respectively: </p><formula xml:id="formula_7">(x h opt ,? h opt ) = arg min (x h ,? h )?? h 1 P h (x h ,? h ) (|(x i ,? i ) ? (d hx (x i ,? i ) ,d hy (x i ,? i ) ) ? (x h ,? h )|),<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>V-COCO. V-COCO was constructed based on the MS-COCO database <ref type="bibr" target="#b28">[29]</ref>. Its training and validation sets contain 5,400 images, while its testing set includes 4,946 images. It covers 80 object categories, 26 interaction categories and 234 HOI categories. The mean average precision of Scenario 1 role (mAP role ) <ref type="bibr" target="#b30">[31]</ref> is used for evaluation. <ref type="bibr" target="#b29">[30]</ref> is a large-scale HOI detection dataset with more than 150,000 annotated instances. It contains 38,118 and 9,658 images for training and testing, respectively. There are a total of 80 object categories, 117 verb categories, and 600 HOI categories. Those HOI categories with fewer than 10 training samples are referred to as "rare" categories and the remaining ones are called as "non-rare" categories; specifically, there are 138 rare and 462 non-rare categories in total. There are two modes of mAP on HICO-DET, namely the Default (DT) mode and the Known-Object (KO) mode. In DT mode, each HOI category is evaluated on all testing images; while in KO mode, one HOI is only evaluated on images that contain its associated object category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HICO-DET. HICO-DET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use the Hourglass-104 model <ref type="bibr" target="#b27">[28]</ref>, pre-trained on the MS-COCO <ref type="bibr" target="#b28">[29]</ref>, as the backbone of GGNet. Moreover, we adopt a lightweight network, named DLA-34 <ref type="bibr" target="#b35">[36]</ref>, as the backbone to perform ablation studies on HICO-DET for shortening experimental cycle. GGNet is trained using the Adam optimizer with an initial learning rate of 1.5e-5 (1.5e-4) and batch size of 7 (23) on V-COCO (HICO-DET) for 120 epochs. For all the experiments, the learning rate is reduced by multiplying 0.1 at the 90th epoch. Resolution of input images is 512 ? 512 and the output stride d of backbone is set to 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Studies</head><p>We perform ablation studies on both V-COCO and HICO-DET datasets to demonstrate the effectiveness of each component of GGNet. Our baseline is constructed by removing the gaze steps from GGNet, and replacing the HNA loss and APM module with their counterparts in PPDM <ref type="bibr" target="#b9">[10]</ref>. Experimental results are tabulated in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Effectiveness of the HNA Loss. As analyzed in Section 3.3, a serious problem of imbalance exists between positive and negative samples for each interaction category. We therefore propose the HNA loss to handle this problem by highlighting the hard negative samples. The adoption of HNA loss promotes the performance of our model by 1.45% (0.42%) mAP on V-COCO (HICO-DET).</p><p>We further evaluate the optimal value of the hyperparameters ? in in the HNA loss. The experimental results are provided in the supplementary material.</p><p>Effectiveness of the Glance-Gaze Strategy. The glanceand-gaze strategy is developed to infer ActPoints to represent the interaction area for one human-object pair. The Gaze Step 1 is found to promote the HOI detection performance by 1.52% and 0.47% in terms of mAP on V-COCO and HICO-DET. Moreover, when the ActPoints is refined by Gaze Step 2, the performance is further improved by 0.40% and 0.49% mAP on V-COCO and HICO-DET, respectively.</p><p>Effectiveness of APM. As the interaction category affects the human-object spatial layout in an HOI instance, we propose the APM module that assigns each interaction category a unique location regressor to facilitate matching of both the human and object points. The adoption of APM promotes the performance of our model by 0.29% and 0.49% in terms of mAP on V-COCO and HICO-DET, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drop-one-out Study. We further perform a drop-one-out study in which each proposed component is removed individually. In particular, as Gaze Step 2 is built based on Gaze</head><p>Step 1, we remove both Gaze Steps 1 and 2 in the experiment where "Gaze # 1" is dropped out. These experimental results further demonstrate that each proposed component is indeed effective at promoting HOI detection performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons with Variants of GGNet</head><p>Comparisons with Variants of the Gaze Step. We compare the performance of gaze step with some possible variants by changing the number of ActPoints, the number of gaze steps, and whether layers are shared in different gaze steps. Experimental results are summarized in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>First, we set the number of gaze steps to 1 and change the number of ActPoints. As the results show, our model achieves the best performance when the number of Act-Points is 25; this may be because a small number of Act-Points is insufficient to cover the entire interaction area, while too many ActPoints will increase the complexity of searching their locations.</p><p>Second, we compare the performance of different numbers of gaze steps. The number of ActPoints is set to 25 here. When the number of gaze steps increases from 1 to 2, our model is promoted by 0.51% in terms of mAP; notably, the performance is not further promoted through the addition of more gaze steps, which may be because this increases the difficulty of model optimization.</p><p>Third, we try sharing the parameters of the two 3 ? 3 Conv layers that generate F 1 and F 2 in <ref type="figure" target="#fig_7">Figure 5</ref>. As <ref type="table" target="#tab_2">Table  2</ref> shows, the performance of our model decreases by 0.29% in terms of mAP. One reason for this is that Gaze Step 2 captures more fine-grained features for interaction prediction; therefore, it is better for the two gaze steps to adopt independent Conv layers. Comparisons with Variants of Feature Aggregation In two-stage methods, it is a common practice to aggregate features of the human instance, the object instance, and their union box for interaction prediction purposes <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Accordingly, in this experiment, we compare the performance of the above feature aggregation approach with our proposed glance-and-gaze strategy. The experimental results are tabulated in <ref type="table" target="#tab_3">Table 3</ref>. All methods in this table are constructed based on our baseline model. Here, "I + H" ("I + O") means that we concatenate the features of each pixel in F 0 (in <ref type="figure" target="#fig_7">Figure 5</ref>) with those of one human (object) center point. "I + H + O" denotes that both features of the human and object center points are aggregated. The human and object center points are obtained via the point matching strategy in <ref type="bibr" target="#b9">[10]</ref>. Moreover, "ActPoints w/o glance and <ref type="table">Table 4</ref>. Performance comparisons on V-COCO. ? denotes methods that are reproduced using their open-source codes. 'A', 'P', 'S', and 'L' represent the appearance feature, human pose feature, spatial feature, and language feature, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Feature AProle gaze" means that we adopt one deformable Conv layer to aggregate features of the predicted coarse ActPoints on F 0 for interaction prediction. The structure of these models is outlined in more detail in the supplementary material.</p><p>From the above, we can make the following observations. First, performance is promoted when either human or object features are aggregated. However, one person may interact with different objects, while a single object may also be interacted by multiple persons. Therefore, features for the center points of human and object instances may lack specific information for each human-object pair. By contrast, our proposed ActPoints adaptively capture features in the discriminative area for each human-object pair; therefore, it outperforms "I + H + O" by 0.93% mAP. Second, when the glance and gaze steps are omitted, the performance of ActPoints drops by 1.59% mAP. These comparisons demonstrate the effectiveness of our proposed glanceand-gaze strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons with State-of-the-Art Methods</head><p>We compare the performance of GGNet with state-ofthe-art methods on both V-COCO and HICO-DET.</p><p>As shown in <ref type="table">Table 4</ref>, GGNet outperforms all state-ofthe-art methods by significant margins on V-COCO. In particular, with the same backbone model, GGNet outperforms one of the most recent one-stage methods, i.e. PPDM <ref type="bibr" target="#b9">[10]</ref> by a large margin of 3.6% in terms of mAP. Moreover, although recent two-stage methods adopt various types of features to promote HOI detection performance, GGNet still outperforms all of these by at least 1.4% in terms of mAP while utilizing appearance features only.</p><p>As shown in <ref type="table">Table 5</ref>, on HICO-DET, GGNet still outperforms state-of-the-art methods by clear margins in different settings of "Backbone Sharing". In particular, GGNet outperforms PPDM <ref type="bibr" target="#b9">[10]</ref>, by 1.74% (2.78%), 2.70% (3.58%), <ref type="table">Table 5</ref>. Performance comparisons on HICO-DET. "Backbone Sharing" represents methods that use the same feature backbone for object detection and interaction detection. denotes two-stage methods that first pre-train their object detectors on COCO, and then further fine-tune the object detectors on HICO-DET. ? denotes methods that use the same object detection results during inference. As <ref type="table">Table 5</ref> shows, several recent two-stage methods have adopted two separate backbones for object detection and interaction prediction. In particular, two approaches have fine-tuned both backbones on the HICO-DET databases, thereby achieving superior performance <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b16">[17]</ref> (marked with in <ref type="table">Table 5</ref>). For its part, however, GGNet has a single shared backbone for object detection and interaction prediction. To facilitate fair comparison, we further test the performance of GGNet with the same object detection results provided by the object detector in DRG <ref type="bibr" target="#b18">[19]</ref>; this setting is denoted as GGNet ? . As shown in <ref type="table">Table 5</ref>, GGNet ? outperforms DRG ? <ref type="bibr" target="#b18">[19]</ref> by a large margin of 4.64% (5.52%) mAP in DT (KO) mode.</p><p>The above comparisons also show that the adoption of a separate object detector can significantly promote GGNet's performance. This may be because the object detection and interaction prediction tasks conflict with each other, as they require different features. It is also worth noting that adopting two backbones does not alter the one-stage nature of GGNet, as the object detection and interaction prediction tasks still run in parallel, regardless of whether or not they adopt the same backbone. The above experiments further justify the effectiveness of GGNet. sen from V-COCO. We can observe that the interaction points, i.e. the green points in first column, are often located in the background area; therefore, their own features are vague to represent the interaction category. In comparison, ActPoints capture cues from discriminative object and human parts, both of which are important for interaction prediction. Moreover, the ActPoints refined by Gaze Step 2 are usually located at more important object and human parts than the ActPoints sampled by Gaze Step 1. In the supplementary material, we also present qualitative comparisons between GGNet and PPDM in terms of HOI detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative Visualization Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Existing one-stage HOI detection methods typically utilize the features of predefined interaction areas for interaction prediction, while these artificially defined areas are usually vague to represent interactions. In this paper, we propose a novel one-stage network, namely GGNet, which adaptively samples a set of action-aware points (ActPoints) via glance and gaze steps. We also propose an action-aware point matching approach that robustly matches target human and objects for each detected interaction. Moreover, a novel hard negative attentive loss is devised to improve the optimization of GGNet. Extensive experiments results show that GGNet outperforms start-of-the-arts on both V-COCO and HICO-DET datasets.</p><p>This supplementary material includes five sections. Section A illustrates the structure of GGNet in the inference stage. Section B conducts ablation study on the value of hyper-parameter ?. Section C carries out the sensitivity analysis for ? 1 . Section D shows the structure of "Variants of Feature Aggregation" in Section 5.2 of the main paper. Section E visualizes the HOI detection results of GGNet and PPDM; some failure cases of GGNet for HOI detection are also presented here.</p><p>A. Structure of GGNet in the Inference Stage <ref type="figure" target="#fig_7">Figure 5</ref> illustrates the structure of GGNet in the inference stage. During inference, the glance step and the first gaze step are only utilized to infer ActPoints; therefore, some layers in these two steps are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study on the Value of ?</head><p>Experiments are conducted on the V-COCO database. The experimental results are summarized in <ref type="table" target="#tab_6">Table 6</ref>. We can observe that the HNA loss achieves the best performance when ? is set to 7. Experiments are conducted on the V-COCO database. The experimental results are listed in <ref type="table" target="#tab_7">Table 7</ref>. We can observe that the GGNet achieves the best performance when ? 1 is set to 0.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Structure of Variants for Feature Aggregation</head><p>In this section, we show the structure of "Variants of Feature Aggregation" in <ref type="table" target="#tab_3">Table 3</ref> of the main paper. All methods in <ref type="table" target="#tab_3">Table 3</ref> share the same structure of the human-object pair matching module as the baseline model. Besides, they all adopt the ordinary V -dimensional element-wise focal loss <ref type="bibr" target="#b9">[10]</ref> for optimization. The structure of Model "I + H" ("I + O"). The model "I + H" is illustrated in <ref type="figure">Figure 6</ref>. To obtain the human feature for each human-object pair, we first attach one human (H) branch on the backbone model. The H branch runs in parallel with the interaction point detection (I) branch. The H branch is realized using a 3 ? 3 Conv layer with ReLU, followed by a 1 ? 1 Conv layer and a sigmoid layer. To enable the feature maps H 1 to be action-aware, we apply a V -dimensional element-wise focal loss to the H branch as supervision.</p><p>Next, we utilize the offset predicted by the human-object pair matching module, i.e. the point matching branch in <ref type="figure">Figure 6</ref>, to predict the human center point for each interaction point in F 1 . Features of the human center point are extracted on H 1 using the bilinear sampling <ref type="bibr" target="#b25">[26]</ref> and are further concatenated with the features of the interaction point. The concatenated features are processed by two successive 1 ? 1 Conv layers for interaction prediction.</p><p>The  <ref type="figure">Figure 7</ref> presents the qualitative comparisons between GGNet and PPDM <ref type="bibr" target="#b9">[10]</ref> in terms of HOI detection results on HICO-DET. We can observe that PPDM fails to predict interaction categories for some images. This is because the interaction points often locate at the background area or unimportant human body area; therefore, their features are ambiguous in semantics for interaction prediction. In comparison, GGNet infers the interaction categories accurately, as the discriminative interaction areas can be captured by our proposed glance-and-gaze strategy. Qualitative comparisons on V-COCO are shown in <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Visualization Results</head><p>We also present some failure cases of GGNet in terms of HOI detection in <ref type="figure">Figure 9</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Comparisons of interaction area definition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Human-object pairs with different interaction categories present different spatial characteristics, i.e. the relative location between one human-object pair. Green points represent interaction points, each yellow arrow specifies the human instance in the pair, while each blue arrow indicates the object instance in the pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Loss Formulation. First, we introduce the Gaussian heatmap masks M ? [?1, 1] H d ? W d ?V , which are used to mark both positive samples and hard negative samples. For a ground truth HOI sample (v i , o i ) with an interaction point located at (x i , y i ), M xiyivi is set to 1 and it is also used as the center of a Gaussian distribution in the v i -th channel of M. The value of the elements in this distribution is within [0, 1]. The interaction category and object category for this HOI sample are denoted as v i and o i , respectively. Second, we infer a set of HOI samples {(v j , o i )} as hard negatives for the v j -th interaction category, with the help of the labeled positive sample (v i , o i ). If (v j , o i ) is not labeled as a positive sample, then we set M xiyivj as -1 and it is also used as the center of another Gaussian distribution in the v j -th channel of M. The value of the elements in this distribution is within [?1, 0].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 Figure 4 .</head><label>44</label><figDesc>visualizes the interaction points and ActPoints predicted by GGNet. Here, three images are randomly cho-Visualization of results in glance and gaze steps. The first column shows the detected interaction point in the Glance Step; the second and third columns visualize the adaptively sampled ActPoints by Gaze Step 1 and Gaze Step 2, respectively. The color of each ActPoint reflects its weight, i.e. discriminative power.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>model "I + O" can be constructed in a similar manner by replacing the above H branch with an object (O) branch. Features of the object center are utilized to augment the features of the corresponding interaction point. The Structure of Model "I + H + O". This model can be constructed by adding both the H and O branches. The features of human center, object center, and the interaction point are concatenated for interaction prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Overview of GGNet in the inference stage. The Glance step and Gaze Step 1 are only used to infer the ActPoints, and the irrelevant layers are discarded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .Figure 7 .Figure 8 .Figure 9 .</head><label>6789</label><figDesc>Overview of the model "I + H" in the training stage. The model is composed of four branches, namely the interaction point detection branch, the human branch, the point matching branch, and the object detection branch. The four branches run in parallel. Features of each interaction point and those of the corresponding human center point are concatenated for interaction prediction. ? denotes the concatenation operation in the channel dimension. Qualitative comparisons between GGNet and PPDM on HICO-DET. The first and second rows show the predictions by PPDM and GGNet respectively. Cyan denotes the interaction points and red stands for ActPoints. Moreover, the human and objects are represented using yellow and blue, respectively. If a person has interaction with an object, they are linked by a green line. We show the top-1 triplet according to the prediction confidence per image. Qualitative comparisons between GGNet and PPDM on V-COCO. Failure cases of GGNet for HOI detection on HICO-DET. The ground-truth interaction and the predicted one are typed in black and red, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Object Detection Offset field Gaze Step 1 Residual offset field</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gaze Step 2</cell></row><row><cell></cell><cell>3 ? 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 ? 1</cell></row><row><cell></cell><cell></cell><cell>F 2</cell><cell></cell><cell></cell><cell></cell><cell>G 2</cell><cell>Interaction Prediction</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell></row><row><cell>Input Image I</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3 ? 3</cell><cell></cell><cell></cell><cell cols="2">5 ? 5</cell><cell>1 ? 1</cell></row><row><cell></cell><cell></cell><cell>F 1</cell><cell>Offset field</cell><cell>G 1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>&lt;human ride snowboard&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Interaction Point</cell><cell>Glance Step</cell><cell>APM</cell></row><row><cell>F</cell><cell>3 ? 3</cell><cell></cell><cell>5 ? 5</cell><cell>1 ? 1</cell><cell></cell></row><row><cell></cell><cell></cell><cell>F 0</cell><cell></cell><cell>3 ? 3</cell><cell>1 ? 1</cell><cell>human</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>snowboard</cell></row><row><cell>Backbone</cell><cell>Glance</cell><cell cols="2">Gaze Step 1</cell><cell>Gaze Step 2</cell><cell cols="2">Action-aware Point Matching</cell><cell>Object Detection</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>promoted HOI</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">detection accuracy by overcoming the so-called verb poly-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">semy problem, while Kim et al. [15] modeled the relation-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation studies on each component of GGNet. DLA-34 is adopted as backbone for experiments on HICO-DET.</figDesc><table><row><cell></cell><cell></cell><cell>Components</cell><cell></cell><cell></cell><cell>mAP</cell><cell></cell></row><row><cell>Methods</cell><cell cols="6">HNA Loss Gaze # 1 Gaze # 2 APM V-COCO HICO-DET (DT)</cell></row><row><cell>Our Baseline</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.06</cell><cell>20.16</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.51</cell><cell>20.58</cell></row><row><cell>Incremental</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>54.03</cell><cell>21.05</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>54.43</cell><cell>21.54</cell></row><row><cell></cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell>53.55</cell><cell>21.03</cell></row><row><cell>Drop-one-out</cell><cell></cell><cell>-</cell><cell>--</cell><cell></cell><cell>52.60 54.21</cell><cell>21.19 21.65</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>54.43</cell><cell>21.54</cell></row><row><cell>GGNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>54.72</cell><cell>22.03</cell></row><row><cell cols="7">APM. The optimal object point (x o opt ,? o opt ) can be inferred</cell></row><row><cell>similarly.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>where P h (x h ,? h ) denotes the object detection score for hu- man point (x h ,? h ); (d hx (x i ,? i ) ,d hy (x i ,? i ) ) denotes the predicted offset from the interaction point to the human point by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparisons with variants of the GazeStep on V-COCO.</figDesc><table><row><cell cols="4"># ActPoints # Gaze Step Sharing AProle</cell></row><row><cell>9</cell><cell>1</cell><cell>-</cell><cell>53.54</cell></row><row><cell>25</cell><cell>1</cell><cell>-</cell><cell>54.32</cell></row><row><cell>49</cell><cell>1</cell><cell>-</cell><cell>53.81</cell></row><row><cell>25</cell><cell>1</cell><cell>-</cell><cell>54.21</cell></row><row><cell>25</cell><cell>2</cell><cell>-</cell><cell>54.72</cell></row><row><cell>25</cell><cell>3</cell><cell>-</cell><cell>54.34</cell></row><row><cell>25</cell><cell>2</cell><cell></cell><cell>54.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with variants of feature aggregation on V-COCO. "I", "H", and "O" denote the features of interaction point, human center point, and object center point, respectively.</figDesc><table><row><cell>Methods</cell><cell>AP role</cell></row><row><cell>I (our baseline)</cell><cell>51.06</cell></row><row><cell>I + H</cell><cell>51.84</cell></row><row><cell>I + O</cell><cell>51.71</cell></row><row><cell>I + H + O</cell><cell>52.32</cell></row><row><cell>ActPoints w/o glance and gaze</cell><cell>51.66</cell></row><row><cell>ActPoints + glance and gaze</cell><cell>53.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Ablation study on the value of ?. 54.33 54.28 54.72 54.45 C. Sensitivity analysis for ? 1</figDesc><table><row><cell>?</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>mAP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>role</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Sensitivity analysis for ?1.</figDesc><table><row><cell>? 1</cell><cell>0.1 0.5</cell><cell>1</cell></row><row><cell cols="3">mAP role 54.72 54.01 53.28</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">More formally, (x (t) k , y (t) k ) is updated as follows: (x (t) k , y (t) k ) = (x (t?1) k , y (t?1) k ) + (?x (t) k , ?y (t) k ),(2)(?x(t) k , ?y (t) k ) = T (t) of f set (G t?1 ),(3)where t stands for the t-th gaze step, while (?x</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">UnionDet: Union-Level Detector Towards Real-Time Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang1y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scaling Human-Object Interaction Recognition Through Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">No-Frills Human-Object Interaction Detection: Factorization, Layout Encodings, and Training Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transferable Interactiveness Knowledge for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose-aware Multi-level Feature Network for Human Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Amplifying key cues for human-object-interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VSGNet: Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ican: Instance-centric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Contextual Attention for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Polysemy deciphering network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting Human-Object Interactions with Action Cooccurrence Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual Compositional Learning for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DRG: Dual Relation Graph for Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Human-Object Interaction Detection using Interaction Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to Detect Human-Object Interactions With Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detailed 2D-3D Joint Representation for Human-Object Interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Schneider and others. Saccade target selection and object recognition: Evidence for a common attentional mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision research</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sac-cadeNet: A Fast and Accurate Object Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Visual semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gps-net: Graph property sensing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cascaded human-object interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Polysemy Deciphering Network for Robust Human-Object Interaction Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
