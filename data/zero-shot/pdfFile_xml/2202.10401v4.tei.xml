<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vision-Language Pre-Training with Triple Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
							<email>jinyu.yang@mavs.uta.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University Of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Duan</surname></persName>
							<email>duajiali@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University Of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Tran</surname></persName>
							<email>sontran@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University Of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University Of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampath</forename><surname>Chanda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University Of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Chen</surname></persName>
							<email>liquchen@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University Of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Zeng</surname></persName>
							<email>zengb@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University Of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trishul</forename><surname>Chilimbi</surname></persName>
							<email>trishulc@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University Of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>jzhuang@uta.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University Of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Vision-Language Pre-Training with Triple Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-language representation learning largely benefits from image-text alignment through contrastive losses (e.g., InfoNCE loss). The success of this alignment strategy is attributed to its capability in maximizing the mutual information (MI) between an image and its matched text. However, simply performing cross-modal alignment (CMA) ignores data potential within each modality, which may result in degraded representations. For instance, although CMA-based models are able to map image-text pairs close together in the embedding space, they fail to ensure that similar inputs from the same modality stay close by. This problem can get even worse when the pre-training data is noisy. In this paper, we propose triple contrastive learning (TCL) for visionlanguage pre-training by leveraging both cross-modal and intra-modal self-supervision. Besides CMA, TCL introduces an intra-modal contrastive objective to provide complementary benefits in representation learning. To take advantage of localized and structural information from image and text input, TCL further maximizes the average MI between local regions of image/text and their global summary. To the best of our knowledge, ours is the first work that takes into account local structure information for multi-modality representation learning. Experimental evaluations show that our approach is competitive and achieves the new state of the art on various common down-stream vision-language tasks such as image-text retrieval and visual question answering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Self-supervision is an active research topic both in vision and language representation learning. Numerous methods have been proposed with an impressive performance on challenging tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40]</ref>. A typical approach is to pre-train a model on massive amounts of unlabeled data in a self-supervised manner, then fine-tune it for downstream tasks (e.g., zero-shot learning and transfer learning) of interest. In the vision, self-supervision can be carried out using exemplars <ref type="bibr" target="#b12">[13]</ref>, predicting the relative position between two random patches <ref type="bibr" target="#b10">[11]</ref> or via solving jigsaw <ref type="bibr" target="#b31">[32]</ref>. In language, masked language modeling (MLM) is widely used as the method of choice for self-supervision.</p><p>Inspired by the success of self-supervision in individual modalities, there is a surging interest in self-supervised vision-language pre-training (VLP) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>, which is essential for multi-modal tasks such as visual question answering (VQA), image-text retrieval, and visual entailment. These tasks heavily rely on joint multi-modal embeddings which are typically obtained by modeling interactions between vision and language features. To achieve this goal, various VLP frameworks are proposed by exploiting massive imagetext pairs in the past few years <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>, where the key insight is to apply a fusion encoder to the concatenation of vision and language features to learn joint representations. Although simple and effective, this strategy suffers from the problem that vision and language features lie in different embedding spaces, which makes the feature fusion quite challenging <ref type="bibr" target="#b25">[26]</ref>. To mitigate this problem, the most recent state of the art <ref type="bibr" target="#b25">[26]</ref> disentangles the learning process into two stages: i) first align cross-modal features by using a contrastive loss (i.e., InfoNCE <ref type="bibr" target="#b32">[33]</ref>) to pull the embeddings of matched image-text pairs together while pushing those of non-matched pairs apart; then ii) apply a fusion encoder to the aligned image and text representations to learn joint embeddings. Specifically, stage 1 aims to maximize the mutual information (MI) between matched image-text pair (I, T ) through InfoNCE loss, which is spurred by the fact that I and T represent two "views" of the same semantic <ref type="bibr" target="#b45">[46]</ref>. However, the limitation of stage 1 lies in that: simply performing cross-modal alignment (CMA) cannot fully guarantee the expressiveness of the learned features that is essential for joint multi-modal representation learning. The main reason is that I and T are unable to fully describe each other. For instance, the text in <ref type="figure" target="#fig_0">(Figure 1 A)</ref> only focus on salient objects in the paired image, while overlooking other detailed and fine-grained information. To align I and T , only co-occurring features are captured by CMA. This is also evidenced by <ref type="bibr" target="#b22">[23]</ref>, where the performance of CMA-based features on image-text retrieval is far greater than intra-modal retrieval (image-image and text-text). Furthermore, the pre-training datasets are usually collected from the web and are inherently noisy. This leads to learning degraded representations, where cross-modal features fail to capture certain key concepts.</p><p>As transformer became increasingly popular in both vision and language tasks, existing VLP methods adopted transformer architecture for extracting visual and linguistic features. Specifically, [CLS] tokens from the vision transformer (e.g., ViT <ref type="bibr" target="#b11">[12]</ref>) and the text transformer (e.g., BERT <ref type="bibr" target="#b9">[10]</ref>) are used to represent the global information of the input. For instance, ALBEF <ref type="bibr" target="#b25">[26]</ref> maximizes MI between vision [CLS] and text <ref type="bibr">[CLS]</ref>. However, global MI maximization fails to consider localized and structural information in the input <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>. One potential side-effect is that it encourages the encoder to mainly extract information from certain unrelated/noisy image patches or text tokens that dominate MI.</p><p>In this paper, we introduce a novel VLP framework called triple contrastive learning (or TCL for short). The key idea is to learn desirable representations by leveraging both crossmodal and intra-modal self-supervision, aiming to make it easier for the fusion encoder to learn multi-modal interactions. To achieve this goal, TCL introduces three contrastive modules: cross-modal alignment (CMA), intra-modal contrastive (IMC), and local MI maximization (LMI), all of which rely on MI maximization. Specifically, i) CMA pulls the embeddings of matched image-text pairs together while pushing those of non-matched pairs apart by maximizing global MI between matched image and text; ii) complementary to CMA, IMC maximizes agreement between differently augmented views of the same data example through maximizing their global MI; iii) LMI encourages high MI between the global representation and every local region (e.g., image patches and text tokens) of the input, which is designed to remedy the side-effects that are introduced by the global MI maximization. The combination of these three modules allows us to i) learn representations that are semantically meaningful not only for cross-modal image-text pairs but also for intra-modal inputs; ii) capture the structural and localized information by extracting relevant features that are shared across local patches/tokens.</p><p>Our main contributions can be summarized as ? We leverage both cross-modal and intra-modal selfsupervision to provide complementary benefits in representation learning, which facilitates modeling of better joint multi-modal features in the fusion encoder;</p><p>? Rather than simply relying on global information for multi-modal contrastive learning, we propose to take advantage of localized and structural information in both image and text input by maximizing local MI maximization between local regions and their global summary;</p><p>Comprehensive empirical studies demonstrate that TCL achieves a new state of the art on a wide range of vi-sion+language benchmarks, such as image-text retrieval and VQA. Specifically, on zero-shot image-text retrieval tasks, our method achieves significant improvement than ALIGN <ref type="bibr" target="#b22">[23]</ref> (a mean recall of 79.5% vs 70.9% on MSCOCO). It is noteworthy that ALIGN is pre-trained on 1.8B imagetext pairs, which is approximately 350? larger than TCL (5M). By pre-training TCL on a large-scale dataset with 14M image-text pairs, we observe a significant performance boost, implying its potential for further improvement with larger datasets. To investigate the effectiveness of each component in TCL, comprehensive ablation studies are also carried out with detailed analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Vision-Language Pre-training (VLP) Inspired by the success of self-supervised learning in intra-modal tasks, there is a surging interest in developing pre-training objectives for tasks with multiple modalities (e.g., vision and language). For instance, to leverage a much broader source of supervision from text, pioneering work CLIP <ref type="bibr" target="#b38">[39]</ref> predicts which text goes with which image, resulting in a taskagnostic model that is even competitive with task-specific supervised models. ALIGN <ref type="bibr" target="#b22">[23]</ref> further scales up CLIP by leveraging a noisy dataset that covers more than one billion image alt-text pairs. Despite these advancements, CLIP and ALIGN are mainly designed for vision-based downstream tasks and ignore the interaction between multiple modalities during pre-training. To fit in vision+language tasks (such as VQA <ref type="bibr" target="#b17">[18]</ref> and visual reasoning), recent studies propose to learn joint multi-modal representations of image content and natural language. Among them, OSCAR <ref type="bibr" target="#b27">[28]</ref>, UNIMO <ref type="bibr" target="#b26">[27]</ref>, VILLA <ref type="bibr" target="#b15">[16]</ref>, and UNITER <ref type="bibr" target="#b7">[8]</ref> use an object detector (e.g., Faster R-CNN <ref type="bibr" target="#b41">[42]</ref>) to capture vision features first, then a multi-layer transformer <ref type="bibr" target="#b44">[45]</ref> is applied to the concatenation of the extracted vision features and text features to learn joint embeddings. However, such kind of strategy suffers from limitations such as i) extracting region features using an object detector is computationally inefficient and ii) the quality of visual features is largely limited by the predefined visual vocabulary in pre-trained object detectors. To address this issue, rather than rely on region-based visual features, SOHO <ref type="bibr" target="#b20">[21]</ref> takes a whole image as input and extracts compact image features through a visual dictionary, which favors 10 times faster inference time than region-based methods. ViLT <ref type="bibr" target="#b23">[24]</ref> totally discards convolutional visual features and adopts vision transformer <ref type="bibr" target="#b11">[12]</ref> to model long-range dependencies over a sequence of fixed-size non-overlapping image patches. Although these aforementioned methods achieve remarkable performance, they fail to conduct image-text alignment before fusion, which makes it challenging to learn the interaction between different modalities.</p><p>To remedy this, ALBEF <ref type="bibr" target="#b25">[26]</ref> applies a contrastive loss to align image and text features before modeling their joint representations, which delivers the state-of-the-art performance. Our method shares similar spirits with ALBEF, but with clear differences as follows: i) instead of only performing cross-modal alignment (CMA), we propose to leverage both cross-modal and intra-modal self-supervision to enforce the learned representations are semantic meaningful. The rationale is that cross-modal alignment alone may result in feature degeneration problem: although features from different modalities are well-separated, those from the same modality fall within a narrow cone and have high similarity. ii) we introduce local alignment to the cross-modal scenario by maximizing mutual information (MI) between local regions and global representations. Compared with the global alignment strategy used in ALBEF, maximizing local MI encourages our model to learn features that are shared across image patches/text tokens. Furthermore, local alignment prevents simply capturing noise or unrelated features.</p><p>CODIS <ref type="bibr" target="#b14">[15]</ref> is a concurrent work, which adopts a teacherstudent distillation paradigm to guide the learning process. Different from our method, CODIS performs feature alignment using cluster representations.</p><p>Mutual Information Maximization Mutual information (MI) aims to measure the relationship between random variables or determine the amount of shared information. MI is widely used in unsupervised feature learning, where the key idea is to maximize MI between the input and output <ref type="bibr" target="#b29">[30]</ref>. However, the MI estimation of high dimensional random variables is quite difficult and intractable <ref type="bibr" target="#b34">[35]</ref>, especially for deep neural networks. To this end, MINE <ref type="bibr" target="#b1">[2]</ref> exploits dual optimization to offer a general-purpose estimator of MI. Another alternative is InfoNCE <ref type="bibr" target="#b32">[33]</ref>, which is a categorical cross-entropy loss that identifies the positive sample amongst a set of negative samples. InfoNCE is proved to be a lower bound of MI <ref type="bibr" target="#b32">[33]</ref>, such that minimizing InfoNCE loss can indirectly maximize MI. However, existing studies simply maximize MI between the complete input and output (i.e., global MI maximization), which is proven to be insufficient for meaningful representation learning <ref type="bibr" target="#b19">[20]</ref>. DIM <ref type="bibr" target="#b19">[20]</ref> addresses this limitation by introducing local MI, i.e., maximizing the average MI between local regions of image input and the encoder output. AMDIM <ref type="bibr" target="#b0">[1]</ref> further extends DIM to maximize MI between features extracted from independent augmentations of the same image.</p><p>Both DIM and AMDIM are conducted in intra-modal tasks. In contrast, we introduce local MI maximization for multi-modal problems to benefit cross-modal representation learning. Specifically, we encourage high MI between the global representation and every local region (e.g., image patches and text tokens) of the input. This enables more transferable representations which are evidenced by our empirical studies. Furthermore, rather than using a CNN-based network, our local MI is built upon the transformer architecture. Therefore, the sequential patch tokens in the transformer actually give us free access to local features without the need to pull local information from intermediate lay-ers. Our experiments show that patch embeddings from the last layer outperform intermediate-layer patches with the transformer backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first describe the model architecture of our method <ref type="figure" target="#fig_0">(Figure 1)</ref>, followed by the uni-modal representation learning. After that, we detail the proposed triple contrastive learning modules: cross-modal alignment (CMA), Intra Modal Contrastive (IMC), and Local MI maximization (LMI). In the end, we brief two pre-training objectives, i.e., image-text matching (ITM) and masked language modeling (MLM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>An overview of our method is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, which contains a vision encoder g(?) for learning visual features from the image input, a text encoder h(?) for learning linguistic features from the text input, and a fusion encoder for learning multi-modal interactions. All of these encoders adopt transformer-based architecture <ref type="bibr" target="#b44">[45]</ref>, which are detailed in section 4.3. For each encoder, we maintain a paired momentum encoder that is implemented by momentum-based moving average strategy by following the same setting in <ref type="bibr" target="#b18">[19]</ref>.</p><formula xml:id="formula_0">Formally, ?? = m?? + (1 ? m)? g , where?(?) is momen- tum vision encoder, m ? [0, 1] is a momentum coefficient.</formula><p>Similarly, we use?(?) to denote the momentum text encoder. Uni-modal encoders g(?) and h(?) are used to learn robust visual and linguistic features from the given input, then an alignment module is applied to the learned features to align both cross-modal and intra-modal representations before fusion. We detail each component in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Uni-modal Representation Learning</head><p>Given an image-text pair (I, T ), two separate augmentations are applied to the image to obtain two correlated "views", i.e., I 1 and I 2 . Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, we consider two random "views" of the same image under random data augmentation as a positive pair. Each augmented image is split into fixed-size patches which are then linearly mapped and embedded with positional information <ref type="bibr" target="#b11">[12]</ref>. Similar to BERT <ref type="bibr" target="#b9">[10]</ref>, a class token [CLS] is prepended to the image patches, serving as the representation of the whole image. The obtained sequential embeddings of I 1 are finally fed into</p><formula xml:id="formula_1">g(?) to learn desired visual representations {v cls , v 1 , ..., v M },</formula><p>where M is the total number of image patches. For I 2 , we use?(?) to learn its representations {v cls ,v 1 , ...,v M }. For the text input T , we follow <ref type="bibr" target="#b9">[10]</ref> to obtain {t cls , t 1 , ..., t N } and {t cls ,t 1 , ...,t N } by h(T ) and?(T + ), where N is the length of text tokens, T + = T .</p><p>To model the interaction between image and text features, previous VLP work directly apply a fusion encoder to the concatenation of {v cls , v 1 , ..., v M } and {t cls , t 1 , ..., t N } to learn joint multi-modal embeddings. However, the most obvious drawback of this strategy is that visual and linguistic  features lie in different embedding spaces, which is challenging for the fusion encoder to learn their interactions <ref type="bibr" target="#b25">[26]</ref>. To alleviate this limitation, we propose an alignment module that is applied to the learned visual and linguistic features before fusion. Specifically, our alignment module contains three contrastive learning objectives, i.e., CMA, IMC, and LMI. We discuss each objective below and show that they play a complementary role in the feature alignment and can benefit multi-modal feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-Modal Alignment (CMA)</head><p>The goal of CMA is to pull embeddings of the matched image-text pair (sampled from the joint distribution) together while pushing those of unmatched pairs apart (sampled from the product of marginal distributions). In other words, CMA aims to maximize the MI between the image and text that are matched, which are assumed to describe the same semantic meaning. For instance, the text in <ref type="figure" target="#fig_0">Figure 1</ref> (A) describes high-level information (e.g., the occurrence of certain events or presence of certain objects) in the paired image. Since direct maximization of MI for continuous and highdimensional variables is intractable <ref type="bibr" target="#b1">[2]</ref>, we instead minimize InfoNCE loss <ref type="bibr" target="#b32">[33]</ref> which represents the lower bound of MI. Formally, the InfoNCE loss for image-to-text is defined as: </p><formula xml:id="formula_2">I 1 , T + ) = f v (v cls ) Tf t (t cls ), where f v (?) andf t (?)</formula><p>are two projection heads that map representations to the space where InfoNCE loss is applied. To maintain the negative text samplesT , following <ref type="bibr" target="#b25">[26]</ref>, we use a large queue that keeps the most recent K projected representationsf t (t cls ). Similarly, the loss of text-to-image is formulated by: </p><p>where sim(T,</p><formula xml:id="formula_4">I 2 ) = f t (t cls ) Tf v (v cls ), f t (?)</formula><p>andf v (?) are two projection heads.? = {? 1 , ...,? K } is a queue of negative image examples which store the most recent K projected featuresf v (v cls ). Taken together, we define the loss of CMA as:</p><formula xml:id="formula_5">L cma = 1 2 [L nce (I 1 , T + ,T ) + L nce (T, I 2 ,?)]<label>(3)</label></formula><p>Intuitively, by minimizing L cma , we encourage the visual features and linguistic features to be aligned well in the embedding space and in turn ease the feature fusion. However, CMA loss 1 ignores the self-supervision within each modality, thus failing to guarantee the desirable expressiveness of learned features. The reason is that i) text usually cannot fully describe the paired image. For instance, although the text in <ref type="figure" target="#fig_0">Figure 1 (A)</ref> captures most of the salient objects in the image, it overlooks detailed features of each object, such as the cloth of the man. Therefore, simply pulling embeddings of image-text pair together results in degraded representations <ref type="figure" target="#fig_0">(Figure 1 B)</ref>; and ii) image-text pairs used for pre-training are inherently noisy, which makes the problem in i) even worse. To mitigate these limitations, we propose to further make use of intra-modal self-supervision by introducing Intra-Modal Contrastive (IMC) objective as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Intra-Modal Contrastive (IMC)</head><p>Different from CMA, IMC attempts to learn the semantic difference between positive and negative samples within the same modality. For the visual modality, we consider two random "views" (I 1 , I 2 ) of the same image I under random data augmentation as a positive pair. Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, we maximize agreement between (I 1 , I 2 ) by using the contrastive loss L nce (I 1 , I 2 ,?). Similar to Equation 2, we define sim(I 1 ,</p><formula xml:id="formula_6">I 2 ) = f v (v cls ) Tf v (v cls ).</formula><p>For the text input, we follow <ref type="bibr" target="#b16">[17]</ref> to take a text and predict itself in a contrastive objective. This is achieved by considering standard dropout as minimal data augmentation for the text, and applying independently sampled dropout masks for identical positive pairs, i.e., T + = T . Different from <ref type="bibr" target="#b16">[17]</ref> that uses in-batch negatives, we use the same negative text queueT in Equation 1 instead. The contrastive objective can be described by L nce (T, T + ,T ), where sim(T, T + ) = f t (t cls ) Tf t (t cls ). Overall, we minimize the following objective to guarantee reasonable intra-modal representation learning.</p><formula xml:id="formula_7">L imc = 1 2 [L nce (T, T + ,T ) + L nce (I 1 , I 2 ,?)]<label>(4)</label></formula><p>Specifically, our model is encouraged to learn representations that keep alignment between semantically-related positive pairs within a modality. Most importantly, L imc enforces the uniformity of the whole representation space of image and text such that the embeddings are uniformly distributed <ref type="bibr" target="#b46">[47]</ref>. Therefore, CMA and IMC are designed to play a complementary role in the representation learning: i) CMA maps matched image-text pair close in the embedding space, and ii) IMC maximizes agreement between differently augmented views of the same data example. Combining them together improves the quality of the learned representations <ref type="figure" target="#fig_0">(Figure 1 B)</ref> and can further facilitate joint multi-modal learning in the fusion encoder. One limitation of IMC is that it simply performs the contrastive objective on [CLS] tokens of vision encoders and text encoders, where [CLS] tokens are assumed to represent the global information of the input. In other words, IMC maximizes the global MI between differently augmented views. However, the drawbacks of global MI maximization lie in that: i) it ignores the localized and structural information in the input <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>; ii) certain unrelated local regions may dominate the MI, resulting in the model that is biased to learning unrelated features. For instance, noisy patches can represent a larger "quantity" of information than semantic-meaningful patches that occur repeatedly <ref type="bibr" target="#b19">[20]</ref>. To remedy this issue, we introduce local MI maximization into multi-modal representation learning as detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Local MI Maximization (LMI)</head><p>The goal of local MI maximization is to encourage high MI between the global representation and every local region (e.g., image patches and text tokens) of the input. Rather than considering the [CLS] token pair (e.g., (v cls ,v cls ) of (I 1 , I 2 )) as a positive pair, we pair [CLS] token from one augmented version, with patch embeddings in the other independently augmented version of the input. Without loss of generality, we take the vision input (I 1 , I 2 ) as an example. Specifically, we consider {v i } M i=1 as positive examples of v cls , while patch embeddings from other images in the same batch are used to build up negative examples. Similarly, {t j } N j=1 are considered as positive examples of t cls , while text tokens from other in-batch texts are negative examples. We maximize the average MI between global and local regions by minimizing the following loss:</p><formula xml:id="formula_8">L lmi = 1 2 1 M M i=1 L nce (I 1 , I i 2 ,? l ) + 1 N N j=1 L nce (T, T j + ,T l ) (5) where sim(I 1 , I i 2 ) = f v (v cls ) Tf v (v i ), sim(T, T j + ) = f t (t cls ) Tf t (t j )</formula><p>,? l andT l are in-batch negative image and text patch embeddings, respectively. Therefore, minimizing L lmi allows our model to encode the representations of data that are shared across all patches, rather than encoder representation from certain patches which dominate MI. Another perspective is that local MI maximization encourages the model to predict local from the global representation, which forces the model to also capture fine-grained information and in turn to benefit joint representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Image-Text Matching (ITM)</head><p>To fuse vision and language representations, we adopt ITM which is widely used in previous VLP studies. Given an image-text pair, ITM predicts whether they are matched (positive examples) or not (negative examples), which can be regarded as a binary classification problem. Following <ref type="bibr" target="#b25">[26]</ref>, the fusion encoder takes {v cls , v 1 , ..., v M } and {t cls , t 1 , ..., t N } as input. We use [CLS] token of the fusion encoder as the joint representation of the input image-text pair, which is then fed into a fully-connected layer to predict the matching probability ?(I, T ). We assume that each image-text pair (I, T ) sampled from the pre-training datasets is a positive example (with label 1) and construct negative examples (with label 0) through batch-sampling <ref type="bibr" target="#b25">[26]</ref>. The ITM loss is defined as:</p><formula xml:id="formula_9">L itm = E p(I,T ) H(?(I, T ), y (I,T ) )<label>(6)</label></formula><p>where H(; ) is the cross-entropy, y (I,T ) denotes the label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Masked Language Modeling (MLM)</head><p>We adopt MLM from BERT <ref type="bibr" target="#b9">[10]</ref>, which aims to predict the ground truth labels of masked text tokens T msk . Specifically, we randomly mask out text tokens with a probability of 15%, and replace them with a special [MASK] token 80% of the time, and 10% with random words, and leave it unchanged for the remaining 10% of the time <ref type="bibr" target="#b9">[10]</ref>. Different from BERT, our MLM is conditioned on both surrounding text tokens of T msk and image representations. The MLM loss is defined as:</p><formula xml:id="formula_10">L mlm = E p(I,T msk ) H(?(I, T msk ), y T msk )<label>(7)</label></formula><p>where ?(I, T msk ) is the predicted probability of T msk , and y T msk is ground truth. The overall training objective of our model is:</p><formula xml:id="formula_11">L = L cma + L imc + L lmi + L itm + L mlm (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pre-training Datasets</head><p>Following previous experimental protocols <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26]</ref>, we use COCO <ref type="bibr" target="#b28">[29]</ref>, Visual Genome (VG) <ref type="bibr" target="#b24">[25]</ref>, Conceptual Captions (CC) <ref type="bibr" target="#b42">[43]</ref>, and SBU Captions <ref type="bibr" target="#b33">[34]</ref> as the pre-training dataset in our study, where a total of 4.0M unique images and 5.1M image-text pairs are covered. We term this dataset as a 4M dataset in our study. To prove that our method can be applied to large-scale datasets, we further use CC12M <ref type="bibr" target="#b2">[3]</ref>. Together with the 4M dataset, we, therefore, reach largescale pre-training data with 14.97M unique images and 16M image-text pairs ( <ref type="table" target="#tab_1">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Downstream Tasks</head><p>Image-Text Retrieval includes two tasks: (1) image as query and text as targets (TR); (2) text as query and image as targets (IR). The pre-trained model is evaluated on Flickr30K <ref type="bibr" target="#b36">[37]</ref> and COCO <ref type="bibr" target="#b28">[29]</ref> by following both fine-tuning and zeroshot settings. For the fine-tuning setting, the pre-trained model is fine-tuned on the training data and evaluated on the validation/test data. For the zero-shot setting, the pre-trained model is directly evaluated on the test data. In particular, for zero-shot retrieval on Flickr30K, we follow <ref type="bibr" target="#b25">[26]</ref> to evaluate the model fine-tuned on COCO.</p><p>Visual Question Answering (VQA) <ref type="bibr" target="#b17">[18]</ref> aims to predict the answer given an image and a question (in text format), which requires an understanding of vision, language, and commonsense knowledge to answer. We consider this task as a generation problem by following the same setting in <ref type="bibr" target="#b25">[26]</ref>.</p><p>Specifically, an answer decoder is fine-tuned to generate the answer from the 3,192 candidates.</p><p>Visual Entailment (SNLI-VE) <ref type="bibr" target="#b47">[48]</ref> predicts whether a given image semantically entails a given text, which is a three-classes classification problem. Specifically, the class or relationship between any given image-text pair can be entailment, neutral, or contradictory. Compared with VQA, this task requires fine-grained reasoning.</p><p>Visual Reasoning (NLVR 2 ) <ref type="bibr" target="#b43">[44]</ref> determines whether a natural language caption is true about a pair of photographs. We evaluate our model on NLVR 2 dataset which contains 107,292 examples of human-written English sentences paired with web photographs. Since this task takes a text and two images as input, we extend our model by following <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>All of our experiments are performed on 8 NVIDIA A100 GPUs with PyTorch framework <ref type="bibr" target="#b35">[36]</ref>. Our vision encoder is implemented by ViT-B/16 with 12 layers and 85.8M parameters. Both the text encoder and the fusion encoder are implemented by a 6-layer transformer. They are initialized by the first 6 layers and the last 6 layers of BERT base (123.7M parameters), respectively. We set K = 65, 536 and m = 0.995. For the pre-training stage, the model is trained for 30 epochs with a batch size of 512. We use mini-batch AdamW optimizer <ref type="bibr" target="#b30">[31]</ref> with a weight decay of 0.02. The learning rate is initialized as 1e ? 5 and is warmed up to 1e ? 4 after 2,000 training iterations. We then decrease it by the cosine decay strategy to 1e ? 5. For data augmentation, a 256?256-pixel crop is taken from a randomly resized image and then undergoes random color jittering, random grayscale conversion, random Gaussian Blur, random horizontal flip, and RandAugment <ref type="bibr" target="#b8">[9]</ref>. During the fine-tuning stage, the image resolution is increased to 384?384 and the positional encoding is interpolated according to the number of image patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Image-Text Retrieval</head><p>To assess the generalization of the learned representations, the common practice is to perform the zero-shot transfer of the trained model to downstream tasks. We evaluate our model by benchmarking the zero-shot image-text retrieval tasks on Flickr30K and COCO datasets by following the standard evaluation protocol. As shown in <ref type="table">Table 2</ref>, our approach achieves the best performance while outperforming the existing state-of-the-art by a large margin. Compared with ViLT <ref type="bibr" target="#b23">[24]</ref> which directly uses a transformer encoder to model the interaction between word and image patch embeddings, we improve +9.5% (average) on COCO and +12.2% (average) on Flickr30K, revealing the necessity of conducting cross-modal alignment before fusion. AL-BEF <ref type="bibr" target="#b25">[26]</ref> is closely related to our work, which aligns image and text embeddings first, then uses a fusion encoder to learn joint representations. Furthermore, ALBEF shares the same pre-training datasets with our method, thus making them comparable. However, ALBEF ignores the intra-modal supervision, therefore the expressiveness of the learned fea-    <ref type="table" target="#tab_3">Table 3</ref>. On the medium-sized COCO dataset, we surpass ALBEF [26] by 2.5% absolute TR/R@1 and 2.2% absolute IR/R@1, revealing that our model can further benefit from fully-supervised training. We also compete favorably against prior baselines on the small-sized Flickr30K dataset. The only exception is ALIGN <ref type="bibr" target="#b22">[23]</ref>, which outperforms our method by +0.48% averaged (89.69% vs 89.21%) on COCO and Flickr30K, while at the expense of huge computational resources. This is especially problematic for the scenario/researchers with limited budgets. We believe that our method can also largely benefit from a much larger pre-training dataset, which is evidenced in section 4.6. <ref type="table" target="#tab_4">Table 4</ref> shows the performance comparison on VQA, VE, and NLVR 2 which requires image+text as inputs. In other words, to make succeed in these tasks, the model is supposed to have the capability in learning joint multi-modal embeddings. Among five out of six criteria, we deliver state-of-theart results, implying that explicitly considering cross-modal alignment and intra-modal supervision contribute to the fea- ture fusion. Note that VinVL <ref type="bibr" target="#b48">[49]</ref> outperforms our method, the main reason is that its pre-training corpus contains visual QA datasets, including GQA <ref type="bibr" target="#b21">[22]</ref>, VQA <ref type="bibr" target="#b17">[18]</ref>, and VG-QAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">VQA, VE, and NLVR 2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>To learn the effectiveness of the newly proposed modules (i.e., IMC and LMI) in improving the multi-modal representation learning, we perform ablation studies on image-text retrieval tasks shown in <ref type="table">Table 5</ref>. Since ALBEF <ref type="bibr" target="#b25">[26]</ref> is implemented by using loss function CMA+ITM+MLM, we thus use the results in ALBEF as the baseline. We investigate two choices of IMC, i) IMC (w/o aug): only random crop, random horizontal flip, and RandomAugment are applied to the input image and set I 1 = I 2 by following ALBEF; ii) IMC: I 1 and I 2 are two augmented views of the input image with stronger data augmentation as discussed in section 4.3. Both strategies can improve the performance by a large margin, while stronger augmentation works better that is consistent with previous studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. The performance is further improved by incorporating LMI, indicating the importance of localized and structural information in representation learning.</p><p>During the pre-training stage, each image is split into 256 patches with a size 16 ? 16. To rule out the probability that small image patches may not contain enough information for local MI maximization, we apply global average pooling to the last-layer patch embeddings {v 1 , ...,v M }, resulting in M = 16 patches for LMI in Equation 5. To maintain the spatial relationship among patches, we reshape {v 1 , ...,v M } to the original 3D image space then apply the pooling operation. Notably, different from <ref type="bibr" target="#b19">[20]</ref> which uses feature maps from the intermediate layer as local information, we use patch embeddings pulled from the last layer. We examine these two choices on image-text retrieval tasks as shown in  <ref type="table">Table 7</ref>. Ablation study of the size of pre-training datasets. R@1 is reported. <ref type="table">Table 6</ref> and observe the importance of image patch pooling. In addition, the performance of using last-layer patches is comparable to, if not better than, using patches from intermediate layers (i.e., 9th layer in?(?) and 4th layer in?(?)). We suspect that the difference between features learned by CNNs and vision transformers lead to this observation <ref type="bibr" target="#b40">[41]</ref>.</p><p>To study the impact of training on larger-scale datasets, we perform an ablation study on 14M datasets by using +IMC (w/o aug) as shown in <ref type="table">Table 7</ref>. We could clearly see that the larger scale dataset gave a significant boost in performance. We hypothesis that our model has the potential for further improvement if pre-trained on further large datasets.</p><p>We further investigate the importance of the momentum coefficient m and observe that m = 0.5 reaches the best performance (see supplementary). This is different from MoCo <ref type="bibr" target="#b18">[19]</ref> which claims that a reasonable momentum should be in 0.99?0.9999. We leave this as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations</head><p>The learned representations may tend to features present in the available data. If there are underrepresented groups, the model may be biased and perform worse on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a new vision-language pretraining framework named TCL (short for triple contrastive learning). Different from previous studies that simply align image and text representations through a cross-modal contrastive loss, TCL further considers intra-modal supervision to guarantee that the learned representations are also meaningful within each modality, and in turn benefits cross-modal alignment and joint multi-modal embedding learning. To incorporate the localized and structural information in representation learning, TCL further introduces the local MI which maximizes the mutual information between the global representation and the local information from image patches or text tokens. Experimental results on widely used benchmarks show that TCL outperforms existing SOTA methods by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(A): An overview of our framework which consists of a vision encoder, a text encoder, and a fusion encoder. Each encoder has a paired momentum encoder updated by the momentum-based moving average. For the image input, we apply two separate data augmentation operators (a and b) which are sampled from the same family of augmentations. The alignment module contains three contrastive objectives (i.e., CMA, IMC, and LMI) for both cross-modal and intra-modal representation learning (make it easier for the fusion encoder to learn joint multi-modal embeddings). (B): The motivation of leveraging both cross-modal and intra-modal supervision. The original image (pink) is augmented to two different views (green). For CMA only, the middle image only has a positive text example (green) and treats other texts (red) as negatives. Its embedding (blue cirble) would be close to its positive text example. By incorporating IMC, it has two positive examples (one text and one image) and two sets of negative examples (one from text and one from image) and tends to learn more reasonable embeddings (blue square).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>L</head><label></label><figDesc>nce (I 1 , T + ,T ) = ?E p(I,T ) log e (sim(I1,T+)/? ) K k=1 e (sim(I1,T k )/? ) (1) where ? is a temperature hyper-parameter,T = {T 1 , ...,T K } is a set of negative text examples that are not matched to I 1 , sim(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L</head><label></label><figDesc>nce (T, I 2 ,?) = ?E p(I,T ) log e (sim(T,I2)/? ) K k=1 e (sim(T,? k )/? )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Statistics of pre-training datasets.</figDesc><table><row><cell></cell><cell>COCO</cell><cell>VG</cell><cell>SBU</cell><cell>CC</cell><cell>CC12M</cell></row><row><cell># images</cell><cell>113K</cell><cell>100K</cell><cell>859K</cell><cell>2.92M</cell><cell>10.97M</cell></row><row><cell># text</cell><cell>567K</cell><cell>769K</cell><cell>859K</cell><cell>2.92M</cell><cell>10.97M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison of fine-tuned image-text retrieval on Flickr30K and COCO datasets. For completeness, we also provide the results of ALIGN<ref type="bibr" target="#b25">[26]</ref> which uses 1.8B image-text pairs (1.2B unique images) for pre-training.</figDesc><table><row><cell>Method</cell><cell>#Images</cell><cell cols="6">VQA test-dev test-std dev test-P val NLVR 2 SNLI-VE test</cell></row><row><cell>OSCAR [28]</cell><cell>4M</cell><cell cols="4">73.16 73.44 78.07 78.36</cell><cell>?</cell><cell>?</cell></row><row><cell>UNITER [8]</cell><cell>4M</cell><cell cols="6">72.70 72.91 77.18 77.85 78.59 78.28</cell></row><row><cell>ViLT [24]</cell><cell>4M</cell><cell>71.26</cell><cell>?</cell><cell cols="2">75.7 76.13</cell><cell>?</cell><cell>?</cell></row><row><cell>UNIMO [27]</cell><cell>4M</cell><cell cols="2">73.29 74.02</cell><cell>?</cell><cell>?</cell><cell cols="2">80.0 79.1</cell></row><row><cell>VILLA [16]</cell><cell>4M</cell><cell cols="6">73.59 73.67 78.39 79.30 79.47 79.03</cell></row><row><cell>ALBEF [26]</cell><cell>4M</cell><cell cols="6">74.54 74.70 80.24 80.50 80.14 80.30</cell></row><row><cell>Ours</cell><cell>4M</cell><cell cols="6">74.90 74.92 80.54 81.33 80.51 80.29</cell></row><row><cell>VinVL [49]</cell><cell>6M</cell><cell cols="4">75.95 76.12 82.05 83.08</cell><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison on vision+language tasks.</figDesc><table><row><cell>attributed to the consideration of intra-modal supervision.</cell></row><row><cell>Overall, the representations learned by our method are more</cell></row><row><cell>general and transferable than existing baselines.</cell></row><row><cell>For fine-tuned experiments, we set up new benchmark</cell></row><row><cell>results as shown in</cell></row><row><cell>tures cannot be guaranteed. Compared with ALBEF, our</cell></row><row><cell>method brings +2.7% TR/R@1 boost and +3.4% IR/R@1</cell></row><row><cell>boost on MSCOCO (5K) dataset by explicitly leveraging</cell></row><row><cell>the intra-modal information from both global and local per-</cell></row><row><cell>spectives. Details of intra-modal representation analysis are</cell></row><row><cell>referred to the supplementary. It is worth mentioning that</cell></row><row><cell>our method demonstrates a significant improvement over</cell></row><row><cell>ALIGN [23], i.e., a mean of 79.5% vs 70.9% on COCO and</cell></row><row><cell>94.0% vs 92.2% on Flickr30K. Note, ALIGN is pre-trained</cell></row><row><cell>on 1.8B image-text pairs which is approximately 360? times</cell></row><row><cell>more image-text pairs than our model. This observation sug-</cell></row><row><cell>gests that our method is more data-efficient which is mainly</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>TR IR TR IR TR IR TR IR CMA+ITM+MLM68.7 50.1 90.5 76.8 73.1 56.8 94.3 82.8 +IMC (w/o aug) 71.1 52.2 92.0 78.6 75.0 58.6 94.5 82.9 +IMC 71.4 53.3 92.1 78.9 75.6 58.8 95.1 83.1 +IMC+LMI (Ours) 71.4 53.5 93.0 79.6 75.6 59.0 94.9 84.0 Ablation study of each component on image-text retrieval tasks. The R@1 is reported. For CMA+ITM+MLM, we use the results in ALBEF<ref type="bibr" target="#b25">[26]</ref>. Ablation study of image patch pooling and intermediate local feature on image-text retrieval. R@1 is reported.</figDesc><table><row><cell></cell><cell></cell><cell>Zero-Shot</cell><cell>Fine-Tune</cell></row><row><cell>Module</cell><cell></cell><cell cols="2">MSCOCO Flickr30K MSCOCO Flickr30K</cell></row><row><cell></cell><cell></cell><cell>Zero-Shot</cell><cell>Fine-Tune</cell></row><row><cell cols="2">Pooling Intermediate</cell><cell cols="2">MSCOCO Flickr30K MSCOCO Flickr30K</cell></row><row><cell></cell><cell></cell><cell cols="2">TR IR TR IR TR IR TR IR</cell></row><row><cell></cell><cell></cell><cell cols="2">71.5 52.9 92.4 79.1 75.7 58.6 94.6 83.3</cell></row><row><cell></cell><cell>?</cell><cell cols="2">71.4 52.9 91.5 77.9 75.7 58.6 94.4 82.3</cell></row><row><cell>?</cell><cell>?</cell><cell cols="2">71.8 53.2 93.2 79.2 75.6 58.7 94.8 82.8</cell></row><row><cell>?</cell><cell></cell><cell cols="2">71.4 53.5 93.0 79.6 75.6 59.0 94.9 84.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>TR IR TR IR TR IR TRIR +IMC (w/o aug) (4M) 71.1 52.2 92.0 78.6 75.0 58.6 94.5 82.9 +IMC (w/o aug) (14M) 72.7 54.1 94.6 83.6 77.9 60.9 96.2 86.0</figDesc><table><row><cell></cell><cell>Zero-Shot</cell><cell>Fine-Tune</cell></row><row><cell>Module</cell><cell cols="2">MSCOCO Flickr30K MSCOCO Flickr30K</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/uta-smile/TCL 2 This work was done while Jinyu Yang was interning at Amazon</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">ALBEF<ref type="bibr" target="#b25">[26]</ref> applies a special case of Lcma by setting I 1 = I 2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>This work was partially supported by US National Science Foundation IIS-1553687 and Cancer Prevention and Research Institute of Texas (CPRIT) award (RP190107).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00910</idno>
		<title level="m">Learning representations by maximizing mutual information across views</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
		<title level="m">Mine: mutual information neural estimation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3558" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feilong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duzhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09061</idno>
		<title level="m">Vlp: A survey on visionlanguage pre-training</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A survey of vision-language pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne Xin</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10936</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-modal alignment using representation codebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiali</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trishul</forename><surname>Chilimbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for vision-and-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06195</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seeing out of the box: End-toend pre-training for vision-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Vilt: Vision-andlanguage transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03334</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07651</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15409</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1143" to="1151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimation of entropy and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1191" to="1253" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Sacheti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Do vision transformers see like convolutional neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08810</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00491</idno>
		<title level="m">Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno>PMLR, 2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.06706</idno>
		<title level="m">Visual entailment: A novel task for fine-grained image understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
