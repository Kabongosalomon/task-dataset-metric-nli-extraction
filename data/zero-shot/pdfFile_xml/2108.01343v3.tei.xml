<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">I3CL: Intra-and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
							<email>dubo@whu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>jing.zhang1@sydney.edu.au.j.liuis</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Liu</surname></persName>
							<email>liu-juhua@whu.edu.cncorrespondingauthor.d.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Dacheng Tao</surname></persName>
							<email>dacheng.tao@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">search Center for Multimedia Software</orgName>
								<orgName type="department" key="dep2">Institute of Arti-ficial Intelligence</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="laboratory">Hubei Key Laboratory of Multimedia and Network Communica-tion Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">with School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engi-neering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">with Research Center for Graphic Communica-tion, Printing and Packaging, and Institute of Artificial In-telligence</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Tao is with JD Explore Academy, China and School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">I3CL: Intra-and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Text Detection ? Collaborative Learn- ing ? Semi-supervised Learning ? Deep Learning ? Transformer</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing methods for arbitrary-shaped text detection in natural scenes face two critical issues, i.e., 1) fracture detections at the gaps in a text instance; and 2) inaccurate detections of arbitrary-shaped text instances with diverse background context. To address these issues, we propose a novel method named Intraand Inter-Instance Collaborative Learning (I3CL). Specifically, to address the first issue, we design an effective convolutional module with multiple receptive fields, which is able to collaboratively learn better character and gap feature representations at local and long ranges inside a text instance. To address the second issue, we devise an instance-based transformer module to exploit the dependencies between different text instances and a global context module to exploit the semantic context from the shared background, which are able to collaboratively learn more discriminative text feature representation. In this way, I3CL can effectively exploit the intra-and inter-instance dependencies together in a unified end-to-end trainable frame-B. Du and J. Ye are with National Engineering Re-). This work was done during Jian Ye's internship at JD Explore Academy.</p><p>work. Besides, to make full use of the unlabeled data, we design an effective semi-supervised learning method to leverage the pseudo labels via an ensemble strategy. Without bells and whistles, experimental results show that the proposed I3CL sets new state-of-the-art results on three challenging public benchmarks, i.e., an F-measure of 77.5% on ArT, 86.9% on Total-Text, and 86.4% on CTW-1500. Notably, our I3CL with the ResNeSt-101 backbone ranked the 1 st place on the ArT leaderboard. Code is available at github.com/ViTAE-Transformer/ViTAE-Transformer-Scene-Text-Detection. Input 256 Conv 1x7 64 Conv 7x7 64 Conv 7x1 64 Conv 1x5 64 Conv 5x5 64 Conv 5x1 64 Conv 1x3 64 Conv 3x3 64 Conv 3x1 64 Conv 1x1 256 shortcut Relu</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a key procedure for text reading, scene text detection has gradually become an active topic in the computer vision community due to its wide range of applications <ref type="bibr" target="#b55">(Zhang and Tao, 2020)</ref>, such as autonomous driving, scene parsing, and visual-impaired navigation. Many excellent methods have been proposed recently thanks to the success of deep learning <ref type="bibr" target="#b8">Dai et al., 2021;</ref><ref type="bibr" target="#b12">He et al., 2017;</ref><ref type="bibr" target="#b17">Liu et al., 2020a;</ref><ref type="bibr" target="#b62">Zhou et al., 2017;</ref>. However, many issues in this task remain open and challenging, such as fracture detections at the gaps in a text instance and inaccurate detections of text instances with diverse background context, due to various factors including irregular shapes, complex fonts, and variable scales.</p><p>Most of the previous methods <ref type="bibr" target="#b13">(Liao et al., 2017;</ref><ref type="bibr" target="#b27">Shi et al., 2017;</ref><ref type="bibr" target="#b62">Zhou et al., 2017)</ref> are designed for horizontal or multi-oriented text detection and have encountered troubles in dealing with arbitrary-shaped text.</p><p>(1)</p><p>(2) (3) <ref type="figure">Fig. 1</ref>: From left to right, a text image, the result of Mask R-CNN, and the result of our I3CL. Existing instance segmentation-based methods suffer from fracture detections due to the gaps inside the text (the bottom digital nameplate) and inaccurate detection due to the arbitrary shapes of different instances. Our I3CL produces much better results thanks to the intra-and inter-instance collaborative learning.</p><p>Baseline I3CL <ref type="figure">Fig. 2</ref>: Existing Mask R-CNN based methods fail to detect the gaps inside the text and produce fracture detection while our I3CL model can address this issue through collaborative learning.</p><p>Some methods propose to represent curved text with a set of characters, which is time-consuming and requires complex post-processing. In recent studies, inspired by Mask R-CNN , instance segmentationbased approach is proposed to address the problem of detecting text of various shapes. Nevertheless, simply applying Mask R-CNN to scene text detection also has some thorny problems. As illustrated in <ref type="figure">Figure 2</ref>, one of the main problems is the fracture detection at the gaps in a text instance. When detecting text with extremely scattered and misaligned characters, the detection model may produce low text feature responses in the regions of gaps between characters because of its weak text feature representation capacity in these regions. As a result, the text detector will suffer from fracture detections. Therefore, how to learn a strong text feature representation for both characters and gaps in the text instance matters for improving the detection performance. Besides, another problem is the inaccurate detection of text instances due to diverse background, such as false positives, missed detections, as well as incomplete contours. Although existing methods learn to detect all text instances within an image through end-to-end modeling, they treat them as individual instances during training. Consequently, existing methods have difficulties in distinguishing texts from the complex background and are prone to generate inaccurate detection results. In this paper, we argue that the text instances within an image probably have some kind of commonness. It refers to the common properties between different text instances due to similar font, color, size, and shared background context, which represent the semantic information of text instances and are completely different from the background semantic. Similar to the term of long-range dependencies between pixels within texture regions or objects in context, we term the relationship between text instances sharing the commonness as the long-range dependencies. How to exploit the dependencies between text instances and leverage the global context from the same background matters for learning a strong text feature representation.</p><p>To address these issues, we proposed a novel scene text detector based on Intra-and Inter-Instance Collaborative Learning (I3CL), which can effectively detect arbitrary-shaped scene texts. On the one hand, we first observe that the gaps in a text contain useful semantic information distinct from the background, since they are connected to the characters on both sides. We suspect that existing methods have limited performance because they are trapped by the limited receptive fields and thus have weak representation capacity for these gap regions. Based on the observation, we propose an intra-instance collaborative learning module, which treats a text as a combination of characters and gaps and learns discriminative features for them. Specifically, it consists of a cascade of three convolutional blocks, in each of which we use two convolutional layers with asymmetric horizontal and vertical convolutional kernels, and a convolutional layer with a regular convolutional kernel in parallel to them. In this way, it can model both character and gap regions in multioriented texts via an ensemble of paths with different receptive fields. On the other hand, to exploit the dependencies between different text instances, we propose an inter-instance collaborative learning module based on an instance-based transformer structure and a global context module, where the texture instance features are used as a token sequence to model the dependencies while the global context from the same background will be learned to supplement the above text features. By integrating these modules into a unified end-to-end trainable network, I3CL can learn a more discriminative feature representation for arbitrary-shaped scene text detection. In addition, to use unlabeled data to improve the performance, we design a simple yet effective pseudo label generation method based on an ensemble strategy, which can mitigate the problems of missed and false detections when producing reliable pseudo labels.</p><p>The contribution of this work is four-fold. Firstly, we devise an intra-instance collaborative learning module to learn a unified feature representation for both character and gap regions in the text instance. Secondly, we devise an inter-instance collaborative learning module to exploit the dependencies between text instances within an image. Thirdly, we propose a pseudo label generation method based on an ensemble strategy to harvest the unlabeled data in a semi-supervised learning (SSL) framework. Finally, Our I3CL model outperforms existing methods and sets new state-of-the-art results on three challenging public benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Scene Text Detection</head><p>Regression-based methods follow the generic object detection framework and localize texts by directly regressing the bounding boxes of text instances. For example, EAST <ref type="bibr" target="#b62">(Zhou et al., 2017)</ref> used efficient pixellevel regression for text objects without using the anchor mechanism and proposal generation. Based on SSD <ref type="bibr" target="#b18">(Liu et al., 2016)</ref>, TextBoxes <ref type="bibr" target="#b13">(Liao et al., 2017)</ref> modified the aspect ratio of anchors and added a text-box layer using a horizontal convolutional kernel. Further, TextBoxes++ <ref type="bibr" target="#b14">(Liao et al., 2018)</ref> applied quadrilaterals regression for multi-oriented text instances. SegLink  proposed to employ fully convolutional networks to detect text segments and model their link relationships. DGGR  first used a graph convolutional network to model relational reasoning of text components, and then grouped them into text results by linking merging. Although these methods have achieved good performance for quadrilateral text detection, most of them can not handle irregular shaped texts well due to the limited geometric representation ability.</p><p>Segmentation-based methods can accurately describe scene texts in various shapes using pixel-level segmentation masks. For example, TextSnake <ref type="bibr" target="#b24">(Long et al., 2018)</ref> proposed a flexible and general text representation for arbitrary-shaped texts by predicting the text center line and text regions with geometry attributes. PSENet <ref type="bibr" target="#b37">(Wang et al., 2019b)</ref> generated whole text boundary by performing progressive scale expansion of text regions using different scale kernels. Inspired by Mask R-CNN , SPCNet <ref type="bibr" target="#b41">(Xie et al., 2019)</ref> proposed a supervised pyramid context network to detect arbitrary-shaped texts based on instance segmentation. CRAFT <ref type="bibr" target="#b0">(Baek et al., 2019)</ref> detected the text by clustering characters boxes according to exploring affinity between characters. For real-time detection, DB <ref type="bibr" target="#b15">(Liao et al., 2020)</ref> designed a differentiable binarization module to perform the binarization process in a segmentation network. TextFuseNet <ref type="bibr" target="#b50">(Ye et al., 2020)</ref> adopt a multi-path fusion architecture to fuse three levels of features for text detection. However, these methods still suffer from fracture detections and inaccurate detections. Moreover, these methods treat each text instance as an individual object for learning and training, and pays no attention to the adverse influence caused by gaps in a text, which make them suffer from fracture detections and inaccurate detections. In contrast to them, we propose a novel idea of intra-and interinstance collaborative learning to learn better feature representation by exploiting the intra-instance characteristics and inter-instance dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Collaborative Learning</head><p>Collaborative learning (CL) has been widely used in different visual tasks. For example, Wang et al. <ref type="bibr" target="#b34">(Wang et al., 2018)</ref> proposed a collaborative learning framework of object detection by enforcing partial feature sharing and prediction consistency to train a weakly supervised learner and a strongly supervised learner jointly. CDCL  presented a Cross-Dataset Collaborative Learning method to improve the generalization and discrimination of feature representations for semantic segmentation. <ref type="bibr" target="#b28">Song et al. (Song and Chai, 2018)</ref> introduced a collaborative learning network where multiple classifier heads of the same network are simultaneously trained to improve generalization and robustness to label noise. <ref type="bibr" target="#b61">Zhang et al.(Zhang et al., 2020c)</ref> proposed to improve text detection via collaborative training of weakly supervised text classification network and supervised text detection network. In the context of scene text detection, existing methods pay little attention to the gaps in the text and handle text instances separately, resulting in a weak text feature representation ability. By contrast, we propose a novel collaborative learning model to learn a unified feature representation for both characters and gaps in the text and exploit the dependencies between different instances, which is an instance-level collaborative learning different from the task-level collaborative learning in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-training with Pseudo Labels</head><p>Self-training using pseudo labels is a learning paradigm associated with constructing models in semi-superv- ised learning, which leverages the model's own confident predictions to produce the pseudo labels for unlabeled data <ref type="bibr" target="#b56">Zhang et al., 2021a</ref><ref type="bibr" target="#b58">Zhang et al., , 2019b</ref>. Xie et al.  proposed a Noise Student method inspired by knowledge distillation with equal-or-larger student models. Zhang et al.  proposed to use the category centers of the source domain features as guiding anchors, which can be used to determine the active features of the target domain and generate pseudo labels for semantic segmentation. Zou et al. <ref type="bibr" target="#b66">(Zou et al., 2019)</ref> proposed the confidence regularized self-training to avoid putting overconfident pseudo labels on wrong classes, which may leading to deviated solutions with propagated errors. Zhang et al.  proposed to use the feature distances from prototypes to estimate the likelihood of pseudo labels to facilitate online correction in the course of training. <ref type="bibr" target="#b49">Yang et al. (Yang et al., 2021)</ref> designed multiple detection heads that predict pseudo labels for each other to provide complementary information. Unlike the above methods that may suffer from the erroneous pseudo labels from a single model, we proposed a new pseudo labeling method based on an ensemble strategy to produce reliable pseudo labels for text detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The overall framework of the proposed I3CL model is illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>. As shown, the basis of I3CL pipeline is built upon the Mask R-CNN framework. Firstly, the input text image is fed into the backbone network with an FPN <ref type="bibr" target="#b16">(Lin et al., 2017)</ref> architecture to generate a multi-scale feature pyramid, denoted as {P 2 , P 3 , P 4 , P 5 }, which have the same size as the input image with the down-sampling factors of {4, 8, 16, 32}, respectively. Secondly, the Intra-Instance Collaborative Learning (Intra-CL) module is used to further refine the text fracture of both characters and gaps implicitly on the feature maps at each scale of the feature pyramid. The detailed network structure of Intra-CL will be presented in Section 3.2. Next, we use a region proposal network (RPN) to produce text proposals for subsequent procedures. After that, box regression and mask prediction are carried out in two parallel branches. The box branch further refines and classifies text proposals. In the mask branch, we devise an Inter-Instance Collaborative Learning (Inter-CL) module to perform collaborative learning among all positive text instances. The text features from the Inter-CL module and the original ROIAlign module are fused into more discriminative features and used in instance segmentation to generate precise text contours. The detailed network structure of Inter-CL will be described in Section 3.3. Besides, a pseudo label generation method based on ensemble strategy for semi-supervised learning will be introduced in Section 3.4 in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intra-Instance Collaborative Learning</head><p>Existing methods focus on learning discriminative features for the character regions in the text instance while paying little attention to the gap regions between the characters, which may result in fracture detections (i.e., false positives detection) at the gaps due to a weak feature representation ability. In this paper, we treat the text instance as a spaced combination of both Text Region</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Character Region</head><p>Gap Region <ref type="figure">Fig. 4</ref>: The text region consists of spaced character regions and gap regions. Compared with the background, the gap regions are surrounded by characters in both sides, and contain rich text-related information in a long range. Therefore, it requires exploiting long-range dependencies between characters, between gaps, as well as between characters and gaps to learn a complete representation for the whole text instance. characters and gaps, as illustrated in <ref type="figure">Figure 4</ref>. In other words, the characters are spaced by the gaps while the gaps are also surrounded by characters on both sides, indicating that there are long-range dependencies between characters, between gaps, as well as between characters and gaps. To exploit the dependencies and learn a unified discriminative feature representation for both characters and gaps, we propose the Intra-CL module consists of a cascade of three convolutional blocks with multiple receptive fields.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 5</ref>, the Intra-CL module is composed of a cascade of three convolutional blocks. Each block contains three parallel convolutional layers with asymmetric horizontal and vertical convolutional kernels, i.e., k ? 1 and 1 ? k, as well as a regular k ? k convolutional kernel. In our work, k is set to 7, 5, and 3, respectively. The features from the three layers are summed and fed into the subsequent block. In this way, the Intra-CL module indeed contains an ensemble of paths with multiple receptive fields. We also add a residual connection between the input feature and the fused feature of the last block since it has been proved that learning with residual connections is much easier and converges faster. It is noteworthy that we use asymmetric kernels to enable the Intra-CL module adapt to multi-oriented texts. Besides, we employ a large kernel at the first block and smaller ones in the subsequent blocks because the Intra-CL module is expected to learn long-range dependencies between characters and gaps at first and then gradually focus on the central region of either the character or gap to learn a discriminative feature representation. An ablation study of the design of the Intra-CL module is conducted in Section 4.3.</p><p>Unlike the inception-like modules in IncepText <ref type="bibr" target="#b48">(Yang et al., 2018)</ref> enlarging received field for horizontal text detection by stacking separable convolutions (1 ? k and k ? 1) sequentially and convolution layers with different It consists of a cascade of three convolutional blocks, each of which contains two convolutional layers with asymmetric horizontal and vertical convolutional kernels, i.e., k ? 1 and 1 ? k, and a convolutional layer with a regular k ? k convolutional kernel in parallel to them. In this way, it can model both character and gap regions in multi-oriented texts via an ensemble of paths with different receptive fields. kernels in parallel, I3CL aims to learn text representations in longer ranges for arbitrary-shaped text detection. To this end, first, we utilize separable convolutions in parallel and stack convolution layers with different kernels sequentially. Second, we use large kernels at the shallow layers and small kernels at the deep layers to make the network gradually focus on the central region of either the character or gap to learn a complete text representation. Third, the parallel and serial structure in Intra-CL implies an ensemble of 3? 3 ? 3 = 27 paths, each of which corresponds to a unique combination of convolution kernels and has a specific receptive field.</p><p>By deploying the proposed Intra-CL module at each scale of the feature pyramid, our detection model can exploit the long-range dependencies between characters and gaps in text region through the information flows among different paths and implicitly learn a unified feature representation for both characters and gaps, therefore effectively mitigating the fracture detection issue due to the gaps in a text instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inter-Instance Collaborative Learning</head><p>Following Mask R-CNN , we use the RoIAlign to extract the RoI features of size H ? W ? C from the multi-scale feature pyramid for M positives proposals, which will be fed into the box branch and mask branch, respectively. To model the dependencies between text instances, we applied the transformer structure in Inter-CL module as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Firstly, the M RoI features are fed into a 1 ? 1 convolution layer to reduce their channel dimension from C to C 0 . Then, their spatial resolution is also reduced from H ? W to h ? w by using Adaptive Max-Pooling. Next, we flatten each feature into a vector of size 1 ? (h ? w ? C 0 ). In this way, we obtain a sequence of M token features (denoting as q), whose feature dimension has been significantly reduced. The sequence q is fed into a transformer encoder, which has three regular encoder layers with four heads of self-attention layers. And the output feature dimension of the feedforward network in the transformer is h ? w ? C 0 . Via the multi-head attention module, the long-range dependencies between different text instances in an image can be captured by adaptively attending to specific text instances that have similar background context or font appearance for any text instance. In this collaborative learning way, the representation ability of learned features can be improved. Afterward, the sequence q will be reshaped to a set of enhanced 2D visual features of size h ? w ? C 0 , which will be upsampled using bilinear interpolation and transformed using a 1 ? 1 convolution layer to recover the feature dimension as H ? W ? C. In this paper, the typical setting of the aforementioned parameters are C=256, C 0 =32, H=W =14, h=w=3, and M is the number of positive instance proposals. The whole process can be described as follows:</p><formula xml:id="formula_0">q = Reshape(AdaptiveMaxpool(Conv 1?1 (f))), (1) q TE = TransformerEncoder(q),<label>(2)</label></formula><formula xml:id="formula_1">q * = Conv 1?1 (BilinearInterp(Reshape(q TE ))),<label>(3)</label></formula><p>where f denotes the RoI features of M text instances, q T E denotes the learned features by the transformer encoder, and q * is the recovered 2D visual features. Existing methods typically detect texts according to the RoI feature that hardly pay attention to the global context from shared background for text instances within an image, which may tend to produce inaccurate detection results. We introduce two different structure designs in Inter-CL module to extract global context, which can be seen in <ref type="figure" target="#fig_3">Figure 7</ref>. 1) The first structure is built upon a pixel-based transformer. After obtaining the unified representation from all the levels of the Attention map of the dependencies between text instances. The darker the color, the closer the dependency between two instances. As shown, the dependencies between different text instances are influenced by background context, font, and scale, etc. feature pyramid like in TextFuseNet <ref type="bibr" target="#b50">(Ye et al., 2020)</ref>, we flatten the feature maps into a sequence of tokens, where each token is a feature vector at a specific pixel position on the feature maps as shown in <ref type="figure" target="#fig_3">Figure 7</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Encoder</head><formula xml:id="formula_2">M R R Maxpool Reshape R M G GlobalAvgpool (a) (b) Conv 1?1 G Conv 1?1 G Conv 1?1 G Conv 1?1 G</formula><p>In this way, we extract the global context by modeling the long-range dependencies between different pixels on the feature maps. 2) For the second structure shown in <ref type="figure" target="#fig_3">Figure 7</ref>(b), each level of feature pyramid is aggregated into a global context vector by a 1 ? 1 convolution layer and global average pooling, and then we fuse global context from different scales through element-wise summation. The global context will be fused with the original RoI features and the enhanced RoI features from the transformer encoder via element-wise summation to generate the discriminative text representation for text instance segmentation as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. As we have discussed, the text instances within an image probably have strong dependencies since they may share a same background or have a same font style, color, and scale, as illustrated in <ref type="figure">Figure 6</ref>(a). Based on the Inter-CL module, we can effectively model dependencies between text instances via the self-attention mechanism as demonstrated in <ref type="figure">Figure 6</ref>(b), which is beneficial for learning discriminative feature representation. It is noteworthy that we have not utilized the Inter-CL module in the box branch for the following three reasons. First, there are both positive and negative text samples in the box branch, while we only need to model the dependencies between different positive text instances rather than those negative ones, which are primarily used for training the classifier. Second, there are a lot of negative samples which will result in a long sequence and a bulk of computations if we directly apply the Inter-CL module based on them. Third, since we derive the detection results from the predicted masks rather than the quadrilateral bounding boxes for the arbitrary-shaped scene texts, therefore we only deploy the Inter-CL module in the mask branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semi-supervised Learning</head><p>SSL has been widely applied in various deep learning tasks, which can effectively use unlabeled data to improve performance. Among them, self-training based on pseudo labels is one of the most common methods in SSL. However, existing methods obtain pseudo labels from the detection results only via a customized confidence threshold, ignoring the errors of missed and false detections in object regression. To mitigate the side effect of the problem, we propose a more reliable pseudo label generation method as described below.</p><p>Specifically, we first train three teacher models A, B, and C with different data augmentations on labeled data, by which these models will focus on respective corresponding scenes and learn different text representations. Second, the three models perform multi-scale testing on unlabeled data to avoid missed detections as much as possible. Third, reliable pseudo labels for unlabeled data will be generated from the three sets of detection results through an ensemble strategy, which is described in Algorithm 1.</p><p>As shown, Det A , Det B , and Det C denote the detection result sets of model A, B, and C respectively. We define a text instance as a triplet of (m, b, s), which Algorithm 1: Pseudo-label Generation</p><formula xml:id="formula_3">Input: Det A = {text i = (m i , b i , s i )} I i=0 , Det B = {text j = (m j , b j , s j )} J j=0 , Det C = {text k = (m k , b k , s k )} K k=0 Output: L p = {text q = (m q , b q , w q )} Q q=0 begin L p = {}; for text i ? Det A do if ? text j ? Det B and text k ? Det C , iou ij &gt; T and iou ik &gt; T then m q = Overlap-Mask(m i ,m j ,m k ) b q = Soft-Box(b i ,b j ,b k ) w q = s i * s j * s k L p = L p (m q , b q , w q ) end else if ? text j ? Det B and iou ij &gt; T then m q = Overlap-Mask(m i ,m j ) b q = Soft-Box(b i ,b j ) w q = s i * s j * ? L p = L p (m q , b q , w q ) end else if ? text j ? Det C and iou ik &gt; T then m q = Overlap-Mask(m i ,m k ) b q = Soft-Box(b i ,b k ) w q = s i * s k * ? L p = L p (m q , b q , w q ) end else continue end end end</formula><p>are the mask, bounding box, and score of the text. L p represents the final pseudo labels set including triplets of (m, b, w), in which w is the corresponding loss weight of proposals matched with this pseudo label during the training. For each text instance in Det A , we retrieve the presence of text instances with high similarity in Det B and Det C . If similar text instances appear in both Det B and Det C , we consider these instances to be highly reliable, and then fuse them into a pseudo label with the multiplication of scores as w. If similar text instances exist only in Det B or Det C , we consider these text instances to be weakly reliable and the w of fused text instance will be decayed. In contrast, the text instance will be ignored when there is no similar text instance in Det B or Det C . We calculate the Intersection over Union (IoU) score to evaluate the similarity between text instances. T and ? represent the IoU threshold and decay weight, and are set to 0.8 and 0.5 respectively. Soft-Box means the process of boxes merging by calculating the average of coordinates of boxes as the new coordinates of the pseudo labeled box. Meanwhile, we obtain the overlap of the text masks as the pseudo labeled mask in Overlap-Mask process. Finally, we train a student  <ref type="figure">Fig. 8: (a)</ref>-(c) are the detection results from different teacher models, and (d) is the generated reliable pseudo label based on above results. model on the combination of labeled data and pseudo labeled data.</p><p>Producing pseudo labels through multiple models mitigates the issue of missed detections caused by a single model with insufficient detection capability. Moreover, a pseudo label is jointly determined by multiple models, minimizing the problem of false positives and generating more accurate pseudo labels. The visualization example of the proposed pseudo labels generation method is shown in <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>Following Mask R-CNN, our model is trained in a multi-task manner, where a classification task, a box regression task, and an instance segmentation task are involved. Specifically, the final loss function is defined as follows:</p><formula xml:id="formula_4">L = L rpn + L box + L mask<label>(4)</label></formula><p>where L rpn and L box denote the loss functions in the RPN and box branch, both of which consist of a Cross-Entropy (Binary or Sof tmax) loss L cls and a Smooth L1 loss L reg for classification and box regression, respectively. L mask denotes the loss function in the mask branch, which is a Binary Cross-Entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the performance of I3CL model on three public benchmarks, i.e., ArT , Total-Text (Ch'ng and Chan, 2017), and CTW-1500 <ref type="bibr" target="#b51">(Yuliang et al., 2017)</ref>, in which horizontal, quadrilateral, and curved texts exist simultaneously in most of the images. We first conduct comprehensive ablation studies to verify the effectiveness of proposed modules, and then compare I3CL with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>SynthText <ref type="bibr" target="#b10">(Gupta et al., 2016</ref>) is a dataset consisting of 800k synthetic images and 8 million text instances. We use it to pre-train our I3CL model.</p><p>ArT  is newly released dataset in the ICDAR2019 Robust Reading Challenge on Arbitrary-Shaped Text. It is the most challenging arbitrary-shaped text dataset containing Chinese texts, English texts, and other mixed symbols. It has a total of 10,166 images, including 5,603 training images and 4,563 testing images. Text instances in ArT are labeled by polygons with adaptive number of key points.</p><p>Total-Text (Ch'ng and Chan, 2017) is a dataset that includes English texts of various shapes. It contains 1,255 images for training and 300 images for testing. All text instances are labeled by word-level polygons.</p><p>CTW-1500 <ref type="bibr" target="#b51">(Yuliang et al., 2017)</ref> is an English dataset focusing on curved texts, which consists of 1,000 training images and 500 testing images. Different from Total-Text, text instances in CTW-1500 are labeled by text-line-level polygons.</p><p>We follow the same standard evaluation protocols by using Recall, Precision, and F-measure as the evaluation metrics, which are provided by the dataset creators or competition organizers. F-measure is the major evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement our proposed I3CL model based on the Detectron2 codebase with PyTorch. All experiments are performed using Nvidia Tesla V100 (16G) GPUs. The model is trained on 4 GPUs and tested on 1 GPU. As in the most of previous methods, we choose the ResNet-50 with the FPN as the backbone encoder.</p><p>Training. The whole training can be roughly divided into three main stages. Firstly, we pre-train a base model on SynthText dataset for about 300k iterations. Secondly, for each benchmark dataset, we finetune the base model using the corresponding real-world images for 30 epochs. In particular, considering that SynthText only contains English texts, we further pretrain the base model on the LSVT  and ICDAR2019-MLT <ref type="bibr" target="#b25">(Nayef et al., 2019)</ref> datasets before fine-tuning on ArT to enhance the ability of the model regarding Chinese, following the common practice in <ref type="bibr" target="#b1">(Baek et al., 2020)</ref>. Finally, based on the proposed pseudo-label generation method, we choose the test data of LSVT as the unlabeled data for ArT, and make Total-Text and CTW-1500 act as unlabeled data for each other to further fine-tune the model in a semisupervised learning way.</p><p>The batch size during pre-training is set to 8, and reduced to 4 during fine-tuning. We adopt the Stochastic gradient descent (SGD) optimizer to optimize our network with a weight decay of 0.0001 and a momentum of 0.9. During pre-training, the initial learning rate is set to 0.001 in the first 100k iterations and divided by 10 for the remaining iterations. For all experiments during fine-tuning, the initial learning rate is set to 0.0005 in the first 10 epochs and divided by 10 at 20 and 30 epochs. The shorter sides of images are randomly resized to different scales (i.e., 800, 1,000, 1,200), and the upper limit of longer side is 1,800. Data augmentation strategies such as random noise, brightness adjusting, and color change are applied to increase the diversity of training data.</p><p>Inference. During inference, we only perform a single-scale test to evaluate our model. The shorter side of the test images was scaled to 1,000 while keeping the aspect ratio unchanged, and the maximum size of the longer side is limited to 2,100, 1,875, and 1,800 on three datasets. The function of f indContours in OpenCV is used to generate polygon contours of text instances from predicted masks as the final detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Effectiveness of each module. We conduct an ablation study on ArT, Total-Text, and CTW-1500 to verify the effectiveness of each proposed module in this paper. For each dataset, we trained four models by adding the proposed modules gradually. "Baseline" denotes the original Mask R-CNN baseline model. "Intra-CL" denotes the model using the Intra-Instance Collaborative Learning module. "Inter-CL(GCT)" and "Inter-CL(GCG)" denote the model using the Inter-Instance Collaborative Learning module with the global context structure based on transformer or global average pooling, respectively. "SSL" refers to that I3CL model is trained in the semi-supervised learning way. The results are summarized in <ref type="table">Table 1.</ref> As can be seen, Intra-CL improves the performance of the baseline model consistently on all three datasets, e.g., 1.0%, 1.4%, and 1.3% gains in terms of the Fmeasure on ArT, Total-Text, and CTW-1500, respectively. In addition, integrating it with Inter-CL(GCT) <ref type="table">Table 1</ref>: Ablation study of the key components in our I3CL model on different datasets. "Intra-CL" represents the Intra-Instance Collaborative Learning module. "Inter-CL(GCT)" and "Inter-CL(GCG)" refer to Inter-Instance Collaborative Learning module with the global context structure based on transformer or global average pooling, respectively. "R", "P", and "F" represent Recall, Precision, and F-measure, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ArT  further brings absolute performance gains in terms of the F-measure increase by 1.4%, 1.3%, and 1.7%, respectively. By contrast, the combination of Intra-CL and Inter-CL(GCG) achieves a better gain of 1.6%, 1.6%, and 1.8% on F-measure but contains fewer parameters. In terms of F-measure, there is no significant gap between the two modules. The reason why GCG module works slightly better may be that each pixel of text RoI features integrates the complete global context. In terms of parameter, GCG module is much smaller than GCT module. Moreover, the combination of GCG module and GCT module in Inter-CL has no obvious advantage on F-measure and parameter. To this end, we choose the Inter-CL(GCG) as the default setting of Inter-CL module in I3CL. Finally, the I3CL model trained in a semi-supervised learning way achieves a gain of 3.5%, 3.6%, and 3.7% in terms of the F-measure over the baseline on the three datasets, respectively. Moreover, there is a similar trend of improvement in the Precision and Recall.</p><p>To prove that the improvements are obtained by addressing the two previous limitations, we select 100 images in each dataset, on which Mask R-CNN baseline is prone to produce a large number of fracture detections and inaccurate detections, as the difficult subset, and compare the performance gap between the Mask R-CNN baseline and the proposed I3CL on the three subsets. As shown in <ref type="table" target="#tab_2">Table 2</ref>, I3CL achieves a remarkable gain of 8.2%, 7.5%, and 7.9% in terms of F-measure on the three subsets respectively. Some visual results of the Mask R-CNN baseline and our I3CL model are shown in <ref type="figure" target="#fig_4">Figure 9</ref>. As can be seen, Mask R-CNN produces fracture detections, missed detections, as well as incomplete text contours indicated by the red boxes, while our I3CL model can effectively mitigate these issues and produce complete and accurate text masks. These quantitative and qualitative results demonstrate that our I3CL model benefits from the intra-and interinstance collaborative learning and learns a better and more discriminative feature representation than Mask R-CNN baseline model, which help the text detector do well in detecting difficult texts.</p><p>Different settings of the Intra-CL module. Ablation studies are also conducted on the ArT dataset to investigate the impact of different settings of the Intra-CL module, e.g., the number of convolutional branches in each block, feature fusion method, and the order of convolutional kernels in the cascade. The results are listed in <ref type="table" target="#tab_2">Table 2</ref>. "1-path" denotes using a single convolutional branch with a regular k?k convolutional kernel. "2-path" denotes using two convolutional branches with asymmetric horizontal and vertical convolutional kernels. "3-path" denotes the default setting that contains all three branches as shown in <ref type="figure" target="#fig_1">Figure 5</ref>. "cat" and "sum" denote the feature fusion method, i.e., concatenation and element-wise sum, respectively. "kernel:753" denotes using the 7 ? 7 convolutional kernel in the first block and 5 ? 5 and 3 ? 3 kernels subsequently.</p><p>As shown, although both "1-path" and "2-path" can improve the performance marginally, "3-path" brings more improvement over the baseline model in terms of the F-measure, i.e., a gain of 1.0%, confirming the value of using different kernels for modeling the multioriented texts. Besides, there is no significant difference between the concatenation and element-wise sum for feature fusion. We choose the latter one as the de-    <ref type="table">Table 6</ref>: Evaluation results on the ArT dataset. " ?", " ?", and " ?" indicate that the results are collected from , official website of ArT, and our experiments using official released code, respectively. fault setting. When the order of convolutional kernels in the cascade is reversed, i.e., from "kernel:753" to "kernel:357", we observe a performance drop of 0.7% in terms of the F-measure. We suspect the reason is that using a large kernel at the first block can effectively model long-range dependencies between characters, between gaps, and between characters and gaps, and using smaller kernels subsequently can gradually guide the Intra-CL module focus on the central region of the receptive field to learn a discriminative fea-ture representation for either character or gap regions. However, when reversing the order, the Intra-CL module may struggle in extracting long-range context and be prone to noisy features in later blocks due to the large receptive fields. Besides, we conducted an ablation study on the setting of kernel sizes in the Intra-CL module. As shown in <ref type="table" target="#tab_4">Table 4</ref>, compared with other settings, "kernel:753" achieves a better trade-off between performance and parameters. Although "kernel:773" delivers slightly better results in terms of recall and F-measure, its precision score is worse than that of "kernel:753" while increasing the number of parameters by 0.8M. As a result, we use "kernel:753" as the default setting in the Intra-CL module. Different settings of pseudo label generation. Moreover, we also conduct an ablation study to compare different settings of pseudo label generation on the ArT dataset. "Threshold Filter" denotes the common practice that selects the detection results from a single model as pseudo labels via a fixed threshold of confidence score. "Ensemble" represents the ensemble strategy based on multiple models without Inter-Mask and Soft-Box to refine the masks and boxes respectively. We use "Ensemble + Overlap-Mask + Soft-Box" as the default setting in our pseudo label generation. As shown in <ref type="table" target="#tab_5">Table 5</ref>, due to the missed and false detections, "Threshold Filter" results in a severe side effect on the performance of the baseline model, i.e. 0.9%, 1.2%, and 1.1% drop on the three metrics respectively. In contrast, "Ensemble + Overlap-Mask" achieves the highest Precision of 84.1% among all models with ResNet-50 backbone on ArT. Furthermore, the default "Ensemble + Overlap-Mask + Soft-Box" can bring considerable performance gains of 0.9%, 0.9%, and 0.9% on the three metrics respectively, which verifies the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-art Methods</head><p>Evaluation on ArT. We evaluate the effectiveness of the proposed I3CL model in detecting arbitrary-shaped mixed-language text on ArT dataset. The evaluation results of I3CL and previous methods are presented in <ref type="table">Table 6</ref>. I3CL has achieved the best performance in terms of Recall and F-measure without using semisupervised learning, which surpasses the current best method, i.e., TextFuseNet ? <ref type="bibr" target="#b50">(Ye et al., 2020)</ref>, by a large margin of 1.2% in terms of the F-measure. When applying the semi-supervised learning, a more compelling result can be achieved, i.e., 72.2% on Recall and 77.5% on F-measure. To the best of our knowledge, I3CL is the first method achieving an F-measure over 77.0% on ArT using the ResNet-50 backbone.</p><p>Evaluation on Total-Text. We evaluate the effectiveness of the proposed I3CL model in detecting word-level arbitrary-shaped English text on Total-Text dataset. As shown in <ref type="table" target="#tab_7">Table 7</ref>, similarly, I3CL sets a new state-ofthe-art result of 86.3% F-measure on Total-Text. Furthermore, our detector with full implementations outperforms the latest state-of-the-art method, i.e., FCENet , by 1.1% F-measure. Moreover, I3CL achieves the highest Precision of 89.8%, which has a gain of 0.5% over the previous best method. In addition, our I3CL is the only one exceeding 86.0% in terms of the F-measure in all contenders. Evaluation on CTW-1500. We evaluate the effectiveness of the proposed I3CL model in detecting textline-level arbitrary-shaped English text on CTW-1500 dataset. The results are summarized in <ref type="table" target="#tab_12">Table 10</ref>. As can be seen, our model achieves the best results with Precision of 88.4% and F-measure of 86.5% while keeping highly competitive results on Recall. Compared with the previous best method FCENet , I3CL outperforms it by 1.2% on Recall, 0.8% on Precision, and 1.0% on F-measure. Note that since the text instances in CTW-1500 are labelled by text-line-level polygons rather than word-level annotations in Totaltext, our method can learn better local and long-range features to handle such challenging cases and produce more accurate detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Competition on ArT Leaderboard</head><p>To explore the upper limit of the detection performance of I3CL, we join the melee on the ArT leaderboard. More complex experiments are conducted to improve the performance of I3CL on ArT dataset, including using stronger backbones and other common tricks during the training and testing.</p><p>Backbone. We adopt ResNet <ref type="bibr" target="#b11">(He et al., 2016)</ref> and its variants, i.e., ResNeXt <ref type="bibr" target="#b43">(Xie et al., 2017)</ref> and ResNeSt <ref type="bibr" target="#b54">(Zhang et al., 2020a)</ref>, with different depths of {50,101,152} as the backbone. Besides, we also adopt ResNet-50 pretrained with RegionCL <ref type="bibr" target="#b45">(Xu et al., 2021a)</ref> pretext task and transformer-based ViTAEv2-S <ref type="bibr" target="#b59">(Zhang et al., 2022)</ref> backbone for further experiments. The comparisons of different backbones can be seen in <ref type="table" target="#tab_11">Table 9</ref>. As shown, RegionCL contrastive learning on ResNet-50 backbone apparently assists the downstream scene text detection task. Due to the superior feature representation by incorporating transformers with intrinsic inductive bias, ViTAEv2-S surpasses the base ResNet-50 with a large margin using similar parameters, i.e., 2.3% gain on Fmeasure. What's more, deeper backbones bring effective gains on all three evaluation metrics compared to the base ResNet-50. Among all the backbones we have tried, ResNeSt-101 stands out with the highest values on Recall, Precision, and F-measure at single-scale testing, i.e., 75.1%, 86.3%, and 80.3%, which are even better than the deeper ResNeXt-152. After comprehensive consideration about the size, training speed, and memory consumption of the model, we chose the ResNeSt-101 as the final backbone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Venue Backbone R P F Mask-TTD  TIP'19 Res50 74.5 79.1 76.7 TextSnake <ref type="bibr" target="#b24">(Long et al., 2018)</ref> ECCV'18 VGG16 74.5 82.7 78.4 ATRR <ref type="bibr" target="#b39">(Wang et al., 2019d)</ref> CVPR'19 VGG16 76.2 80.9 78.5 MSR <ref type="bibr" target="#b47">(Xue et al., 2019)</ref> IJCAI'19 Res50 74.8 83.8 79.0 CSE <ref type="bibr" target="#b22">(Liu et al., 2019c)</ref> CVPR'19 Res34 79.1 81.4 80.2 SAST <ref type="bibr" target="#b36">(Wang et al., 2019a)</ref> MM'19 Res50 76.9 83.8 80.2 TextDragon  ICCV'19 VGG16 75.7 85.6 80.3 TextRay <ref type="bibr" target="#b32">(Wang et al., 2020a)</ref> MM'20 Res50 77.9 83.5 80.6 TextField  TIP'19 VGG16 79.9 81.2 80.6 PSENet <ref type="bibr" target="#b37">(Wang et al., 2019b)</ref> CVPR'19 Res50 78.0 84.0 80.9 SegLink++ <ref type="bibr" target="#b30">Tang et al. (2019)</ref> PR'19 VGG16 80.9 82.1 81.5 MS-CAFA  TMM'19 Res50 78.6 84.6 81.5 SPCNet <ref type="bibr" target="#b41">(Xie et al., 2019)</ref> AAAI'19 Res50 82.8 83.0 82.9 LOMO  CVPR'19 Res50 79.3 87.6 83.3 CRAFT <ref type="bibr" target="#b0">(Baek et al., 2019)</ref> CVPR'19 VGG16 79.9 87.6 83.6 CRNet <ref type="bibr" target="#b63">(Zhou et al., 2020)</ref> MM'20 Res50 82.5 85.8 84.1 Boundary  AAAI'20 Res50 83.5 85.2 84.3 ABCNet  CVPR'20 Res50 81.3 87.9 84.5 DB <ref type="bibr" target="#b15">(Liao et al., 2020)</ref> AAAI'20 Res50-DCN 82.5 87.1 84.7 PAN <ref type="bibr" target="#b38">(Wang et al., 2019c)</ref> ICCV'19 Res18 81.0 89.3 85.0 TextPerception <ref type="bibr" target="#b26">(Qiao et al., 2020)</ref> AAAI'20 Res50 81.8 88.8 85.2 PCR <ref type="bibr" target="#b8">(Dai et al., 2021)</ref> CVPR'21 DLA34 82.0 88.5 85.2 TextFuseNet <ref type="bibr" target="#b50">(Ye et al., 2020)</ref> IJCAI'20 Res50 83.2 87.5 85.3 ContourNet  CVPR'20 Res50 83.9 86.9 85.4 DGGR  CVPR'20 VGG16 84.9 86.5 85.7 FCENet  CVPR   <ref type="bibr" target="#b63">(Zhou et al., 2020)</ref> MM'20 Res50 80.9 87.0 83.8 ContourNet  CVPR'20 Res50 84.1 83.7 83.9 DGGR  CVPR'20 VGG16 83.0 85.9 84.5 TextPerception <ref type="bibr" target="#b26">(Qiao et al., 2020)</ref> AAAI'20 Res50 81.9 87.5 84.6 PCR <ref type="bibr" target="#b8">(Dai et al., 2021)</ref> CVPR Multi-scale Testing. We resized the maximum size of the longer side to {1,500, 1,800, 2,100, 2,400, 2,700}, and the shorter side is scaled to {1,000, 1,300, 1,500}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICDAR2019-ArT</head><p>Total-Text CTW-1500 Results from different scales are aggregated and NMS is used to suppress the redundant text instances to get final detection results. As can be seen in <ref type="table" target="#tab_12">Table  10</ref>, compared to single-scale testing, multi-scale testing achieves a margin of 0.9% on F-measure over the baseline.</p><p>Soft-NMS <ref type="bibr" target="#b2">(Bodla et al., 2017)</ref>. NMS is replaced by Soft-NMS during the testing. We tried two decay strategies of confidence score, including linear decay and Gaussian decay. A finding in our experiment is that Soft-NMS with linear decay is better for ArT, and the F-measure was increased from 81.2% to 82.0%.</p><p>Mixup <ref type="bibr" target="#b53">(Zhang et al., 2017)</ref>. We implemented Mixup by pasting two text images and their labels in a fixed transparency ratio, i.e., 1:1. Besides, we paste text images and non-text images together in a random transparency ratio. Those non-text images include landscape, architectural and animal images, increasing the diversity of background. In this way, I3CL achieves 80.8% in terms of F-measure at single-scale testing without Soft-NMS and 82.3% at multi-scale testing with Soft-NMS.</p><p>Model Ensemble. We ensemble the detected text boxes of different models to obtain better final results, such as models training with different backbones, different data augmentation strategies, and different iterations. Similarly in multi-scale testing, detection results from models using different training strategies are aggregated and then we use Soft-NMS to remove redundant text instances. As shown in <ref type="table" target="#tab_12">Table 10</ref>, our I3CL ultimately achieves an extremely impressive F-measure of 84.0% on ArT dataset.</p><p>In summary, the proposed I3CL sets new state-ofthe-art on the ArT, Total-Text, and CTW-1500 for arbitrary-shaped scene text detection. Specifically, I3CL with ResNeSt-101 backbone achieves an impressive detection performance and ranks the 1 st place on the ArT leaderboard. Some visual results are shown in <ref type="figure" target="#fig_5">Figure 10</ref>. As can be seen, I3CL can well handle different challenging cases including various shapes, extremely small scales, large gaps between characters, diverse font styles, and backgrounds, showing great potential for real-world applications.  In this section, we discuss the model complexity of our I3CL, including parameter, computation, and inference speed.</p><p>Parameter. As shown in <ref type="table">Table 1</ref>, I3CL brings considerable performance improvements of over 3% F-measure on different datasets with 17.8% parameters increase. The main parameter increase comes from transformer in Inter-CL module. ContourNet  using Deformable ROI pooling with 256.3M parameters achieved 67.2%, 85.4%, and 83.9% in terms of F-measure on ArT, Total-Text, and CTW-1500, respectively. In contrast, I3CL sets new state-of-the-art results with 77.5%, 86.9%, and 86.5% on the three datasets and maintains a better trade-off between the model size and the performance with only 52.2M parameters, which proves that effectively model design for specific problems is important.</p><p>Computation. As known, transformer often brings heavy computation due to the self-attention mechanism. Unlike the image classification task in <ref type="bibr" target="#b46">(Xu et al., 2021b)</ref>, <ref type="bibr" target="#b59">(Zhang et al., 2022)</ref> and <ref type="bibr" target="#b23">(Liu et al., 2021)</ref>, the computation of transformer encoder for modeling the dependencies between text instances in Inter-CL module depends on the number of text instances and the dimension of sequence features. After statistics, the average number of text instances on the images of the three datasets is 9. Benefit by the dimension reduction of the input sequence features and reasonable depth of transformer structure, transformer in Inter-CL module only increases about 0.05 GFLOPs computation on average. Compare with the 204.8 GFLOPs computation of the Mask R-CNN baseline, we consider that the computation increases of transformer structure when modeling the dependencies between text instances in Inter-CL module is acceptable. The total computation of I3CL can be seen in <ref type="table">Table 1</ref>. Overall, I3CL brings considerable performance improvements of over 3% Fmeasure on different datasets with 20.7% computation increase.</p><p>Inference Speed. I3CL achieves an inference speed of 7.6 fps on CTW-1500 dataset, which is slightly slower than 9.1 fps of the Mask R-CNN baseline. The comparison results of speed between I3CL and some previous methods can be seen in <ref type="table">Table 11</ref>. When testing on CTW-1500, I3CL surpasses PSENet <ref type="bibr" target="#b37">(Wang et al., 2019b)</ref> and ContourNet  both on F-measure and speed (i.e., 86.5% vs 82.2% and 83.9% and 7.6 fps vs 3.9 fps and 4.5 fps). However, compared with DB <ref type="bibr" target="#b15">(Liao et al., 2020)</ref>, though I3CL outperforms it by a large margin on F-measure (i.e., 86.5% vs 83.5%), our method lags behind on speed (i.e., 7.6 fps vs 22.0 fps). Overall, Although the inability to achieve realtime detection is a congenital limitation of the twostage detector, I3CL still has obvious advantages over some previous methods on inference speed. <ref type="table">Table 11</ref>: Comparision results of speed between I3CL and some previous methods on CTW-1500 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Venue Backbone FPS CSE <ref type="bibr" target="#b22">(Liu et al., 2019c)</ref> CVPR'19 Res34 2.6 PSENet <ref type="bibr" target="#b37">(Wang et al., 2019b)</ref> CVPR'19 Res50 3.9 MSR <ref type="bibr" target="#b47">(Xue et al., 2019)</ref> IJCAI'19 Res50 4.3 LOMO  CVPR'19 Res50 4.4 ContourNet  CVPR'20 Res50 4.5 TextField  TIP'19 VGG16 6.0 TextFuseNet <ref type="bibr" target="#b50">(Ye et al., 2020)</ref> IJCAI'20 Res50 7.3 PCR <ref type="bibr" target="#b8">(Dai et al., 2021)</ref> CVPR'21 DLA34 11.8 DB <ref type="bibr" target="#b15">(Liao et al., 2020)</ref> AAAI'20 Res50 22.0 I3CL -Res50 7.6 (a) (b) <ref type="figure">Fig. 11: (a)</ref> A test image with two horizontal text regions. (b) Failure detection. In (b), green polygons represent true positives, while red polygons represent false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitation</head><p>In this section, we discuss the limitations of the proposed I3CL model. Although achieving state-of-theart performance on three challenging benchmarks, our method is not outstanding enough in terms of speed, which can not meet the requirement for real-time applications. In the future, we plan to investigate efficient instance segmentation pipelines and fast implementation as well as other effective and lightweight modules for collaborative learning. In addition, our model may generate linguistically ambiguous text proposals when detecting text arranged in multiple rows and columns. A failure detection example is shown in <ref type="figure">Figure 11</ref>. In the future, domain knowledge of linguistics can be utilized to design more effective modules as well as grouping strategies for proposal generation and filtering to mitigate the issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we first identify two issues in arbitraryshaped text detection, i.e., fracture detection and inaccurate detection, and then argue that collaborative learning of both character and gap regions in text and long-dependencies between text instances within an image matters for mitigating the two issues. To validate the idea, we make the first attempt to propose a novel intra-and inter-instance collaborative learning model named I3CL, where an Intra-CL module based on a cascade of convolutional blocks with multiple receptive fields and an Inter-CL module based on a text instance transformer are devised. Besides, a new method of pseudo label generation based on ensemble strategy is proposed for semi-supervised learning of scene text detection. Comprehensive empirical studies on three public benchmarks demonstrate the effectiveness of the proposed I3CL model and its superiority over existing methods. We hope this study can open a new perspective for text detection and encourage more follow-up work in modeling long-range dependencies within and between text instances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>The overall pipeline of the proposed I3CL. Based on the Mask R-CNN, it refines the feature map at each scale of the feature pyramid via the Intra-Instance Collaborative Learning module, and further embeds the features of text instances and global context using the Inter-Instance Collaborative Learning module in the mask branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>The architecture of the Intra-Instance Collaborative Learning module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig</head><label></label><figDesc>. 6: (a) Different text instances on an image. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 :</head><label>7</label><figDesc>(a) Global context extraction structure based on a Transformer encoder. (b) Global context extraction structure based on global average pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 :</head><label>9</label><figDesc>Detection results of Mask R-CNN (second row) and our I3CL model (third row). Mask R-CNN produces fracture detections(a-c), and inaccurate detections such as false positives(d), missed detections(e), as well as incomplete text contours(f), while our I3CL model can effectively mitigate these issues and generate more accurate detection results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 :</head><label>10</label><figDesc>Some visual results of our I3CL model on the ArT, Total-Text, and CTW-1500 datasets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Score?97.7% Score? 76.1% (b) Score? 79.3% Score? 95.1% Score? 80.4% Score? 99.6% Lossweight? 31.8% Lossweight?92.5%</head><label></label><figDesc></figDesc><table><row><cell>(a)</cell><cell>(c)</cell><cell>(d)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison results between the Mask R-CNN baseline and I3CL on three difficult subsets.</figDesc><table><row><cell>Method</cell><cell>R</cell><cell>ArT subset P</cell><cell>F</cell><cell>R</cell><cell>Total-Text subset P</cell><cell>F</cell><cell>R</cell><cell>CTW-1500 subset P</cell><cell>F</cell></row><row><cell>Baseline</cell><cell>61.6</cell><cell>52.3</cell><cell>56.6</cell><cell>70.6</cell><cell>84.8</cell><cell>77.1</cell><cell>74.0</cell><cell>75.0</cell><cell>74.5</cell></row><row><cell>I3CL</cell><cell cols="9">68.8(?7.2) 61.2(?8.9) 64.8(?8.2) 80.0(?9.4) 89.8(?5.0) 84.6(?7.5) 82.1(?8.1) 82.6(?7.6) 82.4(?7.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of different settings of the Intra-CL module on the ArT dataset.</figDesc><table><row><cell>Method</cell><cell>R</cell><cell>P</cell><cell>F</cell></row><row><cell>Baseline</cell><cell>68.9</cell><cell>80.0</cell><cell>74.0</cell></row><row><cell>Intra-CL (1-path)</cell><cell>69.2</cell><cell>80.6</cell><cell>74.4</cell></row><row><cell>Intra-CL (2-path)</cell><cell>69.1</cell><cell>80.9</cell><cell>74.5</cell></row><row><cell>Intra-CL (3-path, kernel:753, cat)</cell><cell>69.1</cell><cell>81.8</cell><cell>74.9</cell></row><row><cell cols="2">Intra-CL (3-path, kernel:753, sum) 69.4</cell><cell>81.7</cell><cell>75.0</cell></row><row><cell>Intra-CL (3-path, kernel:357, sum)</cell><cell>69.0</cell><cell>80.5</cell><cell>74.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the setting of kernel sizes in the Intra-CL module on the ArT dataset.</figDesc><table><row><cell>Method</cell><cell>R</cell><cell>P</cell><cell>F</cell><cell>Parameter</cell></row><row><cell>kernel:753</cell><cell>69.4</cell><cell>81.7</cell><cell>75.0</cell><cell>46.7M</cell></row><row><cell>kernel:975</cell><cell>69.3</cell><cell>81.6</cell><cell>75.0</cell><cell>48.20M</cell></row><row><cell>kernel:777</cell><cell>69.6</cell><cell>80.9</cell><cell>74.8</cell><cell>48.10M</cell></row><row><cell>kernel:555</cell><cell>69.5</cell><cell>81.0</cell><cell>74.8</cell><cell>46.4M</cell></row><row><cell>kernel:333</cell><cell>69.5</cell><cell>80.7</cell><cell>74.7</cell><cell>45.1M</cell></row><row><cell>kernel:775</cell><cell>69.6</cell><cell>80.6</cell><cell>74.7</cell><cell>47.8M</cell></row><row><cell cols="2">kernel:773 69.7</cell><cell>81.5</cell><cell>75.1</cell><cell>47.5M</cell></row><row><cell>kernel:755</cell><cell>69.1</cell><cell>81.5</cell><cell>74.7</cell><cell>47.0M</cell></row><row><cell>kernel:733</cell><cell>69.6</cell><cell>81.0</cell><cell>74.9</cell><cell>46.3M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of different settings of pseudo label generation on the ArT dataset.</figDesc><table><row><cell>Method</cell><cell>R</cell><cell>P</cell><cell>F</cell></row><row><cell>Baseline</cell><cell>71.3</cell><cell>82.7</cell><cell>76.6</cell></row><row><cell>Threshold Filter</cell><cell>70.4</cell><cell>81.5</cell><cell>75.5</cell></row><row><cell>Ensemble</cell><cell>71.8</cell><cell>83.0</cell><cell>77.0</cell></row><row><cell>Ensemble + Overlap-Mask</cell><cell>71.5</cell><cell>84.1</cell><cell>77.3</cell></row><row><cell cols="2">Ensemble + Overlap-Mask + Soft-Box 72.2</cell><cell>83.6</cell><cell>77.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Evaluation results on the Total-Text dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Evaluation results on the CTW-1500 dataset.</figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>Backbone</cell><cell>R</cell><cell>P</cell><cell>F</cell></row><row><cell>CTD (Liu et al., 2019b)</cell><cell>PR'19</cell><cell>Res50</cell><cell>69.8</cell><cell>77.4</cell><cell>73.4</cell></row><row><cell>TextSnake (Long et al., 2018)</cell><cell>ECCV'18</cell><cell>VGG16</cell><cell>85.3</cell><cell>67.9</cell><cell>75.6</cell></row><row><cell>CSE (Liu et al., 2019c)</cell><cell>CVPR'19</cell><cell>Res34</cell><cell>76.0</cell><cell>81.1</cell><cell>78.4</cell></row><row><cell>Mask-TTD (Liu et al., 2019a)</cell><cell>TIP'19</cell><cell>Res50</cell><cell>79.0</cell><cell>79.7</cell><cell>79.4</cell></row><row><cell>ATRR (Wang et al., 2019d)</cell><cell>CVPR'19</cell><cell>VGG16</cell><cell>80.2</cell><cell>80.1</cell><cell>80.1</cell></row><row><cell>SAE (Tian et al., 2019)</cell><cell>CVPR'19</cell><cell>Res50</cell><cell>77.8</cell><cell>82.7</cell><cell>80.1</cell></row><row><cell>LOMO (Zhang et al., 2019a)</cell><cell>CVPR'19</cell><cell>Res50</cell><cell>76.5</cell><cell>85.7</cell><cell>80.8</cell></row><row><cell>SAST (Wang et al., 2019a)</cell><cell>MM'19</cell><cell>Res50</cell><cell>77.1</cell><cell>85.3</cell><cell>81.0</cell></row><row><cell>SegLink++ (Tang et al., 2019)</cell><cell>PR'19</cell><cell>VGG16</cell><cell>79.8</cell><cell>82.8</cell><cell>81.3</cell></row><row><cell>TextField (Xu et al., 2019)</cell><cell>TIP'19</cell><cell>VGG16</cell><cell>79.8</cell><cell>83.0</cell><cell>81.4</cell></row><row><cell>ABCNet (Liu et al., 2020b)</cell><cell>CVPR'20</cell><cell>Res50</cell><cell>78.5</cell><cell>84.4</cell><cell>81.4</cell></row><row><cell>MSR (Xue et al., 2019)</cell><cell>IJCAI'19</cell><cell>Res50</cell><cell>78.3</cell><cell>85.0</cell><cell>81.5</cell></row><row><cell>TextRay (Wang et al., 2020a)</cell><cell>MM'20</cell><cell>Res50</cell><cell>80.4</cell><cell>82.8</cell><cell>81.6</cell></row><row><cell>PSENet (Wang et al., 2019b)</cell><cell>CVPR'19</cell><cell>Res50</cell><cell>79.7</cell><cell>84.8</cell><cell>82.2</cell></row><row><cell>DB (Liao et al., 2020)</cell><cell>AAAI'20</cell><cell>Res50-DCN</cell><cell>80.2</cell><cell>86.9</cell><cell>83.4</cell></row><row><cell>CRAFT (Baek et al., 2019)</cell><cell>CVPR'19</cell><cell>VGG16</cell><cell>81.1</cell><cell>86.0</cell><cell>83.5</cell></row><row><cell>TextDragon (Feng et al., 2019)</cell><cell>ICCV'19</cell><cell>VGG16</cell><cell>82.8</cell><cell>84.5</cell><cell>83.6</cell></row><row><cell>PAN (Wang et al., 2019c)</cell><cell>ICCV'19</cell><cell>Res18</cell><cell>81.2</cell><cell>86.4</cell><cell>83.7</cell></row><row><cell>CRNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Results of I3CL without SSL using different backbones on ArT dataset. ? and ? represent the Re-gionCL<ref type="bibr" target="#b45">(Xu et al., 2021a)</ref> with finetuning and without finetuning on the ImageNet training data, respectively. * indicates that the whole detection model is implemented in MMDetection<ref type="bibr" target="#b3">(Chen et al., 2019)</ref>.</figDesc><table><row><cell>Backbone</cell><cell>R</cell><cell>P</cell><cell>F</cell></row><row><cell>ResNet-50</cell><cell>71.3</cell><cell>82.7</cell><cell>76.6</cell></row><row><cell>ResNet-50 w/ RegionCL ?</cell><cell>72.6</cell><cell>81.9</cell><cell>77.0</cell></row><row><cell>ResNet-50 w/ RegionCL ?</cell><cell>73.5</cell><cell>81.6</cell><cell>77.3</cell></row><row><cell>ViTAEv2-S*</cell><cell>75.4</cell><cell>82.8</cell><cell>78.9</cell></row><row><cell>ResNeXt-101</cell><cell>74.1</cell><cell>85.5</cell><cell>79.4</cell></row><row><cell>ResNeSt-101</cell><cell>75.1</cell><cell>86.3</cell><cell>80.3</cell></row><row><cell>ResNeXt-152</cell><cell>74.9</cell><cell>86.0</cell><cell>80.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Results of I3CL using different tricks withResNeSt-101 backbone on ArT dataset.</figDesc><table><row><cell>MS Testing Soft-NMS Mixup Model Ensemble</cell><cell>F</cell></row><row><cell></cell><cell>80.3</cell></row><row><cell></cell><cell>81.2</cell></row><row><cell></cell><cell>82.0</cell></row><row><cell></cell><cell>82.3</cell></row><row><cell></cell><cell>84.0</cell></row><row><cell>5 Discussion about Model Complexity</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Character region awareness for text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9365" to="9374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Character region attention for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="504" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Softnms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno>arXiv:190607155</idno>
		<title level="m">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recursive context routing for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="160" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Icdar2019 robust reading challenge on arbitrary-shaped textrrc-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Document Analysis and Recognition (ICDAR)</title>
		<meeting>International Conference on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1571" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Ch&amp;apos;ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Document Analysis and Recognition</title>
		<meeting>International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep multi-scale context aware feature aggregation for curved scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1969" to="1984" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Progressive contour regression for arbitrary-shape scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7393" to="7402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Textdragon: An end-to-end framework for arbitrary shaped text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9076" to="9085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
	<note type="report_type">Mask rcnn</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Textboxes++: A singleshot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time scene text detection with differentiable binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11474" to="11481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Asts: A unified framework for arbitrary shape text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5924" to="5936" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European conference on computer vision</title>
		<meeting>European conference on computer vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Arbitrarily shaped scene text detection with a mask tightness text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2918" to="2930" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Curved scene text detection via transverse and longitudinal sequence connection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="337" to="345" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abcnet: Real-time scene text spotting with adaptive bezier-curve network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9809" to="9818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards robust curve text detection with conditional spatial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7269" to="7278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno>arXiv:210314030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Icdar2019 robust reading challenge on multilingual scene text detection and recognition-rrcmlt-2019</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Busta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Burie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Cl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1582" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text perceptron: Towards end-to-end arbitrary-shaped text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11899" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2550" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<idno>arXiv:180511761</idno>
		<title level="m">Collaborative learning for deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1557" to="1562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Seglink++: Detecting dense and arbitrary-shaped scene text by instance-aware component grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106954</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning shape-aware embedding for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4234" to="4243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Textray: Contour-based geometric modeling for arbitraryshaped scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">All you need is boundary: Toward arbitrary-shaped text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12160" to="12167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Collaborative learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv:180203531</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Crossdataset collaborative learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<idno>arXiv:210311351</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A single-shot arbitrarilyshaped text detector based on context attended multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1277" to="1285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shape robust text detection with progressive scale expansion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9336" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient and accurate arbitraryshaped text detection with pixel aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8440" to="8449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Arbitrary shape scene text detection with adaptive text region representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contournet: Taking a further step toward accurate arbitrary-shaped scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11753" to="11762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scene text detection with supervised pyramid context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9038" to="9045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Selftraining with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Textfield: Learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5566" to="5579" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>arXiv:211112309</idno>
		<title level="m">Regioncl: Can simple region swapping contribute to contrastive learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MSR: multi-scale shape regression for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, ijcai.org</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, ijcai.org</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="989" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Inceptext: A new inception-text module with deformable PSROI pooling for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1071" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Interactive self-training with mean teachers for semisupervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5941" to="5950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Textfusenet: Scene text detection with richer fused features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="516" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lianwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shuaitao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<idno>arXiv:171202170</idno>
		<title level="m">Detecting curve text in the wild: New dataset and new solution</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Look more than once: An accurate detector for text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10552" to="10561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>arXiv:171009412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno>arXiv:200408955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7789" to="7817" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards high performance human keypoint detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2639" to="2662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="433" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>arXiv:220210108</idno>
		<title level="m">Vision transformer advanced by exploring inductive bias for image recognition and beyond</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep relational reasoning graph network for arbitrary shape text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9699" to="9708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Collaborative learning network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="6788" to="6793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Crnet: A center-aware representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2571" to="2580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Textmountain: Accurate scene text detection via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107336</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fourier contour embedding for arbitraryshaped text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bvkv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="5981" to="5990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
