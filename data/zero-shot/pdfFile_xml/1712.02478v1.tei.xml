<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Understanding shadows from a single image spontaneously derives into two types of task in previous studies, containing shadow detection and shadow removal. In this paper, we present a multi-task perspective, which is not embraced by any existing work, to jointly learn both detection and removal in an end-to-end fashion that aims at enjoying the mutually improved benefits from each other. Our framework is based on a novel STacked Conditional Generative Adversarial Network (ST-CGAN), which is composed of two stacked CGANs, each with a generator and a discriminator. Specifically, a shadow image is fed into the first generator which produces a shadow detection mask. That shadow image, concatenated with its predicted mask, goes through the second generator in order to recover its shadow-free image consequently. In addition, the two corresponding discriminators are very likely to model higher level relationships and global scene characteristics for the detected shadow region and reconstruction via removing shadows, respectively. More importantly, for multi-task learning, our design of stacked paradigm provides a novel view which is notably different from the commonly used one as the multi-branch version. To fully evaluate the performance of our proposed framework, we construct the first large-scale benchmark with 1870 image triplets (shadow image, shadow mask image, and shadow-free image) under 135 scenes. Extensive experimental results consistently show the advantages of ST-CGAN over several representative state-of-the-art methods on two large-scale publicly available datasets and our newly released one.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Both shadow detection and shadow removal reveal their respective advantages for scene understanding. The accurate recognition of shadow area (i.e., shadow detection) provides adequate clues about the light sources <ref type="bibr" target="#b24">[25]</ref>, illu- * co-first author mination conditions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, object shapes <ref type="bibr" target="#b36">[37]</ref> and geometry information <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Meanwhile, removing the presence of shadows (i.e., shadow removal) in images is of great interest for the downstream computer vision tasks, such as efficient object detection and tracking <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>. Till this end, existing researches basically obey one of the following pipelines for understanding shadows: Detection only. In the history of shadow detection, a series of data-driven statistical learning approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref> have been proposed. Their main objective is to find the shadow regions, in a form of an image mask that separates shadow and non-shadow areas.</p><p>Removal only. A list of approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43</ref>] simply skips the potential information gained from the discovery of shadow regions and directly produces the illumination attenuation effects on the whole image, which is also denoted as a shadow matte <ref type="bibr" target="#b42">[43]</ref>, to recover the image with shadows removed naturally.</p><p>Two stages for removal. Many of the shadow removal methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b49">50]</ref> generally include two seperated steps: shadow localization and shadow-free reconstruction by exploiting the intermediate results in the awareness of shadow regions.</p><p>It is worth noting that the two targets: shadow mask in detection and shadow-free image in shadow removal, share a fundamental characteristic essentially. As shown in <ref type="figure" target="#fig_0">Figure  1</ref>, the shadow mask is posed as a two-binary map that segments the original image into two types of region whereas Shadow Detection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shadow Removal</head><p>Real/Fake Real/Fake  the shadow removal mainly focuses on one type of that and needs to discover the semantic relationship between the two areas, which indicates the strong correlations and possible mutual benefits between these two tasks.</p><p>Besides, most of the previous methods, including shadow detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref> and removal <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b0">1]</ref> are heavily based on local region classifications or low-level feature representations, failing to reason about the global scene semantic structure and illumination conditions. Consequently, a most recent study <ref type="bibr" target="#b35">[36]</ref> in shadow detection introduced a Conditional Generative Adversarial Network (CGAN) <ref type="bibr" target="#b32">[33]</ref> which is proved to be effective for the global consistency. For shadow removal, Qu et al. <ref type="bibr" target="#b42">[43]</ref> also proposed a multi-context architecture with an end-toend manner, which maintained a global view of feature extraction.</p><p>Since no existing approaches have explored the joint learning aspect of these two tasks, in this work, we propose a STacked Conditional Generative Adversarial Network (ST-CGAN) framework and aim to tackle shadow detection and shadow removal problems simultaneously in an end-to-end fashion. Besides making full use of the potential mutual promotions between the two tasks, the global perceptions are well preserved through the stacked adversarial components. Further, our design of stacked modules is not only to achieve a multi-task purpose, but also inspired from the connectivity pattern of DenseNet <ref type="bibr" target="#b13">[14]</ref>, where outputs of all preceding tasks are used as inputs for all subsequent tasks. Specifically, we construct ST-CGAN by stacking two generators along with two discriminators. In <ref type="figure" target="#fig_1">Figure  2</ref>, each generator takes every prior target of tasks (includ-ing the input) and stacks them as its input. Similarly, the discriminator attempts to distinguish the concatenation of all the previous tasks' targets from the real corresponding ground-truth pairs or triplets.</p><p>Importantly, the design of the proposed stacked components offers a novel perspective for multi-task learning in the literature. Different from the commonly used multibranch paradigm (e.g., Mask R-CNN <ref type="bibr" target="#b12">[13]</ref>, in which each individual task is assigned with a branch), we stack all the tasks that can not only focus on one task once a time in different stages, but also share mutual improvements through forward/backward information flows. Instead, the multibranch version aims to learn a shared embedding across tasks by simply aggregating the supervisions from each individual task.</p><p>To validate the effectiveness of the proposed framework, we further construct a new large-scale Dataset with Image Shadow Triplets (ISTD) consisting of shadow, shadow mask and shadow-free image to match the demand of multitask learning. It contains 1870 image triplets under 135 distinct scenarios, in which 1330 is assigned for training whilst 540 is for testing.</p><p>Extensive experiments on two large-scale publicly available benchmarks and our newly released dataset show that ST-CGAN performs favorably on both detection and removal aspects, comparing to several state-of-the-art methods. Further, we empirically demonstrate the advantages of our stacked joint formula over the widely used multi-branch version for shadow understanding. To conclude, the main contributions of this work are listed as follows:</p><p>? It is the first end-to-end framework which jointly learns shadow detection and shadow removal with superior performances on various datasets and on both the two tasks.</p><p>? A novel STacked Conditional Generative Adversarial Network (ST-CGAN) with a unique stacked joint learning paradigm is proposed to exploit the advantages of multi-task training for shadow understanding.</p><p>? The first large-scale shadow dataset which contains image triplets of shadow, shadow mask and shadow-free image is publicly released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Shadow Detection. To improve the robustness of shadow detection on consumer photographs and web quality images, a series of data-driven approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b55">56]</ref> have been taken and been proved to be effective. Recently, Khan et al. <ref type="bibr" target="#b21">[22]</ref> first introduced deep Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b44">[45]</ref> to automatically learn features for shadow regions/boundaries that significantly outperforms the previous state-of-the-art. A multikernel model for shadow region classification was proposed by Vicente et al. <ref type="bibr" target="#b47">[48]</ref> and it is efficiently optimized based on leastsquares SVM leave-one-out estimates. More recent work of Vicente et al. <ref type="bibr" target="#b48">[49]</ref> used a stacked CNN with separated steps, including first generating the image level shadow-prior and training a patch-based CNN which produces shadow masks for local patches. Nguyen et al. <ref type="bibr" target="#b35">[36]</ref> presented the first application of adversarial training for shadow detection and developed a novel conditional GAN architecture with a tunable sensitivity parameter. Shadow Removal. Early works are motivated by physical models of illumination and color. For instance, Finlayson et al. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> provide the illumination invariant solutions that work well only on high quality images. Many existing approaches for shadow removal include two steps in general. For the removal part of these two-stage solutions, the shadow is erased either in the gradient domain <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b1">2]</ref> or the image intensity domain <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23</ref>]. On the contrary, a few works <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b41">42]</ref> recover the shadow-free image by intrinsic image decomposition and preclude the need of shadow prediction in an end-to-end manner. However, these methods suffer from altering the colors of the non-shadow regions. Qu et al. <ref type="bibr" target="#b42">[43]</ref> further propose a multicontext architecture which consists of three levels (global localization, appearance modeling and semantic modeling) of embedding networks, to explore shadow removal in an end-to-end and fully automatic framework. CGAN and Stacked GAN. CGANs have achieved impressive results in various image-to-image translation problems, such as image superresolution <ref type="bibr" target="#b26">[27]</ref>, image inpaint-ing <ref type="bibr" target="#b40">[41]</ref>, style transfer <ref type="bibr" target="#b27">[28]</ref> and domain adaptation/transfer <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b29">30]</ref>. The key of CGANs is the introduction of the adversarial loss with an informative conditioning variable, that forces the generated images to be with high quality and indistinguishable from real images. Besides, recent researches have proposed some variants of GAN, which mainly explores the stacked scheme of its usage. Zhang et al. <ref type="bibr" target="#b53">[54]</ref> first put forward the StackGAN to progressively produce photo-realistic image synthesis with considerably high resolution. Huang et al. <ref type="bibr" target="#b15">[16]</ref> design a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations for the purpose of generating more qualified images. Therefore, our proposed stacked form is distinct from all the above relevant versions in essence.</p><p>Multi-task Learning. The learning hypothesis is biased to prefer a shared embedding learnt across multiple tasks. The widely adopted architecture of multi-task formulation is a shared component with multi-branch outputs, each for an individual task. For example, in Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> and MultiNet <ref type="bibr" target="#b46">[47]</ref>, 3 parallel branches for object classification, bounding-box regression and semantic segmentation respectively are utilized. Misra et al. <ref type="bibr" target="#b33">[34]</ref> propose "crossstitch" unit to learn shared representations from multiple supervisory tasks. In Multi-task Network Cascades <ref type="bibr" target="#b3">[4]</ref>, all tasks share convolutional features, whereas later task also depends the output of a preceding one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A new Dataset with Image Shadow Triplets -ISTD</head><p>Existing publicly available datasets are all limited in the view of multi-task settings. Among them, SBU <ref type="bibr" target="#b50">[51]</ref> and UCF <ref type="bibr" target="#b55">[56]</ref> are prepared for shadow detection only, whilst SRD <ref type="bibr" target="#b42">[43]</ref>, UIUC <ref type="bibr" target="#b11">[12]</ref> and LRSS <ref type="bibr" target="#b9">[10]</ref> are constructed for the purpose of shadow removal accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Amount Content of Images Type SRD <ref type="bibr" target="#b42">[43]</ref> 3088 shadow/shadow-free pair UIUC <ref type="bibr" target="#b11">[12]</ref> 76 shadow/shadow-free pair LRSS <ref type="bibr" target="#b9">[10]</ref> 37 shadow/shadow-free pair SBU <ref type="bibr" target="#b50">[51]</ref> 4727 shadow/shadow mask pair UCF <ref type="bibr" target="#b55">[56]</ref> 245 shadow/shadow mask pair ISTD (ours) 1870 shadow/shadow mask/shadow-free triplet To facilitate the evaluation of shadow understanding methods, we have constructed a large-scale Dataset with Image Shadow Triplets called ISTD 1 . It contains 1870 triplets of shadow, shadow mask and shadow-free image under 135 different scenarios. To the best of our knowledge, ISTD is the first large-scale benchmark for simultaneous evaluations of shadow detection and shadow removal. Detailed comparisons with previous popular datasets are listed in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>In addition, our proposed dataset also contains a variety of properties in the following aspects:</p><p>? Illumination: Minimized illumination difference between a shadow image and the shadow-free one is obtained. When constructing the dataset, we pose a camera with a fixed exposure parameter to capture the shadow image, where the shadow is cast by an object. Then the occluder is removed in order to get the corresponding shadow-free image. More evidences are given in the 1st and 3rd row of ? Scenes: 135 different types of ground materials, e.g., 6th-8th column in <ref type="figure" target="#fig_2">Figure 3</ref>, are utilized to cover as many complex backgrounds and different reflectances as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>We propose STacked Conditional Generative Adversarial Networks (ST-CGANs), a novel stacked architecture that enables the joint learning for shadow detection and shadow removal, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In this section, we first describe the formulations with loss functions, training procedure, and then present the network details of ST-CGAN, followed by a subsequent discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">STacked Conditional Generative Adversarial Networks</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b8">[9]</ref> consists of two players: a generator G and a discriminator D. These two players are competing in a zero-sum game, in which the generator G aims to produce a realistic image given an input z, that is sampled from a certain noise distribution. The discriminator D is forced to classify if a given image is generated by G or it is indeed a real one from the dataset. Hence, the adversarial competition progressively facilitates each other, whilst making G's generation hard for D to differentiate from the real data. Conditional Generative Adversarial Networks (CGANs) <ref type="bibr" target="#b32">[33]</ref> extends GANs by introducing an additional observed information, named conditioning variable, to both the generator G and discriminator D.</p><p>Our ST-CGAN consists of two Conditional GANs in which the second one is stacked upon the first. For the first CGAN of ST-CGAN in <ref type="figure" target="#fig_1">Figure 2</ref>, both the generator G 1 and discriminator D 1 are conditioned on the input RGB shadow image x. G 1 is trained to output the corresponding shadow mask G 1 (z, x), where z is the random sampled noise vector. We denote the ground truth of shadow mask for x as y, to which G 1 (z, x) is supposed to be close. As a result, G 1 needs to model the distribution p data (x, y) of the dataset. The objective function for the first CGAN is:</p><formula xml:id="formula_0">L CGAN1 (G 1 , D 1 ) = E x,y?p data (x,y) [log D 1 (x, y)]+ E x?p data (x),z?pz(z) [log(1 ? D 1 (x, G 1 (z, x)))].<label>(1)</label></formula><p>We further eliminate the random variable z to have a deterministic generator G 1 and thus the Equation <ref type="formula" target="#formula_0">(1)</ref> is simplified to:</p><formula xml:id="formula_1">L CGAN1 (G 1 , D 1 ) = E x,y?p data (x,y) [log D 1 (x, y)]+ E x?p data (x) [log(1 ? D 1 (x, G 1 (x)))].<label>(2)</label></formula><p>Besides the adversarial loss, the classical data loss is adopted that encourages a straight and accurate regression of the target:</p><formula xml:id="formula_2">L data1 (G 1 ) = E x,y?p data (x,y) ||y ? G 1 (x)||.<label>(3)</label></formula><p>Further in the second CGAN of <ref type="figure" target="#fig_1">Figure 2</ref>, by applying the similar formulations above, we have:  <ref type="table">Table 2</ref>. The architecture for generator G1/G2 of ST-CGAN. Cvi means a classic convolutional layer whilst CvTi stands for a transposed convolutional layer that upsamples a feature map. Cv4 (?3) indicates that the block of Cv4 is replicated for additional two times, three in total. "#C in" and "#C out" denote for the amount of input channels and output channels respectively. "before" shows the immediate layer before a block and "after" gives the subsequent one directly. "link" explains the specific connections that lie in U-Net architectures <ref type="bibr" target="#b43">[44]</ref> in which ? decides the direction of connectivity, i.e., Cv0 ? CvT11 bridges the output of Cv0 concatenated to the input of CvT11. LReLU is short for Leaky ReLU activation <ref type="bibr" target="#b30">[31]</ref> and BN is a abbreviation of Batch Normalization <ref type="bibr" target="#b16">[17]</ref>.  </p><formula xml:id="formula_3">L data 2 (G2|G1) = E x,r?p data (x,r) ||r ? G2(x, G1(x))||,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><formula xml:id="formula_4">L CGAN2 (G 2 , D 2 |G 1 ) = E x,y,r?p data (x,y,r) [log D 2 (x, y, r)] + E x?p data (x) [log(1 ? D 2 (x, G 1 (x), G 2 (x, G 1 (x))))],<label>(5)</label></formula><p>where r denotes for x's corresponding shadow-free image and G 2 takes a combination of x and G 1 (x) as inputs whereas D 2 differentiates the concatenation of outputs from G 1 and G 2 , conditioned on x, from the real pairs. Till this end, we can finally conclude the entire objective for the joint learning task which results in solving a mini-max problem where the optimization aims to find a saddle point:</p><formula xml:id="formula_5">min G1,G2 max D1,D2 L data1 (G 1 ) + ? 1 L data2 (G 2 |G 1 ) + ? 2 L CGAN1 (G 1 , D 1 ) + ? 3 L CGAN2 (G 2 , D 2 |G 1 ).<label>(6)</label></formula><p>It is regarded as a two-player zero-sum game. The first player is a team consisting of two generators (G 1 , G 2 ). The second player is a team containing two discriminators (D 1 , D 2 ). In order to defeat the second player, the members of the first team are encouraged to produce outputs that are close to their corresponding ground-truths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architecture and Training Details</head><p>Generator. The generator is inspired by the U-Net architecture <ref type="bibr" target="#b43">[44]</ref>, which is originally designed for biomedical image segmentation. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. The detailed structure of G 1 /G 2 , similar to <ref type="bibr" target="#b17">[18]</ref>, is listed in the <ref type="table">Table 2</ref>. Discriminator. For D 1 , it receives a pair of images as inputs, composed of an original RGB scene image and a shadow mask image that generates 4-channel feature-maps as inputs. The dimensionality of channels increases to 7 for D 2 as it accepts an additional shadow-free image. <ref type="table" target="#tab_4">Table 3</ref> gives more details of these two discriminators.  Training/Implementation settings. Our code is based on pytorch <ref type="bibr" target="#b20">[21]</ref>. We train ST-CGAN with the Adam solver <ref type="bibr" target="#b23">[24]</ref> and an alternating gradient update scheme is applied. Specifically, we first adopt a gradient ascent step to update D 1 , D 2 with G 1 , G 2 fixed. We then apply a gradient descent step to update G 1 , G 2 with D 1 , D 2 fixed. We initialize all the weights of ST-CGAN by sampling from a zeromean normal distribution with standard deviation 0.2. During training, augmentations are adopted by cropping (image size 286 ? 256) and flipping (horizontally) operations. A practical setting for ?, where ? 1 = 5, ? 2 = 0.1, ? 3 = 0.1, is used. The Binary Cross Entropy (BCE) loss is assigned for the objective of image mask regression and L1 loss is utilized for the shadow-free image reconstruction respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>The stacked term. The commonly used form of multitask learning is the multi-branch version. It aims to learn a shared representation, which is further utilized for each task in parallel. <ref type="figure" target="#fig_5">Figure 4</ref> implies that our stacked design differs quite a lot from it. We conduct the multi-task learning in such a way that each task can focus on its individual feature embeddings, instead of a shared embedding across tasks, whilst they still enhance each other through the stacked connections, in a form of a forward/backward information flow.</p><p>The following experiments also confirm the effectiveness of our architecture on the two tasks, compared with the multibranch one, which can be found in <ref type="table" target="#tab_8">Table 8</ref>.</p><p>The adversarial term. Moreover, Conditional GANs (CGANs) are able to effectively enforce higher order consistencies, to learn a joint distribution of image pairs or triplets. This confers an additional advantage to our method, as we implement our basic component to be CGAN and per-  <ref type="table">Table 5</ref>. Detection with quantitative results using BER, smaller is better. For our proposed architecture, we use image pairs of SBU training set together with their roughly generated shadow-free images by Guo et al. <ref type="bibr" target="#b11">[12]</ref> to form image triplets for training. The best and second best results are marked in red and blue colors, respectively. form a stacked input into the adversarial networks, when compared with nearly most of previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To comprehensively evaluate the performance of our proposed method, we perform extensive experiments on a variety of datasets and evaluate ST-CGAN in both detection and removal measures, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We mainly utilize two large-scale publicly available datasets 2 including SBU <ref type="bibr" target="#b50">[51]</ref> and UCF <ref type="bibr" target="#b55">[56]</ref>, along with our newly collected dataset ISTD. SBU <ref type="bibr" target="#b50">[51]</ref> has 4727 pairs of shadow and shadow mask image. Among them, 4089 pairs are for training and the rest is for testing. UCF <ref type="bibr" target="#b55">[56]</ref> has 245 shadow and shadow mask pairs in total, which are all used for testing in the following experiments. ISTD is our new released dataset consisting of 1870 triplets, which is suitable for multi-task training. It is randomly divided into 1330 for training and 540 for testing. <ref type="bibr" target="#b1">2</ref> Note that we do not include the large-scale SRD dataset in this work because it is currently unavailable for the authors' <ref type="bibr" target="#b42">[43]</ref> personal reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Compared Methods and Metrics</head><p>For detection part, we compare ST-CGAN with the stateof-the-art StackedCNN <ref type="bibr" target="#b50">[51]</ref>, cGAN <ref type="bibr" target="#b35">[36]</ref> and scGAN <ref type="bibr" target="#b35">[36]</ref>. To evaluate the shadow detection performance quantitatively, we follow the commonly used terms <ref type="bibr" target="#b35">[36]</ref> to compare the provided ground-truth masks and the predicted ones with the main evaluation metric, which is called Balance Error Rate (BER):</p><formula xml:id="formula_6">BER = 1 ? 1 2 ( T P T P + F N + T N T N + F P ),<label>(7)</label></formula><p>along with separated per pixel error rates per class (shadow and non-shadow).</p><p>For removal part, we use the publicly available source codes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b7">8]</ref> as our baselines. In order to perform a quantitative comparison, we follow <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43]</ref> and use the root mean square error (RMSE) in LAB color space between the ground truth shadow-free image and the recovered image as measurement, and then evaluate the results on the whole image as well as shadow and non-shadow regions separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Detection Evaluation</head><p>For detection, we utilize the cross-dataset shadow detection schedule, similar in <ref type="bibr" target="#b35">[36]</ref>, to evaluate our method. We first train our proposed ST-CGAN on the ISTD training set.  <ref type="table">Table 6</ref>. Removal with quantitative results using RMSE, smaller is better. The original difference between the shadow and shadow-free images is reported in the third column. We perform multi-task training on ISTD and compare it with three state-of-the-art methods. The best and second best results are marked in red and blue colors, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Type Aspects</head><p>Ours Ours (-  <ref type="table">Table 7</ref>. Component analysis of ST-CGAN on ISTD by using RMSE for removal and BER for detection, smaller is better. The metrics related to shadow and non-shadow part are also provided. The best and second best results are marked in red and blue colors, respectively.</p><formula xml:id="formula_7">D 1 ) Ours (-D 2 ) Ours (-G 1 -D 1 ) Ours (-G 2 -D 2 ) Removal</formula><p>The evaluations are thus conducted on three datasets with three state-of-the-art approaches in <ref type="table" target="#tab_5">Table 4</ref>. As can be seen, ST-CGAN outperforms StackedCNN and cGAN by a large margin. In terms of BER, we obtain a significant 14.4% error reduction on SBU and 18.1% on ISTD respectively, compared to scGAN. Next, we switch the training set to SBU's training data. Considering our framework requires image triplets that SBU cannot offer, we make an additional pre-processing step. In order to get the corresponding shadow-free image, we use the shadow removal code <ref type="bibr" target="#b11">[12]</ref> to generate them as coarse labels. We also test these trained models on the three datasets. Despite the inaccurate shadow-free groundtruths, our proposed framework still significantly improves the overall performances. Specifically, on the SBU test set, ST-CGAN achieves an obvious improvement with 10.5% error reduction of BER over the previous best record from scGAN.</p><p>In <ref type="figure" target="#fig_6">Figure 5</ref>, we demonstrate the comparisons of the detection results qualitatively. As shown in <ref type="figure" target="#fig_6">Figure 5</ref> (a) and 5 (b), ST-CGAN is not easily fooled by the lower brightness area of the scene, comparing to cGAN and scGAN. Our method is also precise in detecting shadows cast on bright areas such as the line mark in <ref type="figure" target="#fig_6">Figure 5</ref> (c) and 5 (d). The proposed ST-CGAN is able to detect more fine-grained shadow details (e.g., shadow of leaves) than other methods, as shown in <ref type="figure" target="#fig_6">Figure 5</ref> (e) and 5 (f).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Removal Evaluation</head><p>For removal, we compare our proposed ST-CGAN with the three state-of-the-art methods on ISTD dataset, as shown in <ref type="table">Table 6</ref>. The RMSE values are reported. We evaluate the performance of different methods on the shadow regions, non-shadow regions, and the whole image. The proposed ST-CGAN achieves the best performance among all the compared methods by a large margin. Notably, the error of non-shadow region is very close to the original one, which indicates its strong ability to distinguish the nonshadow part of an image. The advantage of removal also partially comes from the joint learning scheme, where the well-trained detection block provides more clear clues of shadow and shadow-free areas.</p><p>We also demonstrate the comparisons of the removal results. As shown in <ref type="figure" target="#fig_6">Figure 5</ref>, although Yang <ref type="bibr" target="#b52">[53]</ref> can recover shadow-free image, it alters the colors of both shadow and nonshadow regions. Guo <ref type="bibr" target="#b10">[11]</ref> and Gong <ref type="bibr" target="#b7">[8]</ref> fail to detect shadow accurately, thus both of their predictions are incomplete especially in shadow regions. Moreover, due to the difficulty of determining the environmental illuminations and global consistency, all the compared baseline models produce unsatisfactory results on the semantic regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Component Analysis of ST-CGAN</head><p>To illustrate the effects of different components of ST-CGAN, we make a series of ablation experiments by progressively removing different parts of it. According to both the removal and the detection performances in <ref type="table">Table 7</ref>, we find that each individual component is necessary and indispensable for the final excellent predictions. Moreover, the last two columns of <ref type="table">Table 7</ref> also demonstrate that without the stacked joint learning, a single module consisting of one generator and one discriminator performs worse consistently. It further implies the effectiveness of our multi-task architecture on both shadow detection and shadow removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Stacked Joint vs. Multi-branch Learning</head><p>We further modify our body architecture into a multibranch version, where each branch is designed for one task respectively. Therefore, the framework aims to learn  a shared embedding which is supervised by two tasks, as shown in the bottom of <ref type="figure" target="#fig_7">Figure 6</ref>. For a clear explanation, the illustration of comparisons between ours and the multibranch one is also given. With all other training settings fixed, we fairly compare our proposed ST-CGAN with the multi-branch version quantitatively on the measurements of both detection and removal on ISTD dataset. <ref type="table" target="#tab_8">Table 8</ref> reports that our stacked joint learning paradigm consistently outperforms the multi-branch version in every single aspect of the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed STacked Conditional Generative Adversarial Network (ST-CGAN) to jointly learn shadow detection and shadow removal. Our frame- work has at least four unique advantages as follows: 1) it is the first end-to-end approach that tackles shadow detection and shadow removal simultaneously; 2) we design a novel stacked mode, which densely connects all the tasks in the purpose of multi-task learning, that proves its effectiveness and suggests the future extension on other types of multiple tasks; 3) the stacked adversarial components are able to preserve the global scene characteristics hierarchically, thus it leads to a fine-grained and natural recovery of shadow-free images; 4) ST-CGAN consistently improves the overall performances on both the detection and removal of shadows. Moreover, as an additional contribution, we publicly release the first large-scale dataset which contains shadow, shadow mask and shadow-free image triplets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>We propose an end-to-end stacked joint learning architecture for two tasks: shadow detection and shadow removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the proposed ST-CGAN. It consists of two stacked CGANs: one for shadow detection and another for shadow removal, which are marked in different colors. The intermediate outputs are concatenated together as the subsequent components' input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An illustration of several shadow, shadow mask and shadow-free image triplets in ISTD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .?</head><label>3</label><figDesc>Shapes: Various shapes of shadows are built by different objects, such as umbrellas, boards, persons, twigs and so on. See the 2nd row of Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>An illustration of information flows which indicates the mutual promotions between tasks of the proposed stacked scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Comparison of shadow detection and removal results of different methods on ISTD dataset. Note that our proposed ST-CGAN simultaneously produces the detection and removal results, whilst others are either for shadow detection or for shadow removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Illustrations of our stacked joint learning (top) and common multi-branch learning (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Input image: concatenation over channels : skip connection between mirror layers Color : CGAN for shadow detection Color : CGAN for shadow removal : fake pair/triplet for discriminator : real pair/triplet for discriminator</figDesc><table><row><cell>1</cell><cell>2</cell></row><row><cell>Input image</cell><cell></cell><cell>Input image</cell></row><row><cell cols="2">Detected shadow</cell><cell>Detected shadow</cell></row><row><cell></cell><cell></cell><cell>Shadow removal image</cell></row><row><cell></cell><cell>1</cell><cell>2</cell></row><row><cell>OR</cell><cell>OR</cell></row><row><cell></cell><cell></cell><cell>Input image</cell></row><row><cell>Input image</cell><cell></cell><cell>GT shadow</cell></row><row><cell>GT shadow</cell><cell></cell><cell>Ground truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons with other popular shadow related datasets. Ours is unique in the content and type, whilst being in the same order of magnitude to the most large-scale datasets in amount.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>CvT 6 CvT 7 (?3) CvT 8 CvT 9CvT 10 CvT 11G 1 /G 2</figDesc><table><row><cell>Network</cell><cell>Layer</cell><cell>Cv 0</cell><cell>Cv 1</cell><cell>Cv 2</cell><cell>Cv 3</cell><cell>Cv 4 (?3)</cell><cell>Cv 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>#C in</cell><cell>3/4</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>1024</cell><cell>1024</cell><cell>512</cell><cell>256</cell><cell>128</cell></row><row><cell></cell><cell>#C out</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>1/3</cell></row><row><cell></cell><cell>before</cell><cell>-</cell><cell>LReLU</cell><cell>LReLU</cell><cell>LReLU</cell><cell>LReLU</cell><cell cols="2">LReLU ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell><cell>ReLU</cell></row><row><cell></cell><cell>after</cell><cell>-</cell><cell>BN</cell><cell>BN</cell><cell>BN</cell><cell>BN</cell><cell>-</cell><cell>BN</cell><cell>BN</cell><cell>BN</cell><cell>BN</cell><cell>BN</cell><cell>Tanh</cell></row><row><cell></cell><cell>link</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? CvT 11 ? CvT 10 ? CvT 9 ? CvT 8 ? CvT 7 - - Cv 4 ? Cv 3 ? Cv 2 ? Cv 1 ? Cv 0 ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>The architectures for discriminator D1/D2 of ST-CGAN. Annotations are kept the same with Table 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Detection with quantitative results using BER, smaller is better. For our proposed architecture, we use image triplets of ISTD training set. These models are tested on three datasets. The best and second best results are marked in red and blue colors, respectively.</figDesc><table><row><cell cols="6">Using ISTD Train Detection Aspects StackedCNN [51] cGAN [36] scGAN [36] ours</cell></row><row><cell></cell><cell>Shadow</cell><cell>11.29</cell><cell>24.07</cell><cell>9.1</cell><cell>9.02</cell></row><row><cell>SBU [51] (%)</cell><cell>Non-shadow</cell><cell>20.49</cell><cell>13.13</cell><cell>17.41</cell><cell>13.66</cell></row><row><cell></cell><cell>BER</cell><cell>15.94</cell><cell>18.6</cell><cell>13.26</cell><cell>11.34</cell></row><row><cell></cell><cell>Shadow</cell><cell>10.56</cell><cell>23.23</cell><cell>9.09</cell><cell>8.77</cell></row><row><cell>UCF [56] (%)</cell><cell>Non-shadow</cell><cell>27.58</cell><cell>15.61</cell><cell>23.74</cell><cell>23.59</cell></row><row><cell></cell><cell>BER</cell><cell>18.67</cell><cell>19.42</cell><cell>16.41</cell><cell>16.18</cell></row><row><cell></cell><cell>Shadow</cell><cell>7.96</cell><cell>10.81</cell><cell>3.22</cell><cell>2.14</cell></row><row><cell>ISTD (%)</cell><cell>Non-shadow</cell><cell>9.23</cell><cell>8.48</cell><cell>6.18</cell><cell>5.55</cell></row><row><cell></cell><cell>BER</cell><cell>8.6</cell><cell>9.64</cell><cell>4.7</cell><cell>3.85</cell></row><row><cell cols="6">Using SBU Train Detection Aspects StackedCNN [51] cGAN [36] scGAN [36] ours</cell></row><row><cell></cell><cell>Shadow</cell><cell>9.6</cell><cell>20.5</cell><cell>7.8</cell><cell>3.75</cell></row><row><cell>SBU [51] (%)</cell><cell>Non-shadow</cell><cell>12.5</cell><cell>6.9</cell><cell>10.4</cell><cell>12.53</cell></row><row><cell></cell><cell>BER</cell><cell>11.0</cell><cell>13.6</cell><cell>9.1</cell><cell>8.14</cell></row><row><cell></cell><cell>Shadow</cell><cell>9.0</cell><cell>27.06</cell><cell>7.7</cell><cell>4.94</cell></row><row><cell>UCF [56] (%)</cell><cell>Non-shadow</cell><cell>17.1</cell><cell>10.93</cell><cell>15.3</cell><cell>17.52</cell></row><row><cell></cell><cell>BER</cell><cell>13.0</cell><cell>18.99</cell><cell>11.5</cell><cell>11.23</cell></row><row><cell></cell><cell>Shadow</cell><cell>11.33</cell><cell>19.93</cell><cell>9.5</cell><cell>4.8</cell></row><row><cell>ISTD (%)</cell><cell>Non-shadow</cell><cell>9.57</cell><cell>4.92</cell><cell>8.46</cell><cell>9.9</cell></row><row><cell></cell><cell>BER</cell><cell>10.45</cell><cell>12.42</cell><cell>8.98</cell><cell>7.35</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Dataset Removal aspects Original Guo et al. [12] Yang et al. [53] Gong et al. [8] ours</figDesc><table><row><cell></cell><cell>Shadow</cell><cell>32.67</cell><cell>18.95</cell><cell>19.82</cell><cell>14.98</cell><cell>10.33</cell></row><row><cell>ISTD</cell><cell>Non-shadow</cell><cell>6.83</cell><cell>7.46</cell><cell>14.83</cell><cell>7.29</cell><cell>6.93</cell></row><row><cell></cell><cell>All</cell><cell>10.97</cell><cell>9.3</cell><cell>15.63</cell><cell>8.53</cell><cell>7.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Comparisons between stacked learning (ours) and multibranch learning with removal and detection results on ISTD dataset.</figDesc><table><row><cell>Task Type</cell><cell>Aspects</cell><cell cols="2">Multi-branch Ours</cell></row><row><cell></cell><cell>Shadow</cell><cell>11.54</cell><cell>10.33</cell></row><row><cell>Removal</cell><cell>Non-shadow</cell><cell>7.13</cell><cell>6.93</cell></row><row><cell></cell><cell>All</cell><cell>7.84</cell><cell>7.47</cell></row><row><cell></cell><cell>Shadow</cell><cell>2.34</cell><cell>2.14</cell></row><row><cell>Detection (%)</cell><cell>Non-shadow</cell><cell>7.2</cell><cell>5.55</cell></row><row><cell></cell><cell>BER</cell><cell>4.77</cell><cell>3.85</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">ISTD dataset is available in https://drive.google.com/file/d/1I0qw-65KBA6np8vIZzO6oeiOvcDBttAY/view?usp=sharing</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shadow removal using intensity surfaces and texture anchor points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hel-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1202" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape, illumination, and reflectance from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1670" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving shadow suppression in moving object detection with hsv color information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sirotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="334" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Entropy minimization for shadow removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="57" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Removing shadows from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Hordley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="823" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the removal of shadows from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Hordley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive shadow removal and ground truth for variable scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cosker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<publisher>University of Bath</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to remove soft shadows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gryka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single-image shadow detection and removal using paired regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2033" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Paired regions for shadow detection and removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2956" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What characterizes a shadow boundary under the sun and sky</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="898" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04357</idno>
		<title level="m">Stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning(ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating geo-temporal location of stationary cameras using shadow trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">N</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="318" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rendering synthetic objects into legacy photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">157</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction to pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ketkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning with Python</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="195" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic feature learning for robust shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1939" to="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic shadow detection and removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="431" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimating natural illumination from a single outdoor image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting ground shadows in outdoor consumer photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="322" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Texture-consistent shadow removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="437" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00848</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Moving shadow and object detection in traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mikic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Kogut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crossstitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Editing soft shadows in a digital photograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shadow detection with conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4510" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attached shadow coding: Estimating surface normals from shadows under unknown reflectance and lighting conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1693" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust shadow and illumination estimation using a mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panagopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="651" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Illumination estimation and cast shadow detection through a higher-order graphical model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panagopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="673" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simultaneous cast shadows, illumination and geometry inference using hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panagopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="437" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pixel-wise orthogonal decomposition for color illumination invariant and shadowfree image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2220" to="2239" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deshadownet: A multi-context embedding deep network for shadow removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recovering intrinsic images from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1459" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zoellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multinet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07695</idno>
		<title level="m">Real-time joint semantic reasoning for autonomous driving</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Leave-oneout kernel optimization for shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3388" to="3396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Noisy label recovery for shadow detection in unfamiliar domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3783" to="3792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Leave-oneout kernel optimization for shadow detection and removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Large-scale training of shadow detectors with noisily-annotated shadow examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Natural shadow matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Shadow removal using bilateral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4361" to="4368" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stackgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03242</idno>
		<title level="m">Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Shadow remover: Image shadow removal based on illumination recovering optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4623" to="4636" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning to recognize shadows in monochromatic natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="223" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
