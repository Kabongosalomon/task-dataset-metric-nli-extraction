<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cell Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Giusti</surname></persName>
							<email>lorenzo.giusti@uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University or Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Battiloro</surname></persName>
							<email>claudio.battiloro@uniroma1.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Sapienza University or Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Testa</surname></persName>
							<email>lucia.testa@uniroma1.it</email>
							<affiliation key="aff2">
								<orgName type="institution">Sapienza University or Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><forename type="middle">Di</forename><surname>Lorenzo</surname></persName>
							<email>paolo.dilorenzo@uniroma1.it</email>
							<affiliation key="aff3">
								<orgName type="institution">Sapienza University or Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefania</forename><surname>Sardellitti</surname></persName>
							<email>stefania.sardellitti@uniroma1.it</email>
							<affiliation key="aff4">
								<orgName type="institution">Sapienza University or Rome</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Barbarossa</surname></persName>
							<email>sergio.barbarossa@uniroma1.it</email>
							<affiliation key="aff5">
								<orgName type="institution">Sapienza University or Rome</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cell Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since their introduction, graph attention networks achieved outstanding results in graph representation learning tasks. However, these networks consider only pairwise relationships among nodes and then they are not able to fully exploit higher-order interactions present in many real world data-sets. In this paper, we introduce Cell Attention Networks (CANs), a neural architecture operating on data defined over the vertices of a graph, representing the graph as the 1skeleton of a cell complex introduced to capture higher order interactions. In particular, we exploit the lower and upper neighborhoods, as encoded in the cell complex, to design two independent masked self-attention mechanisms, thus generalizing the conventional graph attention strategy. The approach used in CANs is hierarchical and it incorporates the following steps: i) a lifting algorithm that learns edge features from node features; ii) a cell attention mechanism to find the optimal combination of edge features over both lower and upper neighbors; iii) a hierarchical edge pooling mechanism to extract a compact meaningful set of features. The experimental results show that CAN is a low complexity strategy that compares favorably with state of the art results on graph-based learning tasks.</p><p>Preprint. Preliminary work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) find applications in a plethora of fields, like computational chemistry <ref type="bibr" target="#b0">[1]</ref>, social networks <ref type="bibr" target="#b1">[2]</ref> and physics simulations <ref type="bibr" target="#b2">[3]</ref>. Since their introduction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, GNNs have shown remarkable results in learning tasks when data are defined over a graph domain, where the flexibility of neural networks is coupled with prior knowledge about data relationships, expressed in terms of the underlying topology. The literature on GNNs is large and numerous techniques have been studied, usually categorized in spectral <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and non-spectral methods <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>.</p><p>Generally speaking, the idea is learning representations of node attributes using local aggregation, where the neighborhood is formally represented by the graph topology. By leveraging this basic but powerful idea, outstanding performance has been achieved in many traditional tasks such as classification for nodes or entire graphs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref> or link prediction <ref type="bibr" target="#b11">[12]</ref> as well as more specialized ones such as protein folding <ref type="bibr" target="#b12">[13]</ref> and neural algorithmic reasoning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. At the same time, a major performance boost to deep learning algorithms has been offered by the inclusion of attention mechanisms, introduced to handle sequence-based tasks <ref type="bibr" target="#b15">[16]</ref>, enabling for variable sized inputs, to concentrate on their most important features. Then, pioneering works introduced graph attention networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> achieving state-of-the-art results in most of the aforementioned tasks.</p><p>Graphs can be also seen as a simple instance of a topological space, able to capture pairwise interactions through the presence of an edge between any pair of directly interacting nodes. However, despite their overwhelming popularity, graph-based representations are unable to consistently model higher order relations, which play a crucial role in many practical applications. Examples where multiway relations cannot be reduced to an ensemble of pairwise relations are gene regulatory networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, where some reactions occur only when a set of multiple (not only two) genes interact, or applications in network neuroscience <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. To overcome this problem, many architectures defined on general hypergraphs have been proposed <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>, and some works incorporating attention mechanisms for hypergraphs neural networks have been published <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>However, in the meanwhile, pioneering works on Topological Signal Processing <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> demonstrated the benefit of processing signals defined on simplicial and cell complexes, which are specific examples of hyper-graphs with a rich algebraic description, and can easily encode multi-way relationships hidden in the data in a low-complexity fashion. Consequently, there was a natural interest in the design of (deep) neural networks architectures able to learn from data defined on simplicial and cell complexes, as summarized in the following state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Works</head><p>Despite Topological Deep Learning is an emerging research area that has been introduced quite recently, numerous pioneering works appeared in this field. In <ref type="bibr" target="#b31">[32]</ref>, message passing neural networks (MPNNs) <ref type="bibr" target="#b0">[1]</ref> are adapted to simplicial complexes (SCs), and a Simplicial Weisfeiler-Lehman (SWL) colouring procedure for distinguishing non-isomorphic SCs is introduced. The aggregation and updating functions are able to process the data exploiting lower and upper neighbourhoods. The architectures in <ref type="bibr" target="#b31">[32]</ref>, namely Message Passing Simplicial Networks (MPSNs), are a generalization of Graph Isomorphism Network (GIN) <ref type="bibr" target="#b32">[33]</ref>. In <ref type="bibr" target="#b33">[34]</ref>, message passing neural networks able to handle data defined over regular cell complexes are introduced under the name of CW Networks (CWNs), and they are proven to be not less powerful than the 3-WL test. The work in ( <ref type="bibr" target="#b34">[35]</ref>) introduced Neural Sheaf Diffusion Models, neural architectures grounded in the theory of cellular sheaves able to improve learning performance on graph-based tasks especially in heterophilic settings. In <ref type="bibr" target="#b35">[36]</ref>, a novel attention neural architecture that operates on data defined on simplicial complexes leveraging masked self-attention layers is introduced, taking into account lower and upper neighbourhoods and introducing a proper technique to process the harmonic component of the data based on the design of a sparse projection operator. A similiar architecture is proposed in <ref type="bibr" target="#b36">[37]</ref>, which re-weights the interactions between neighbouring simplices through an orientation equivariant attention mechanism. However, none of these works considered masked self-attention mechanisms for architectures designed to handle data defined over cell complexes. Finally, the work in <ref type="bibr" target="#b37">[38]</ref> introduced a broad class of attentional architectures operating on generalized higher-order domains called Combinatorial Complexes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contribution</head><p>The aim of this paper is to introduce cell attention networks, i.e., a fully attentional low-complexity architecture able to learn from data defined over the nodes of a graph by incorporating the graph within a cell complex and working at the edge-level in order to extract higher order interactions. The idea is to implement an attention mechanism that handles relations among nearby edges, where the neighborhood is formally represented by a cell complex. To this aim, we exploit a hierarchical approach that lifts up node features to derive edge features, and then it computes the attention coefficients between nearby edges to find the optimal combination of edge features. In particular, we devise an attentional lift mechanism that learns the data over the edges of the complex leveraging a self-attention mechanism over the features of the vertices that are on their boundaries. Consequently, our architecture is also equipped with a cellular lifting map algorithm that embeds the graph domain into a regular cell complex (CW). Other graph lifting techniques have been used in previous works, such as clique complexes <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, or more sophisticated structures based on incidence tensors <ref type="bibr" target="#b40">[41]</ref>.Finally, please notice that the proposed architecture presents nice features of explainability due to its fully attention-driven design: by simply inspecting the attention coefficients, it is possible to understand the contribution of the single cells for the learning task. For instance, in a computational chemistry task, the upper attention coefficients may represent the importance of the interaction between two atoms inside the rings of a molecule. Also, considering a network traffic problem, the pooling attention coefficients may tell us which links can be considered obsolete, or even detect communities in social networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Cell Complexes</head><p>In this section we recall the basics of regular cell complexes, which are topological spaces provided with a rich algebraic structure that enables an efficient representation of high-order interaction systems, and we explain their relation with usual graphs G = (V, E). In particular, we will first introduce the definition of a regular cell complex, then we will describe few additional properties enabling the representation of cell complexes via boundary operators.</p><p>Definition 1 (Regular cell complex) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42]</ref>. A regular cell complex is a topological space C together with a partition {X ? } ??P C of subspaces X ? of C called cells, where P C is the indexing set of C, such that 1. For each c ? C, every sufficient small neighborhood of c intersects finitely many X ? ; 2. For all ? ,? we have that X ? ? X ? = ? iff X ? ? X ? , where X ? is the closure of the cell;</p><p>3. Every X ? is homeomorphic to R n? for some n ? ; 4. For every ? ? P C there is a homeomorphism ? of a closed ball in R n? to X ? such that the restriction of ? to the interior of the ball is a homeomorphism onto X ? .</p><p>Condition <ref type="bibr" target="#b1">(2)</ref> implies that the indexing set P C has a poset structure, given by ? ? ? iff X ? ? X ? . This is known as the face poset of C. The regularity condition <ref type="bibr" target="#b3">(4)</ref> implies that all topological information about C is encoded in the poset structure of P C . Then, a regular cell complex can be identified with its face poset.</p><p>Definition 2 (k-skeleton). A k-skeleton of a cell complex C, denoted C (k) , is the subcomplex of C consisting of cells of dimension at most k.</p><p>From Definition 1 and Definition 2, it is trivial to check the 0-skeleton of a cell complex is the set of vertices C (0) = V and the 1-skeleton is the underlying graph C (1) = G(V, E); we refer to 2-cells as polygons and, in general, there is little interest with dimensions above two. Regular cell complexes can be described via an incidence relation (boundary relation) with a reflexive and transitive closure that is consistent with the partial order introduced in Definition 1. The boundary relation describes which cells are on the boundary of other cells.</p><p>Definition 3 (Boundary relation).We have the boundary relation ? ? ? iff dim(?) ? dim(? ) and there is no cell ? such that ? ? ? ? ? .</p><p>We can use the previous definitions to define the four types of (local) adjacencies present in cell complexes, following the approach from <ref type="bibr" target="#b33">[34]</ref>:</p><p>Definition 4 (Cell complex adjacencies) <ref type="bibr" target="#b33">[34]</ref>. For a cell complex C and a cell ? ? P C , we define:</p><p>? The boundary adjacent cells B(?) = {? | ? ? ?}, are the lower-dimensional cells on the boundary of ?. For instance, the boundary cells of an edge are its vertices and the boundary cells of a polygon are its edges.</p><p>? The co-boundary adjacent cell CB(?) = {? | ? ? ? }, are the higher-dimensional cells with ? on their boundary. For instance, the co-boundary cells of a vertex are the edges having that vertex as an endpoint and the co-boundary of an edge are the polygons having that edge as one of its sides.</p><p>? The lower adjacent cells N d (?) = {? | ?? such that ? ? ? and ? ? ? }, are the cells of the same dimension as ? that share a lower dimensional cell on their boundary. The line graph adjacencies between the edges are a classic example of this.</p><p>? The upper adjacent cells N u (?) = {? | ?? such that ? ? ? and ? ? ?}. These are the cells of the same dimension as ? that are on the boundary of the same higher-dimensional cell as ?.</p><p>Finally, it is possible to incorporate a graph G in a higher order cell complex C G by attaching polygons to closed paths of edges having no internal chords. . In this figure, it is possible to distinguish the topology of P by the color attached to its elements,i.e., the polygons. Two polygons share the same color if the have the same number of boundary elements, i.e., triangles are orange, squares are green, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Algebraic Representation</head><p>Let us now introduce an algebraic representation of the cell complexes based on the incidence relations among cells. We need first to introduce the orientation of a cell complex by generalizing the concept of orientation of a simplex <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43]</ref>. To define the orientation of a k-cell, we may apply a simplicial decomposition <ref type="bibr" target="#b42">[43]</ref>, which consists in subdividing the cell into a set of internal k-simplices, so that, by orienting a single internal simplex, the orientation propagates to the entire cell. Defining the transposition as the permutation of two elements, two orientations are equivalent if each of them can be recovered from the other through an even number of transpositions. Given an oriented cell complex C, let S k denote the number of cells in dimension k, and ? i and ? j denote two cells of the complex with dimension dim(? j ) = k and dim(? i ) = k ? 1 respectively. The k ? th signed boundary matrix B k ? R S k?1 ?S k of C is:</p><formula xml:id="formula_0">[B k ] i,j = 1, if ? i ? + ? j ?1, if ? i ? ? ? j 0, otherwise<label>(1)</label></formula><p>where we use the notation ? + to indicate coherent orientation between two cells and ? ? to indicate a opposite orientation between two cells.</p><p>As a particular case, let us consider a cell complex of order two C = {V, E, P}, where V, E, P denote the set of 0, 1 and 2-cells, i.e., vertices, edges and polygons, respectively. We denote their cardinality by |V| = V , |E| = E and |P| = P . Then, the two incidence matrices describing the connectivity of the complex are B 1 ? R V ?E and B 2 ? R E?P , where B 2 can be written as in <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_1">B 2 = [B T , B Q , . . . , B P S ]</formula><p>, with B T , B Q and B P S denoting the incidences between edges and, respectively, triangles, quadrilaterals, up to polygons with P S sides (where each polygon does not include any internal chord between any pair of its vertices). An interesting property of the incidence matrices is that B k B k+1 = 0, for all k.</p><p>Finally, the structure of a K-cell complex can be described through the higher-order combinatorial Laplacians defined as <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>:</p><formula xml:id="formula_2">L 0 = B 1 B T 1 , L k = L d k + L u k , for k = 1, . . . , K ? 1, L K = B T K B K ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">L d k = B T k B k and L u k = B k+1 B T k+1</formula><p>are respectively the lower and upper Laplacians, encoding the lower and upper adjacencies of the k-order cells. Note that L 0 corresponds to the combinatorial Laplacian used for graph representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data over Cell Complexes</head><p>Let C k be the set of k-th order cells in a cell complex C. In most of the cases the focus is on complex C (2) of order up to two, thus a set of vertices V with |V| = V , a set of edges E with |E| = E and a set of polygons P with |P| = P are considered, resulting in C 0 = V (cells of order 0), C 1 = E (cells of order 1) and C 2 = P (cells of order 2). In <ref type="figure" target="#fig_0">Fig. 1</ref> we sketch an example of a cell complex of order 2.</p><p>A k-cell signal is defined as a mapping from the set of all k-cells contained in the complex to real numbers:</p><p>x k : C k ? R, k = 0, 1, . . . K.</p><p>(3) The order of the signal is one less the cardinality of the elements of C k . Therefore, for a complex C <ref type="bibr" target="#b1">(2)</ref> , the k-cell signals are defined as the following mappings:</p><formula xml:id="formula_4">x 0 : V ? R, x 1 : E ? R, x 2 : P ? R,<label>(4)</label></formula><p>representing vertex, edge and polygon signals, respectively.</p><p>In this work, we will consider only vertex and edge signals. In particular, we will refer to an instance of the former as x i while the instance of the latter will be referred as x e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cell Attention Networks</head><p>The aim of this work is to extend Graph Attention Networks introduced in [11] to account for multi-way relationships, i.e., performing a masked self-attention mechanisms at the edge level. The proposed hierarchical architecture, which we refer to as Cell Attention Network (CAN) (4), starts with the embedding of the input graphs in regular cell complexes via a skeleton-preserving cellular lifting map and an attentional lift procedure enabling the derivation of edge features from node features. Then, we introduce a novel edge-level attentional message passing scheme. After each round of message passing, we perform a novel edge pooling operation and a local readout to reduce complexity; finally, after the last message-passing round, a global readout is applied. As in the previous sections, we denote the input graph(s) with G = (V, E) and the input node features of node i ? V with x i ? R Fn .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cellular Lifting Map</head><p>We first need to incorporate input graphs in regular cell complexes. To address this challenge, we exploit the notion of skeleton-preserving cellular lifting map presented in <ref type="bibr" target="#b33">[34]</ref> and defined as:</p><p>Definition 5 (Skeleton-Preserving Cellular Lifting Map) <ref type="bibr" target="#b33">[34]</ref>. A cellular lifting map s : G ? C G is a skeleton preserving function that incorporates a graph G into a regular cell complex C G , such that, for any graph G, the 1-skeleton (i.e., the underlying graph) of s(G) and G are isomorphic .</p><p>Informally, Definition 5 just requires that the lifting map keeps the underlying graph structure unchanged. Several cellular lifting map can be exploited, in this work we opted for a lifting map that attach cells to all the induced (or chordless) cycles, where k can be considered a hyperparameter to be chosen arbitrarily, and which controls the maximum size of the polygons (2-cells) of the complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attentional lift</head><p>After the Cellular Lifting Map, we need to learn edge features, thus performing a lift operation on the node features that we refer to as attentional lift. To this aim, we exploit a masked multi-head self-attention mechanism <ref type="bibr" target="#b10">[11]</ref>. The procedure is based on the computation of F 0 attention heads such that, for each pair of nodes i,j ? V connected by an edge e ? E, the corresponding edge features x e ? R F 0 are given by the concatenation of the resulting attention scores.</p><p>Definition 6 (Attentional Lift). An Attentional Lift is a learnable function g : R Fn ? R Fn ? R F 0 , of the form:</p><formula xml:id="formula_5">x e = g(x i , x j ) = F 0 || k=1 a k n (x i , x j ), ?e ? E.<label>(5)</label></formula><p>where a k n : R Fn ? R Fn ? R is the k-th (shared across nodes) learnable attention function, and || is the concatenation operator. Since the order of the nodes connected by an edge should not change the corresponding lifted edge features, we assume that the functions a k n are symmetric. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cell Attention</head><p>In this section we introduce Cellular Attentional Message-passing, an attentional message passing scheme operating at edges level on the learned edges features of Eq. (5) exploiting the connectivity given by the regular cell complex C G computed via the cellular lifting map of Definition 5. Before describing the proposed scheme, please notice that, as previously introduced, we will perform an edge pooling operation after the message-passing round at each layer l ? {1, ..., L}, meaning that the architecture will produce a sequence of cell complexes {C l } l such that C l+1 ? C l (due to the fact that the corresponding edge sets are such that E l+1 ? E l ); we will describe the edge pooling in details in the next section. As already introduced in Section 2, there are two types of adjacencies that can be exploited when dealing with cell complexes. In particular, since the message exchange happen at the edges level, in each layer l, our message-passing scheme exploits upper and lower edge adjacencies N l d (e) and N l u (e), associated with the cell complex C l , l = 1, . . . , L. At each layer l, we introduce a learnable upper attention function a l u : R F l ? R F l ? R, responsible to evaluate the reciprocal importance of two edges that are part of the same polygon, and a lower attention function a l d : R F l ? R F l ? R, responsible to evaluate the reciprocal importance of two edges that share a common node. Therefore, edges embedding are updated in the l ? th message passing round as:</p><formula xml:id="formula_6">h l e = ? l h l e , k?N l d (e) a l d (h l e , h l k ) ? l d (h l k ), k?N l u (e) a l u (h l e , h l k ) ? l u (h l k ) ? R F l+1 , ?e ? E l ,<label>(6)</label></formula><p>where is any permutation invariant (aggregation) operator (e.g., sum, mean, max, ...), ? l is a (possibly) learnable function, ? l u and ? l d are learnable functions sharing the weights with a l u and a l d , respectively (as usual in attentional settings), h 0 e = x e , C 0 = C G (thus E 0 = E). Obviously, multihead attention can be trivially injected following the usual concatenation or averaging approach <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36]</ref>. A pictorial example of how upper and lower attention work is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Edge Pooling</head><p>In this section, we present a self-attention edge pooling technique, adopting a variation of the method used in <ref type="bibr" target="#b45">[46]</ref>. Let h l e ? R F l+1 be the hidden feature vector associated to edge e obtained via attentional message-passing after the l-th message-passing round. The edge attention pooling operation consists in computing a self-attention score ? l e ? R for each edge of the complex via a pooling learnable attention function a l p : R F l+1 ? R :</p><p>? l e = a l p h l e ?e ? E l .</p><p>Let k ? (0, 1] be the pooling ratio, i.e., the fraction of the edges that will be retained over the number of edges in input to the self-attention edge pooling layer. At this point, we keep the k|E l | </p><formula xml:id="formula_8">({? l e } e?E l , k|E l | )} ? E l where top-k({? l e } e?E l , k|E l | )</formula><p>is the set of the highest k|E l | self-attention scores. Finally, the feature vectors that will be kept after the pooling stage are scaled as:</p><formula xml:id="formula_9">h l+1 e = ? l e h l e , ?e ? E l+1 .<label>(8)</label></formula><p>After the edge pooling, we consequently need to adjust the structure of the cell complex C l to obtain a consistent updated complex C l+1 . To this aim, we apply the procedure depicted in <ref type="figure" target="#fig_2">Fig. 3</ref>: If an edge e belongs to E l but is not contained in E l+1 , the lower connectivity is updated by disconnecting the nodes that are on the boundary of e, while the upper connectivity is updated by removing the polygons that have e on their boundaries.</p><p>Finally, we considered also a hierarchical version of the aforementioned self-attention edge pooling operation as in <ref type="bibr" target="#b46">[47]</ref>. To this aim we employ a (by-layer) readout operation on the hidden feature {h l+1 e } e?E l+1 to obtain an aggregate embedding of the whole complex C l+1 as:</p><formula xml:id="formula_10">h C l+1 = e?E l+1 h l+1 e .<label>(9)</label></formula><p>Then, after the last hidden layer, a final (global) readout operation is performed, e.g., by aggregating all the previously computed complexes embeddings:</p><formula xml:id="formula_11">h C = l h C l .<label>(10)</label></formula><p>Finally, the result of the final aggregation is fed to a multi-layer perceptron (MLP) if needed for the learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CAN Architecture and Symmetries</head><p>In summary, a Cell Attention Network with input graph(s) G = (V, E) and input node features {x i } i?V is defined as the stack of: (i) a skeleton-preserving cellular lifting map to obtain a regular cell complex C G from a graph G; (ii) a multi-head attentional lift to obtain edge features {x e } e?E from node features {x i } i?V ; (iii) a stack of L cell attention layers, each of them composed by a message passing round as in Eq. <ref type="formula" target="#formula_6">(6)</ref>, an edge pooling stage as in Eq. <ref type="formula" target="#formula_7">(7)</ref> and Eq. <ref type="formula" target="#formula_9">(8)</ref>, and a (by-layer) readout function as in Eq. (9); (v) a (global) readout function, e.g. as in Eq. <ref type="bibr" target="#b9">(10)</ref>. A schematic view of the whole architecture is illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. Finally, we present the following result about equivariance properties of the proposed architecture: Proof. We can assert, w.l.o.g., that Attentional Lift is permutation equivariant by construction. The operation g(?) and a(?) are both learnable functions acting on edges, and since a(?) is symmetric by  definition, both a(?) and g(?) are symmetric w.r.t. to the vertices that are endpoint of the edges we are considering. This leads to have the whole function permutation equivariant.</p><p>The scheme followed in Edge Pooling is the selecting top ? k element of edge set referring to self-attentional coefficients ? e . In order to select the top ? k elements of E, the vector ? e must be sorted. So no matter what is the permutation on the set, after sorting we obtain always the same result. For this reason Edge Pooling is permutation invariant.</p><p>Finally, since the proposed CAN architecture is the composition of a permutation equivariant function (i.e., the attentional lift) and a permutation invariant function (i.e., the edge pooling), it readily follows that CAN are permutation equivariant. Please notice that the proposed architecture without the pooling stage is clearly permutation equivariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section we asses the performance of the proposed architecture when solving several real-world graph classification problems, focusing on well known molecular benchmarks on TUDataset <ref type="bibr" target="#b47">[48]</ref>. In every experiment, if the dataset is equipped with edge features, we concatenate them to the result of the lift layer (Eq. <ref type="formula" target="#formula_5">(5)</ref>). We included small molecules with class labels such as MUTAG <ref type="bibr" target="#b48">[49]</ref> and PTC <ref type="bibr" target="#b49">[50]</ref>. In the former dataset, the task is to identify mutagenic molecular compounds for potentially commercial drugs, while in the latter the goal is to identify chemical compounds based on their carcinogenicity in rodents. The PROTEINS dataset <ref type="bibr" target="#b50">[51]</ref> is composed mainly by macromolecules. Here, nodes represent secondary structure elements and are annotated by their type. Nodes are connected by an edge if the two nodes are neighbours on the amino acid sequence or one of three nearest neighbors in space; the task is to understand if a protein is an enzyme or not.</p><p>Using these type of data in a Cell Complex based architecture has an underlying importance since molecules have polyadic structures. Finally, NCI1 and NCI109 are two datasets aimed at identifying chemical compounds against the activity of non-small lung cancer and ovarian cancer cells <ref type="bibr" target="#b51">[52]</ref>.</p><p>Considering the aforementioned datasets, we compare CAN with other state of the art techniques in graph representation learning. Since there are no official splits for the training and test sets, to validate the proposed architecture, we followed the method used in <ref type="bibr" target="#b33">[34]</ref>: we run a 10-fold cross-validation reporting the maximum of the average validation accuracy across folds. The performance of CAN is reported in <ref type="table" target="#tab_1">Table 2</ref>, along with those of graph kernel methods: Random Walk Kernel (RWK, <ref type="bibr" target="#b52">[53]</ref>), Graph Kernel (GK, <ref type="bibr" target="#b53">[54]</ref>), Propagation Kernels (PK, <ref type="bibr" target="#b54">[55]</ref>), Weisfeiler-Lehman graph kernels (WLK, <ref type="bibr" target="#b55">[56]</ref>); other GNNs: Diffusion-Convolutional Neural Networks (DCNN, <ref type="bibr" target="#b9">[10]</ref>), Deep Graph Convolutional Neural Network (DGCNN, <ref type="bibr" target="#b56">[57]</ref>), Invariant and Equivariant Graph Networks (IGN, <ref type="bibr" target="#b57">[58]</ref>), Graph Isomorphism Networks (GIN, <ref type="bibr" target="#b32">[33]</ref>), Provably Powerful Graph Networks (PPGNs, <ref type="bibr" target="#b58">[59]</ref>), Natural Graph Networks (NGN, <ref type="bibr" target="#b59">[60]</ref>), Graph Substructure Network (GSN <ref type="bibr" target="#b60">[61]</ref>) and topological networks: Simplicial Isomorphism Network (SIN, <ref type="bibr" target="#b31">[32]</ref>, Cell Isomorphism Network (CIN, <ref type="bibr" target="#b33">[34]</ref>). As we can see from <ref type="table" target="#tab_1">Table 2</ref>, CAN achieves the best performance on four out of five benchmarks, while performing very similarly to CIN in the last experiment (i.e., NCI109). Since CAN has a much lower computational complexity than CIN (cf. Appendix C), these results support the validity and the performance obtained of the proposed architecture. The tested models have been implemented using PyTorch <ref type="bibr" target="#b61">[62]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Discussion</head><p>In this work we presented Cell Attention Networks (CANs), novel neural architectures operating on data defined over the nodes of a graph incorporated into a a regular cell complex, exploiting generalized masked self-attention mechanisms. It builds on skeleton-preserving cellular lifting maps, a novel attentional features lift and a novel edge-level attentional message-passing scheme with two attention functions that operate on the upper and lower connectivities induced by the cell complex. The proposed architecture is also equipped with a novel hierarchical edge pooling technique that leverage a self-attention mechanism to downsample the data in the network's hidden layers while extracting significant features for the learning task. The Cell Attention Network architecture proposed and tested in the previous sections shows promising results and it is grounded in the theory of regular cell complexes; however, some directions can be explored to enrich the proposed formulation. In particular, a signal processing perspective <ref type="bibr" target="#b30">[31]</ref> can be exploited to reinterpret and modify the proposed architecture following a similar approach to <ref type="bibr" target="#b35">[36]</ref>; an expressivity analysis can be carried out based on the renewed Weisfeiler-Lehman approach <ref type="bibr" target="#b32">[33]</ref>, on its generalization to cell complexes <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38]</ref>, or based on spectral approaches <ref type="bibr">[64]</ref>. We leave these problems to be addressed in future works.</p><p>[ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Model Implementation</head><p>In our experiments, we employ cell attention networks to regular cell complexes of order two obtained by applying the structural lifting map to the original graphs, i.e. we consider nodes as 0-cells and edges as 1-cells, and the chordless cycles of size up to R = 6 as 2-cells. In our case, each node of the original graphs is always equipped with an input feature vector.</p><p>Throughout all experiments, we employ cell attention networks with the following structure. The attentional lift mechanism in Eq. (5) is given by:</p><formula xml:id="formula_12">h 0 e = x e = F 0 || k=1 ? n a k n T [x i ||x j ] x e a k n , i, j ? B(e),<label>(11)</label></formula><p>wherex e is the input feature vector of the edge e. If not provided by the specific benchmark,x e can be considered as an empty vector. Also, a k n ? R 2Fn is the vector of attention coefficients associated to the k-th feature of the input edge feature vector, and ? n is the non-linear activation function for the lift layer. Please notice that the employed functions a k n are not symmetric, but they give the best learning performance on the proposed tasks.</p><p>The lower and upper attentional functions a d (h e , h k ) and a u (h e , h k ) in Eq. (6) are chosen as two independent masked self-attention schemes. They can be chosen following any of the known approaches from graphs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">65]</ref>. In this paper we follow the approach from <ref type="bibr" target="#b10">[11]</ref>: formally, let</p><formula xml:id="formula_13">? l,d e,k = ? a (a l d ) T W l d h l e ||W l d h l k (12) ? l,u e,k = ? a (a l u ) T W l u h l e ||W l u h l k ,<label>(13)</label></formula><p>where a l d , a l u ? R F l are two independent vectors of attention coefficients, W l d , W l u ? R F l+1 ?F l are two learnable linear transformations shared by the lower and upper neighbourhoods of the complex, respectively, and ? a is a pointwise non-linear activation. The coefficients ? u and ? d in Eq. <ref type="bibr" target="#b11">(12,</ref><ref type="bibr" target="#b12">13)</ref> represent the importance of the features of edge k when exchanging messages with edge e over lower and upper neighborhoods, respectively. It worth to emphasize that since the attention schemes are decoupled, these importance coefficients will be different over the upper and lower neighborhoods.</p><p>In line with the approach of <ref type="bibr" target="#b10">[11]</ref>, we make coefficients easily comparable across different edge by normalizing them across all choices of k using the softmax function: </p><p>Thus, for layer l we have that a l d (h l e , h l k ) = ? l,d e,k and a l u (h l e , h l k ) = ? l,u e,k . Once the attention coefficients have been normalized, to update the representation of an edge e, a linear combination of the edge features and the normalized attention coefficients corresponding to them is computed for both the lower and upper neighbourhoods and the results are aggregated alongside with the current edge representation. Formally:</p><formula xml:id="formula_15">h l e = ? (1 + ?) W l s h l e + k?N l d (e) ? l,d e,k W l d h l k + k?N l u (e) ? l,u e,k W l u h l k ) ,<label>(16)</label></formula><p>here W l s ? R F l+1 ?F l is a shared linear transformation applied to the current hidden representation of the edges of the complex. Notice that the functions ? d (h l k ) and ? u (h l k ) in Eq. (6) are implemented respectively as: W l d h l k and W l u h l k . In the pooling layer, the hidden feature vectors are updated using Eq. </p><p>where the vector a l p plays the role of a collection of attention coefficient that weight the features of h l e to compute the corresponding score ? l e , which that represents the importance of edge e in the learning task. Following the approach of <ref type="bibr" target="#b45">[46]</ref>, the weight (a l p ) Thl e of the edge e is also forwarded into a non-linear activation function ? p to produce the score ? l e ? R, which is then multiplied toh l e to obtain h l+1 e . Readout operations are performed as follows: If the pooling approach is hierarchical, the readout is performed layer-wise. In particular, we choose the sum as the permutation equivariant aggregation function of Eq. (9 which results in a hierarchical representation of the complex i.e. a collection {h C l } L?1 l=0 of hidden representations. Then, Eq. <ref type="formula" target="#formula_0">(10)</ref> is computed as the sum over the collection defined previously.</p><p>In the case of a global pooling, the readout is computed only in the last layer, which constitute the overall representation of the complex:</p><formula xml:id="formula_17">h C = h C L?1 = e?E L?1 h L?1 e .<label>(18)</label></formula><p>Once h C is obtained, it is forwarded into a 2-Layer MLP with ? as activation function to perform the prediction.</p><p>In all layers, we adopt a Batch Normalization technique [66] and all training operations are performed with the AdamW optimization algorithm [67]. In <ref type="table" target="#tab_4">Table 3</ref> we report the hyper-parameters used in our experiments for each dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Complexity Analysis</head><p>In this section, we review the computational complexity of each operation involved the proposed architecture referring to the model implementation define above:</p><p>Cellular Lifting Map: Although this operation can be precomputed for the entire dataset and the connectivity results stored for a later usage, it worth to elicit its complexity noticing that for some applications the storage of the upper and lower connectivity for the entire dataset might be not possible. We considered Cellular Lifting Maps that assign 2-cells to all the chordless cycles of a graph with a maximum number of nodes in the cycles up to R as maximum cycle size. The chord-less cycles in a graph can thus be enumerated in O((|E| + |V |R) polylog|V |) time [68]. Similar to <ref type="bibr" target="#b33">[34]</ref>, in our experimental setup we have that R can upper bounded by a small constant. Thus, the complexity of this operation can be approximated to be linear in the size of the complex.</p><p>Attentional Lift: The complexity of this operation consists of a multi-head attention message passing scheme over the entire graph <ref type="bibr" target="#b10">[11]</ref>. For a single node pair i, j ? V connected by an edge e ? E, the attentional lift defined in Eq. (5) can be decomposed into F 0 independent self-attention schemes. Each attention scheme requires O(F n ) computations, where F n is the number of input node features. Thus, for the pair i, j, the attentional lift is performed in O(F 0 F n ), where F 0 is a parameter to be chosen as the number of input edge features. Accounting all the edges of the complex yields an amount of O(|E|F 0 F n )) operations to lift the given node features into edge ones.</p><p>Cell Attention Layer: This operation consists in two independent masked self-attention message passing schemes over the upper and lower neighbourhoods of the complex, namely cell attention, an inner linear transformation of the edges' features and an outer point-wise nonlinear activation (Eq. <ref type="formula" target="#formula_6">(6)</ref>). For the lower neighbourhood, a single edge e receives at most 2 messages for each cell in its coboundary, CB(e) (cf <ref type="figure" target="#fig_0">Fig. 2.1 )</ref>; thus, for a single edge the attention over the lower neighbourhood of the complex is O(|CB(e)|), where |CB(e)| is the number of cells that are co-faces of the edge e. Regarding the upper neighbourhood, if R is the maximum ring size we have that a single edge e receives O(R ? |CB(e)|) messages (cf. <ref type="figure" target="#fig_1">Fig. 2.2</ref>. Recalling that R is upper bounded by a small constant <ref type="bibr" target="#b33">[34]</ref>, cell attention is an O(|CB(e)|) operation for both neighbourhoods of an arbitrary edge e i.e. linear in the size of the complex. The inner linear transformation that propagates the information contained in h l e is upper bounded by O(F l ). Extending this to all edges of the complex, we have that the complexity of a cell attention layer can be rewritten as O(|E l | F l ). In the case of a multi-head cell attention, the complexity receives an overhead induced by the number of attention heads involved within the layer, i.e., a multiplication by a factor H c , the number of cell attention heads.</p><p>Attentional Pooling: The operations involved in the pooling layer can be decomposed in: (i) computing the self-attention scores for each edge of the complex (? l e in Eq. <ref type="formula" target="#formula_7">(7)</ref>); (ii) select the highest k |E l | values from a collection of self-attention scores (top-k({? l e } e?E l , k|E l | )); and (iii) adjust the connectivity of the complex (see <ref type="figure" target="#fig_2">Fig. 3</ref>). To compute the computational complexity of this layer it is convenient to see the selection operation as a combination of a sorting algorithm over a collection of self-attention scores and a selection of the first k |E l | elements from the sorted collection. Since the computations involved in (i) and (iii) are linear in the dimension of the complex, the overall complexity of this layer in can be upper bounded by the sorting algorithm, i.e., O(|E l | log(|E l |)).</p><p>In practice, all the computations involved in a cell attention network are local formulations completely disjoint from each other. Thus, using an efficient GPU-based implementation, we can rewrite all the analysis in terms of the longest sequential chain of operations in a concurrent execution over the edges of the input domain, i.e., O(log(|E|)). For an in-depth concurrency analysis we refer readers to <ref type="bibr">[69]</ref>, where the authors report a complete taxonomy of parallelism in GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Learnable Parameters</head><p>The total number of learnable parameters of a CAN can be decomposed into:</p><p>Cellular Lifting Map: Lifting the input graph G to a cell complex C is an operation that assign a cell ? to all the chord-less cycles of G up to a maximum cycle size R. Intuitively this operation does not involve any parameter to be learned during the network's training phase. Thus, the number of learnable parameters for the lift is O(1).</p><p>Attentional Lift: In the context of lifting a pair of graph node features x i , x j ? R Fn to a signal defined over the edge of the complex, x e ? R F 0 , we have to learn a vector of attention coefficients a n ? R 2Fn for each input edge feature. The vector a n has a number of learnable parameters in ? (F n ). Accounting multiple input edge features, the overall number of parameters for the attention lift operation is ?(F 0 F n ), where F 0 is the number of input edge features computed as multiple independent attention heads.</p><p>Cell Attention: In terms of learnable parameters, a single cell attention layer is composed of: two independent vectors of attention coefficients a l d , a l u ? R F l for properly weighting the lower and upper neighbourhoods, respectively. Moreover, the layer is equipped with three linear transformations, W l s , W l d , W l u ? R F l ?F l+1 acting respectively on: h l e , the hidden feature vector of edge e at layer l and the hidden feature vectors h l k in the lower and upper neighbourhoods of the edge e. Thus, the number of learnable parameters of a cell attention layer is O(F l F l+1 ).</p><p>Pooling: For this layer, learnable parameters are employed only in computing the self-attention scores (? l e , Eq. <ref type="formula" target="#formula_7">(7)</ref>). The shared vector of attentional scores' coefficients a l p ? R F l , similarly to the lift layer, is known to have a number of learnable parameters in ? F l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Ablation Study</head><p>In this section we take a detailed look at the performance of each operation involved in cell attention networks by performing different ablation studies and show their individual importance and contributions.</p><p>In particular, we followed the same experimental setup used in section 4 by fixing the hyper-parameters as in <ref type="table" target="#tab_4">Table 3</ref> and removing one-by-one the cell attention network operations: (i) removing the lift refers to assign a feature x e to an edge e using a linear function that takes the feature vectors x i , x j from the vertices i, j ? B(e), i.e. a simple scalar product between x i and x j (x e = x i , x j ); (ii) removing the lower attention can be intended as initializing the lower attention coefficients as: a l d = 1 and left them unmodified during the update step in the training phase; (iii) similarly, for the upper attention we replicated the idea of (ii) but now only the upper attention coefficients are involved, i.e. a l u = 1 and kept fixed for the entire optimization stage; (iv) removing the attention means to remove both the upper and lower attention simultaneously as explained in (ii) and (iii); removing the pooling means to detach the pooling layer from the network and remove eventual intermediate readout computations involved in the hierarchical pooling setup.</p><p>As shown in <ref type="figure" target="#fig_7">Figure 5</ref> and in <ref type="table" target="#tab_5">Table 4</ref>, we observe a decrease in the overall performance when removing parts of the cell attention network architecture as expected. Of particular interest is the ablation study on NCI1, which shows a slightly higher accuracy in every case we kept the attention coefficients fixed and without the pooling but a drastic drop in the performance when the edge features are no longer learned. Moreover we see that there are no evident "patters" inside the ablation study with the except that for NCI109 we observe the same behaviour of NCI1 when removing the lift layer. This ). The ablation study shows the benefits of incorporating all the proposed operations into the message passing procedure when operating on data defined over cell complexes. fact can be explained by noticing that the aforementioned datasets experience, on average, a very similar topology <ref type="table" target="#tab_0">(Table 1)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of a geometric cell complex C = (V, E, P)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of upper and lower attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the proposed edge pooling procedure edges belonging to the set E l+1 = {e : e ? E l and ? l e ? top-k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 1 .</head><label>1</label><figDesc>Cell Attention Networks are permutation invariant.In literature, a GNN is permutation invariant if a permutation of nodes produces the same output without the permutation. More formally, a GNN f (?) taking an input graph G with adjacency matrix A and input node features matrix X = {x i } i?V is (node) permutation invariant if f (PAP T , PX) = f (A, X) for any permutation matrix P. In the same way, Cell Attention Networks are permutation invariant w.r.t. permutations of nodes, edges and polygons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of a cell attention network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 8 )</head><label>8</label><figDesc>by scaling the features h l e with the corresponding score ? l e : ?e ? E l+1 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>TUDataset: Results of the ablation of different CAN features with respect to Table 2 (g.t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Details of the datasets used in our experiments.</figDesc><table><row><cell>Info</cell><cell cols="5">MUTAG PTC PROTEINS NCI1 NCI109</cell></row><row><cell># Graphs</cell><cell>188</cell><cell>336</cell><cell>1113</cell><cell>4110</cell><cell>4127</cell></row><row><cell># Classes</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell># Node Feat.</cell><cell>7</cell><cell>20</cell><cell>3</cell><cell>37</cell><cell>38</cell></row><row><cell># Edge Feat.</cell><cell>4</cell><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Avg. Nodes</cell><cell>17.93</cell><cell>13.97</cell><cell>39.06</cell><cell>29.87</cell><cell>29.68</cell></row><row><cell>Avg. Edges</cell><cell>19.79</cell><cell>14.32</cell><cell>72.82</cell><cell>32.30</cell><cell>32.13</cell></row><row><cell>Avg. 3 Cells.</cell><cell>0.00</cell><cell>0.04</cell><cell>27.40</cell><cell>0.04</cell><cell>0.04</cell></row><row><cell>Avg. 4 Cells.</cell><cell>0.00</cell><cell>0.01</cell><cell>14.08</cell><cell>0.03</cell><cell>0.03</cell></row><row><cell>Avg. 5 Cells.</cell><cell>0.36</cell><cell>0.19</cell><cell>5.68</cell><cell>0.75</cell><cell>0.74</cell></row><row><cell>Avg. 6 Cells.</cell><cell>2.5</cell><cell>1.12</cell><cell>8.72</cell><cell>2.66</cell><cell>2.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on TUDatasets. The first part shows the accuracy of graph kernel methods, while the second assess graph neural networks. Cell attention networks scores top on four out of five experiments.</figDesc><table><row><cell>Dataset</cell><cell>MUTAG</cell><cell>PTC</cell><cell cols="2">PROTEINS NCI1</cell><cell>NCI109</cell></row><row><cell>RWK</cell><cell>79.2?2.1</cell><cell>55.9?0.3</cell><cell>59.6?0.1</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">GK(k=3) 81.4?1.7</cell><cell>55.7?0.5</cell><cell>71.4?0.3</cell><cell>62.5?0.3</cell><cell>62.4?0.3</cell></row><row><cell>PK</cell><cell>76.0?2.7</cell><cell>59.5?2.4</cell><cell>73.7?0.7</cell><cell>82.5?0.5</cell><cell>N/A</cell></row><row><cell>WLK</cell><cell>90.4?5.7</cell><cell>59.9?4.3</cell><cell>75.0?3.1</cell><cell>86.0?1.8</cell><cell>N/A</cell></row><row><cell>DCNN</cell><cell>N/A</cell><cell>N/A</cell><cell>61.3?1.6</cell><cell>56.6?1.0</cell><cell>N/A</cell></row><row><cell cols="2">DGCNN 85.8?1.8</cell><cell>58.6?2.5</cell><cell>75.5?0.9</cell><cell>74.4?0.5</cell><cell>N/A</cell></row><row><cell>IGN</cell><cell>83.9?13.0</cell><cell>58.5?6.9</cell><cell>76.6?5.5</cell><cell>74.3?2.7</cell><cell>72.8?1.5</cell></row><row><cell>GIN</cell><cell>89.4?5.6</cell><cell>64.6?7.0</cell><cell>76.2?2.8</cell><cell>82.7?1.7</cell><cell>N/A</cell></row><row><cell>PPGNs</cell><cell>90.6?8.7</cell><cell>66.2?6.6</cell><cell>77.2?4.7</cell><cell>83.2?1.1</cell><cell>82.2?1.4</cell></row><row><cell>NGN</cell><cell>89.4?1.6</cell><cell>66.8?1.7</cell><cell>71.7?1.0</cell><cell>82.4?1.3</cell><cell>N/A</cell></row><row><cell>GSN</cell><cell>92.2?7.5</cell><cell>68.2?7.2</cell><cell>76.6?5.0</cell><cell>83.5?2.0</cell><cell>N/A</cell></row><row><cell>SIN</cell><cell>N/A</cell><cell>N/A</cell><cell>76.4?3.3</cell><cell>82.7?2.1</cell><cell>N/A</cell></row><row><cell>CIN</cell><cell>92.7?6.1</cell><cell>68.2?7.2</cell><cell>77.0?4.3</cell><cell>83.6?1.4</cell><cell>84.0 ? 1.6</cell></row><row><cell>CAN</cell><cell cols="5">94.1 ? 4.8 72.8 ? 8.3 78.2 ? 2.0 84.5 ? 1.6 83.6 ? 1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The datasets have been taken from the PyTorch Geometric library[63]. The operations involved during cellular lifting maps use the code provided by<ref type="bibr" target="#b33">[34]</ref> under MIT license. PyTorch, NumPy, SciPy and are made available under the BSD license, Matplotlib under the PSF license, graph-tool under the GNU LGPL v3 license. PyTorch Geometric is made available under the MIT license. All the experimental results have been made on NVIDIA? GeForce RTX 3090 GPUs with 10,496 CUDA cores and 24GB GPU memory. The operative system used for the experiment was Ubuntu 22.04 LTS 64-bit. See Appendix A for an extensive description of the tested architectures and an ablation study 1 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>63] M. Fey and J. E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. 9 [64] C.I. Kanatsoulis and A. Ribeiro. Graph neural networks are more powerful than we think, 2022. 10 [65] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021. 13 [66] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448-456.</figDesc><table><row><cell>PMLR, 2015. 14</cell></row><row><cell>[67] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint</cell></row><row><cell>arXiv:1711.05101, 2017. 14</cell></row><row><cell>[68] R. A. Ferreira, R. Grossi, R. Rizzi, G. Sacomoto, and M.F. Sagot. Amortized ?(|v|)-delay</cell></row><row><cell>algorithm for listing chordless cycles in undirected graphs. CoRR, abs/1408.1265, 2014. 15</cell></row><row><cell>[69] M. Besta and T. Hoefler. Parallel and distributed graph neural networks: An in-depth concurrency</cell></row><row><cell>analysis, 2022. 16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameter used for the experiments on TUDatasets.</figDesc><table><row><cell>Parameter</cell><cell>MUTAG</cell><cell>PTC</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>NCI109</cell></row><row><cell>Lift Heads</cell><cell>1</cell><cell>32</cell><cell>256</cell><cell>107</cell><cell>116</cell></row><row><cell>Lift Activation</cell><cell>ReLU</cell><cell>ELU</cell><cell>ELU</cell><cell>ELU</cell><cell>Sigmoid</cell></row><row><cell>Lift Dropout</cell><cell>0.0</cell><cell>0.0</cell><cell>0.05</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Hidden Features</cell><cell>[32, 32]</cell><cell>[32, 8]</cell><cell>[128, 128]</cell><cell cols="2">[32, 16, 64, 8] [64, 8, 8, 32, 8]</cell></row><row><cell>Attention Heads</cell><cell>[1, 1]</cell><cell>[2, 1]</cell><cell>[1, 1]</cell><cell>[3, 5, 4, 5]</cell><cell>[5, 7, 4, 7, 7]</cell></row><row><cell>Attention Aggregation</cell><cell>-</cell><cell>cat</cell><cell>-</cell><cell>cat</cell><cell>cat</cell></row><row><cell>Attention Activation</cell><cell>LReLU</cell><cell>LReLU</cell><cell>Tanh</cell><cell>Tanh</cell><cell>Tanh</cell></row><row><cell>Activation</cell><cell>ELU</cell><cell>ELU</cell><cell>Tanh</cell><cell>ELU</cell><cell>GELU</cell></row><row><cell>MLP Neurons</cell><cell>8</cell><cell>4</cell><cell>128</cell><cell>256</cell><cell>32</cell></row><row><cell>Batch Size</cell><cell>64</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell></row><row><cell>Neg. Slope</cell><cell>0.1</cell><cell>0.1</cell><cell>0.3</cell><cell>0.08</cell><cell>0.07</cell></row><row><cell>Pool Ratio</cell><cell>1.0</cell><cell>0.75</cell><cell>0.6</cell><cell>0.5</cell><cell>0.75</cell></row><row><cell>Pool Type</cell><cell>Hier.</cell><cell>Glob.</cell><cell>Hier.</cell><cell>Glob.</cell><cell>Glob.</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>0.6</cell><cell>0.3</cell><cell>0.15</cell><cell>0.05</cell></row><row><cell>Learning Rate</cell><cell>3e ? 3</cell><cell>1e ? 3</cell><cell>3e ? 3</cell><cell>3e ? 4</cell><cell>3e ? 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Analysis of the impact of the operations involved in cell attention networks. No Upper Attention ?1.1 ? 5.3 ?2.5 ? 5.7 ?3.4 ? 3.5 +1.6 ? 1.8 ?3.3 ? 4.3 No Lower Attention +1.0 ? 4.4 ?9.1 ? 4.1 ?3.5 ? 2.6 +0.9 ? 1.2 ?2.2 ? 4.5 No Lift ?2.7 ? 6.1 ?3.7 ? 5.2 ?9.4 ? 4.3 ?15.9 ? 2.6 ?17.3 ? 2.7</figDesc><table><row><cell>Feature Removed</cell><cell>MUTAG</cell><cell>PTC</cell><cell>PROTEINS</cell><cell>NCI1</cell><cell>NCI109</cell></row><row><cell>No Pooling</cell><cell cols="3">?4.8 ? 6.2 ?2.9 ? 3.9 ?3.0 ? 5.3</cell><cell>+1.4 ? 0.6</cell><cell>?2.3 ? 2.1</cell></row><row><cell>No Attention</cell><cell cols="3">?1.9 ? 4.8 ?8.2 ? 2.9 ?2.9 ? 2.9</cell><cell>+0.7 ? 0.8</cell><cell>?7.5 ? 7.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code implementation for the proposed architecture is available at: https://github.com/ lrnzgiusti/can</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The world wide web conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph neural networks in particle physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Vlimant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21001</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd International Conference on Learning Representations (ICLR)</title>
		<meeting>of the 2nd International Conference on Learning Representations (ICLR)<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th International Conference on Learning Representations (ICLR</title>
		<meeting>of the 5th International Conference on Learning Representations (ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Advancing mathematics by guiding human intuition with ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Toma?ev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tanburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Juh?sz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">600</biblScope>
			<biblScope unit="issue">7887</biblScope>
			<biblScope unit="page" from="70" to="74" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<title level="m">Neural algorithmic reasoning. Patterns</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">100273</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Edgenets: Edge varying graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Isufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Signal transduction in cancer. Cold Spring Harbor perspectives in medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Brugge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6098</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From networks to optimal higher-order models of complex systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Scholtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature physics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="313" to="320" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two&apos;s company, three (or more) is a simplex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghrist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bassett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cliques and cavities in the human connectome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Sizemore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Vettel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Betzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bassett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="145" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">You are allset: A multiset function framework for hypergraph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hypergcn: A new method for training graph convolutional networks on hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nimishakavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/1809.09401</idno>
		<title level="m">Hypergraph neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hyper-sagnn: a self-attention based graph neural network for hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural message passing for multi-relational ordered and recursive hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yadati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3275" to="3289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hypergraph attention networks for multimodal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14569" to="14578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hypergraph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1560" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Topological signal processing over simplicial complexes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barbarossa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sardellitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2992" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Topological signal processing over cell complexes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sardellitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barbarossa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Testa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 55th Asilomar Conference on Signals, Systems, and Computers</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weisfeiler and lehman go topological: Message passing simplicial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>PMLR, 2021. 2, 4</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weisfeiler and lehman go cellular: Cw networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Simplicial attention neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Battiloro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Di</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sardellitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barbarossa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simplicial attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Wei Jin</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2022 (Non-Archival) Workshop on Geometrical and Topological Representation Learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>Submitted (under review</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Higher-order attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hajij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zamzmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Papamarkou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Miolane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzm?n-S?enz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ramamurthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Why topology for machine learning and knowledge extraction? Machine Learning and Knowledge Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Network motifs: Simple building blocks of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Itzkovitzn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Out-of-sample representation learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Albooyeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Toward a spectral theory of cellular sheaves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghrist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. and Comput. Topology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="315" to="358" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Discrete calculus: Applied analysis on graphs for computational science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Polimeni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Harmonische funktionen und randwertaufgaben in einem komplex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eckmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comment. Math. Helv</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="240" to="255" />
			<date type="published" when="1944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hodge Laplacians on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Geometry and Topology in Statistical Inference</title>
		<editor>S. Mukherjee</editor>
		<imprint>
			<publisher>AMS</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">76</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jovanovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01287</idno>
		<title level="m">Towards sparse hierarchical graph classifiers</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Derivation and validation of toxicophores for mutagenicity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kazius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcguire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bursi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="312" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Helma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="108" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning theory and kernel machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09902</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Natural graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3636" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improving graph neural network expressivity via subgraph isomorphism counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouritsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
