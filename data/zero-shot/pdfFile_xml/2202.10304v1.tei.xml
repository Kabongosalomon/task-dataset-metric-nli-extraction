<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyi</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
						</author>
						<title level="a" type="main">Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Scene Text Detection</term>
					<term>Arbitrary Shapes</term>
					<term>Real-Time</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, segmentation-based scene text detection methods have drawn extensive attention in the scene text detection field, because of their superiority in detecting the text instances of arbitrary shapes and extreme aspect ratios, profiting from the pixel-level descriptions. However, the vast majority of the existing segmentation-based approaches are limited to their complex post-processing algorithms and the scale robustness of their segmentation models, where the post-processing algorithms are not only isolated to the model optimization but also time-consuming and the scale robustness is usually strengthened by fusing multi-scale feature maps directly. In this paper, we propose a Differentiable Binarization (DB) module that integrates the binarization process, one of the most important steps in the post-processing procedure, into a segmentation network. Optimized along with the proposed DB module, the segmentation network can produce more accurate results, which enhances the accuracy of text detection with a simple pipeline. Furthermore, an efficient Adaptive Scale Fusion (ASF) module is proposed to improve the scale robustness by fusing features of different scales adaptively. By incorporating the proposed DB and ASF with the segmentation network, our proposed scene text detector consistently achieves state-of-the-art results, in terms of both detection accuracy and speed, on five standard benchmarks. ! ? This work was done when M. Liao and Z. Zou were with the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R EADING text in scene images <ref type="bibr" target="#b36">[36]</ref> is of great importance in both academia and industry due to its abundant real-world applications, including office automation, visual search, geo-location, and blind auxiliary. Scene text detection, which aims to localize the text instances in the images, is an essential component in scene text reading. Although huge progress has been achieved in recent years, scene text detection is still challenging due to the diverse scales, irregular shapes, and extreme aspect ratios of the text instances.</p><p>As a mainstream of scene text detection, segmentationbased scene text detectors usually have advantages in handling text instances of irregular shapes and extreme aspect ratios due to their pixel-level representation and local prediction. However, most of them rely on complex postprocessing algorithms to group the pixels into text regions, resulting in a considerable time cost in the inference period. For instance, PSENet <ref type="bibr" target="#b49">[49]</ref> applied a progressive scale expansion algorithm to integrate multi-scale results and Tian et al. <ref type="bibr" target="#b46">[46]</ref> adopted pixel embedding to group the pixels by calculating the feature distances among pixels. Besides, they mostly boosted the scale robustness of the segmentation network by applying a feature-pyramid <ref type="bibr" target="#b28">[29]</ref> or U-Net <ref type="bibr" target="#b42">[42]</ref> structure to fuse the feature maps of different scales, which   <ref type="bibr" target="#b59">[59]</ref>, in terms of both accuracy (F-measure) and speed. Our method achieves the ideal tradeoff between effectiveness and efficiency. did not explicitly fuse the multi-scale features adaptively for the text instances of different scales.</p><p>A basic post-processing pipeline is described in <ref type="figure" target="#fig_2">Fig. 2</ref> (following the blue arrows): First, the probability map produced from the segmentation network is converted to a binary image by applying a step function with a constant threshold; Then, some heuristic techniques like pixel clustering are used to group pixels into text regions. The abovementioned two processes are standalone, without participating in the training process of the segmentation network, which may cause low detection accuracy. For more effective post-processing procedures while keeping the efficiency, we propose to insert the binarization operation into the arXiv:2202.10304v1 [cs.CV] 21 Feb 2022 segmentation network for joint optimization (following the red arrows in <ref type="figure" target="#fig_2">Fig. 2</ref>). First, a threshold map is predicted adaptively, where the thresholds can be diverse in different regions. This design is inspired by the observation that the boundary regions of the text instances may be with lower confidence scores than the central regions in the segmentation results. Then, we introduce an approximate function for binarization called Differentiable Binarization (DB), which binarizes the segmentation map using the threshold map. In this manner, the segmentation network is jointly optimized with the binarization process, leading to better detection results. Different from the previous methods that directly fused the multi-scale feature maps to improve the scale robustness of the segmentation network, we propose an Adaptive Scale Fusion (ASF) module to adaptively fuse the multi-scale feature maps. ASF integrates a spatial attention module into a stage-wise attention module. The stage-wise attention module learns the weights of the feature maps of different scales and the spatial attention module learns the attention across the spatial dimensions, leading to scale-robust feature fusion.</p><p>This paper is an extension of its conference version DBNet <ref type="bibr" target="#b25">[26]</ref>. It extends the conference version from two aspects. First, it proposes an ASF module to further enhance the scale robustness of the segmentation model, without obvious loss of efficiency. The improvements brought by the proposed ASF are positively related to the scale distribution of the scene text benchmarks. Second, we give a more comprehensive theoretical analysis for the proposed DB module.</p><p>An accurate, robust, and efficient scene text detector, named DBNet++, is created by integrating the proposed DB module and ASF module into a segmentation network. Profiting from the joint optimization with the segmentation network, DB not only improves the quality of the segmentation results but also contributes to a simpler postprocessing algorithm. By applying ASF to the segmentation network, its ability to detect the text instances of diverse scales is distinctly strengthened. The prominent advantages of DBNet++ over the previous state-of-the-art methods are concluded as follows:</p><p>1) Jointly optimized with the proposed DB module, our segmentation network can produce highly robust segmentation results, significantly improving the text detection results. 2) As DB can be removed in the inference period without sacrificing the accuracy, there is no extra memory/time cost for inference. 3) The scale robustness of the segmentation model can be efficiently improved by the proposed ASF module. 4) DBNet++ achieves consistently state-of-the-art accuracy on five scene text detection benchmarks, including horizontal, multi-oriented, and curved text. The rest paper is organized as follows: Sec. 2 reviews the relevant scene text detection methods. We describe DBNet++ in Sec. 3. The experiments are discussed and analyzed in Sec. 4. The conclusions are summarized in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text Detection</head><p>The early scene text detection methods usually detected the individual characters or components and then grouped them into words. Neumann and Matas <ref type="bibr" target="#b41">[41]</ref> proposed to locate characters by classifying Extremal Regions (ERs). They posed the character detection problem as an efficient sequential selection from the set of Extremal Regions. Then, the detected ERs were grouped into words. Jaderberg et al. <ref type="bibr" target="#b20">[21]</ref> first generated word candidates with a proposal generator. Then, the word candidates were filtered by a random forest classifier. Finally, the remaining word candidates were refined with a regression network.</p><p>Recently, deep learning has dominated the scene text detection area. The deep-learning-based scene text detection methods can be roughly classified into three categories according to the granularity of the predicted target: regressionbased methods, part-based methods, and segmentationbased methods.</p><p>Regression-based methods are a series of models which directly regress the bounding boxes of the text instances. TextBoxes <ref type="bibr" target="#b24">[25]</ref> modified the anchors and the scale of the convolutional kernels based on SSD <ref type="bibr" target="#b30">[30]</ref> for text detection. TextBoxes++ <ref type="bibr" target="#b23">[24]</ref> and DMPNet <ref type="bibr" target="#b31">[31]</ref> applied quadrilaterals regression to detect multi-oriented text. SSTD <ref type="bibr" target="#b14">[15]</ref> proposed an attention mechanism to roughly identifies text regions. RRD <ref type="bibr" target="#b26">[27]</ref> decoupled the classification and regression by using rotation-invariant features for classification and rotation-sensitive features for regression, for better effect on multi-oriented and long text instances. EAST <ref type="bibr" target="#b65">[65]</ref> and Deep-Reg <ref type="bibr" target="#b16">[17]</ref> are anchor-free methods, which applied pixel-level regression for multi-oriented text instances. DeRPN <ref type="bibr" target="#b54">[54]</ref> proposed a dimension-decomposition region proposal network to handle the scale problem in scene text detection. Regression-based methods usually enjoy simple postprocessing algorithms (e.g. non-maximum suppression). However, most of them are limited to represent accurate bounding boxes for irregular shapes, such as curved shapes.</p><p>Part-based methods firstly detect the small parts of the text instances and then link/combine them into word/textline bounding boxes. SegLink <ref type="bibr" target="#b43">[43]</ref> regressed the bounding boxes of the text segment and predicted their links, to deal with long text instances. SegLink++ <ref type="bibr" target="#b44">[44]</ref> further proposed an instance-aware component grouping algorithm to separate the close text instances more effectively and improved the linking algorithm to fit the arbitrary-shape text instances. These methods usually are skilled at detecting long text lines. However, the linking algorithms are quite complex with hand-crafted super-parameters, which makes them hard to tune.</p><p>Segmentation-based methods usually combine pixel-level prediction and post-processing algorithms to get the bounding boxes. Zhang et al. <ref type="bibr" target="#b63">[63]</ref> detected multi-oriented text by semantic segmentation and MSER-based algorithms. Text border is used in Xue et al. <ref type="bibr" target="#b56">[56]</ref> to split the text instances. Mask TextSpotter <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b38">[38]</ref> detected arbitrary-shape text instances in an instance segmentation manner based on Mask R-CNN <ref type="bibr" target="#b12">[13]</ref>. PSENet <ref type="bibr" target="#b49">[49]</ref> proposed progressive scale expansion by segmenting the text instances with different scale kernels. Pixel embedding is proposed in Tian et al. <ref type="bibr" target="#b46">[46]</ref> to cluster the pixels from the segmentation results. PSENet <ref type="bibr" target="#b49">[49]</ref> and SAE <ref type="bibr" target="#b46">[46]</ref> proposed new post-processing algorithms for the segmentation results, resulting in lower inference speed. Instead, our method focuses on improving the segmentation results by including the binarization process into the training period, without the loss of the inference speed.</p><p>Fast scene text detection methods focus on both the accuracy and the inference speed. TextBoxes <ref type="bibr" target="#b24">[25]</ref>, TextBoxes++ <ref type="bibr" target="#b23">[24]</ref>, SegLink <ref type="bibr" target="#b43">[43]</ref>, and RRD <ref type="bibr" target="#b26">[27]</ref> achieved fast text detection by following the detection architecture of SSD <ref type="bibr" target="#b30">[30]</ref>. EAST <ref type="bibr" target="#b65">[65]</ref> proposed to use an anchor-free design to achieve a good tradeoff between accuracy and speed. Most of them can not deal with text instances of irregular shapes, such as curved shapes. Compared to the previous fast scene text detectors, our method not only runs faster but also can detect text instances of arbitrary shapes. Recently, PAN <ref type="bibr" target="#b50">[50]</ref> proposed to adopt a low computational-cost segmentation head and learnable post-processing for scene text detection, yielding an efficient and accurate arbitrary-shaped text detector. Our proposed DBNet++ performs more accurately and more efficiently owing to the simple and efficient differentiable binarization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Mechanisms for Image Classification</head><p>Some previous image classification methods use channel attention and spatial attention to enhance the accuracy of image classification. Wang et al. <ref type="bibr" target="#b48">[48]</ref> proposed residual attention network for image classification. It adopted a mixture of channel attention and spatial attention to produce a soft mask for the features. Hu et al. <ref type="bibr" target="#b18">[19]</ref> proposed a "Squeezeand-Excitation" block that recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels. Woo et al. <ref type="bibr" target="#b52">[52]</ref> proposed to adopt a channel attention module and a spatial attention module to refine the features. These methods mainly refine the independent features by various types of attention. Our proposed adaptive scale fusion focuses on fusing the features of different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Scale Feature Fusion and Context Enhancement for Semantic Segmentation</head><p>Context is critical for semantic segmentation methods. Context and scale are two highly related concepts, where the context can help perceive large-scale objects/scenes while the multi-scale fusion strategies can usually provide more context. Thus, multi-scale feature fusion is commonly applied in semantic segmentation methods. <ref type="bibr" target="#b35">[35]</ref> firstly proposed the fully convolutional network to fuse multi-scale features with upsampling layers. U-net <ref type="bibr" target="#b42">[42]</ref> applied the skip connection which directly connected the low-level features and high-level features while inherited the structure from the FCN. PSPNet <ref type="bibr" target="#b64">[64]</ref> and Deeplabv3 <ref type="bibr" target="#b3">[4]</ref> proposed a pyramid pooling module (PPM) and an Atrous Spatial Pyramid Pooling (ASPP) for multi-scale feature fusion respectively. RefineNet <ref type="bibr" target="#b27">[28]</ref>, Deeplabv3+ <ref type="bibr" target="#b4">[5]</ref>, DFN <ref type="bibr" target="#b60">[60]</ref>, and SPGNet <ref type="bibr" target="#b5">[6]</ref> adopted encoder-decoder structure which fuse high-level and low-level features to get more discriminating feature. Compared to these multi-scale feature fusion methods, our proposed ASF learns the weights of multi-scale features along with the attention across both the scale and the spatial dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Scale Feature Fusion FCN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Enhancement with Attention Mechanisms</head><p>Attention mechanisms are popular in the semantic segmentation methods for enhancing the context. ANN <ref type="bibr" target="#b67">[67]</ref>, CCNet <ref type="bibr" target="#b19">[20]</ref>, GCNet <ref type="bibr" target="#b2">[3]</ref>, DANet <ref type="bibr" target="#b10">[11]</ref> and ACFNet <ref type="bibr" target="#b62">[62]</ref> all used selfattention to fuse different features for contextual information. Choi et al. <ref type="bibr" target="#b6">[7]</ref> used height-driven attention to get height-dimensional information from multi-scale feature. Compared to these methods that applied attention mechanisms to the spatial dimensions to enhance the context, our proposed ASF focuses on the attentional fusion of multiscale features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>The architecture of our proposed method is shown in <ref type="figure">Fig. 3</ref>. Firstly, the input image is fed into a feature-pyramid backbone. Secondly, the pyramid features are up-sampled to the same scale and passed to the Adaptive Scale Fusion (ASF) module to produce contextual feature F . Then, feature F is used to predict both the probability map (P ) and the threshold map (T ). After that, the approximate binary map (B) is calculated by P and F . In the training period, the supervision is applied on the probability map, the threshold map, and the approximate binary map, where the probability map and the approximate binary map share the same supervision. In the inference period, the bounding boxes can be obtained easily from the approximate binary map or the probability map by a box formation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adaptive Scale Fusion</head><p>The features of different scales are with different perceptions and receptive fields, thus they focus on describing the text instances of different scales. For example, the shallow or large-size features can perceive details of the small text instances but can not capture a global view of the large text instances while the deep or small-size features are opposite. To take full advantage of the features of different scales, feature-pyramid <ref type="bibr" target="#b28">[29]</ref> or U-Net <ref type="bibr" target="#b42">[42]</ref> structures are commonly adopted in semantic segmentation methods. Different from most of the semantic segmentation methods that fuse the features of different scales by simply cascading or summing up, our proposed Adaptive Scale Fusion is designed to dynamically fuse the features of different scales.</p><p>As shown in <ref type="figure">Fig. 4</ref>, the features of different scales are scaled into the same resolution before being fed into the ASF module. Assuming that the input feature maps consist of N feature maps X ? R N ?C?H?W = {X i } N ?1 i=0 , where N is set to 4. Firstly, we concatenate the scaled input features X and then a 3 ? 3 convolutional layer is followed to obtain an intermediate feature S ? R C?H?W . Secondly, the attention weights A ? R N ?H?W can be calculated by applying a spatial attention module to the feature S. Thirdly, the attention weights A can be split into N parts along the channel dimension and weighted multiply with corresponding scaled feature to get the fused feature F ? R N ?C?H?W . In this way, the scale attention is defined as:</p><formula xml:id="formula_0">S = Conv(concat([X 0 , X 1 , ..., X N ?1 ])) A = Spatial Attention(S) F = concat([E 0 X 0 , E 1 X 1 , ..., E N ?1 X N ?1 ])<label>(1)</label></formula><p>where concat indicates the concatenation operator; Conv represents the 3 ? 3 convolutional operator; Spatial Attention indicates a spatial attention module, which is illustrated in <ref type="figure">Fig. 4</ref>. The spatial attention mechanism in the ASF makes the attention weights more flexible across the spatial dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Binarization</head><p>Standard Binarization Given a probability map P ? R H?W produced by a segmentation network, where H and W indicate the height and width of the map, it is essential to convert it to a binary map B ? R H?W , where pixels with value 1 are considered as valid text areas. Usually, this binarization process can be described as follows:</p><formula xml:id="formula_1">B i,j = 1 if P i,j ? t, 0 otherwise.<label>(2)</label></formula><p>where t is the predefined threshold and (i, j) indicates the coordinate point in the map. Differentiable Binarization The standard binarization described in Eq. 2 is not differentiable. Thus, it can not be optimized along with the segmentation network in the training period. To solve this problem, we propose to perform binarization with an approximate step function:</p><formula xml:id="formula_2">B i,j = 1 1 + e ?k(Pi,j ?Ti,j )<label>(3)</label></formula><p>whereB is the approximate binary map; T is the adaptive threshold map learned from the network; k indicates the amplifying factor. k is set to 50 empirically. This approximate binarization function behaves similar to the standard binarization function (see <ref type="figure">Fig 5a)</ref> but is differentiable thus can be optimized along with the segmentation network in the training period. The differentiable binarization with adaptive thresholds can not only help differentiate text regions from the background, but also separate text instances which are closely jointed. Some examples are illustrated in <ref type="figure" target="#fig_10">Fig.9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis of Differentiable Binarization</head><p>The reasons that DB improves the performance can be explained by the gradients in the backpropagation. Let's take the binary cross-entropy loss as an example. The binary cross-entropy loss can be expressed as:</p><formula xml:id="formula_3">L bce = ? 1 N H i=1 W j=1? i,j log(y i,j ) + (1 ?? i,j )log(1 ? y i,j ) (4)</formula><p>where y i,j ? [0, 1] and? i,j ? {0, 1} indicate the output value with logits and the target value. Thus, in the segmentation task, the loss l + for positive labels and l ? for negative labels are:</p><formula xml:id="formula_4">l + = ? log(y i,j ) l ? = ? log(1 ? y i,j )<label>(5)</label></formula><p>Without Considering Activation Function The differential of the segmentation loss can be calculated with the chain rule:</p><formula xml:id="formula_5">?l + ?y i,j = ?1 y i,j ?l ? ?y i,j = 1 1 ? y i,j<label>(6)</label></formula><formula xml:id="formula_6">Let x i,j = P i,j ? T i,j . The DB function can be expressed as f (x) = 1 1+e ?kx i,j .</formula><p>Similarly, the loss l + b for positive labels and l ? b for negative labels are:</p><formula xml:id="formula_7">l + b = ? log 1 1 + e ?kxi,j l ? b = ? log(1 ? 1 1 + e ?kxi,j )<label>(7)</label></formula><p>The differential of the losses with the DB function are as follows:</p><formula xml:id="formula_8">?l + b ?x i,j = ?ke ?kxi,j 1 + e ?kxi,j ?l ? b ?x i,j = k 1 + e ?kxi,j<label>(8)</label></formula><p>The numerical comparison of the derivatives of the losses in Eq. 6 and Eq. 8 are also shown in <ref type="figure">Fig. 5b</ref> and <ref type="figure">Fig. 5c</ref> respectively, from which we can perceive:</p><p>(1) The magnitude of the differential around the boundary value. For the standard binary cross-entropy loss with logits (top), the magnitudes of ?l + ?yi,j and ?l ? ?yi,j are quite small around the boundary value (0.5) between positive value (&gt; 0.5) and negative value (&lt; 0.5). As a result, the backpropagation or the feedback may not be significant when a predicted value is ambiguous, such as 0.  ?xi,j is determined by the amplifying factor k. Thus, the proposed differentiable binarization tends to work more stable for some extremely small or large values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Considering Activation Function</head><p>In practice, the activation function (Sigmoid) is applied in both formulations, which could bound the derivatives and alleviate the problem of "The least upper bound and the greatest lower bound". Considering the Sigmoid function as follows:</p><formula xml:id="formula_9">y i,j = 1 1 + e ?vi,j x i,j = 1 1 + e ?v db i,j ? T i,j ,<label>(9)</label></formula><p>where ?v i,j and v db i,j indicate the output values before the Sigmoid activations; T i,j indicates the adaptive threshold. In this way, the differential of the losses can be updated by chain rule as follows:</p><formula xml:id="formula_10">?l + ?v i,j = ?e ?vi,j 1 + e ?vi,j ?l ? ?v i,j = 1 1 + e ?vi,j (10) ?l + b ?v db i,j = ?ke ?k( 1 1+e ?v db i,j ?Ti,j )?v db i,j (1 + e ?kv db i,j ) 2 (1 + e ?k( 1 1+e ?v db i,j ?Ti,j ) ) ?l ? b ?v db i,j = k 1 + e ?kv db i,j<label>(11)</label></formula><p>Without loss of generality, Eq. 10 and Eq. 11 can be visualized as <ref type="figure" target="#fig_7">Fig. 6</ref>, by setting k = 50 and T i,j = 0.5. We can perceive from <ref type="figure" target="#fig_7">Fig. 6</ref> that the differentiable binarization enlarges the feedback of backpropagation when the wrongly predicted values are near the boundary value. Thus, the proposed differentiable binarization makes the model focus on optimizing the prediction of ambiguous regions. Besides, the Sigmoid function alleviates the problem of "The least upper bound and the greatest lower bound" and DB further decreases the penalty for extremely small/large values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Adaptive Threshold</head><p>The threshold map in <ref type="figure" target="#fig_1">Fig. 1</ref> is similar to the text border map in <ref type="bibr" target="#b56">[56]</ref> from appearance. However, the motivation and usage of the threshold map are different from the text border map. The threshold map with/without supervision is visualized in <ref type="figure" target="#fig_8">Fig. 7</ref>. The threshold map would highlight the text border region even without supervision for the threshold map. This indicates that the border-like threshold map is beneficial to the final results. Thus, we apply border-like supervision on the threshold map for better guidance. An ablation study about the supervision of the adaptive threshold is discussed in the Experiments section. For the usage, the text border map in <ref type="bibr" target="#b56">[56]</ref> is used to split the text instances while our threshold map is served as thresholds for the binarization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Deformable Convolution</head><p>Deformable convolution <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b66">[66]</ref> can provide a flexible receptive field for the model, which is especially beneficial to the text instances of extreme aspect ratios. Following <ref type="bibr" target="#b66">[66]</ref>, modulated deformable convolutions are applied in all the 3 ? 3 convolutional layers in stages conv3, conv4, and conv5 in the ResNet-18 or ResNet-50 backbone <ref type="bibr" target="#b13">[14]</ref>. The label generation for the probability map is inspired by PSENet <ref type="bibr" target="#b49">[49]</ref>. Given a text image, each polygon of its text regions is described by a set of segments: <ref type="bibr" target="#b11">(12)</ref> n is the number of vertexes, which may be different in different datasets, e.g, 4 for the ICDAR 2015 dataset <ref type="bibr" target="#b21">[22]</ref> and 16 for the CTW1500 dataset <ref type="bibr" target="#b32">[32]</ref>. Then the positive area is generated by shrinking the polygon G to G s using the Vatti clipping algorithm <ref type="bibr" target="#b47">[47]</ref>. The offset D of shrinking is computed from the perimeter L and area A of the original polygon:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Label Generation</head><formula xml:id="formula_11">G = {S k } n k=1</formula><formula xml:id="formula_12">D = A(1 ? r 2 ) L<label>(13)</label></formula><p>where r is the shrink ratio, set to 0.4 empirically. With a similar procedure, we can generate labels for the threshold map. Firstly the text polygon G is dilated with the same offset D to G d . We consider the gap between G s and G d as the border of the text regions, where the label of the threshold map can be generated by computing the distance to the closest segment in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Optimization</head><p>The loss function L can be expressed as a weighted sum of the loss for the probability map L s , the loss for the binary map L b , and the loss for the threshold map L t :</p><formula xml:id="formula_13">L = L s + ? ? L b + ? ? L t<label>(14)</label></formula><p>According to the numeric values of the losses, ? and ? are set to 1.0 and 10 respectively. We apply a binary cross-entropy (BCE) loss for both L s and L b . To overcome the unbalance of the number of positives and negatives, hard negative mining is used in the BCE loss by sampling the hard negatives.</p><formula xml:id="formula_14">L s = L b = i?S l y i log x i + (1 ? y i ) log (1 ? x i )<label>(15)</label></formula><p>S l is the sampled set where the ratio of positives and negatives is 1 : 3. It consists of all the positives and the top-k negatives (sorted by the values of the predicting probability), where k is 3 times the number of positives. L t is computed as the sum of L1 distances between the prediction and label inside the dilated text polygon G d :</p><formula xml:id="formula_15">L t = i?R d |y * i ? x * i |<label>(16)</label></formula><p>where R d is a set of indexes of the pixels inside the dilated polygon G d ; y * is the label for the threshold map.</p><p>In the inference period, we can either use the probability map or the approximate binary map to generate text bounding boxes, which produces almost the same results. For better efficiency, we use the probability map so that the threshold branch can be removed. The box formation process consists of three steps: (1) the probability map/the approximate binary map is firstly binarized with a constant threshold (0.2), to get the binary map; (2)the connected regions (shrunk text regions) are obtained from the binary map; (3) the shrunk regions are dilated with an offset D the Vatti clipping algorithm <ref type="bibr" target="#b47">[47]</ref>. D is calculated as</p><formula xml:id="formula_16">D = A ? r L<label>(17)</label></formula><p>where A is the area of the shrunk polygon; L is the perimeter of the shrunk polygon; r is set to 1.5 empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The scene text datasets used in the experiments are described as follows.</p><p>SynthText <ref type="bibr" target="#b11">[12]</ref> is a synthetic dataset which consists of 800k images. These images are synthesized from 8k background images. This dataset is only used to pre-train our model.</p><p>MLT-2017 dataset 1 is a multi-language dataset. It includes 9 languages representing 6 different scripts. There are 7, 200 training images, 1, 800 validation images, and 9, 000 1. https://rrc.cvc.uab.es/?ch=8 testing images in this dataset. We use both the training set and the validation set in the finetune period.</p><p>MLT-2019 dataset 2 is a multi-language dataset. It is an extension of MLT-2017. It includes 10 languages representing 7 different scripts. The languages include Chinese, Japanese, Korean, English, French, Arabic, Italian, German, Bangla, and Hindi (Devanagari). There are 10, 000 training images, 2, 000 validation images, and 10, 000 testing images in this dataset. We use the training set in the finetune period.</p><p>ICDAR 2015 dataset <ref type="bibr" target="#b21">[22]</ref> consists of 1, 000 training images and 500 testing images, which are captured by Google glasses with a resolution of 720 ? 1280. The text instances are labeled at the word level.</p><p>MSRA-TD500 dataset <ref type="bibr" target="#b59">[59]</ref> is a multi-language dataset that includes English and Chinese. There are 300 training images and 200 testing images. The text instances are labeled at the text-line level. Following the previous methods <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b65">[65]</ref>, we include extra 400 training images from HUST-TR400 <ref type="bibr" target="#b58">[58]</ref>.</p><p>CTW1500 dataset <ref type="bibr" target="#b32">[32]</ref> mainly focuses on curved text. It consists of 1, 000 training images and 500 testing images. The text instances are annotated at the text-line level.</p><p>Total-Text dataset <ref type="bibr" target="#b7">[8]</ref> includes the text of various shapes, including horizontal, multi-oriented, and curved. They are 1, 255 training images and 300 testing images. The text instances are labeled at the word level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>For all the models, we first pre-train them with the Syn-thText dataset for 100k iterations. Then, we finetune the models on the corresponding real-world datasets for 1200 epochs. The training batch size is set to 16. We follow a "poly" learning rate policy where the learning rate at the current iteration equals the initial learning rate multiplying (1 ? iter max iter ) power , where the initial learning rate is set to 0.007 and power is 0.9. We use a weight decay of 0.0001 and a momentum of 0.9. The max iter means the maximum number of iterations, which depends on the maximum epochs.</p><p>The data augmentation for the training data includes: (1) Random rotation with an angle range of (?10 ? , 10 ? ); (2) Random cropping; (3) Random Flipping. All the processed images are re-sized to 640?640 for better training efficiency.</p><p>In the inference period, we keep the aspect ratio of the test images and re-size the input images by setting a suitable height for each dataset. The inference speed is tested with a batch size of 1, with a single GTX 1080Ti GPU. The inference time cost consists of the model forward time cost and the post-processing time cost. The post-processing time cost is about 30% of the inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conduct an ablation study on the MSRA-TD500 dataset and the CTW1500 dataset to show the effectiveness of the modules including differentiable binarization, deformable convolution, and adaptive scale fusion.     CTW1500 dataset by providing more flexible and adaptive attention weights across the spatial dimension.</p><formula xml:id="formula_17">_ _ g i _ _ ii ? ? ? ? ? } $ r o 0 d ? 4 ?? ---1 -,., . ? ? ? ? ? ? ?? I ? ? ??II1\\ $ P "0ATE8 \ 00 ? MAINSPAN ?) 42OOFEET % ??Of0IIC??"'.7650 fT.?? ?"'???????ll?? WIRfS???q?????Z1572 10TIII WIRf USfD... 80.000??? ?? 0,"l =?.500??n? ? ??????????s Sons Co. ????ngNew? r ? ? ?? z ".-</formula><p>Comparisons with PPM and CCA We compare our proposed ASF with a multi-scale feature fusion module (Pyramid Pooling Module, PPM <ref type="bibr" target="#b64">[64]</ref>) and a context enhancement module (Criss-Cross Attention, CCA <ref type="bibr" target="#b19">[20]</ref>). We integrate them into the encoder of the DBNet for a fair comparison. As shown in Tab. 4, the proposed ASF outperforms the PPM and CCA in terms of both the detection accuracy and the inference speed. The experimental results demonstrate that the proposed ASF is more effective than PPM and CCA. Our proposed ASF performs better than PPM and CCA on the text detection task because that ASF is not designed to simply enlarge the receptive field or introduce more context for the segmentation model. It fuses the multi-scale feature maps with stage-wise and spatial-wise attention weights, which can be guided by the scales of the corresponding text regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparisons with Previous Methods</head><p>We compare our proposed method with previous methods on five standard benchmarks, including two benchmarks for curved text, one benchmark for multi-oriented text, and two multi-language benchmarks for long text lines. Some qualitative results are visualized in <ref type="figure" target="#fig_10">Fig. 9</ref>. Curved Text Detection We prove the shape robustness of our method on two curved text benchmarks (Total-Text and CTW1500). As shown in Tab. 5 and Tab. 6, our method achieves state-of-the-art performance both on accuracy and speed. Specifically, "DBNet++ (ResNet-50)" outperforms the previous state-of-the-art method by 1.0% and 1.6% on the Total-Text and the CTW1500 dataset. "DBNet (ResNet-50)" runs faster than all previous methods and the speed can be further improved by using a ResNet-18 backbone, with a small performance drop. Compared to the recent fast text detector <ref type="bibr" target="#b50">[50]</ref>, DBNet++ achieves better accuracy with a comparable inference speed. Multi-Oriented Text Detection The ICDAR 2015 dataset is a multi-oriented text dataset that contains lots of small     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparisons with the Conference Version</head><p>The major extension of this paper over the conference version is the proposed ASF module. Some qualitative results  are shown in <ref type="figure" target="#fig_1">Fig. 10</ref> and more results are shown in the appendix. As shown, DBNet++ performs better in detecting the text instances of diverse scales, especially for the large-scale text instances. In contrast, DBNet may generate inaccurate bounding boxes or discrete bounding boxes for large-scale text instances. This indicates that the proposed ASF module strengthens the scale robustness of the text detection model.</p><p>The quantitative results in the standard scene text benchmarks show that the proposed DBNet++ outperforms the conference version in terms of accuracy with little speed drop. Specifically, The accuracy increases 0.5% (1.3%), 2.9% (1.9%), 0.8% (0.0%), 3.6% (2.3%), and 1.3% (1.0%) in terms of F-measure on the Total-Text dataset, the CTW1500 dataset, the ICDAR 2015 dataset, the MSRA-TD500 dataset, and the MLT-2019 dataset, respectively, with the backbone of ResNet-18 (ResNet-50).</p><p>The performance improvements on the CTW1500 dataset and the MSRA-TD500 dataset are more significant than those on the Total-Text dataset and the ICDAR 2015 dataset. Thus, we visualize the scale distributions of these datasets in <ref type="figure" target="#fig_1">Fig. 11</ref> for further analysis. As shown in <ref type="figure" target="#fig_1">Fig. 11</ref>, the scale distributions of the ICDAR 2015 dataset and the Total-Text dataset are less diverse than those of the MSRA-TD500 dataset and the CTW1500 dataset. As shown in <ref type="figure" target="#fig_1">Fig. 11</ref> (c) and <ref type="figure" target="#fig_1">Fig. 11 (d)</ref>, the performance improvements approximately have a positive correlation with the diversity of the scales, which quantitatively reflects that DBNet++ is superior to DBNet on scale robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Limitation</head><p>One limitation of our method is that it is difficult to deal with cases "text inside text", which means that a text instance is inside another text instance. Although the shrunk text regions are helpful to the cases that the text instance is not in the center region of another text instance, it fails when the text instance is exactly located in the center region of another text instance. This is a common limitation for segmentation-based scene text detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have presented a novel framework for detecting arbitrary-shape scene text, which improves the segmentation-based scene text detection methods from two aspects: (1) A differentiable binarization module is proposed to integrate the binarization process into the training period;</p><p>(2) The proposed ASF module efficiently enhances the scale robustness of the segmentation network. Both two modules significantly improve the text detection accuracy. The experiments have verified that our method (ResNet-50 backbone) consistently outperforms the state-the-the-art methods on five standard scene text benchmarks, in terms of speed and accuracy. In particular, even with a lightweight backbone (ResNet-18), our method can achieve competitive performance on all the testing datasets with real-time inference speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>The comparisons of several recent scene text detection methods on the MSRA-TD500 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>A traditional pipeline (blue flow) and our pipeline (red flow). Dashed arrows are the inference only operators; solid arrows indicate differentiable operators in both training and inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>Architecture of our proposed DBNet++, where the Adaptive Scale Fusion module is shown inFig. 4. Illustration of the Adaptive Scale Fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>8 Fig. 5 :</head><label>85</label><figDesc>The derivative of losses in Eq. Numerical comparisons of different functions and derivatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>4 or 0.6; For the binary cross-entropy with differentiable binarization (bottom), the magnitudes of ?l + b ?xi,j and ?l ? b?xi,j are large around the boundary value (0) between positive value (&gt; 0) and negative value (&lt; 0), where is augmented by the amplifying factor k. Thus, the proposed differentiable binarization helps to produce more distinctive predictions around the boundary value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 2 )</head><label>2</label><figDesc>The least upper bound and the greatest lower bound. For the standard binary cross-entropy loss with logits (top), there is no greatest lower bound for ?l + ?yi,j and no least upper bound for ?l ? ?yi,j ; For the binary cross-entropy with differentiable binarization (bottom), the greatest lower bound of ?l + b ?xi,j and the least upper bound of ?l ? b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>The derivative of losses in Eq. 10 and Eq. 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>The threshold map with/without supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Label generation. The annotation of text polygon is visualized in red lines. The shrunk and dilated polygon are displayed in blue and green lines, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>Some visualization results on text instances of various shapes, including curved text, multi-oriented text, vertical text, and long text lines. For each unit, the top right is the threshold map; the bottom right is the probability map.TABLE1: Detection results with different settings of deformable convolution, differentiable binarization and adaptive scale fusion module. "DConv" indicates deformable convolution. "ASF" indicates adaptive scale fusion module. "P", "R", and "F" indicate precision, recall, and f-measure respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>The scale distributions of the test sets of different datasets. The points in (a) and (b) represent the text instances of various scales. The scales of the bounding boxes are measured by the width and height of their minimum bounding rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The detailed experimental results are shown in Tab. 1, Tab. 2, and Tab. 3.</figDesc><table /><note>2. https://rrc.cvc.uab.es/?ch=15</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Differentiable Binarization In Tab. 1, we can see that our proposed DB improves the performance significantly for both ResNet-18 and ResNet-50 on the two datasets. For the ResNet-18 backbone, DB achieves 3.7% and 4.9% performance gain in terms of F-measure on the MSRA-TD500 dataset and the CTW1500 dataset. For the ResNet-50 backbone, DB brings 3.2% (on the MSRA-TD500 dataset) and 4.6% (on the CTW1500 dataset) improvements. Moreover, since DB can be removed in the inference period, the speed is the same as the one without DB.</figDesc><table><row><cell>Backbone</cell><cell>DConv</cell><cell>DB</cell><cell>ASF</cell><cell>P</cell><cell cols="2">MSRA-TD500 R F</cell><cell>FPS</cell><cell>P</cell><cell cols="2">CTW1500 R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>ResNet-18</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>85.5</cell><cell>70.8</cell><cell>77.4</cell><cell>66</cell><cell>76.3</cell><cell>72.8</cell><cell cols="2">74.5</cell><cell>59</cell></row><row><cell>ResNet-18</cell><cell></cell><cell>?</cell><cell>?</cell><cell>86.8</cell><cell>72.3</cell><cell>78.9</cell><cell>62</cell><cell>80.9</cell><cell>75.4</cell><cell cols="2">78.1</cell><cell>55</cell></row><row><cell>ResNet-18</cell><cell>?</cell><cell></cell><cell>?</cell><cell>87.3</cell><cell>75.8</cell><cell>81.1</cell><cell>66</cell><cell>82.4</cell><cell>76.6</cell><cell cols="2">79.4</cell><cell>59</cell></row><row><cell>ResNet-18</cell><cell>?</cell><cell>?</cell><cell></cell><cell>84.9</cell><cell>78.5</cell><cell>81.6</cell><cell>53</cell><cell>83.5</cell><cell>75.9</cell><cell cols="2">79.5</cell><cell>45</cell></row><row><cell>ResNet-18</cell><cell></cell><cell></cell><cell>?</cell><cell>90.4</cell><cell>76.3</cell><cell>82.8</cell><cell>62</cell><cell>84.8</cell><cell>77.5</cell><cell cols="2">81.0</cell><cell>55</cell></row><row><cell>ResNet-18</cell><cell></cell><cell>?</cell><cell></cell><cell>87.1</cell><cell>79.9</cell><cell>83.3</cell><cell>55</cell><cell>86.4</cell><cell>80.8</cell><cell cols="2">83.5</cell><cell>40</cell></row><row><cell>ResNet-18</cell><cell></cell><cell></cell><cell></cell><cell>87.9</cell><cell>82.5</cell><cell>85.1</cell><cell>55</cell><cell>86.7</cell><cell>81.3</cell><cell cols="2">83.9</cell><cell>40</cell></row><row><cell>ResNet-50</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>84.6</cell><cell>73.5</cell><cell>78.7</cell><cell>40</cell><cell>81.6</cell><cell>72.9</cell><cell cols="2">77.0</cell><cell>27</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>?</cell><cell>?</cell><cell>90.5</cell><cell>77.9</cell><cell>83.7</cell><cell>32</cell><cell>86.2</cell><cell>78.0</cell><cell cols="2">81.9</cell><cell>22</cell></row><row><cell>ResNet-50</cell><cell>?</cell><cell></cell><cell>?</cell><cell>86.6</cell><cell>77.7</cell><cell>81.9</cell><cell>40</cell><cell>84.3</cell><cell>79.1</cell><cell cols="2">81.6</cell><cell>27</cell></row><row><cell>ResNet-50</cell><cell>?</cell><cell>?</cell><cell></cell><cell>84.5</cell><cell>83.2</cell><cell>83.8</cell><cell>32</cell><cell>83.3</cell><cell>79.1</cell><cell cols="2">81.2</cell><cell>24</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell>?</cell><cell>91.5</cell><cell>79.2</cell><cell>84.9</cell><cell>32</cell><cell>86.9</cell><cell>80.2</cell><cell cols="2">83.4</cell><cell>22</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>?</cell><cell></cell><cell>90.7</cell><cell>83.5</cell><cell>86.9</cell><cell>29</cell><cell>89.2</cell><cell>81.4</cell><cell cols="2">85.1</cell><cell>21</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell></cell><cell>91.5</cell><cell>83.3</cell><cell>87.2</cell><cell>29</cell><cell>87.9</cell><cell>82.8</cell><cell cols="2">85.3</cell><cell>21</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">deformable convolution.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Backbone The proposed detector with the ResNet-50 back-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">bone achieves better performance than the ResNet-18 but</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">runs slower. Specifically, The best ResNet-50 model outper-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">forms the best ResNet-18 model by 2.1% (on the MSRA-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">TD500 dataset) and 2.4% (on the CTW1500 dataset), with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">approximate double time cost.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Supervision of Threshold Map Although the threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">maps with/without supervision are similar in appearance,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">the supervision can bring performance gain. As shown in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Tab. 3, the supervision improves 0.7% (ResNet-18) and 2.6%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(ResNet-50) on the MLT-2017 dataset.</cell><cell></cell><cell></cell></row></table><note>Deformable Convolution As shown in Tab. 1, the de- formable convolution can also brings 1.5% ? 5.0% perfor- mance gain since it provides a flexible receptive field for the backbone, with small extra time costs. For the MSRA- TD500 dataset, the deformable convolution increase the F- measure by 1.5% (with ResNet-18) and 5.0% (with ResNet- 50). For the CTW1500 dataset, 3.6% (with ResNet-18) and 4.9% (with ResNet-50) improvements are achieved by theAdaptive Scale Fusion As shown in Tab. 1, the adap- tive scale fusion module improves the F-measure by 2.3% and 1.9% on the MSRA-TD500 dataset and the CTW1500 dataset, respectively. The inference speed decreases slightly. As shown in Tab. 2, the spatial attention in the ASF module brings 0.5% and 1.0% on the MSRA-TD500 dataset and the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Detection results with different settings of ASF. "Spatial" means spatial attention in the adaptive scale fusion module; "Scale" means adaptive scale fusion. Some visualization results of DBNet and DBNet++ on text instances of various shapes, including curved text, vertical text and multi-oriented text. For each unit, the top is the result of DBNet; the bottom is the result of DBNet++. More results are shown in the appendix.</figDesc><table><row><cell>Base Method</cell><cell>Scale</cell><cell>Spatial</cell><cell>P</cell><cell cols="2">MSRA-TD500 R F</cell><cell>FPS</cell><cell>P</cell><cell cols="2">CTW1500 R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>DBNet (ResNet-50)</cell><cell>?</cell><cell>?</cell><cell>91.5</cell><cell>79.2</cell><cell>84.9</cell><cell>32</cell><cell>86.9</cell><cell>80.2</cell><cell cols="2">83.4</cell><cell>22</cell></row><row><cell>DBNet (ResNet-50)</cell><cell></cell><cell>?</cell><cell>92.2</cell><cell>81.8</cell><cell>86.7</cell><cell>30</cell><cell>85.4</cell><cell>83.2</cell><cell cols="2">84.3</cell><cell>21</cell></row><row><cell>DBNet (ResNet-50)</cell><cell></cell><cell></cell><cell>91.5</cell><cell>83.3</cell><cell>87.2</cell><cell>29</cell><cell>87.9</cell><cell>82.8</cell><cell cols="2">85.3</cell><cell>21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="6">Effect of supervising the threshold map on the</cell></row><row><cell cols="6">MLT-2017 dataset. "Thr-Sup" denotes applying supervision</cell></row><row><cell cols="2">on the threshold map.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell>Thr-Sup</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>ResNet-18</cell><cell>?</cell><cell>81.3</cell><cell>63.1</cell><cell>71.0</cell><cell>41</cell></row><row><cell>ResNet-18</cell><cell></cell><cell>81.9</cell><cell>63.8</cell><cell>71.7</cell><cell>41</cell></row><row><cell>ResNet-50</cell><cell>?</cell><cell>81.5</cell><cell>64.6</cell><cell>72.1</cell><cell>19</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>83.1</cell><cell>67.9</cell><cell>74.7</cell><cell>19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>Comparisons with multi-scale feature fusion and context enhancement modules in semantic segmentation methods. "PPM": Pyramid Pooling Module; "CCA": Criss-Cross Attention.</figDesc><table><row><cell>Base Method</cell><cell>Module</cell><cell>P</cell><cell cols="2">MSRA-TD500 R F</cell><cell>FPS</cell><cell>P</cell><cell>R</cell><cell cols="2">CTW1500</cell><cell>F</cell><cell>FPS</cell></row><row><cell>DBNet (ResNet-50)</cell><cell>PPM [64]</cell><cell>91.2</cell><cell>79.7</cell><cell>85.1</cell><cell>23</cell><cell>87.0</cell><cell cols="2">79.9</cell><cell cols="2">83.3</cell><cell>16</cell></row><row><cell>DBNet (ResNet-50)</cell><cell>CCA [20]</cell><cell>92.9</cell><cell>80.9</cell><cell>86.5</cell><cell>22</cell><cell>88.4</cell><cell cols="2">81.5</cell><cell cols="2">84.8</cell><cell>15</cell></row><row><cell>DBNet (ResNet-50)</cell><cell>ASF(ours)</cell><cell>91.5</cell><cell>83.3</cell><cell>87.2</cell><cell>29</cell><cell>87.9</cell><cell cols="2">82.8</cell><cell cols="2">85.3</cell><cell>21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Detection results on the Total-Text dataset. The values in the bracket mean the height of the input images. "*" indicates testing with multiple scales.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>TextSnake [37]</cell><cell>82.7</cell><cell>74.5</cell><cell>78.4</cell><cell>-</cell></row><row><cell>ATRR [51]</cell><cell>80.9</cell><cell>76.2</cell><cell>78.5</cell><cell>-</cell></row><row><cell>Mask TextSpotter [38]</cell><cell>82.5</cell><cell>75.6</cell><cell>78.6</cell><cell>-</cell></row><row><cell>TextField [55]</cell><cell>81.2</cell><cell>79.9</cell><cell>80.6</cell><cell>-</cell></row><row><cell>LOMO [61]*</cell><cell>87.6</cell><cell>79.3</cell><cell>83.3</cell><cell>-</cell></row><row><cell>CRAFT [1]</cell><cell>87.6</cell><cell>79.9</cell><cell>83.6</cell><cell>-</cell></row><row><cell>CSE [34]</cell><cell>81.4</cell><cell>79.1</cell><cell>80.2</cell><cell>-</cell></row><row><cell>PSENet-1s [49]</cell><cell>84.0</cell><cell>78.0</cell><cell>80.9</cell><cell>3.9</cell></row><row><cell>PAN [50]</cell><cell>89.3</cell><cell>81.0</cell><cell>85.0</cell><cell>39.6</cell></row><row><cell>DBNet (ResNet-18) (800) [26]</cell><cell>88.3</cell><cell>77.9</cell><cell>82.8</cell><cell>50</cell></row><row><cell>DBNet (ResNet-50) (800) [26]</cell><cell>87.1</cell><cell>82.5</cell><cell>84.7</cell><cell>32</cell></row><row><cell>DBNet++ (ResNet-18) (800)</cell><cell>87.4</cell><cell>79.6</cell><cell>83.3</cell><cell>48</cell></row><row><cell>DBNet++ (ResNet-50) (800)</cell><cell>88.9</cell><cell>83.2</cell><cell>86.0</cell><cell>28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 :</head><label>6</label><figDesc>Detection results on the CTW1500 dataset. The methods with "*" are collected from<ref type="bibr" target="#b32">[32]</ref>. The values in the bracket mean the height of the input images.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>CTPN*</cell><cell>60.4</cell><cell>53.8</cell><cell>56.9</cell><cell>7.14</cell></row><row><cell>EAST*</cell><cell>78.7</cell><cell>49.1</cell><cell>60.4</cell><cell>21.2</cell></row><row><cell>SegLink*</cell><cell>42.3</cell><cell>40.0</cell><cell>40.8</cell><cell>10.7</cell></row><row><cell>TextSnake [37]</cell><cell>67.9</cell><cell>85.3</cell><cell>75.6</cell><cell>1.1</cell></row><row><cell>TLOC [32]</cell><cell>77.4</cell><cell>69.8</cell><cell>73.4</cell><cell>13.3</cell></row><row><cell>PSENet-1s [49]</cell><cell>84.8</cell><cell>79.7</cell><cell>82.2</cell><cell>3.9</cell></row><row><cell>SAE [46]</cell><cell>82.7</cell><cell>77.8</cell><cell>80.1</cell><cell>3</cell></row><row><cell>PAN [50]</cell><cell>86.4</cell><cell>81.2</cell><cell>83.7</cell><cell>39.8</cell></row><row><cell>DBNet (ResNet-18) (1024) [26]</cell><cell>84.8</cell><cell>77.5</cell><cell>81.0</cell><cell>55</cell></row><row><cell>DBNet (ResNet-50) (1024) [26]</cell><cell>86.9</cell><cell>80.2</cell><cell>83.4</cell><cell>22</cell></row><row><cell>DBNet++ (ResNet-18) (1024)</cell><cell>86.7</cell><cell>81.3</cell><cell>83.9</cell><cell>40</cell></row><row><cell>DBNet++ (ResNet-18) (800)</cell><cell>84.3</cell><cell>81.0</cell><cell>82.6</cell><cell>49</cell></row><row><cell>DBNet++ (ResNet-50) (1024)</cell><cell>88.5</cell><cell>82.0</cell><cell>85.1</cell><cell>21</cell></row><row><cell>DBNet++ (ResNet-50) (800)</cell><cell>87.9</cell><cell>82.8</cell><cell>85.3</cell><cell>26</cell></row><row><cell cols="5">and low-resolution text instances. In Tab. 7, we can see</cell></row><row><cell cols="5">that "DBNet++ (ResNet-50) (1152)" and "DBNet (ResNet-</cell></row><row><cell cols="5">50) (1152)" achieve the state-of-the-art performance on ac-</cell></row><row><cell cols="5">curacy. Compared to EAST [65], "DBNet++ (ResNet-50)</cell></row><row><cell cols="5">(736)" outperforms it by 7.2% on accuracy and runs twice</cell></row><row><cell cols="5">faster. Compared to PAN [50], "DBNet++ (ResNet-18) (736)"</cell></row><row><cell cols="5">performs better in terms of accuracy and inference speed.</cell></row></table><note>Multi-Language Text Detection Our method is robust on multi-language text detection. As shown in Tab. 8 and Tab. 9, "DBNet++ (ResNet-50)" is superior to previous methods on accuracy and speed. For the accuracy, "DBNet++ (ResNet- 50)" surpasses the previous state-of-the-art method by 3.1% and 3.3% on the MSRA-TD500 dataset and the MLT-2019 dataset respectively. For the speed, "DBNet++ (ResNet-18) (736)" is faster than the previous fastest method [50] while</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7 :</head><label>7</label><figDesc>Detection results on the ICDAR 2015 dataset. The values in the bracket mean the height of the input images.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>CTPN [45]</cell><cell>74.2</cell><cell>51.6</cell><cell>60.9</cell><cell>7.1</cell></row><row><cell>EAST [65]</cell><cell>83.6</cell><cell>73.5</cell><cell>78.2</cell><cell>13.2</cell></row><row><cell>SSTD [15]</cell><cell>80.2</cell><cell>73.9</cell><cell>76.9</cell><cell>7.7</cell></row><row><cell>WordSup [18]</cell><cell>79.3</cell><cell>77</cell><cell>78.2</cell><cell>-</cell></row><row><cell>Lyu et al. [39]</cell><cell>94.1</cell><cell>70.7</cell><cell>80.7</cell><cell>3.6</cell></row><row><cell>TextBoxes++ [24]</cell><cell>87.2</cell><cell>76.7</cell><cell>81.7</cell><cell>11.6</cell></row><row><cell>RRD [27]</cell><cell>85.6</cell><cell>79</cell><cell>82.2</cell><cell>6.5</cell></row><row><cell>MCN [33]</cell><cell>72</cell><cell>80</cell><cell>76</cell><cell>-</cell></row><row><cell>TextSnake [37]</cell><cell>84.9</cell><cell>80.4</cell><cell>82.6</cell><cell>1.1</cell></row><row><cell>PSENet-1s [49]</cell><cell>86.9</cell><cell>84.5</cell><cell>85.7</cell><cell>1.6</cell></row><row><cell>SPCNet [53]</cell><cell>88.7</cell><cell>85.8</cell><cell>87.2</cell><cell>-</cell></row><row><cell>LOMO [61]</cell><cell>91.3</cell><cell>83.5</cell><cell>87.2</cell><cell>-</cell></row><row><cell>CDAFT [1]</cell><cell>89.8</cell><cell>84.3</cell><cell>86.9</cell><cell>-</cell></row><row><cell>SAE(720) [46]</cell><cell>85.1</cell><cell>84.5</cell><cell>84.8</cell><cell>3</cell></row><row><cell>SAE(990) [46]</cell><cell>88.3</cell><cell>85.0</cell><cell>86.6</cell><cell>-</cell></row><row><cell>PAN [50]</cell><cell>84.0</cell><cell>81.9</cell><cell>82.9</cell><cell>26.1</cell></row><row><cell>DBNet (ResNet-18) (736) [26]</cell><cell>86.8</cell><cell>78.4</cell><cell>82.3</cell><cell>48</cell></row><row><cell>DBNet (ResNet-50) (1152) [26]</cell><cell>91.8</cell><cell>83.2</cell><cell>87.3</cell><cell>12</cell></row><row><cell>DBNet++ (ResNet-18) (736)</cell><cell>90.1</cell><cell>77.2</cell><cell>83.1</cell><cell>44</cell></row><row><cell>DBNet++ (ResNet-50) (1152)</cell><cell>90.9</cell><cell>83.9</cell><cell>87.3</cell><cell>10</cell></row><row><cell cols="5">achieving better accuracy on the MSRA-TD500 dataset. The</cell></row><row><cell cols="5">speed can be further accelerated to 80 FPS ("DBNet++</cell></row><row><cell cols="4">(ResNet-18) (512)") by decreasing the input size.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 8 :</head><label>8</label><figDesc>Detection results on the MSRA-TD500 dataset. The values in the bracket mean the height of the input images.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>He et al. [16]</cell><cell>71</cell><cell>61</cell><cell>69</cell><cell>-</cell></row><row><cell>DeepReg [17]</cell><cell>77</cell><cell>70</cell><cell>74</cell><cell>1.1</cell></row><row><cell>RRPN [40]</cell><cell>82</cell><cell>68</cell><cell>74</cell><cell>-</cell></row><row><cell>RRD [27]</cell><cell>87</cell><cell>73</cell><cell>79</cell><cell>10</cell></row><row><cell>MCN [33]</cell><cell>88</cell><cell>79</cell><cell>83</cell><cell>-</cell></row><row><cell>PixelLink [10]</cell><cell>83</cell><cell>73.2</cell><cell>77.8</cell><cell>3</cell></row><row><cell>Lyu et al. [39]</cell><cell>87.6</cell><cell>76.2</cell><cell>81.5</cell><cell>5.7</cell></row><row><cell>TextSnake [37]</cell><cell>83.2</cell><cell>73.9</cell><cell>78.3</cell><cell>1.1</cell></row><row><cell>Xue et al. [56]</cell><cell>83.0</cell><cell>77.4</cell><cell>80.1</cell><cell>-</cell></row><row><cell>MSR [57]</cell><cell>87.4</cell><cell>76.7</cell><cell>81.7</cell><cell>-</cell></row><row><cell>CRAFT [1]</cell><cell>88.2</cell><cell>78.2</cell><cell>82.9</cell><cell>8.6</cell></row><row><cell>SAE [46]</cell><cell>84.2</cell><cell>81.7</cell><cell>82.9</cell><cell>-</cell></row><row><cell>PAN [50]</cell><cell>84.4</cell><cell>83.8</cell><cell>84.1</cell><cell>30.2</cell></row><row><cell>DBNet (ResNet-18) (512) [26]</cell><cell>85.7</cell><cell>73.2</cell><cell>79.0</cell><cell>82</cell></row><row><cell>DBNet (ResNet-18) (736) [26]</cell><cell>90.4</cell><cell>76.3</cell><cell>82.8</cell><cell>62</cell></row><row><cell>DBNet (ResNet-50) (736) [26]</cell><cell>91.5</cell><cell>79.2</cell><cell>84.9</cell><cell>32</cell></row><row><cell>DBNet++ (ResNet-18) (512)</cell><cell>89.7</cell><cell>76.5</cell><cell>82.6</cell><cell>80</cell></row><row><cell>DBNet++ (ResNet-18) (736)</cell><cell>87.9</cell><cell>82.5</cell><cell>85.1</cell><cell>55</cell></row><row><cell>DBNet++ (ResNet-50) (736)</cell><cell>91.5</cell><cell>83.3</cell><cell>87.2</cell><cell>29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 9 :</head><label>9</label><figDesc>Detection results on the MLT-2019 dataset. *CRAFTS used character-level annotations and integrated a recognition model.</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>FPS</cell></row><row><cell>PSENet [49]</cell><cell>73.5</cell><cell>59.6</cell><cell>65.8</cell><cell>-</cell></row><row><cell>CRAFTS* [2]</cell><cell>79.5</cell><cell>59.6</cell><cell>68.1</cell><cell>-</cell></row><row><cell>DBNet (ResNet-18) [26]</cell><cell>75.3</cell><cell>60.2</cell><cell>66.9</cell><cell>19</cell></row><row><cell>DBNet (ResNet-50) [26]</cell><cell>78.3</cell><cell>64.0</cell><cell>70.4</cell><cell>10</cell></row><row><cell>DBNet++ (ResNet-18)</cell><cell>77.5</cell><cell>61.0</cell><cell>68.2</cell><cell>18</cell></row><row><cell>DBNet++ (ResNet-50)</cell><cell>78.6</cell><cell>65.4</cell><cell>71.4</cell><cell>10</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Qualitative Comparisons between DBNet and DBNet++</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GroundTruth</head><p>DBNet DBNet++ GroundTruth DBNet</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Character region awareness for text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9365" to="9374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Character region attention for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="504" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision Workshops</title>
		<meeting>Int. Conf. Comput. Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spgnet: Semantic prediction guidance for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5218" to="5228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cars can&apos;t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9373" to="9383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Ch&amp;apos;ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Document Analysis and Recognition</title>
		<meeting>Int. Conf. on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pixellink: Detecting scene text via instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single shot text detector with regional attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3047" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text-attentional convolutional neural network for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2529" to="2541" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wordsup: Exploiting word annotations for character based text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4940" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ICDAR 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Document Analysis and Recognition</title>
		<meeting>Int. Conf. on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time scene text detection with differentiable binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11474" to="11481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Comput</surname></persName>
		</author>
		<title level="m">Vision Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep matching prior network: Toward tighter multi-oriented text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Curved scene text detection via transverse and longitudinal sequence connection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="337" to="345" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning markov clustering networks for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6936" to="6944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards robust curve text detection with conditional spatial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7269" to="7278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: The deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Textsnake: A flexible representation for detecting text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-oriented scene text detection via corner localization and region segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7553" to="7563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3111" to="3122" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Seglink++: Detecting dense and arbitrary-shaped scene text by instanceaware component grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106954</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning shape-aware embedding for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4234" to="4243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A generic solution to polygon clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Vati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="56" to="64" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Shape robust text detection with progressive scale expansion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9336" to="9345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient and accurate arbitrary-shaped text detection with pixel aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8440" to="8449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Arbitrary shape scene text detection with adaptive text region representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scene text detection with supervised pyramid context network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9038" to="9045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Derpn: Taking a further step toward more general object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9046" to="9053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Textfield: Learning a deep direction field for irregular scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5566" to="5579" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Accurate scene text detection through border semantics awareness and bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="355" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">MSR: multi-scale shape regression for scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf. on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="989" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A unified framework for multioriented text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4737" to="4749" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Look more than once: An accurate detector for text of arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Acfnet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6798" to="6807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multioriented text detection with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">EAST: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vision</title>
		<meeting>Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
