<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VQA: Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
						</author>
						<title level="a" type="main">VQA: Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 !</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ?0.25M images, ?0.76M questions, and ?10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We are witnessing a renewed excitement in multi-discipline Artificial Intelligence (AI) research problems. In particular, research in image and video captioning that combines Computer Vision (CV), Natural Language Processing (NLP), and Knowledge Representation &amp; Reasoning (KR) has dramatically increased in the past year <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Part of this excitement stems from a belief that multi-discipline tasks like image captioning are a step towards solving AI. However, the current state of the art demonstrates that a coarse scene-level understanding of an image paired with word n-gram statistics suffices to generate reasonable image captions, which suggests image captioning may not be as "AI-complete" as desired. What makes for a compelling "AI-complete" task? We believe that in order to spawn the next generation of AI algorithms, an ideal task should (i) require multi-modal knowledge beyond a single sub-domain (such as CV) and (ii) have a well-defined quantitative evaluation metric to track progress. For some tasks, such as image captioning, automatic evaluation is still a difficult and open research problem <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In this paper, we introduce the task of free-form and openended Visual Question Answering (VQA). A VQA system takes as input an image and a free-form, open-ended, naturallanguage question about the image and produces a naturallanguage answer as the output. This goal-driven task is applicable to scenarios encountered when visually-impaired users <ref type="bibr" target="#b2">[3]</ref> or intelligence analysts actively elicit visual information. Example questions are shown in <ref type="figure">Fig. 1</ref>. Open-ended questions require a potentially vast set of AI capabilities to answer -fine-grained recognition (e.g., "What kind of cheese is on the pizza?"), object detection (e.g., "How ? * The first three authors contributed equally. What color are her eyes? What is the mustache made of? <ref type="figure">Fig. 1</ref>: Examples of free-form, open-ended questions collected for images via Amazon Mechanical Turk. Note that commonsense knowledge is needed along with a visual understanding of the scene to answer many questions. many bikes are there?"), activity recognition (e.g., "Is this man crying?"), knowledge base reasoning (e.g., "Is this a vegetarian pizza?"), and commonsense reasoning (e.g., "Does this person have 20/20 vision?", "Is this person expecting company?"). VQA <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b2">[3]</ref> is also amenable to automatic quantitative evaluation, making it possible to effectively track progress on this task. While the answer to many questions is simply "yes" or "no", the process for determining a correct answer is typically far from trivial (e.g. in <ref type="figure">Fig. 1</ref>, "Does this person have 20/20 vision?"). Moreover, since questions about images often tend to seek specific information, simple oneto-three word answers are sufficient for many questions. In such scenarios, we can easily evaluate a proposed algorithm by the number of questions it answers correctly. In this paper, we present both an open-ended answering task and a multiplechoice task <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Unlike the open-ended task that requires a free-form response, the multiple-choice task only requires an algorithm to pick from a predefined list of possible answers. We present a large dataset that contains 204,721 images from the MS COCO dataset <ref type="bibr" target="#b31">[32]</ref> and a newly created abstract scene dataset <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b1">[2]</ref> that contains 50,000 scenes. The MS COCO dataset has images depicting diverse and complex scenes that are effective at eliciting compelling and diverse questions. We collected a new dataset of "realistic" abstract scenes to enable research focused only on the high-level reasoning required for VQA by removing the need to parse real images. Three questions were collected for each image or scene. Each question was answered by ten subjects along with their confidence. The dataset contains over 760K questions with around 10M answers. While the use of open-ended questions offers many benefits, it is still useful to understand the types of questions that are being asked and which types various algorithms may be good at answering. To this end, we analyze the types of questions asked and the types of answers provided. Through several visualizations, we demonstrate the astonishing diversity of the questions asked. We also explore how the information content of questions and their answers differs from image captions. For baselines, we offer several approaches that use a combination of both text and state-of-the-art visual features <ref type="bibr" target="#b28">[29]</ref>. As part of the VQA initiative, we will organize an annual challenge and associated workshop to discuss state-of-the-art methods and best practices. VQA poses a rich set of challenges, many of which have been viewed as the holy grail of automatic image understanding and AI in general. However, it includes as building blocks several components that the CV, NLP, and KR <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b3">[4]</ref> communities have made significant progress on during the past few decades. VQA provides an attractive balance between pushing the state of the art, while being accessible enough for the communities to start making progress on the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>VQA Efforts. Several recent papers have begun to study visual question answering <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b2">[3]</ref>. However, unlike our work, these are fairly restricted (sometimes synthetic) settings with small datasets. For instance, <ref type="bibr" target="#b35">[36]</ref> only considers questions whose answers come from a predefined closed world of 16 basic colors or 894 object categories. <ref type="bibr" target="#b18">[19]</ref> also considers questions generated from templates from a fixed vocabulary of objects, attributes, relationships between objects, etc. In contrast, our proposed task involves open-ended, free-form questions and answers provided by humans. Our goal is to increase the diversity of knowledge and kinds of reasoning needed to provide correct answers. Critical to achieving success on this more difficult and unconstrained task, our VQA dataset is two orders of magnitude larger than <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b35">[36]</ref> (&gt;250,000 vs. 2,591 and 1,449 images respectively). The proposed VQA task has connections to other related work: <ref type="bibr" target="#b49">[50]</ref> has studied joint parsing of videos and corresponding text to answer queries on two datasets containing 15 video clips each. <ref type="bibr" target="#b2">[3]</ref> uses crowdsourced workers to answer questions about visual content asked by visually-impaired users. In concurrent work, <ref type="bibr" target="#b36">[37]</ref> proposed combining an LSTM for the question with a CNN for the image to generate an answer. In their model, the LSTM question representation is conditioned on the CNN image features at each time step, and the final LSTM hidden state is used to sequentially decode the answer phrase. In contrast, the model developed in this paper explores "late fusion" -i.e., the LSTM question representation and the CNN image features are computed independently, fused via an element-wise multiplication, and then passed through fullyconnected layers to generate a softmax distribution over output answer classes. <ref type="bibr" target="#b33">[34]</ref> generates abstract scenes to capture visual common sense relevant to answering (purely textual) fill-inthe-blank and visual paraphrasing questions. <ref type="bibr" target="#b46">[47]</ref> and <ref type="bibr" target="#b51">[52]</ref> use visual information to assess the plausibility of common sense assertions. <ref type="bibr" target="#b54">[55]</ref> introduced a dataset of 10k images and prompted captions that describe specific aspects of a scene (e.g., individual objects, what will happen next). Concurrent with our work, <ref type="bibr" target="#b17">[18]</ref> collected questions &amp; answers in Chinese (later translated to English by humans) for COCO images. <ref type="bibr" target="#b43">[44]</ref> automatically generated four types of questions (object, count, color, location) using COCO captions. Text-based Q&amp;A is a well studied problem in the NLP and text processing communities (recent examples being <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b44">[45]</ref>). Other related textual tasks include sentence completion (e.g., <ref type="bibr" target="#b44">[45]</ref> with multiple-choice answers). These approaches provide inspiration for VQA techniques. One key concern in text is the grounding of questions. For instance, <ref type="bibr" target="#b53">[54]</ref> synthesized textual descriptions and QA-pairs grounded in a simulation of actors and objects in a fixed set of locations. VQA is naturally grounded in images -requiring the understanding of both text (questions) and vision (images). Our questions are generated by humans, making the need for commonsense knowledge and complex reasoning more essential. Describing Visual Content. Related to VQA are the tasks of image tagging <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref>, image captioning <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b25">[26]</ref> and video captioning <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b20">[21]</ref>, where words or sentences are generated to describe visual content. While these tasks require both visual and semantic knowledge, captions can often be non-specific (e.g., observed by <ref type="bibr" target="#b52">[53]</ref>). The questions in VQA require detailed specific information about the image for which generic image captions are of little use <ref type="bibr" target="#b2">[3]</ref>. Other Vision+Language Tasks. Several recent papers have explored tasks at the intersection of vision and language that are easier to evaluate than image captioning, such as coreference resolution <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b42">[43]</ref> or generating referring expressions <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b41">[42]</ref> for a particular object in an image that would allow a human to identify which object is being referred to (e.g., "the one in a red shirt", "the dog on the left"). While task-driven and concrete, a limited set of visual concepts (e.g., color, location) tend to be captured by referring expressions. As we demonstrate, a richer variety of visual concepts emerge from visual questions and their answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VQA DATASET COLLECTION</head><p>We now describe the Visual Question Answering (VQA) dataset. We begin by describing the real images and abstract  scenes used to collect the questions. Next, we describe our process of collecting questions and their corresponding answers. Analysis of the questions and answers gathered as well as baselines' &amp; methods' results are provided in following sections. Real Images. We use the 123,287 training and validation images and 81,434 test images from the newly-released Microsoft Common Objects in Context (MS COCO) <ref type="bibr" target="#b31">[32]</ref> dataset. The MS COCO dataset was gathered to find images containing multiple objects and rich contextual information. Given the visual complexity of these images, they are well-suited for our VQA task. The more diverse our collection of images, the more diverse, comprehensive, and interesting the resultant set of questions and their answers. Abstract Scenes. The VQA task with real images requires the use of complex and often noisy visual recognizers. To attract researchers interested in exploring the high-level reasoning required for VQA, but not the low-level vision tasks, we create a new abstract scenes dataset <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref> containing 50K scenes. The dataset contains 20 "paperdoll" human models <ref type="bibr" target="#b1">[2]</ref> spanning genders, races, and ages with 8 different expressions. The limbs are adjustable to allow for continuous pose variations. The clipart may be used to depict both indoor and outdoor scenes. The set contains over 100 objects and 31 animals in various poses. The use of this clipart enables the creation of more realistic scenes (see bottom row of <ref type="figure" target="#fig_1">Fig. 2</ref>) that more closely mirror real images than previous papers <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. See the appendix for the user interface, additional details, and examples. Splits. For real images, we follow the same train/val/test split strategy as the MC COCO dataset <ref type="bibr" target="#b31">[32]</ref> (including testdev, test-standard, test-challenge, test-reserve). For the VQA challenge (see section 6), test-dev is used for debugging and validation experiments and allows for unlimited submission to the evaluation server. Test-standard is the 'default' test data for the VQA competition. When comparing to the state of the art (e.g., in papers), results should be reported on test-standard. Test-standard is also used to maintain a public leaderboard that is updated upon submission. Test-reserve is used to protect against possible overfitting. If there are substantial differences between a method's scores on test-standard and test-reserve, this raises a red-flag and prompts further investigation. Results on test-reserve are not publicly revealed. Finally, test-challenge is used to determine the winners of the challenge. For abstract scenes, we created splits for standardization, separating the scenes into 20K/10K/20K for train/val/test splits, respectively. There are no subsplits (test-dev, test-standard, test-challenge, test-reserve) for abstract scenes. Captions. The MS COCO dataset <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b6">[7]</ref> already contains five single-sentence captions for all images. We also collected five single-captions for all abstract scenes using the same user interface 1 for collection. Questions. Collecting interesting, diverse, and well-posed questions is a significant challenge. Many simple questions may only require low-level computer vision knowledge, such as "What color is the cat?" or "How many chairs are present in the scene?". However, we also want questions that require commonsense knowledge about the scene, such as "What sound does the pictured animal make?". Importantly, questions should also require the image to correctly answer and not be answerable using just commonsense information, e.g., in <ref type="figure">Fig. 1</ref>, "What is the mustache made of?". By having a wide variety of question types and difficulty, we may be able to measure the continual progress of both visual understanding and commonsense reasoning. We tested and evaluated a number of user interfaces for collecting such "interesting" questions. Specifically, we ran pilot studies asking human subjects to ask questions about a given image that they believe a "toddler", "alien", or "smart robot" would have trouble answering. We found the "smart robot" interface to elicit the most interesting and diverse questions. As shown in the appendix, our final interface stated:</p><p>"We have built a smart robot. It understands a lot about images. It can recognize and name all the objects, it knows where the objects are, it can recognize the scene (e.g., kitchen, beach), people's expressions and poses, and properties of objects (e.g., color of objects, their texture). Your task is to stump this smart robot! Ask a question about this scene that this smart robot probably can not answer, but any human can easily answer while looking at the scene in the image."</p><p>To bias against generic image-independent questions, subjects were instructed to ask questions that require the image to answer. The same user interface was used for both the real images and abstract scenes. In total, three questions from unique workers were gathered for each image/scene. When writing a question, the subjects were shown the previous questions already asked for that image to increase the question diversity. In total, the dataset contains over ?0.76M questions. Answers. Open-ended questions result in a diverse set of possible answers. For many questions, a simple "yes" or "no" response is sufficient. However, other questions may require a short phrase. Multiple different answers may also be correct. For instance, the answers "white", "tan", or "off-white" may all be correct answers to the same question. Human subjects may also disagree on the "correct" answer, e.g., some saying "yes" while others say "no". To handle these discrepancies, we gather 10 answers for each question from unique workers, while also ensuring that the worker answering a question did not ask it. We ask the subjects to provide answers that are "a brief phrase and not a complete sentence. Respond matter-offactly and avoid using conversational language or inserting your opinion." In addition to answering the questions, the subjects were asked "Do you think you were able to answer the question correctly?" and given the choices of "no", "maybe", and "yes". See the appendix for more details about the user interface to collect answers. See Section 4 for an analysis of the answers provided. For testing, we offer two modalities for answering the ques- For the open-ended task, the generated answers are evaluated using the following accuracy metric: accuracy = min( # humans that provided that answer 3 , 1)</p><p>i.e., an answer is deemed 100% accurate if at least 3 workers provided that exact answer. <ref type="bibr" target="#b1">2</ref> Before comparison, all responses are made lowercase, numbers converted to digits, and punctuation &amp; articles removed. We avoid using soft metrics such as Word2Vec <ref type="bibr" target="#b38">[39]</ref>, since they often group together words that we wish to distinguish, such as "left" and "right". We also avoid using evaluation metrics from machine translation such as BLEU and ROUGE because such metrics are typically applicable and reliable for sentences containing multiple words. In VQA, most answers (89.32%) are single word; thus there no high-order n-gram matches between predicted answers and ground-truth answers, and low-order n-gram matches degenerate to exact-string matching. Moreover, these automatic metrics such as BLEU and ROUGE have been found to poorly correlate with human judgement for tasks such as image caption evaluation <ref type="bibr" target="#b5">[6]</ref>. For multiple-choice task, 18 candidate answers are created for each question.</p><p>As with the open-ended task, the accuracy of a chosen option is computed based on the number of human subjects who provided that answer (divided by 3 and clipped at 1). We generate a candidate set of correct and incorrect answers from four sets of answers: Correct: The most common (out of ten) correct answer. Plausible: To generate incorrect, but still plausible answers we ask three subjects to answer the questions without seeing the image. See the appendix for more details about the user interface to collect these answers. If three unique answers are not found, we gather additional answers from nearest neighbor questions using a bag-of-words model. The use of these answers helps ensure the image, and not just commonsense knowledge, is necessary to answer the question. Popular: These are the 10 most popular answers. For instance, these are "yes", "no", "2", "1", "white", "3", "red", "blue", "4", "green" for real images. The inclusion of the most popular answers makes it more difficult for algorithms to infer the type of question from the set of answers provided, i.e., learning that it is a "yes or no" question just because "yes" and "no" are present in the answers. Random: Correct answers from random questions in the dataset. To generate a total of 18 candidate answers, we first find the union of the correct, plausible, and popular answers. We include random answers until 18 unique answers are found. The order of the answers is randomized. Example multiple choice questions are in the appendix. Note that all 18 candidate answers are unique. But since 10 different subjects answered every question, it is possible that more than one of those 10 answers be present in the 18 choices. In such cases, according to the accuracy metric, multiple options could have a non-zero accuracy.</p><p>Real Images Abstract Scenes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VQA DATASET ANALYSIS</head><p>In this section, we provide an analysis of the questions and answers in the VQA train dataset. To gain an understanding of the types of questions asked and answers provided, we visualize the distribution of question types and answers. We also explore how often the questions may be answered without the image using just commonsense information. Finally, we analyze whether the information contained in an image caption is sufficient to answer the questions. The dataset includes 614,163 questions and 7,984,119 answers (including answers provided by workers with and without looking at the image) for 204,721 images from the MS COCO dataset <ref type="bibr" target="#b31">[32]</ref> and 150,000 questions with 1,950,000 answers for 50, 000 abstract scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Questions</head><p>Types of Question. Given the structure of questions generated in the English language, we can cluster questions into different types based on the words that start the question. <ref type="figure" target="#fig_3">Fig. 3</ref> shows the distribution of questions based on the first four words of the questions for both the real images (left) and abstract scenes (right). Interestingly, the distribution of questions is quite similar for both real images and abstract scenes. This helps demonstrate that the type of questions elicited by the abstract scenes is similar to those elicited by the real images. There exists a surprising variety of question types, including "What is. . .", "Is there. . .", "How many. . .", and "Does the. . .". Quantitatively, the percentage of questions for different types is shown in <ref type="table" target="#tab_7">Table 3</ref>. Several example questions and answers are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. A particularly interesting type of question is the "What is. . ." questions, since they have a diverse set <ref type="bibr" target="#b1">2</ref>. In order to be consistent with 'human accuracies' reported in Section 4, machine accuracies are averaged over all <ref type="bibr">10 9</ref> sets of human annotators of possible answers. See the appendix for visualizations for "What is. . ." questions.</p><p>Lengths. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the distribution of question lengths. We see that most questions range from four to ten words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Answers</head><p>Typical Answers. <ref type="figure" target="#fig_5">Fig. 5</ref> (top) shows the distribution of answers for several question types. We can see that a number of question types, such as "Is the. . . ", "Are. . . ", and "Does. . . " are typically answered using "yes" and "no" as answers. Other questions such as "What is. . . " and "What type. . . " have a rich diversity of responses. Other question types such as "What color. . . " or "Which. . . " have more specialized responses, such as colors, or "left" and "right". See the appendix for a list of the most popular answers.</p><p>Lengths. Most answers consist of a single word, with the distribution of answers containing one, two, or three words, respectively being 89.32%, 6.91%, and 2.74% for real images and 90.51%, 5.89%, and 2.49% for abstract scenes. The brevity of answers is not surprising, since the questions tend to elicit specific information from the images. This is in contrast</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answers with Images</head><p>Answers without Images with image captions that generically describe the entire image and hence tend to be longer. The brevity of our answers makes automatic evaluation feasible. While it may be tempting to believe the brevity of the answers makes the problem easier, recall that they are human-provided open-ended answers to open-ended questions. The questions typically require complex reasoning to arrive at these deceptively simple answers (see <ref type="figure" target="#fig_1">Fig. 2</ref>). There are currently 23,234 unique one-word answers in our dataset for real images and 3,770 for abstract scenes. 'Yes/No' and 'Number' Answers. Many questions are answered using either "yes" or "no" (or sometimes "maybe") -38.37% and 40.66% of the questions on real images and abstract scenes respectively. Among these 'yes/no' questions, there is a bias towards "yes" -58.83% and 55.86% of 'yes/no' answers are "yes" for real images and abstract scenes. Question types such as "How many. . . " are answered using numbers -12.31% and 14.48% of the questions on real images and abstract scenes are 'number' questions. "2" is the most popular answer among the 'number' questions, making up 26.04% of the 'number' answers for real images and 39.85% for abstract scenes. Subject Confidence. When the subjects answered the questions, we asked "Do you think you were able to answer the question correctly?". <ref type="figure" target="#fig_6">Fig. 6</ref> shows the distribution of responses. A majority of the answers were labeled as confident for both real images and abstract scenes.</p><p>Inter-human Agreement. Does the self-judgment of confidence correspond to the answer agreement between subjects? <ref type="figure" target="#fig_6">Fig. 6</ref> shows the percentage of questions in which (i) 7 or more, (ii) 3?7, or (iii) less than 3 subjects agree on the answers given their average confidence score (0 = not confident, 1 = confident). As expected, the agreement between subjects 7 or more same 3-7 same less than 3 same # of Questions increases with confidence. However, even if all of the subjects are confident the answers may still vary. This is not surprising since some answers may vary, yet have very similar meaning, such as "happy" and "joyful". As shown in <ref type="table" target="#tab_1">Table 1</ref> (Question + Image), there is significant inter-human agreement in the answers for both real images (83.30%) and abstract scenes (87.49%). Note that on average each question has 2.70 unique answers for real images and 2.39 for abstract scenes. The agreement is significantly higher (&gt; 95%) for "yes/no" questions and lower for other questions (&lt; 76%), possibly due to the fact that we perform exact string matching and do not account for synonyms, plurality, etc. Note that the automatic determination of synonyms is a difficult problem, since the level of answer granularity can vary across questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Commonsense Knowledge</head><p>Is the Image Necessary? Clearly, some questions can sometimes be answered correctly using commonsense knowledge alone without the need for an image, e.g., "What is the color of the fire hydrant?". We explore this issue by asking three subjects to answer the questions without seeing the image (see the examples in blue in <ref type="figure" target="#fig_1">Fig. 2</ref>). In <ref type="table" target="#tab_1">Table 1</ref> (Question), we show the percentage of questions for which the correct answer is provided over all questions, "yes/no" questions, and the other questions that are not "yes/no". For "yes/no" questions, the human subjects respond better than chance. For other questions, humans are only correct about 21% of the time. This demonstrates that understanding the visual information is critical to VQA and that commonsense information alone is not sufficient. To show the qualitative difference in answers provided with and without images, we show the distribution of answers for various question types in <ref type="figure" target="#fig_5">Fig. 5</ref> (bottom). The distribution of colors, numbers, and even "yes/no" responses is surprisingly different for answers with and without images. Which Questions Require Common Sense? In order to identify questions that require commonsense reasoning to answer, we conducted two AMT studies (on a subset 10K questions from the real images of VQA trainval) asking subjects -1) Whether or not they believed a question required commonsense to answer the question, and 2) The youngest age group that they believe a person must be in order to be able to correctly answer the question -toddler (3-4), younger child <ref type="formula">(5-8)</ref>, older child (9-12), teenager <ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref><ref type="bibr" target="#b15">(16)</ref><ref type="bibr" target="#b16">(17)</ref>, adult (18+). Each question was shown to 10 subjects. We found that for 47.43% of questions 3 or more subjects voted 'yes' to commonsense, (18.14%: 6 or more). In the 'perceived human age required to answer question' study, we found the following distribution of responses: toddler: 15.3%, younger child: 39.7%, older child: 28.4%, teenager: 11.2%, adult: 5.5%. In <ref type="figure">Figure 7</ref> we show several questions for which a majority of subjects picked the specified age range. Surprisingly the perceived age needed to answer the questions is fairly well distributed across the different age ranges. As expected the questions that were judged answerable by an adult (18+) generally need specialized knowledge, whereas those answerable by a toddler (3-4) are more generic. We measure the degree of commonsense required to answer a question as the percentage of subjects (out of 10) who voted "yes" in our "whether or not a question requires commonsense" study. A fine-grained breakdown of average age and average degree of common sense (on a scale of 0 ? 100) required to answer a question is shown in <ref type="table" target="#tab_7">Table 3</ref>. The average age and the average degree of commonsense across all questions is 8.92 and 31.01% respectively. It is important to distinguish between: 1) How old someone needs to be to be able to answer a question correctly, and 2) How old people think someone needs to be to be able to answer a question correctly. Our age annotations capture the latter -perceptions of MTurk workers in an uncontrolled environment. As such, the relative ordering of question types in <ref type="table" target="#tab_7">Table 3</ref> is more important than absolute age numbers. The two rankings of questions in terms of common sense required according to the two studies were largely correlated (Pearson's rank correlation: 0.58).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Captions vs. Questions</head><p>Do generic image captions provide enough information to answer the questions? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VQA BASELINES AND METHODS</head><p>In this section, we explore the difficulty of the VQA dataset for the MS COCO images using several baselines and novel methods. We train on VQA train+val. Unless stated otherwise, all human accuracies are on test-standard, machine accuracies are on test-dev, and results involving human captions (in gray font) are trained on train and tested on val (because captions are not available for test).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We implemented the following baselines: 1) random: We randomly choose an answer from the top 1K answers of the VQA train/val dataset. 2) prior ("yes"): We always select the most popular answer ("yes") for both the open-ended and multiple-choice tasks. Note that "yes" is always one of the choices for the multiple-choice questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) per Q-type prior:</head><p>For the open-ended task, we pick the most popular answer per question type (see the appendix for details). For the multiple-choice task, we pick the answer (from the provided choices) that is most similar to the picked answer for the open-ended task using cosine similarity in Word2Vec <ref type="bibr" target="#b38">[39]</ref> feature space. 4) nearest neighbor: Given a test image, question pair, we first find the K nearest neighbor questions and associated images from the training set. See appendix for details on how neighbors are found. Next, for the open-ended task, we pick the most frequent ground truth answer from this set of nearest neighbor question, image pairs. Similar to the "per Q-type prior" baseline, for the multiple-choice task, we pick the answer (from the provided choices) that is most similar to the picked answer for the open-ended task using cosine similarity in Word2Vec <ref type="bibr" target="#b38">[39]</ref> feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Methods</head><p>For our methods, we develop a 2-channel vision (image) + language (question) model that culminates with a softmax over K possible outputs. We choose the top K = 1000 most frequent answers as possible outputs. This set of answers covers 82.67% of the train+val answers. We describe the different components of our model below: Image Channel: This channel provides an embedding for the image. We experiment with two embeddings -1) I: The activations from the last hidden layer of VG-GNet <ref type="bibr" target="#b47">[48]</ref> are used as 4096-dim image embedding. 2) norm I: These are 2 normalized activations from the last hidden layer of VGGNet <ref type="bibr" target="#b47">[48]</ref>. Question Channel: This channel provides an embedding for the question. We experiment with three embeddings -1) Bag-of-Words Question (BoW Q): The top 1,000 words in the questions are used to create a bag-of-words representation. Since there is a strong correlation between the words that start a question and the answer (see <ref type="figure" target="#fig_5">Fig. 5</ref>), we find the top 10 first, second, and third words of the questions and create a 30 dimensional bag-of-words representation. These features are concatenated to get a 1,030-dim embedding for the question. 1) For BoW Q + I method, we simply concatenate the BoW Q and I embeddings. 2) For LSTM Q + I, and deeper LSTM Q + norm I <ref type="figure">(Fig. 8)</ref> methods, the image embedding is first transformed to 1024-dim by a fully-connected layer + tanh non-linearity to match the LSTM embedding of the question. The transformed image and LSTM embeddings (being in a common space) are then fused via element-wise multiplication. This combined image + question embedding is then passed to an MLP -a fully connected neural network classifier with 2 hidden layers and 1000 hidden units (dropout 0.5) in each layer with tanh non-linearity, followed by a softmax layer to obtain a distribution over K answers. The entire model is learned end-to-end with a cross-entropy loss. VGGNet parameters are frozen to those learned for ImageNet classification and not fine-tuned in the image channel. We also experimented with providing captions as input to our model. Similar to <ref type="table" target="#tab_1">Table 1</ref>, we assume that a human-generated caption is given as input. We use a bag-of-words representation containing the 1,000 most popular words in the captions as the caption embedding (Caption  possible K answers and multiple-choice picks the answer that has the highest activation from the potential answers. Our quantitative results and analyses suggest that this might be due to the language-model exploiting subtle statistical priors about the question types (e.g. "What color is the banana?" can be answered with "yellow" without looking at the image). For a detailed discussion of the subtle biases in the questions, please see <ref type="bibr" target="#b55">[56]</ref>. The accuracy of our best model (deeper LSTM Q + norm I <ref type="figure">(Fig. 8)</ref>, selected using VQA test-dev accuracies) on VQA teststandard is <ref type="bibr" target="#b57">58</ref>.16% (open-ended) / 63.09% (multiple-choice).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We can see that our model is able to significantly outperform both the vision-alone and language-alone baselines. As a general trend, results on multiple-choice are better than open-ended. All methods are significantly worse than human performance.</p><p>Our VQA demo is available on CloudCV [1] -http://cloudcv. org/vqa. This will be updated with newer models as we develop them.</p><p>To gain further insights into these results, we computed accuracies by question type in <ref type="table" target="#tab_7">Table 3</ref>. Interestingly, for question types that require more reasoning, such as "Is the" or "How many", the scene-level image features do not provide any additional information. However, for questions that can be answered using scene-level information, such as "What sport," we do see an improvement. Similarly, for questions whose answer may be contained in a generic caption we see improvement, such as "What animal". For all question types, the results are worse than human accuracies. We also analyzed the accuracies of our best model (deeper LSTM Q + norm I) on a subset of questions with certain specific (ground truth) answers. In <ref type="figure">Fig. 9</ref>, we show the average accuracy of the model on questions with 50 most frequent ground truth answers on the VQA validation set (plot is sorted by accuracy, not frequency). We can see that the model performs well for answers that are common visual objects such as "wii", "tennis", "bathroom" while the performance is somewhat underwhelming for counts (e.g., "2", "1", "3"), and particularly poor for higher counts (e.g., "5", "6", "10", "8", "7"). In <ref type="figure">Fig. 10</ref>, we show the distribution of 50 most frequently predicted answers when the system is correct on the VQA validation set (plot is sorted by prediction frequency, not accuracy). In this analysis, "system is correct" implies that it has VQA accuracy 1.0 (see section 3 for accuracy metric).</p><p>We can see that the frequent ground truth answers (e.g., "yes", "no", "2", "white", "red", "blue", "1", "green") are more frequently predicted than others when the model is correct.  the age and commonsense perceived by MTurk workers that would be required to answer the question. See the appendix for details. We further analyzed the performance of the model for different age groups on the validation questions for which we have age annotations. In <ref type="figure" target="#fig_9">Fig. 11</ref>, we computed the average accuracy of the predictions made by the model for questions belonging to different age groups. Perhaps as expected, the accuracy of the model decreases as the age of the question increases (from 61.07% at 3 ? 4 age group to 47.83% at 18+ age group).</p><p>In <ref type="figure" target="#fig_1">Fig. 12</ref>, we show the distribution of age of questions for different levels of accuracies achieved by our system on the validation questions for which we have age annotations. It is interesting to see that the relative proportions of different age groups is consistent across all accuracy bins with questions belonging to the age group 5-8 comprising the majority of the predictions which is expected because 5-8 is the most common age group in the dataset (see <ref type="figure">Fig. 7</ref>).    parameters in the following fully-connected layer. Comparing the accuracies in <ref type="table" target="#tab_8">Table 4</ref> and <ref type="table" target="#tab_4">Table 2</ref>, we can see that element-wise fusion performs better by 0.95% for open-ended task and by 1.24% for multiple-choice task. 3) K = 500: In this model, we use K = 500 most frequent answers as possible outputs. Comparing the accuracies in <ref type="table" target="#tab_8">Table 4</ref> and <ref type="table" target="#tab_4">Table 2</ref>, we can see that K = 1000 performs better than K = 500 by 0.82% for open-ended task and by 1.92% for multiple-choice task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) K = 2000:</head><p>In this model, we use K = 2000 most frequent answers as possible outputs. Comparing the accuracies in <ref type="table" target="#tab_8">Table 4</ref> and <ref type="table" target="#tab_4">Table 2</ref>, we can see that K = 2000 performs better then K = 1000 by 0.40% for open-ended task and by 1.16% for multiple-choice task. 5) Truncated Q Vocab @ 5: In this model, the input vocabulary to the embedding layer (which encodes the question words) consists of only those question words which occur atleast 5 times in the training dataset, thus reducing the vocabulary size from 14770 (when all question words are used) to 5134 (65.24% reduction). Remaining question words are replaced with UNK (unknown) tokens. Comparing the accuracies in <ref type="table" target="#tab_8">Table 4</ref> and <ref type="table" target="#tab_4">Table 2</ref>, we can see that truncating the question vocabulary @ 5 performs better than using all questions words by 0.24% for openended task and by 0.17% for multiple-choice task. 6) Truncated Q Vocab @ 11: In this model, the input vocabulary to the embedding layer (which encodes the question words) consists of only those question words which occur atleast 11 times in the training dataset, thus reducing the vocabulary size from 14770 (when all question words are used) to 3561 (75.89% reduction). Remaining question words are replaced with UNK (unknown) tokens. Comparing the accuracies in <ref type="table" target="#tab_8">Table 4</ref> and <ref type="table" target="#tab_4">Table 2</ref>, we can see that truncating the question vocabulary @ 11 performs better than using all questions words by 0.06% for open-ended task and by 0.02% for multiple-choice task. 7) Filtered Dataset: We created a filtered version of the VQA train + val dataset in which we only keep the answers with subject confidence "yes". Also, we keep only those questions for which at least 50% (5 out of 10) answers are annotated with subject confidence "yes". The resulting filtered dataset consists of 344600 questions,   <ref type="table" target="#tab_8">Table 4</ref> shows the performance of the deeper LSTM Q + norm I model when trained on the filtered dataset. Comparing these accuracies with the corresponding accuracies in <ref type="table" target="#tab_4">Table 2</ref>, we can see that the model trained on filtered version performs worse by <ref type="bibr" target="#b0">1</ref>.13% for open-ended task and by 1.88% for multiplechoice task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">VQA CHALLENGE AND WORKSHOP</head><p>We have set up an evaluation server <ref type="bibr" target="#b2">3</ref> where results may be uploaded for the test set and it returns an accuracy breakdown. We are organizing an annual challenge and workshop to facilitate systematic progress in this area; the first instance of the workshop will be held at CVPR 2016 <ref type="bibr" target="#b3">4</ref> . We suggest that papers reporting results on the VQA dataset -1) Report test-standard accuracies, which can be calculated using either of the non-test-dev phases, i.e., "test2015" or "Challenge test2015" on the following links: [oe-real | oe-abstract | mc-real | mc-abstract]. 2) Compare their test-standard accuracies with those on the corresponding test2015 leaderboards [oe-real-leaderboard | oe-abstract-leaderboard | mc-real-leaderboard | mcabstract-leaderboard]. For more details, please see the challenge page 5 . Screenshots of leaderboards for open-ended-real and multiple-choice-real are shown in <ref type="figure" target="#fig_3">Fig. 13</ref>. We also compare the test-standard accuracies of our best model (deeper LSTM Q + norm I) for both open-ended and multiple-choice tasks (real images) with other entries (as of October 28, 2016) on the corresponding leaderboards in <ref type="table" target="#tab_12">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND DISCUSSION</head><p>In conclusion, we introduce the task of Visual Question Answering (VQA). <ref type="bibr">Given</ref>   language question about the image, the task is to provide an accurate natural language answer. We provide a dataset containing over 250K images, 760K questions, and around 10M answers. We demonstrate the wide variety of questions and answers in our dataset, as well as the diverse set of AI capabilities in computer vision, natural language processing, and commonsense reasoning required to answer these questions accurately. The questions we solicited from our human subjects were open-ended and not task-specific. For some application domains, it would be useful to collect task-specific questions.</p><p>For instance, questions may be gathered from subjects who are visually impaired <ref type="bibr" target="#b2">[3]</ref>, or the questions could focused on one specific domain (say sports). Bigham et al. <ref type="bibr" target="#b2">[3]</ref> created an application that allows the visually impaired to capture images and ask open-ended questions that are answered by human subjects. Interestingly, these questions can rarely be answered using generic captions. Training on task-specific datasets may help enable practical VQA applications.</p><p>We believe VQA has the distinctive advantage of pushing the frontiers on "AI-complete" problems, while being amenable to automatic evaluation. Given the recent progress in the community, we believe the time is ripe to take on such an endeavor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX OVERVIEW</head><p>In the appendix, we provide:</p><p>I -Additional analysis comparing captions and Q&amp;A data II -Qualitative visualizations for "What is" questions III -Human accuracy on multiple-choice questions IV -Details on VQA baselines V -"Age" and "Commonsense" of our model VI -Details on the abstract scene dataset VII -User interfaces used to collect the dataset VIII -List of the top answers in the dataset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX -Additional examples from the VQA dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I: CAPTIONS vs. QUESTIONS</head><p>Do questions and answers provide further information about the visual world beyond that captured by captions? One method for determining whether the information captured by questions &amp; answers is different from the information captured by captions is to measure some of the differences in the word distributions from the two datasets. We cast this comparison in terms of nouns, verbs, and adjectives by extracting all words from the caption data (MS COCO captions for real images and captions collected by us for abstract scenes) using the Stanford part-of-speech (POS) 6 tagger <ref type="bibr" target="#b48">[49]</ref>. We normalize the word frequencies from captions, questions, and answers per image, and compare captions vs. questions and answers combined. Using a Kolmogorov-Smirnov test to determine whether the underlying distributions of the two datasets differ, we find a significant difference for all three parts of speech (p &lt; .001) for both real images and abstract scenes. This helps motivate the VQA task as a way to learn information about visual scenes; although both captions and questions &amp; answers provide information about the visual world, they do it from different perspectives, with different underlying biases <ref type="bibr" target="#b19">[20]</ref>, and can function as complementary to one another. <ref type="bibr" target="#b5">6</ref>. Noun tags begin with NN, verb tags begin with VB, adjective tags begin with JJ, and prepositions are tagged as IN.</p><p>We illustrate the similarities and differences between the word distributions in captions vs. questions &amp; answers as Venn-style word clouds <ref type="bibr" target="#b9">[10]</ref> with size indicating the normalized count - <ref type="figure" target="#fig_5">Fig. 15 (nouns)</ref>, <ref type="figure" target="#fig_6">Fig. 16 (verbs)</ref>, and <ref type="figure" target="#fig_15">Fig. 17 (adjectives)</ref> for real images and <ref type="figure" target="#fig_16">Fig. 18 (nouns)</ref>, <ref type="figure" target="#fig_17">Fig. 19 (verbs)</ref>, and <ref type="figure" target="#fig_1">Fig. 20</ref> (adjectives) for abstract scenes. <ref type="bibr" target="#b6">7</ref> The left side shows the top words in questions &amp; answers, the right the top words in captions, and the center the words common to both, with size indicating the harmonic mean of the counts. We see that adjectives in captions capture some clearly visual properties discussed in previous work on vision to language <ref type="bibr" target="#b40">[41]</ref>, such as material and pattern, while the questions &amp; answers have more adjectives that capture what is usual (e.g., "dominant", "approximate", "higher") and other kinds of commonsense properties (e.g., "edible", "possible", "unsafe", "acceptable"). Interestingly, we see that question &amp; answer nouns capture information about "ethnicity" and "hairstyle", while caption nouns capture information about pluralized visible objects (e.g., "cellphones", "daughters") and groups (e.g., "trio", "some"), among other differences. "Man" and "people" are common in both captions and questions &amp; answers.</p><p>One key piece to understanding the visual world is understanding spatial relationships, and so we additionally extract spatial prepositions and plot their proportions in the captions vs. the questions &amp; answers data in <ref type="figure" target="#fig_4">Fig. 14 (left)</ref> for real images and <ref type="figure" target="#fig_4">Fig. 14 (right)</ref> for abstract scenes. We see that questions &amp; 7. Visualization created using http://worditout.com/.</p><p>answers have a higher proportion of specific spatial relations (i.e., "in", "on") compared to captions, which have a higher proportion of general spatial relations (i.e., "with", "near").         </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX II: "WHAT IS" ANALYSIS</head><p>In <ref type="figure" target="#fig_1">Fig. 21</ref>, we show the distribution of questions starting with "What is" by their first five words for both real images and abstract scenes. Note the diversity of objects referenced in the questions, as well as, the relations between objects, such as "holding" and "sitting on". In <ref type="figure" target="#fig_1">Fig. 22</ref>, we show the distribution of answers for "What is" questions ending in different words. For instance, questions ending in "eating" have answers such as "pizza", "watermelon" and "hot dog". Notice the diversity in answers for some questions, such as those that end with "for?" or "picture?". Other questions result in intuitive responses, such as "holding?" and the response "umbrella".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX III: MULTIPLE-CHOICE HUMAN AC-CURACY</head><p>To compute human accuracy for multiple-choice questions, we collected three human answers per question on a random subset of 3,000 questions for both real images and abstract scenes. In <ref type="table" target="#tab_14">Table 6</ref>, we show the human accuracies for multiple choice questions. <ref type="table" target="#tab_14">Table 6</ref> also shows the inter-human agreement for open-ended answer task.</p><p>In comparison to openended answer, the multiple-choice accuracies are more or less same for "yes/no" questions and significantly better (? 15% increase for real images and ? 11% increase for abstract scenes) for "other" questions. Since "other" questions may be ambiguous, the increase in accuracy using multiple choice is not surprising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX IV: DETAILS ON VQA BASELINES</head><p>"per Q-type prior" baseline. We decide on different question types based on first few words of questions in the real images training set and ensure that each question type has at least 30 questions in the training dataset. The most popular answer for each question type is also computed on real images training set. "nearest neighbor" baseline. For every question in the VQA test-standard set, we find its k nearest neighbor questions in the training set using cosine similarity in Skip-Thought <ref type="bibr" target="#b26">[27]</ref> feature space. We also experimented with bag of words and Word2Vec <ref type="bibr" target="#b38">[39]</ref> feature spaces but we obtained the best performance with Skip-Thought. In this set of k questions and their associated images, we find the image which is most similar to the query image using cosine similarity in fc7 feature space. We use the fc7 features from the caffenet model in BVLC Caffe <ref type="bibr" target="#b22">[23]</ref>. The most common ground truth answer of this most similar image and question pair is the predicted answer for the query image and question pair. We pick k = 4 on the test-dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX V: "AGE" AND "COMMONSENSE"</head><p>OF OUR MODEL </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX VI: ABSTRACT SCENES DATASET</head><p>In <ref type="figure" target="#fig_1">Fig. 23 (left)</ref>, we show a subset of the objects that are present in the abstract scenes dataset. For more examples of the scenes generated, please see <ref type="figure" target="#fig_1">Fig. 28</ref>. The user interface used to create the scenes is shown in <ref type="figure" target="#fig_1">Fig. 23 (right)</ref>. Subjects used a drag-and-drop interface to create the scenes. Each object could be flipped horizontally and scaled. The scale of the object determined the rendering order of the objects. Many objects have different attributes corresponding to different poses or types. Most animals have five different discrete poses. Humans have eight discrete expressions and their poses may be continuously adjusted using a "paperdoll" model <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX VII: USER INTERFACES</head><p>In <ref type="figure" target="#fig_1">Fig. 24</ref>, we show the AMT interface that we used to collect questions for images. Note that we tell the workers that the robot already knows the answer to the previously asked question(s), inspiring them to ask different kinds of questions, thereby increasing the diversity of our dataset. <ref type="figure" target="#fig_1">Fig. 25</ref> shows the AMT interface used for collecting answers to the previously collected questions when subjects were shown the corresponding images. <ref type="figure" target="#fig_1">Fig. 26</ref> shows the interface that was used to collect answers to questions when subjects were not shown the corresponding image (i.e., to help in gathering incorrect, but plausible, answers for the multiplechoice task and to assess how accurately the questions can be answered using common sense knowledge alone).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Images Abstract Scenes</head><p>What is What is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Images</head><p>Abstract Scenes <ref type="figure" target="#fig_1">Fig. 22</ref>: Distribution of answers for questions starting with "What is" for a random sample of 60K questions for real images (top) and all questions for abstract scenes (bottom). Each column corresponds to questions ending in different words, such as "doing?", "on?", etc.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX VIII: ANSWER DISTRIBUTION</head><p>The top 250 answers in our real images dataset along with their counts and percentage counts are given below. The answers have been presented in different colors to show the different Part-of-Speech (POS) tagging of the answers with the following color code: yes/no, noun, verb, adjective, adverb, and numeral. "yes" (566613, 22.82%), "no" (381307, 15.35%), "2" (80031, 3.22%), "1" (46537, 1.87%), "white" (41753, 1.68%), "3" (41334, 1.66%), "red" (33834, 1.36%), "blue" (28881, 1.16%), "4" (27174, 1.09%), "green" (22453, 0.9%), "black" (21852, 0.88%), "yellow" (17312, 0.7%), "brown" (14488, 0.58%), "5" (14373, 0.58%), "tennis" (10941, 0.44%),"baseball" (10299, 0.41%), "6" (10103, 0.41%), "orange" (9136, 0.37%), "0" (8812, 0.35%), "bathroom" (8473, 0.34%), "wood" (8219, 0.33%), "right" (8209, 0.33%), "left" (8058, 0.32%), "frisbee" (7671, 0.31%), "pink" (7519, 0.3%), "gray" (7385, 0.3%), "pizza" (6892, 0.28%), "7" (6005, 0.24%), "kitchen" (5926, 0.24%), "8" (5592, 0.23%), "cat" (5514, 0.22%), "skiing" (5189, 0.21%), "skateboarding" (5122, 0.21%), "dog" (5092, 0.21%), "snow" (4867, 0.2%), "black and white" (4852, 0.2%), "skateboard" (4697, 0.19%), "surfing" (4544, 0.18%), "water" (4513, 0.18%), "giraffe" (4027, 0.16%), "grass" (3979, 0.16%), "surfboard" (3934, 0.16%), "wii" (3898, 0.16%), "kite" (3852, 0.16%), "10" (3756, 0.15%), "purple" (3722, 0.15%), "elephant" (3646, 0.15%), "broccoli" (3604, 0.15%), "man" (3590, 0.14%), "winter" (3490, 0.14%), "stop" (3413, 0.14%), "train" (3226, 0.13%), "9" (3217, 0.13%), "apple" (3189, 0.13%), "silver" (3186, 0.13%), "horse" (3159, 0.13%), "banana" (3151, 0.13%), "umbrella" (3139, 0.13%), "eating" (3117, 0.13%), "sheep" (2927, 0.12%), "bear" (2803, 0.11%), "phone" (2772, 0.11%), "12" (2633, 0.11%), "motorcycle" (2608, 0.11%), "cake" (2602, 0.1%), "wine" (2574, 0.1%), "beach" (2536, 0.1%), "soccer" (2504, 0.1%), "sunny" (2475, 0.1%), "zebra" (2403, 0.1%), "tan" (2402, 0.1%), "brick" (2395, 0.1%), "female" (2372, 0.1%), "bananas" (2350, 0.09%), "table" (2331, 0.09%), "laptop" (2316, 0.09%), "hat" (2277, 0.09%), "bench" (2259, 0.09%), "flowers" (2219, 0.09%), "woman" (2197, 0.09%), "male" (2170, 0.09%), "cow" (2084, 0.08%), "food" (2083, 0.08%), "living room" (2022, 0.08%), "bus" (2011, 0.08%), "snowboarding" (1990, 0.08%), "kites" (1979, 0.08%), "cell phone" (1943, 0.08%), "helmet" (1885, 0.08%), "maybe" (1853, 0.07%), "outside" (1846, 0.07%), "hot dog" (1809, 0.07%), "night" (1805, 0.07%), "trees" (1785, 0.07%), "11" (1753, 0.07%), "bird" (1739, 0.07%), "down" (1732, 0.07%), "bed" (1587, 0.06%), "camera" (1560, 0.06%), "tree" (1547, 0.06%), "christmas" (1544, 0.06%), "fence" (1543, 0.06%), "nothing" (1538, 0.06%), "unknown" (1532, 0.06%), "tennis racket" (1525, 0.06%), "red and white" (1518, 0.06%), "bedroom" (1500, 0.06%), "bat" (1494, 0.06%), "glasses" (1491, 0.06%), "tile" (1487, 0.06%), "metal" (1470, 0.06%), "blue and white" (1440, 0.06%), "fork" (1439, 0.06%), "plane" (1439, 0.06%), "airport" (1422, 0.06%), "cloudy" (1413, 0.06%), "15" (1407, 0.06%), "up" (1399, 0.06%), "blonde" (1398, 0.06%), "day" (1396, 0.06%), "teddy bear" (1386, 0.06%), "glass" (1379, 0.06%), "20" (1365, 0.05%), "beer" (1345, 0.05%), "car" (1331, 0.05%), "sitting" (1328, 0.05%), "boat" (1326, 0.05%), "standing" (1326, 0.05%), "clear" (1318, 0.05%), "13" (1318, 0.05%), "nike" (1293, 0.05%), "sand" (1282, 0.05%), "open" (1279, 0.05%), "cows" (1271, 0.05%), "bike" (1267, 0.05%), "chocolate" (1266, 0.05%), "donut" (1263, 0.05%), "airplane" (1247, 0.05%), "birthday" (1241, 0.05%), "carrots" (1239, 0.05%), "skis" (1220, 0.05%), "girl" (1220, 0.05%), "many" (1211, 0.05%), "zoo" (1204, 0.05%), "suitcase" (1199, 0.05%), "old" (1180, 0.05%), "chair" (1174, 0.05%), "beige" (1170, 0.05%), "ball" (1169, 0.05%), "ocean" (1168, 0.05%), "sandwich" (1168, 0.05%), "tie" (1166, 0.05%), "horses" (1163, 0.05%), "palm" (1163, 0.05%), "stripes" (1155, 0.05%), "fall" (1146, 0.05%), "cheese" (1142, 0.05%), "scissors" (1134, 0.05%), "round" (1125, 0.05%), "chinese" (1123, 0.05%), "knife" (1120, 0.05%), "14" (1110, 0.04%), "toilet" (1099, 0.04%), "don't know" (1085, 0.04%), "snowboard" (1083, 0.04%), "truck" (1076, 0.04%), "boy" (1070, 0.04%), "coffee" (1070, 0.04%), "cold" (1064, 0.04%), "fruit" (1064, 0.04%), "walking" (1053, 0.04%), "wedding" (1051, 0.04%), "lot" (1050, 0.04%), "sunglasses" (1047, 0.04%), "mountains" (1030, 0.04%), "wall" (1009, 0.04%), "elephants" (1006, 0.04%), "wetsuit" (998, 0.04%), "square" (994, 0.04%), "toothbrush" (989, 0.04%), "sleeping" (986, 0.04%), "fire hydrant" (977, 0.04%), "bicycle" (973, 0.04%), "overcast" (968, 0.04%), "donuts" (961, 0.04%), "plastic" (961, 0.04%), "breakfast" (955, 0.04%), "tv" (953, 0.04%), "paper" (952, 0.04%), "ground" (949, 0.04%), "asian" (938, 0.04%), "plaid" (936, 0.04%), "dirt" (933, 0.04%), "mirror" (928, 0.04%), "usa" (928, 0.04%), "chicken" (925, 0.04%), "plate" (920, 0.04%), "clock" (912, 0.04%), "luggage" (908, 0.04%), "none" (908, 0.04%), "street" (905, 0.04%), "on table" (904, 0.04%), "spoon" (899, 0.04%), "cooking" (898, 0.04%), "daytime" (896, 0.04%), "16" (893, 0.04%), "africa" (890, 0.04%), "stone" (884, 0.04%), "not sure" (873, 0.04%), "window" (868, 0.03%), "sun" (865, 0.03%), "gold" (860, 0.03%), "people" (856, 0.03%), "racket" (847, 0.03%), "zebras" (845, 0.03%), "carrot" (841, 0.03%), "person" (835, 0.03%), "fish" (835, 0.03%), "happy" (824, 0.03%), "circle" (822, 0.03%), "oranges" (817, 0.03%), "backpack" (812, 0.03%), "25" (810, 0.03%), "leaves" (809, 0.03%), "watch" (804, 0.03%), "mountain" (800, 0.03%), "no one" (798, 0.03%), "ski poles" (792, 0.03%), "city" (791, 0.03%), "couch" (790, 0.03%), "afternoon" (782, 0.03%), "jeans" (781, 0.03%), "brown and white" (779, 0.03%), "summer" (774, 0.03%), "giraffes" (772, 0.03%), "computer" (771, 0.03%), "refrigerator" (768, 0.03%), "birds" (762, 0.03%), "child" (761, 0.03%), "park" (759, 0.03%), "flying kite" (756, 0.03%), "restaurant" (747, 0.03%), "evening" (738, 0.03%), "graffiti" (736, 0.03%), "30" (730, 0.03%), "grazing" (727, 0.03%), "flower" (723, 0.03%), "remote" (720, 0.03%), "hay" (719, 0.03%), "50" (716, 0.03%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX IX: ADDITIONAL EXAMPLES</head><p>To provide insight into the dataset, we provide additional examples. In <ref type="figure" target="#fig_1">Fig. 27, Fig. 28, and Fig. 29</ref>, we show a random selection of the VQA dataset for the MS COCO <ref type="bibr" target="#b31">[32]</ref> images, abstract scenes, and multiple-choice questions, respectively.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>? A. Agrawal, J. Lu and S. Antol are with Virginia Tech. ? M. Mitchell is with Microsoft Research, Redmond. ? C. L. Zitnick is with Facebook AI Research. ? D. Batra and D. Parikh are with Georgia Institute of Technology. Does it appear to be rainy? Does this person have 20/20 vision? Is this person expecting company? What is just under the tree? How many slices of pizza are there? Is this a vegetarian pizza?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Examples of questions (black), (a subset of the) answers given when looking at the image (green), and answers given when not looking at the image (blue) for numerous representative examples of the dataset. See the appendix for more examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>tions: (i) open-ended and (ii) multiple-choice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Distribution of questions by their first four words for a random sample of 60K questions for real images (left) and all questions for abstract scenes (right). The ordering of the words starts towards the center and radiates outwards. The arc length is proportional to the number of questions containing the word. White areas are words with contributions too small to show.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Percentage of questions with different word lengths for real images and abstract scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Distribution of answers per question type for a random sample of 60K questions for real images when subjects provide answers when given the image (top) and when not given the image (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Number of questions per average confidence score (0 = not confident, 1 = confident) for real images and abstract scenes (black lines). Percentage of questions where 7 or more answers are same, 3-7 are same, less than 3 are same (color bars).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 )Fig. 8 :</head><label>28</label><figDesc>LSTM Q: An LSTM with one hidden layer is used to obtain 1024-dim embedding for the question. The embedding obtained from the LSTM is a concatenation of last cell state and last hidden state representations (each being 512-dim) from the hidden layer of the LSTM. Each question word is encoded with 300-dim embedding by a fully-connected layer + tanh non-linearity which is then fed to the LSTM. The input vocabulary to the embedding layer consists of all the question words seen in the training dataset.3) deeper LSTM Q: An LSTM with two hidden layers is used to obtain 2048-dim embedding for the question. The embedding obtained from the LSTM is a concatenation of last cell state and last hidden state representations (each Our best performing model (deeper LSTM Q + norm I). This model uses a two layer LSTM to encode the questions and the last hidden layer of VGGNet<ref type="bibr" target="#b47">[48]</ref> to encode the images. The image features are then 2 normalized. Both the question and image features are transformed to a common space and fused via element-wise multiplication, which is then passed through a fully connected layer followed by a softmax layer to obtain a distribution over answers.being 512-dim) from each of the two hidden layers of the LSTM. Hence 2 (hidden layers) x 2 (cell state and hidden state) x 512 (dimensionality of each of the cell states, as well as hidden states) inFig. 8. This is followed by a fully-connected layer + tanh non-linearity to transform 2048-dim embedding to 1024-dim. The question words are encoded in the same way as in LSTM Q. Multi-Layer Perceptron (MLP): The image and question embeddings are combined to obtain a single embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Pr (system is correct | answer) for 50 most frequent ground truth answers on the VQA validation set (plot is sorted by accuracy, not frequency). System refers to our best model (deeper LSTM Q + norm I). Pr (answer | system is correct) for 50 most frequently predicted answers on the VQA validation set (plot is sorted by prediction frequency, not accuracy). System refers to our best model (deeper LSTM Q + norm I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Pr (system is correct | age of question) on the VQA validation set. System refers to our best model (deeper LSTM Q + norm I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :</head><label>12</label><figDesc>Pr (age of question | system is correct) on the VQA validation set. System refers to our best model (deeper LSTM Q + norm I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 :</head><label>13</label><figDesc>Leaderboard showing test-standard accuracies for VQA Real Image Challenge (Open-Ended) on left and leaderboard showing test-standard accuracies for VQA Real Image Challenge (Multiple-Choice) on right (snapshot from October 28, 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 :</head><label>14</label><figDesc>Proportions of spatial prepositions in the captions and question &amp; answers for real images (left) and abstract scenes (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 :</head><label>15</label><figDesc>Venn-style word clouds<ref type="bibr" target="#b9">[10]</ref> for nouns with size indicating the normalized count for real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 :</head><label>16</label><figDesc>Venn-style word clouds<ref type="bibr" target="#b9">[10]</ref> for verbs with size indicating the normalized count for real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 :</head><label>17</label><figDesc>Venn-style word clouds<ref type="bibr" target="#b9">[10]</ref> for adjectives with size indicating the normalized count for real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 18 :</head><label>18</label><figDesc>Venn-style word clouds<ref type="bibr" target="#b9">[10]</ref> for nouns with size indicating the normalized count for abstract scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 19 :</head><label>19</label><figDesc>Venn-style word clouds<ref type="bibr" target="#b9">[10]</ref> for verbs with size indicating the normalized count for abstract scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 20 :</head><label>20</label><figDesc>Venn-style word clouds<ref type="bibr" target="#b9">[10]</ref> for adjectives with size indicating the normalized count for abstract scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 21 :</head><label>21</label><figDesc>Distribution of questions starting with "What is" by their first five words for a random sample of 60K questions for real images (left) and all questions for abstract scenes (right). The ordering of the words starts towards the center and radiates outwards. The arc length is proportional to the number of questions containing the word. White areas are words with contributions too small to show.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 23 :</head><label>23</label><figDesc>Left: A small subset of the objects present in the abstract scene dataset. Right: The AMT interface for collecting abstract scenes. The light green circles indicate where users can select to manipulate a person's pose. Different objects may be added to the scene using the folders to the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 24 :</head><label>24</label><figDesc>Our AMT interface for collecting the third question for an image, when subjects were shown previous questions that were collected and were asked to ask a question different from previous questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 25 :</head><label>25</label><figDesc>The AMT interface used to collect answers to a question when subjects were shown the image while answering the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig. 26 :</head><label>26</label><figDesc>The AMT interface used to collect answers to a question when subjects were not shown the image while answering the question using only commonsense to collect the plausible, but incorrect, multiple-choice answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 28 :</head><label>28</label><figDesc>Random examples of questions (black), (a subset of the) answers given when looking at the image (green), and answers given when not looking at the image (blue) for numerous representative examples of the abstract scene dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Caption* 57.47 78.97 39.68 44.41 Question + Image 83.30 95.77 83.39 72.67 Question 43.27 66.65 28.52 23.66 Abstract Question + Caption* 54.34 74.70 41.19 40.18 Question + Image 87.49 95.96 95.04 75.<ref type="bibr" target="#b32">33</ref> Test-standard accuracy of human subjects when asked to answer the question without seeing the image (Question), seeing just a caption of the image and not the image itself (Question + Caption), and seeing the image (Question + Image). Results are shown for all questions, "yes/no" &amp; "number" questions, and other questions that are neither answered "yes/no" nor number. All answers are free-form and not multiple-choice. * These accuracies are evaluated on a subset of 3K train questions (1K images).</figDesc><table><row><cell cols="2">Dataset Input</cell><cell cols="2">All Yes/No Number Other</cell></row><row><cell></cell><cell>Question</cell><cell>40.81 67.60</cell><cell>25.77 21.22</cell></row><row><cell>Real</cell><cell>Question +</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 (</head><label>1</label><figDesc>Question + Caption) shows the percentage of questions answered correctly when humanFig. 7: Example questions judged by Mturk workers to be answerable by different age groups. The percentage of questions falling into each age group is shown in parentheses.subjects are given the question and a human-provided caption describing the image, but not the image. As expected, the results are better than when humans are shown the questions alone. However, the accuracies are significantly lower than when subjects are shown the actual image. This demonstrates that in order to answer the questions correctly, deeper image understanding (beyond what image captions typically capture) is necessary. In fact, we find that the distributions of nouns, verbs, and adjectives mentioned in captions is statistically significantly different from those mentioned in our questions + answers (Kolmogorov-Smirnov test, p &lt; .001) for both real images and abstract scenes. See the appendix for details.</figDesc><table><row><cell>3-4 (15.3%)</cell><cell>5-8 (39.7%)</cell><cell>9-12 (28.4%)</cell><cell>13-17 (11.2%)</cell><cell>18+ (5.5%)</cell></row><row><cell>Is that a bird in the sky?</cell><cell>How many pizzas are shown?</cell><cell>Where was this picture taken?</cell><cell>Is he likely to get mugged if he walked</cell><cell>What type of architecture is this?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>down a dark alleyway like this?</cell><cell></cell></row><row><cell>What color is the shoe?</cell><cell>What are the sheep eating?</cell><cell>What ceremony does the cake</cell><cell>Is this a vegetarian meal?</cell><cell>Is this a Flemish bricklaying</cell></row><row><cell></cell><cell></cell><cell>commemorate?</cell><cell></cell><cell>pattern?</cell></row><row><cell>How many zebras are there?</cell><cell>What color is his hair?</cell><cell>Are these boats too tall to fit</cell><cell cols="2">What type of beverage is in the glass? How many calories are in this</cell></row><row><cell></cell><cell></cell><cell>under the bridge?</cell><cell></cell><cell>pizza?</cell></row><row><cell>Is there food on the table?</cell><cell>What sport is being played?</cell><cell>What is the name of the white</cell><cell>Can you name the performer in the</cell><cell>What government document is</cell></row><row><cell></cell><cell></cell><cell>shape under the batter?</cell><cell>purple costume?</cell><cell>needed to partake in this activity?</cell></row><row><cell>Is this man wearing shoes?</cell><cell cols="2">Name one ingredient in the skillet. Is this at the stadium?</cell><cell>Besides these humans, what other</cell><cell>What is the make and model of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>animals eat here?</cell><cell>this vehicle?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.66 70.81 00.39 01.15 per Q-type prior 37.54 71.03 35.77 09.38 39.45 71.02 35.86 13.34 nearest neighbor 42.70 71.89 24.36 21.94 48.49 71.94 26.00 33.56 BoW Q 48.09 75.66 36.70 27.14 53.68 75.71 37.05 38.64 I 28.13 64.01 00.42 03.77 30.53 69.87 00.45 03.76 BoW Q + I 52.64 75.55 33.67 37.37 58.97 75.59 34.35 50.33 LSTM Q 48.76 78.20 35.68 26.59 54.75 78.22 36.82 38.78 LSTM Q + I 53.74 78.94 35.24 36.42 57.17 78.95 35.80 43.41 deeper LSTM Q 50.39 78.41 34.68 30.03 55.88 78.45 35.91 41.13 deeper LSTM Q + norm I 57.75 80.50 36.77 43.08 62.70 80.52 38.22 53.01 Caption 26.70 65.50 02.03 03.86 28.29 69.79 02.06 03.82 BoW Q + C 54.70 75.82 40.12 42.56 59.85 75.89 41.16 52.53</figDesc><table /><note>). For BoW Question + Caption (BoW Q + C) method, we simply concatenate the BoW Q and C embeddings. For testing, we report the result on two different tasks: open- ended selects the answer with highest activation from all Open-Ended Multiple-Choice All Yes/No Number Other All Yes/No Number Other prior ("yes") 29.66 70.81 00.39 01.15 29</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note>Accuracy of our methods for the open-ended and multiple- choice tasks on the VQA test-dev for real images. Q = Question, I = Image, C = Caption. (Caption and BoW Q + C results are on val). See text for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>shows the accuracy of our baselines and methods for both the open-ended and multiple-choice tasks on the VQA test-dev for real images.</figDesc><table><row><cell>As expected, the vision-alone model (I) that completely</cell></row><row><cell>ignores the question performs rather poorly (open-ended:</cell></row><row><cell>28.13% / multiple-choice: 30.53%). In fact, on open-ended</cell></row><row><cell>task, the vision-alone model (I) performs worse than the prior</cell></row><row><cell>("yes") baseline, which ignores both the image and question</cell></row></table><note>(responding to every question with a "yes"). Interestingly, the language-alone methods (per Q-type prior, BoW Q, LSTM Q) that ignore the image perform surprisingly well, with BoW Q achieving 48.09% on open-ended (53.68% on multiple-choice) and LSTM Q achieving 48.76% on open- ended (54.75% on multiple-choice); both outperforming the nearest neighbor baseline (open-ended: 42.70%, multiple- choice: 48.49%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table /><note>Open-ended test-dev results for different question types on real images (Q+C is reported on val). Machine performance is reported using the bag-of-words representation for questions. Questions types are determined by the one or two words that start the question. The percentage of questions for each type is shown in parentheses. Last and second last columns respectively show the average human age and average degree of commonsense required to answer the questions (as reported by AMT workers), respectively. See text for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>In this model, the activations from the last hidden layer of VGGNet<ref type="bibr" target="#b47">[48]</ref> are not 2 -normalized.</figDesc><table><row><cell>shows the accuracy of different ablated versions of</cell></row><row><cell>our best model (deeper LSTM Q + norm I) for both the open-</cell></row><row><cell>ended and multiple-choice tasks on the VQA test-dev for real</cell></row><row><cell>images. The different ablated versions are as follows -</cell></row><row><cell>1) Without I Norm:</cell></row></table><note>Comparing the accuracies in Table 4 and Table 2, we can see that 2 -normalization of image features boosts the performance by 0.16% for open-ended task and by 0.24% for multiple-choice task. 2) Concatenation: In this model, the transformed image and LSTM embeddings are concatenated (instead of element- wise multiplied), resulting in doubling the number of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>No Number Other All Yes/No Number Other Without I Norm 57.59 80.41 36.63 42.84 62.46 80.43 38.10 52.62 Concatenation 56.80 78.49 35.08 43.19 61.46 78.52 36.43 52.54 K = 500 56.93 80.61 36.24 41.39 60.78 80.64 37.44 49.10 K = 2000 58.15 80.56 37.04 43.79 63.86 80.59 38.97 55.20 Truncated Q Vocab @ 5 57.99 80.67 36.99 43.38 62.87 80.71 38.22 53.20 Truncated Q Vocab @ 11 57.81 80.42 36.97 43.22 62.72 80.45 38.30 53.09 Filtered Dataset 56.62 80.19 37.48 40.95 60.82 80.19 37.48 49.57</figDesc><table><row><cell>Open-Ended</cell><cell>Multiple-Choice</cell></row><row><cell>All Yes/</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Accuracy of ablated versions of our best model (deeper</cell></row><row><cell>LSTM Q + norm I) for the open-ended and multiple-choice tasks on</cell></row><row><cell>the VQA test-dev for real images. Q = Question, I = Image. See text</cell></row><row><cell>for details.</cell></row><row><cell>compared to 369861 questions in the original dataset, thus</cell></row><row><cell>leading to only 6.83% reduction in the size of the dataset.</cell></row><row><cell>The filtered dataset has 8.77 answers per question on</cell></row><row><cell>average. We did not filter the test set so that accuracies of</cell></row><row><cell>the model trained on the filtered dataset can be compared</cell></row><row><cell>with that of the model trained on the original dataset. The</cell></row><row><cell>row "Filtered Dataset" in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>an image and an open-ended, natural 3. http://visualqa.org/challenge.html 4. http://www.visualqa.org/workshop.html 5. http://visualqa.org/challenge.html No Number Other All Yes/No Number Other snubi-naverlabs 60.60 82.23 38.22 46.99 64.95 82.25 39.56 55.68 MM PaloAlto 60.36 80.43 36.82 48.33 ----LV-NUS 59.54 81.34 35.67 46.10 64.18 81.25 38.30 55.20 + norm I 58.16 80.56 36.53 43.73 63.09 80.59 37.70 53.64 iBOWIMG ----61.97 76.86 37.30 54.60</figDesc><table><row><cell></cell><cell>Open-Ended</cell><cell cols="2">Multiple-Choice</cell><cell></cell></row><row><cell cols="2">All Yes/ACVT Adelaide 59.44 81.07 37.12 45.83 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>global vision</cell><cell>58.43 78.24 36.27 46.32 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>deeper LSTM Q</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 5 :</head><label>5</label><figDesc>Test-standard accuracy of our best model (deeper LSTM Q + norm I) compared to test-standard accuracies of other entries for the open-ended and multiple-choice tasks in the respective VQA Real Image Challenge leaderboards (as of October 28, 2016).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 6 :</head><label>6</label><figDesc>For each of the two datasets, real and abstract, first two rows are the human accuracies for multiple-choice questions when subjects were shown both the image and the question. Majority vote means we consider the answer picked by majority of the three subjects to be the predicted answer by humans and compute accuracy of that answer for each question. Average means we compute the accuracy of each of the answers picked by the subjects and record their average for each question. The last row is the inter-human agreement for open-ended answers task when subjects were shown both the image and the question. All accuracies are evaluated on a random subset of 3000 questions.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We would like to acknowledge the countless hours of effort provided by the workers on Amazon Mechanical Turk. This work was supported in part by the The Paul G. Allen Family Foundation via an award to D.P., ICTAS at Virginia Tech via awards to D.B. and D.P., Google Faculty Research Awards to D.P. and D.B., the National Science Foundation CAREER award to D.B., the Army Research Office YIP Award to D.B., and a Office of Naval Research grant to D.B.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(l) left of pond (m) 13 (n) plants and cat (o) tree base (p) cement (q) 0 (r) green, blue and yellow Q: What type of wildlife is this park overrun with? </p><p>Q: Does the girl have a lot of toys?  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cloudcv: Large-scale distributed computer vision as a cloud service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Mathialagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chavali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Banik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobile Cloud Visual Media Computing</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="265" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-Shot Learning via Visual Abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VizWiz: Nearly Realtime Answers to Visual Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tatarowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">User Interface Software and Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward an Architecture for Never-Ending Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R H</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1504.00325</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft COCO Captions: Data Collection and Evaluation Server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NEIL: Extracting Visual Knowledge from Web Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mind&apos;s Eye: A Recurrent Visual Representation for Image Caption Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic wordclouds and vennclouds for exploratory data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Interactive Language Learning and Visualization</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical Semantic Indexing for Large Scale Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparing Automatic Evaluation Measures for Image Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Paraphrase-Driven Learning for Open Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open Question Answering over Curated and Extracted Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From Captions to Visual Concepts and Back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Every Picture Tells a Story: Generating Sentences for Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Visual Turing Test for Computer Vision Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PNAS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reporting bias and knowledge extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Knowledge Extraction</title>
		<meeting>the 3rd Workshop on Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>CIKM 2013</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06726</idno>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What Are You Talking About? Text-to-Image Coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Baby Talk: Understanding and Generating Simple Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Sagnik Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Building Large Knowledge-Based Systems; Representation and Inference in the Cyc Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lenat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Guha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Don&apos;t Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Don&apos;t just listen, use your imagination: Leveraging visual common sense for non-visual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ConceptNet -A Practical Commonsense Reasoning Tool-Kit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BT Technology Journal</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neuralbased approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Explain Images with Multimodal Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1410.1090</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Midge: Generating Image Descriptions From Computer Vision Detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attributes in visual reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PRE-CogSci</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generating Expressions that Refer to Visible Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Linking People with &quot;Their&quot; Names using Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Translating Video Content to Natural Language Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Viske: Visual knowledge extraction and question answering by visual verification of relation phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Feature-rich partof-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Joint Video and Text Parsing for Understanding Events and Answering Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based Image Description Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning common sense through visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vendantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visual madlibs: Fill-in-theblank description generation and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Yin and yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1511.05099</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bringing Semantics Into Focus Using Visual Abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning the Visual Interpretation of Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adopting Abstract Images for Semantic Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
