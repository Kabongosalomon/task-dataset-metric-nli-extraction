<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Self-Stitching Graph Network for Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
							<email>chen.zhao@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
							<email>ali.thabet@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video Self-Stitching Graph Network for Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action localization (TAL) in videos is a challenging task, especially due to the large variation in action temporal scales. Short actions usually occupy the major proportion in the data, but have the lowest performance with all current methods. In this paper, we confront the challenge of short actions and propose a multi-level cross-scale solution dubbed as video self-stitching graph network (VSGN). We have two key components in VSGN: video self-stitching (VSS) and cross-scale graph pyramid network (xGPN). In VSS, we focus on a short period of a video and magnify it along the temporal dimension to obtain a larger scale. We stitch the original clip and its magnified counterpart in one input sequence to take advantage of the complementary properties of both scales. The xGPN component further exploits the cross-scale correlations by a pyramid of cross-scale graph networks, each containing a hybrid module to aggregate features from across scales as well as within the same scale. Our VSGN not only enhances the feature representations, but also generates more positive anchors for short actions and more short training samples. Experiments demonstrate that VSGN obviously improves the localization performance of short actions as well as achieving the state-of-the-art overall performance on THUMOS-14 and ActivityNet-v1.3.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays has seen a growing interest in video understanding both from industry and academia, owing to the rapidly produced video content in online platforms. Temporal action localization (TAL) in untrimmed videos is one important task in this area, which aims to specify the start and the end time of an action as well as to identify its category. TAL is not only the key technique of various application such as extracting highlights in sports, but also lays the foundation for other higher-level tasks such as video grounding <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9]</ref> and video captioning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Though many influential works (e.g., <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b1">2]</ref>) in recent years have been continuously breaking <ref type="figure" target="#fig_3">Figure 1</ref>. Short actions are the majority in numbers, but have the lowest performance. a) Distribution of action duration in ActivityNet-v1.3 <ref type="bibr" target="#b4">[5]</ref>. Actions are divided into five duration groups (seconds): XS (0, 30], S <ref type="bibr" target="#b30">(30,</ref><ref type="bibr">60]</ref>, M (60, 120], L (120, 180], and XL (180, inf). b) TAL Performance of different methods on actions of different duration. the record of TAL performance, a major challenge hinders its substantial improvement -large variation in action duration. An action can last from a fraction of a second to minutes in the real-world scenario as well as in the datasets. We plot the distributions of action duration in the dataset ActivityNet-v1.3 <ref type="bibr" target="#b4">[5]</ref> in <ref type="figure" target="#fig_3">Fig. 1 a)</ref>. We notice that actions shorter than 30 seconds dominate the distribution, but their performance is obviously inferior to longer ones with all different TAL methods ( <ref type="figure" target="#fig_3">Fig. 1 b)</ref>). Therefore, the accuracy of short actions is a key factor to determine the performance of a TAL method.</p><p>Why are short actions hard to localize? Short actions have small temporal scales with fewer frames, and therefore, their information is prone to loss or distortion throughout a deep neural network. Most methods in the literature process videos regardless of action duration, which as a consequence sacrifices the performance of short actions. Recently, researchers attempt to incorporate feature pyramid networks (FPN) <ref type="bibr" target="#b20">[21]</ref> from the object detection problem to the TAL problem <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>, which generates different feature scales at different network levels, each level with different sizes for candidate actions. Though by this means short actions may go through fewer pooling layers to avoid being excessively down-scaled, yet their original small scale as the source of the problem still limits the performance.</p><p>Then how can we attack the small-scale problem of short actions? A possible solution is to temporally up-scale videos to obtain more frames to represent an action. Recent literature shows the practice of re-scaling videos via linear interpolation before feeding into a network <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b43">42]</ref>, but these methods actually down-scale rather than upscale videos (e.g., using only 100 frames on AcitivityNet-v1. <ref type="bibr" target="#b2">3</ref>). Even if we can adapt a method to using a larger-scale input, how can we ensure that the up-scaled videos contain sufficient and accurate information for detecting an action? Moreover, it makes the problem even harder that re-scaling is usually not performed on original frames, but on video features which do not satisfy linearity.</p><p>Up-scaling a video could transform a short action into a long one, but may lose important information for localization. Thus both the original scale and the enlarged scale have their limitations and advantages. The original video scale contains the original intact information, while the enlarged one is easier for the network to detect. In contrast to other works that either use the original-scale video or a down-scaled video, in this paper, we use not only the original scale, but also an enlarged scale to take advantage of their complementary properties and mutually enhance their feature representations.</p><p>Specifically, we propose a Video self-Stitching Graph Network (VSGN) for improving performance of short actions in the TAL problem. Our VSGN is a multi-level cross-scale framework that contains two major components: video self-stitching (VSS); cross-scale graph pyramid network (xGPN). In VSS, we focus on a short period of a video and magnify it along the temporal dimension to obtain a larger scale. Then using our self-stitching strategy, we piece together both the original-scale clip and its magnified counterpart into one single sequence as the network input. In xGPN, we progressively aggregate features from cross scales as well as from the same scale via a pyramid of cross-scale graph networks. Hence, we enable direct information pass between the two feature scales. Compared to simply using the up-scaled clip, our VSGN adaptively rectifies distorted features in either scales from one another by learning to localize actions, therefore, it is able to retain more information for the localization task. In addition to enhancing the features, our VSGN augments the datasets with more short actions to mitigate the bias towards long actions during the learning process, and enables more anchors, even those with large scales, to predict short actions.</p><p>We summarize our contributions as follows: 1) To the best of our knowledge, this is the first work that sheds light on the problem of short actions in temporal action localization. We propose a novel solution that utilizes cross-scale correlations of multi-level features to strengthen their representations and facilitate localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2)</head><p>We propose a novel temporal action localization frame-work VSGN, which features two key components: video self-stitching (VSS); cross-scale graph pyramid network (xGPN). For effective feature aggregation, we design a cross-scale graph network for each level in xGPN with a hybrid module of a temporal branch and a graph branch.</p><p>3) VSGN shows obvious improvement on short actions over other concurrent methods, and also achieves new stateof-the-art overall performance. On THUMOS-14, VSGN reaches 52.4% mAP@0.5, compared to previous best score 40.4% under the same features. On ActivityNet-v1.3, VSGN reaches an average mAP of 35.07%, compared to the previous best score 34.26% under the same features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-scale solution in object detection</head><p>Temporal action localization is analogous to the task of object detection in images, though the scale variation in images is not as large as in videos. Multiple methods have been proposed to deal with object scale variation in images.</p><p>A representative work is the feature pyramid network (FPN) <ref type="bibr" target="#b20">[21]</ref>, which generates multi-scale features using an architecture of encoder and decoder pyramids. FPN has become a popular base architecture for many object detection methods in recent years (e.g., <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b33">32]</ref>). Following FPN, some methods are proposed to further improve the architecture for higher efficiency and better accuracy, such as PANet <ref type="bibr" target="#b23">[24]</ref>, NAS-FPN <ref type="bibr" target="#b9">[10]</ref>, BiFPN <ref type="bibr" target="#b30">[30]</ref>. Our proposed cross-scale graph pyramid (xGPN) adopts the idea of FPN and builds a pyramid of video features in the temporal domain instead of images in the spatial domain, and moreover, we embed cross-scale graph networks in the pyramid levels.</p><p>Another perspective to address the scale issue, especially for the small scale, is data augmentation, e.g. mosaic augmentation in YOLOv4 <ref type="bibr" target="#b2">[3]</ref>, which pieces together four images into one large image and crop a center area for training. It helps the model learn to not overemphasize the activations for large objects so as to enhance the performance for small objects. Our VSGN is inspired by mosaic augmentation, but it stitches the same video clip of different scales along the temporal dimension instead of different videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Temporal action localization</head><p>Recent temporal action localization methods can be generally classified into two categories based on the way they deal with the input sequence. In the first category, the works such as BSN <ref type="bibr" target="#b19">[20]</ref>, BMN <ref type="bibr" target="#b18">[19]</ref>, G-TAD <ref type="bibr" target="#b39">[38]</ref>, BC-GNN <ref type="bibr" target="#b1">[2]</ref> re-scale each video to a fixed temporal length (usually a small length such as 100 frames) regardless of the original video duration. Methods using this strategy are efficient owing to the small input scale, but would harm short actions since the information of short ones easily gets lost or distorted during down-scaling. It is also non-trivial to up- scale the videos instead as input for these methods is limited by their architectures. For example, BSN relies on the startness/endness curves to identify proposal candidates, but when more frames are used, the curves will have too many peaks and valleys to generate meaningful proposals. In G-TAD, it tends to find graph neighbors only in the temporal vicinity if too many frames are interpolated and neighboring frames become quite similar (referred to as scaling curse).</p><p>The second category is to use sliding windows to crop the original video into multiple input sequences. This can preserve the original information of each frame. The works R-C3D <ref type="bibr" target="#b38">[37]</ref>, TAL-NET <ref type="bibr" target="#b6">[7]</ref>, PBRNet <ref type="bibr" target="#b22">[23]</ref>, belonging to this category, perform pooling / strided convolution to obtain multi-scale features. Compared to these two categories, our proposed VSGN uses both the original video clip and its up-scaled counterpart, and takes advantage of their complementary properties to enhance their representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Graph neural networks for TAL</head><p>Graph neural networks (GNN) are a useful model for exploiting correlations in irregular structures <ref type="bibr" target="#b15">[16]</ref>. As they become popular in different computer vision fields <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b36">35]</ref>, researchers also find their application in temporal action localization <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b41">40]</ref>. G-TAD <ref type="bibr" target="#b39">[38]</ref> breaks the restriction of the temporal location of video snippets and uses a graph to aggregate features from snippets not located in a temporal neighborhood. It models each snippet as a node and snippet-snippet correlations as edges, and applies edge convolutions <ref type="bibr" target="#b34">[33]</ref> to aggregate features. BC-GNN <ref type="bibr" target="#b1">[2]</ref> improves localization by modelling the boundaries and con-tent of temporal proposals as nodes and edges of a graph neural network. P-GCN <ref type="bibr" target="#b41">[40]</ref> considers each proposal as a graph node, which can be combined with a proposal method to generate better detection results.</p><p>Compared to these methods, our VSGN builds a graph on video snippets as G-TAD, but differently, beyond modelling snippets from the same scale, VSGN exploits correlations between cross-scale snippets and defines a cross-scale edge to break the scaling curse. In addition, our VSGN contains multiple-level graph neural networks in a pyramid architecture whereas G-TAD only uses one scale. . It is comprised of three components: video self-stitching, crossscale graph pyramid network, scoring and localization, which will be elaborated in Sec. 3.2, 3.3, and 3.4, respectively. Before delving into the details, in Sec. 3.1 we first introduce our ideas behind these components to deal with the problem of short actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video Self-Stitching Graph Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">VSGN for Short Actions</head><p>Larger-scale clip. To solve the problem of short action scales, let us first think about how humans react when they find themselves interested in a short video clip that just fleeted away. They would scroll back to the clip and re-play it with a lower speed, by pause-and-play for example. We mimic this process when preparing a video before feeding it into a neural network. We propose to focus on a short period of a video, and magnify it along the temporal dimension to obtain a video clip of a larger temporal scale (VSS in <ref type="figure" target="#fig_0">Fig. 2</ref>, see Sec. 3.2 for details). A larger temporal scale, is not only able to retain more information through the network aggregation and pooling, but also associated with larger anchors which are easier to detect.</p><p>Multi-scale input. The magnification process may inevitably impair the information in the clip, thus the original video clip, which contains the original intact information, is also necessary. To take advantage of the complementary properties of both scales, we design a video stitching technique to piece them together as one single network input (VSS in <ref type="figure" target="#fig_0">Fig. 2</ref>, see Sec. 3.2 for details). This strategy enables the network to process both scales in one single pass, and the clip to have more positive anchors of different scales. It is also an effective way to augment the dataset.</p><p>Cross-scale correlations. The original clip and the magnified clip, albeit different, are highly correlated since they contain the same video content. If we can utilize their correlations and draw connections between their features, then the impaired information in the magnified clip can be rectified by the original clip, and the lost information in the original clip during pooling can be restored by the magnified clip. To this end, we propose a cross-scale graph pyramid network (xGPN in <ref type="figure" target="#fig_0">Fig. 2</ref>, see Sec. 3.3 for details), which aggregates features not only from the same scale but from cross scales, and which progressively enhances the features of both scales at multiple network levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Video Self-Stitching</head><p>The video self-stitching (VSS) component transforms a video into multi-scale input for the network. As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, it takes a video sequence, extracts snippet-level features, cuts into multiple short clips if it is long, up-scales each short clip along the temporal dimension, and stitches together each pair of original and up-scaled clips into one sequence. Please note that in addition to using VSS to gen-erate multi-scale input, we also directly use all original long videos as input in order to detect long actions as well.</p><p>Feature extraction ( <ref type="figure" target="#fig_2">Fig. 3 a)</ref>). Let us denote a video sequence as X = {x t } T t=1 ? R W ?H?T ?3 , where W ? H refers to the spatial resolution and T is the total number of frames. We use a feature encoding method (such as TSN <ref type="bibr" target="#b37">[36]</ref>, I3D <ref type="bibr" target="#b5">[6]</ref>) to extract its features on a snippet basis (one snippet is defined as ? consecutive video frames), namely, we generate one feature vector for each snippet and hence obtain a feature sequence denoted as F</p><formula xml:id="formula_0">= {f t } T /? t=1 ? R T /? ?C , where C feature dimension.</formula><p>Video cutting ( <ref type="figure" target="#fig_2">Fig. 3 b)</ref>). Suppose the requirement for our network input is L number of snippet</p><formula xml:id="formula_1">features F 0 = {f 0 t } L t=1 ? R L?C .</formula><p>We define a short clip as those that contain no more than ?L snippets, where 0 &lt; ? &lt; 1 is called a short factor. If a video is longer than ?L, we need to cut it to into multiple short clips; otherwise, we directly use the whole sequence without cutting. For training, we include as many actions as possible in one short clip, and shift the clip boundary inward to exclude the boundary actions that are cut in halves. If an action is longer than ?L, we don't include it in the video self-stitching stage. Therefore the short clips may vary in length with the cutting position. For inference, we cut long sequences into fixed short clips of length ?L.</p><p>Clip up-scaling ( <ref type="figure" target="#fig_2">Fig. 3 c)</ref>). In order to obtain a larger scale, we magnify each short clip along the temporal dimension via an up-scaling strategy, such as linear interpolation <ref type="bibr" target="#b19">[20]</ref>. For a short clip, the up-scaling ratio depends on its own scale. Specifically, if a short clip contains M snippet features, then it is up-scaled to a length L ? G ? M , where G is a constant representing a gap length (see next paragraph). In other words, the up-scaled clip will fill in the remaining space in the network input F 0 . The shorter a clip is, the longer its up-scaled counterpart will be. This not only makes full use of the input space, but also put more focus on shorter clips.</p><p>Self-stitching ( <ref type="figure" target="#fig_2">Fig. 3 d)</ref>). Then we stitch the original short clip (Clip O) and the up-scaled clip (Clip U) into one single sequence. If we directly concatenate the two clips side by side, one issue arises that the network would easily mistake a stitched sequence for a long sequence and tend to generate predictions spanning across the the two clips. To address this issue, we devise a simple strategy: inserting a gap between the two clips, as shown in <ref type="figure" target="#fig_2">Fig. 3 d)</ref>. We simply fill in zeros in the gap to make the network learn to distinguish a long sequence and a stitched sequence by identifying the zeros. This turns out a highly effective approach (see Sec. 4.3).</p><p>The entire self-stitching process is formulated as follows </p><formula xml:id="formula_2">F 0 = {f t } S+M t=S || L {0} G || L I L?G?M {f t } S+M t=S ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-Scale Graph Pyramid Network</head><p>Inspired by FPN <ref type="bibr" target="#b20">[21]</ref>, which computes multi-scale features with different levels, we propose a cross-scale graph pyramid network (xGPN). It progressively aggregates features from cross scales as well as from the same scale at multiple network levels via a hybrid module of a temporal branch and a graph branch. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our xGPN is composed of a multi-level encoder pyramid and a multi-level decoder pyramid, which are connected by a skip connection <ref type="bibr" target="#b12">[13]</ref> at each level. Each encoder level contains a cross-scale graph network (xGN), deeper levels having smaller temporal scales; each decoder level contains an upscaling network comprised of a de-convolutional <ref type="bibr" target="#b40">[39]</ref> layer, deeper levels having larger temporal scales.</p><p>Cross-scale graph network. The xGN module contains a temporal branch to aggregate features in a temporal neighborhood, and a graph branch to aggregate features from intra-scale and cross-scale locations. Then it pools the aggregated features into a smaller temporal scale. Its architecture is illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>. The temporal branch contains a Conv1d (3, 1) 1 layer. In the graph branch, we build a graph on all the features from both Clip O and Clip U, and apply edge convolutions <ref type="bibr" target="#b34">[33]</ref> for feature aggregation.</p><p>Graph building. We denote a directed graph as G = {V, E}, where V = {v t } J t=1 are the nodes, and E = {E t } J t=1 are the edges pointing to each node. Suppose we have input features F i = {f i t } J t=1 ? R J?C at the i th level. We build such a directed graph that each node v t is a feature f i t and each node has K inward edges formulated as</p><formula xml:id="formula_3">E t = {(v t k , v t ) |1 ? k ? K, t k = t}.</formula><p>The edges fall into one of the following two categories: free edges and crossscale edges.</p><p>We illustrate these two types of edges in <ref type="figure" target="#fig_4">Fig. 4</ref>. We make K/2 edges of a node free edges, which are only determined based on feature similarity between nodes, without considering the source clips. We measure the feature similarity between two nodes v t and v s using negative mean square error (MSE), formulated as ? f i t ? f i s 2 2 /C. As long as a node is among a target node's top K/2 closest neighbors in terms of feature similarity, it has a free edge pointing to the target node. Since free edges have no clip restriction, it can connect features inside a scale or cross scales. We make the other K/2 edges cross-scale edges, which only connect nodes from different clips, meaning that nodes from Clip O can only have cross-scale edges with nodes from Clip U and vice versa. Given a target node, we pick from those nodes satisfying this condition the top K/2 in terms of feature similarity after excluding those that already have free edges with the target node. These cross-scale edges enforce correlations between the stitched two clips of different scales. It enables the two scales to exchange information and mutually enhance representation with their complementary properties. In addition, since it enables edges from beyond a node's temporal vicinity, it resolves the scaling curse (see Sec. 2.2) of using graph networks on interpolated features.</p><p>Feature aggregation. With all the edges of a node f i t , we perform edge convolution operations <ref type="bibr" target="#b34">[33]</ref> to aggregate features of all its correlated nodes. Specifically, we first concatenate the target node f i t with each of its correlated node f i t k , 1 ? k ? K, and apply a multi-layer perceptron (MLP) with weight matrix W = {w c } C c=1 ? R 2C?C to transform each concatenated feature. Then, we take the maximum value in a channel-wise fashion to generate the aggregated featuref i t . This process is formulated as</p><formula xml:id="formula_4">f i t = max t k ?{s|(vs,vt)?Et} f i t k || C f i t T W,<label>(2)</label></formula><p>where || C is concatenation along the channel dimension. We fuse the aggregated features from the graph branch and those from the temporal branch by feature summation. Finally, we generate the features for the next level i+1 after applying activation and pooling. This is formulated as</p><formula xml:id="formula_5">F i+1 = ? max ? F i +F i ,<label>(3)</label></formula><p>whereF i = {f i t } J t is the aggregated features of the graph branch,F is the output of the temporal branch, ? is the rectified linear unit (ReLU), and ? max refers to max pooling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Scoring and Localization</head><p>As shown in the scoring and localization component of <ref type="figure" target="#fig_0">Fig. 2</ref>, we use four modules to predict action locations and scores. In the top area, the location prediction module (M loc ) and the classification (M cls ) module make a coarse prediction directly from each decoder pyramid level. In the bottom area, the boundary adjustment module (M adj ) and the supplementary scoring module (M scr ) further improve the start/end locations and scores of each predicted segment from the top two modules.</p><p>M loc and M cls each contain 4 blocks of Conv1d (3, 1), group normalization (GN) <ref type="bibr" target="#b35">[34]</ref> and ReLU layers, followed by one Conv1d <ref type="figure" target="#fig_3">(1, 1)</ref> to generate the location offsets and the classification scores for each anchor segment <ref type="bibr" target="#b38">[37]</ref>. Here we use pre-defined anchor segments for M loc , whereas for M cls we update the anchor segments by applying their predicted offsets from the M loc module (we use the same mechanism to update the segment boundaries with predicted offsets as in <ref type="bibr" target="#b38">[37]</ref>). These two modules are shared by all the decoder levels.</p><p>To further improve the boundaries generated from M loc , we design M adj inspired by FGD in <ref type="bibr" target="#b22">[23]</ref>. For each updated anchor segment from the M loc , we sample 3 features from around its start and end locations, respectively. Then we temporally concatenate the 3 feature vectors from each location and apply Conv1d(3,1)-ReLU-Conv1d(1,1) to predict start/end offsets. The anchor segment is further adjusted by adding the two offsets to the start and end locations respectively. M scr , comprised of a stack of Conv1d(3,1), ReLU and Conv1d(1,1), predicts actionness/startness/endness scores <ref type="bibr" target="#b19">[20]</ref> for each sequence.</p><p>In training, we use a multi-task loss function based on the output of the four modules, which is formulated as</p><formula xml:id="formula_6">L = L loc + ? cls L cls + ? adj L adj + ? scr L scr ,<label>(4)</label></formula><p>where L loc , L cls , L adj and L scr are the losses corresponding to the four modules, respectively, and ? cls , ? adj , ? scr are their corresponding tradeoff coefficients, which are all empirically set to 1. The losses L loc and L adj are computed based on the distance between the updated / adjusted anchor segments and their corresponding ground-truth actions, respectively. To represent the distance, we adopt the generalized intersection-over-union (GIoU) <ref type="bibr" target="#b28">[28]</ref> and adapt it to the temporal domain. For L cls , we use focal loss <ref type="bibr" target="#b21">[22]</ref> between the predicted classification scores and the ground-truth categories. L scr is computed the same way as the TEM losses in <ref type="bibr" target="#b19">[20]</ref>. To determine whether an anchor segment is positive or negative, we calculate its temporal intersection-over-union (tIoU) with all ground-truth action instances, and use tIoU thresholds 0.6 for L loc and L cls , and 0.7 for L adj .</p><p>In inference, the score of each predicted segment ? = (t s , t e , s) is computed with the confidence score c ? from M cls , the startness probabilities p s and the endness probability p e from M scr , formulated as s = c ? ? p s (t s ) ? p e (t e ).</p><p>In inference, we use the predictions from both Clip O and Clip U. For predictions from Clip U, we shift the boundaries of each detected segments to the beginning of the sequence and down-scale them back to the original scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Setup</head><p>Datasets and evaluation metric. We present our experimental results on two representative datasets THUMOS-14 <ref type="bibr" target="#b13">[14]</ref> and ActivityNet-v1.3 <ref type="bibr" target="#b4">[5]</ref>. THUMOS-14 contains 413 temporally annotated untrimmed videos with 20 action categories, in which 200 videos are for training and 213 videos for validation. ActivityNet-v1.3 has 19994 temporally annotated untrimmed videos in 200 action categories. For both datasets, we use mean Average Precision (mAP) at different tIoU thresholds as the evaluation metric. On THUMOS-14, we use tIoU thresholds {0.3, 0.4, 0.5, 0.6, 0.7}; on ActivityNet-v1.3, we choose 10 values in the range [0.5, 0.95] with a step size 0.05 as tIoU thresholds following the official evaluation practice.</p><p>Implementation Details. In order to achieve higher performance, some works directly process video frames and learn features for the task of temporal action localization (TAL) in an end-to-end fashion <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b22">23]</ref>. However, this has humongous requirements for GPU memory and computational power. Instead, we follow the practice of using offthe-shelf pre-extracted features, without further finetuning on the target TAL task <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b1">2]</ref>. For THUMOS, we sample at original frame rate of each video and pre-extract features using the two-stream network TSN <ref type="bibr" target="#b37">[36]</ref> trained on Kinects <ref type="bibr" target="#b14">[15]</ref>. For ActivityNet, we evaluate on two different types of features: TSN features at 5 fps and I3D <ref type="bibr" target="#b5">[6]</ref> features at 1.5 fps (both networks are trained on Kinetics <ref type="bibr" target="#b14">[15]</ref>).</p><p>We use an input sequence length L = 1280, a channel dimension C = 256 throughout the network and a short factor ? = 0.4. We have 5 levels in the encoder and decoder pyramids respectively, with lengths L/2 (l+1) , where 1 ? l ? 5 is the level index. For each level, we have 2 different anchor sizes {s 1 ? 2 (l?1) , s 2 ? 2 (l?1) }, where s 1 , s 2 are 4, 6 for THUMOS and 32, 48 for ActivityNet. The number of edges for each node is K = 10, and the gap is G = 30.</p><p>The training batch size is 32 for both datasets. We train 10 epochs at learning rate 0.00005 for THUMOS and 15 epochs at learning rate 0.0001 for ActivityNet. We directly predict the 20 action categories for THUMOS, and conduct binary classification and then fuse our prediction scores with video-level classification scores from <ref type="bibr" target="#b37">[36]</ref> for ActivityNet following <ref type="bibr" target="#b19">[20]</ref>. In post-processing, we apply soft-NMS <ref type="bibr" target="#b3">[4]</ref> to suppress redundant predictions, and keep 200 predictions for THUMOS and 100 predictions for Ac-tivityNet for final evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-Art</head><p>We demonstrate the TAL performance of our proposed VSGN compared to recent representative methods in the literature on the two datasets in <ref type="table" target="#tab_0">Table 1 and Table 2</ref>. On both datasets, VSGN achieves state-of-the-art performance, reaching mAP 52.4% at tIoU 0.5 on THUMOS and average mAP 35.07% on ActivityNet. It significantly outperforms all other methods that use the same features. More remarkably, our VSGN which uses pre-extracted features without further finetuning, is on par with and even better than concurrent methods that finetune features end to end for TAL.</p><p>Besides evaluating all actions in general, we also provide average mAPs of short actions for VSGN as well as other methods that have detection results available. Here, we refer to action instances that are shorter than 30 seconds as short actions. On ActivityNet, there are 54.4% short actions, whereas on THUMOS, there are 99.7% short actions. We can see that our performance gains on short actions over other methods are even more evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We provide ablation study for the key components VSS and xGPN in VSGN to verify their effectiveness on the two datasets in <ref type="table">Table 3</ref> and 4, respectively. The baselines are implemented by replacing each xGN module in xGPN with a layer of Conv <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b1">2)</ref> and ReLU, and not using cutting, upscaling and stitching in VSS.</p><p>Video self-stitching (VSS). From both datasets, VSS shows its effectiveness in improving short actions whether used with or without xGPN. For THUMOS, because most actions are short, the overall performance also has a boost with VSS. For ActivityNet, VSS sacrifices long actions since it reduces the bias towards long actions with more short training samples. We design xGPN to mitigate this effect. <ref type="table">Table 2</ref>. Action localization results on validation set of ActivityNet-v1.3, measured by mAPs (%) at different tIoU thresholds and the average mAP. Our VSGN achieves the state-of-the-art average mAP and the highest mAP for short actions. Note that our VSGN, which uses pre-extracted features without further finetuning, significantly outperforms all other methods that use the same pre-extracted features. It is even on par with concurrent methods that finetune the features on ActivityNet for TAL end to end.</p><p>Method 0.5 0.75 0.95 Average Short End-to-end learned/finetuned on ActivityNet for TAL CDC <ref type="bibr" target="#b29">[29]</ref> 45. <ref type="bibr">30 26</ref>.00 0.20 23.80 -R-C3D <ref type="bibr" target="#b38">[37]</ref> 26.80 ----PBRNet <ref type="bibr" target="#b22">[23]</ref> 53. <ref type="bibr">96</ref>   Cross-scale graph pyramid network (xGPN). From <ref type="table">Table 3</ref> and 4, we can see that xGPN obviously improves the performance of short actions as well as the overall performance. On the one hand, xGPN utilizes long-range correlations in multi-level features and benefits actions of various lengths. On the other hand, xGPN enables exploitation of cross-scale correlations when used with VSS, thus further enhancing short actions.</p><p>Clip O and Clip U. In <ref type="table">Table 5</ref>, we compare the per- formance when generating predictions only from Clip O, only from Clip U, and from both Clip O and Clip U with the same well-trained VSGN model. We can see that the two clips still result in different performance even after their features are aggregated throughout the network. Clip O is better at lower tIoU thresholds, whereas Clip U has advantage at a higher tIoU threshold. Combining both predictions can take advantage of the complementary properties of both clips and results in higher performance than either of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Observations of xGPN</head><p>In <ref type="table">Table 6</ref>, we compare VSGN to the model of only using the xGN modules at certain encoder levels. When we only use xGN in one level, having it in the middle level achieves the best performance. Our VSGN uses xGN for all encoder levels, which achieves the best performance. In <ref type="table">Table 7</ref>, we compare the mAP of using different edge types in xGN. Our proposed VSGN uses the top K/2 edges as free edges, and then chooses K/2 cross-scale edges from the rest. The performance drops if we only use K free edges or K crossscale edges. K cross-scale edges is better than K free edge, showing the effectiveness of using cross-scale edges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Computational Complexity</head><p>We compare the inference time of different methods on the ActivityNet validation set on a 1080ti GPU in <ref type="table" target="#tab_4">Table 8</ref>. Compared to end-to-end frameworks such as PBRNet, the methods using pre-extracted features such as BMN, G-TAD and VSGN can re-use the features extracted for other tasks (e.g., action recognition). Moreover, these methods do not introduce complex 3D convolutions in the TAL architecture, therefore, they have obviously lower inference time. Our VSGN has negligible computation in VSS, and has similar cost in xGPN to the GNNs in G-TAD. Addtionally, it uses fewer anchors (1240 vs 4950), and does not have the stage of ROIAlign, so it runs faster than G-TAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, to tackle the challenging problem of large scale variation in the temporal action localization (TAL) problem, we target short actions and propose a multi-level cross-scale solution called video self-stitching graph network (VSGN). It contains a video self-stitching (VSS) component that generates a larger-scale clip and stitches it with the original-scale clip to utilize the complementary properties of different scales. It has a cross-scale graph pyramid network (xGPN) to aggregate features from across different scales as well as from the same scale. This is the first work to focus on the problem of short actions in TAL, and has achieved significant improvement on short action performance as well as overall performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of the proposed video self-stitching graph network (VSGN). Its takes a video sequence and generates detected actions with start/end time as well as their categories. It has three components: video self-stitching (VSS), cross-scale graph pyramid network (xGPN), and scoring and localization (SoL). VSS contains four steps to prepare a video sequence as our network input (red dashed box, seeFig. 3for details). xGPN is composed of multi-level encoder and decoder pyramids. The encoder aggregates features in different levels via a stack of cross-scale graph networks (xGN) (yellow trapezoid area, seeFig. 4for details); the decoder restores the temporal resolution and generates multi-level features for detection. SoL contains four modules, the top two predicting action scores and boundaries, the bottom two producing supplementary scores and adjusting boundaries (blue box area).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>demonstrates the overall architecture of our proposed Video self-Stitching Graph Network (VSGN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Video self-stitching (VSS). a) Snippet-level features are extracted for the entire video. b) Long video is cut into multiple short clips. c) Each video clip is up-scaled along the temporal dimension. d) Original clip (green dots) and up-scaled clip (orange dots) are stitched into one feature sequence with a gap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>where 1 ?</head><label>1</label><figDesc>S, S + M ? T /? . S and S + M are the cutting boundaries, || L denotes the concatenation operation along the temporal dimension, I L?G?M means interpolation with a target length L ? G ? M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Cross-scale graph network (xGN). Top: temporal branch; bottom: graph branch. The two branches are fused by addition, followed by an activation function and pooling. Each dot represents a feature, green dots from Clip O and orange dots from Clip U. In the graph branch, blue arrows represent free edges and the purple arrow represents a cross-scale edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Action detection results on test set of THUMOS-14, measured by mAP (%) at different tIoU thresholds. Our VSGN achieves the highest mAP for tIoU threshold 0.5 (commonly adopted criteria), significantly outperforming all other methods. Re-implementation with the same features as ours. We replace 3D convolutions with 1D convolutions to adapt to the feature dimension.</figDesc><table><row><cell>Method</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>Short</cell></row><row><cell cols="7">End-to-end learned/finetuned on THUMOS for TAL</cell></row><row><cell>TCN [8]</cell><cell>-</cell><cell cols="3">33.3 25.6 15.9</cell><cell>9.0</cell><cell>-</cell></row><row><cell>R-C3D [37]</cell><cell cols="3">44.8 35.6 28.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PBRNet [23]</cell><cell cols="5">58.5 54.6 51.3 41.8 29.5</cell><cell>-</cell></row><row><cell></cell><cell cols="4">Pre-extracted features</cell><cell></cell><cell></cell></row><row><cell>TAL-Net [7]</cell><cell cols="5">53.2 48.5 42.8 33.8 20.8</cell><cell>-</cell></row><row><cell>P-GCN [40]</cell><cell cols="3">63.6 57.8 49.1</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>I.C&amp;I.C [42]</cell><cell cols="5">53.9 50.7 45.4 38.0 28.5</cell><cell>49.1</cell></row><row><cell>MGG [25]</cell><cell cols="5">53.9 46.8 37.4 29.5 21.3</cell><cell>-</cell></row><row><cell>BSN [20]</cell><cell cols="5">53.5 45.0 36.9 28.4 20.0</cell><cell>-</cell></row><row><cell>DBG [18]</cell><cell cols="5">57.8 49.4 39.8 30.2 21.7</cell><cell>-</cell></row><row><cell>BMN [19]</cell><cell cols="5">56.0 47.4 38.8 29.7 20.5</cell><cell>-</cell></row><row><cell>G-TAD [38]</cell><cell cols="5">54.5 47.6 40.2 30.8 23.4</cell><cell>44.2</cell></row><row><cell>BC-GNN [2]</cell><cell cols="5">57.1 49.1 40.4 31.2 23.1</cell><cell>-</cell></row><row><cell cols="6">PBRNet  *  [23] 54.8 49.2 42.3 33.1 23.0</cell><cell>43.6</cell></row><row><cell>VSGN (ours)</cell><cell cols="5">66.7 60.4 52.4 41.0 30.4</cell><cell>56.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Re-implementation with the same features as ours. We replace 3D convolutions with 1D convolutions to adapt to the feature dimension.</figDesc><table><row><cell></cell><cell>34.97 8.98</cell><cell>35.01</cell><cell>-</cell></row><row><cell></cell><cell>Pre-extracted I3D features</cell><cell></cell><cell></cell></row><row><cell>TAL-Net [7]</cell><cell>38.23 18.30 1.30</cell><cell>20.22</cell><cell>-</cell></row><row><cell>P-GCN [40]</cell><cell>48.26 33.16 3.27</cell><cell>31.11</cell><cell>-</cell></row><row><cell>I.C &amp; I.C [42]</cell><cell>43.47 33.91 9.21</cell><cell>30.12</cell><cell>14.8</cell></row><row><cell cols="2">PBRNet  *  [23] 51.32 33.33 7.09</cell><cell>33.08</cell><cell>17.6</cell></row><row><cell>VSGN (ours)</cell><cell>52.32 35.23 8.29</cell><cell>34.68</cell><cell>18.8</cell></row><row><cell></cell><cell>Pre-extracted TSN features</cell><cell></cell><cell></cell></row><row><cell>BSN [20]</cell><cell>46.45 29.96 8.02</cell><cell>30.03</cell><cell>15.0</cell></row><row><cell>BMN [19]</cell><cell>50.07 34.78 8.29</cell><cell>33.85</cell><cell>15.2</cell></row><row><cell>G-TAD [38]</cell><cell>50.36 34.60 9.02</cell><cell>34.09</cell><cell>17.5</cell></row><row><cell>BC-GNN [2]</cell><cell>50.56 34.75 9.37</cell><cell>34.26</cell><cell>-</cell></row><row><cell cols="2">PBRNet  *  [23] 51.41 34.35 8.66</cell><cell>33.90</cell><cell>18.0</cell></row><row><cell>VSGN (ours)</cell><cell>52.38 36.01 8.37</cell><cell>35.07</cell><cell>19.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Effectiveness of VSGN components for THUMOS-14. VSS is highly effective for short actions and xGPN further improves the overall performance. Short 56.78 50.11 42.54 31.14 19.93 45.1 61.41 55.16 45.52 33.43 21.32 48.7 63.77 58.66 50.24 39.44 28.36 53.4 66.69 60.37 52.45 40.98 30.40 56.6 Effectiveness of VSGN components for ActivityNet-v1.3. VSS is highly effective for short actions. xGPN benefits actions of different lengths and improve the overall performance. Baseline VSS xGPN 0.5 0.75 0.95 Avg. Short 51.23 34.91 8.53 34.25 17.5 51.67 35.17 9.79 34.70 18.3 50.87 33.99 9.09 33.79 19.7 52.38 36.01 8.37 35.07 19.9</figDesc><table><row><cell>Baseline VSS xGPN 0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .Table 7 .</head><label>567</label><figDesc>Complementary properties of Clip O and Clip U. Combining predictions from both clips results in higher performance than either of them. Predictions from 0.5 0.75 0.95 Avg. Short Clip O 52.26 36.03 7.98 34.96 19.3 Clip U 51.80 34.79 8.68 34.32 19.3 Clip O + Clip U 52.38 36.01 8.37 35.07 19.9 xGN levels in xGPN. We show the mAPs (%) at different tIoU thresholds, average mAP as well as mAP for short actions (less than 30 seconds) when using xGN at different xGPN encoder levels. The levels in the columns with use xGN and the ones in the blank columns use a Conv1d layer instead. Edge types of each xGN node. We show the mAPs (%) at different tIoU thresholds 0.5, 0.75, 0.95, average mAP as well as mAP for short actions (less than 30 seconds) when using different types of edges in xGN. Edge types 0.5 0.75 0.95 Avg. Short K free 51.59 35.23 7.77 34.48 19.0 K cross-scale 52.33 35.79 7.91 34.75 19.7 K/2 free + K/2 cross-scale 52.38 36.01 8.37 35.07 19.9</figDesc><table><row><cell>xGN levels</cell><cell cols="2">mAP (%) at tIoU threshold mAP (%)</cell></row><row><cell cols="2">1 2 3 4 5 0.5 0.75 0.95 Avg.</cell><cell>Short</cell></row><row><cell></cell><cell>51.22 34.14 8.22 33.82</cell><cell>19.5</cell></row><row><cell></cell><cell>51.92 34.45 8.89 34.17</cell><cell>19.6</cell></row><row><cell></cell><cell>51.61 34.94 9.26 34.46</cell><cell>19.2</cell></row><row><cell></cell><cell>51.10 34.83 8.90 34.19</cell><cell>19.3</cell></row><row><cell></cell><cell>51.10 34.68 8.50 34.03</cell><cell>19.0</cell></row><row><cell></cell><cell>52.38 36.01 8.37 35.07</cell><cell>19.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>Inference time of ActivityNet validation set. Method PBRNet PBRNet * BMN G-TAD VSGN Our re-implementation using the same pre-extracted features.</figDesc><table><row><cell>Time (sec) 1600</cell><cell>128</cell><cell>120</cell><cell>183</cell><cell>158</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For conciseness, we use Conv1d (m, n) to represent 1-D convolutions with kernel size m and stride n.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnosing error in temporal action detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno>abs/1807.10706</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01432</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Soft-nms -improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Temporal localization of moments in video collections with natural language. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7029" to="7038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02739</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Mesh r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5804" to="5813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sudheendra Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
	</analytic>
	<monogr>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive boundary refinement network for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11612" to="11619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haifang Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository (CoRR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Streamlined dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jonghwan Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6581" to="6590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Borderdet: Border feature for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Amir Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mingxing Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10778" to="10787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9626" to="9635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sideaware boundary localization for more precise object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1912.04260</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Point clouds learning with attention-based graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13445</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV</title>
		<meeting>the IEEE international conference on computer vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="10156" to="10165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Matthew D Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03252</idno>
		<title level="m">Graph convolutional networks for temporal action localization</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchorfree detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9756" to="9765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
