<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Spatial Awareness to Improve Crowd Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Qi</forename><surname>Cheng</surname></persName>
							<email>zhiqic@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Southwest Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiu</forename><surname>Li</surname></persName>
							<email>lijunxiu@my</email>
							<affiliation key="aff0">
								<orgName type="institution">Southwest Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
							<email>wuxiaohk@home.swjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Southwest Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Spatial Awareness to Improve Crowd Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aim of crowd counting is to estimate the number of people in images by leveraging the annotation of center positions for pedestrians' heads. Promising progresses have been made with the prevalence of deep Convolutional Neural Networks. Existing methods widely employ the Euclidean distance (i.e., L 2 loss) to optimize the model, which, however, has two main drawbacks: (1) the loss has difficulty in learning the spatial awareness (i.e., the position of head) since it struggles to retain the high-frequency variation in the density map, and (2) the loss is highly sensitive to various noises in crowd counting, such as the zeromean noise, head size changes, and occlusions. Although the Maximum Excess over SubArrays (MESA) loss has been previously proposed by [16] to address the above issues by finding the rectangular subregion whose predicted density map has the maximum difference from the ground truth, it cannot be solved by gradient descent, thus can hardly be integrated into the deep learning framework. In this paper, we present a novel architecture called SPatial Awareness Network (SPANet) to incorporate spatial context for crowd counting. The Maximum Excess over Pixels (MEP) loss is proposed to achieve this by finding the pixel-level subregion with high discrepancy to the ground truth. To this end, we devise a weakly supervised learning scheme to generate such region with a multi-branch architecture. The proposed framework can be integrated into existing deep crowd counting methods and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that our method can significantly improve the performance of baselines. More remarkably, our approach outperforms the state-of-the-art methods on all benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem of crowd counting is described in <ref type="bibr" target="#b15">[16]</ref>. Different from visual object detection, it is impossible to provide bounding boxes for all pedestrians due to the extremely dense crowds. On the other side, when only the total crowd * indicates equal contribution. This work was done when Zhi-Qi Cheng and Jun-Xiu Li were visiting at Microsoft Research. Xiao Wu is the corresponding author. <ref type="figure">Figure 1</ref>: The L 2 loss function has difficulty in learning the spatial awareness and is sensitive to various noises in crowd counting, which will lead to a lower estimation in high-density regions (the first row of each example), and a higher estimation in low-density regions (the second row of each example). Note that the corresponding improvements of our method are shown in <ref type="figure">Figure 5</ref>.</p><p>counts of the images are provided, the training process will become notably difficult since the spatial awareness is completely ignored. Therefore, to preserve as many spatial constraints as possible and reduce annotation cost, the previous work <ref type="bibr" target="#b15">[16]</ref> started to only provide center points of heads and utilizes Gaussian distribution to generate ground truth density maps. It is worth noting that this annotation scheme is widely adopted by subsequent studies.</p><p>Existing crowd counting approaches mainly focus on improving the scale invariance of feature representation, including the multi-column networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b5">6]</ref>, scale aggregation modules <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">47]</ref>, and scale-invariant networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b45">45]</ref>. Despite the architectures of these methods are different, the L 2 loss function is employed by most of them. As a result, the spatial awareness in crowd image is largely ignored, though more scale information is embedded into their features.</p><p>We have examined three state-of-the-art approaches (i.e., MCNN <ref type="bibr" target="#b52">[52]</ref>, CSRNet <ref type="bibr" target="#b16">[17]</ref>, and SANet <ref type="bibr" target="#b2">[3]</ref>) on four crowd counting datasets (i.e., ShanghaiTech <ref type="bibr" target="#b52">[52]</ref>, UCF CC 50 <ref type="bibr" target="#b10">[11]</ref>, WorldExpo'10 <ref type="bibr" target="#b48">[48]</ref>, and UCSD <ref type="bibr" target="#b3">[4]</ref>). Two examples are shown in <ref type="figure">Figure 1</ref>. Similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, we observe that dense-crowd regions are usually underestimated, while sparse-crowd regions are overestimated. Such phenomenon is due to two main factors. First, the pixelwise L 2 loss struggles to retain the high-frequency variation arXiv:1909.07057v1 [cs.CV] <ref type="bibr" target="#b15">16</ref> Sep 2019 in the density map: minimizing L 2 loss encourages finding pixel-wise averages of plausible solutions which are typically overly-smooth and thus have poor spatial awareness <ref type="bibr" target="#b14">[15]</ref>. Second, L 2 loss is highly sensitive to typical noises in crowd counting, including the zero-mean noise, head size changes, and head occlusions. We take a simple statistics and show that the co-occurrence of zero-mean noise and overestimation could reach 96% (6,776 out of 7,044 testing images). We further find that almost all estimated density maps inaccurately predict the head positions or sizes when occlusion occurs, which could result in underestimation in high-density areas. Moreover, the generated ground truth density could also be imprecise due to the annotation error and the fixed variance in Gaussian kernel. It is noted that the corresponding improvements of our method are illustrated in <ref type="figure">Figure 5</ref>.</p><p>To fully utilize the spatial awareness, previous work <ref type="bibr" target="#b15">[16]</ref> proposes a loss named Maximum Excess over SubArrays (MESA) to handle the above problems. Generally speaking, MESA loss attempts to find the rectangular subregion whose predicted density map has the maximum difference from the ground truth. It directly optimizes the counts of this subregion instead of the pixel-level density. Since the set of subregions could include the full image, MESA loss is an upper bound for the count estimation of the entire image. Besides, this loss is only sensitive to the spatial layout of pedestrians and is robust to various noises. However, the complexity of MESA loss function is extremely high. <ref type="bibr" target="#b15">[16]</ref> utilizes Cutting-Plane optimization to obtain an approximate solution. Since this method cannot be solved by the conventional gradient descent, MESA loss has not been employed in any existing CNN-based approach.</p><p>Motivated by the MESA loss, in this paper we present a novel deep architecture called SPatial Awareness Network (SPANet) to retain the high-frequency spatial variations of density. Instead of finding the mismatched rectangular subregion as in MESA, the Maximum Excess over Pixels (MEP) loss is proposed to optimize the pixel-level subregion which has high discrepancy to the ground truth density map. To obtain such pixel-level subregion, the weakly-supervised ranking information <ref type="bibr" target="#b22">[23]</ref> is exploited to generate a mask indicating the pixels with high discrepancies. We further devise a multi-branch architecture to leverage the full image for discrepancy detection by imitating the salience region detection <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b54">54]</ref>, where patches with increasing areas are used for ranking. The proposed framework could be easily integrated into existing CNN-based methods and is end-to-end trainable.</p><p>The main contribution of this work is the proposed Spatial Awareness Network and Maximum Excess over Pixels loss for addressing the issue of crowd counting. The solution also provides the elegant views of what kind of spatial context should be exploited and how to effectively utilize such spatial awareness in crowd images, which are problems not yet fully understood in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Detection-based Methods</head><p>The methods in this category use object detector to locate people in images. Given the individual localization of each people, crowd counting becomes trivial. There are two directions in this line, i.e., detection on 1) whole pedestrians <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b53">53]</ref> and 2) parts of pedestrians <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b43">43]</ref>. Typically, local features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref> are first extracted and then are exploited to train various detectors (e.g., SVM <ref type="bibr" target="#b17">[18]</ref> and AdaBoost <ref type="bibr" target="#b41">[41]</ref>). Though spatial information is well learned in these methods, they are not applicable in challenging situations, such as the high-density clogging crowds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Regression-based Methods</head><p>Different from detection-based methods, regressionbased approaches avoid the hard detection problem and estimate crowd counts from image features. Earlier methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref> usually predict the counts directly from the features, which will lead to poor performance as the spatial awareness is completely ignored. Later methods try to estimate the density map for counting <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>, where the crowd count is obtained by integrating all pixel values over the density map. Though learning the density map somewhat provides the spatial information, their models still have difficulties in preserving the high-frequency variation in the density map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">CNN-based Methods</head><p>Deep CNN based crowd counting methods have shown very strong performance improvements over the shallow learning counterparts. Existing methods mainly focus on coping with the large variation in pedestrian scales, where many multi-column networks are extensively studied. A dual-column network is proposed by <ref type="bibr" target="#b0">[1]</ref> to combine shallow and deep layers for estimating the count. Inspired by this work, a famous three-column network MCNN is proposed by <ref type="bibr" target="#b52">[52]</ref>, which employs different filters on separate columns to obtain features with various scales. Many works have improved MCNN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42]</ref> to further enhance the scale adaptation. Sam et al. <ref type="bibr" target="#b31">[32]</ref> introduce a switching structure, which uses a classifier to assign input image patches to appropriate columns. Recently, Liu et al. <ref type="bibr" target="#b18">[19]</ref> propose a multicolumn network to simultaneously estimate crowd density by detection and regression based models. Ranjan et al. <ref type="bibr" target="#b26">[27]</ref> utilize a two-column network to iteratively train their model with images of different resolution.</p><p>There are a lot of other attempts to further improve the scale invariance, including 1) study on the fusion of various scale information <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46]</ref>, 2) study on multiblob based scale aggregation networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">47]</ref>, 3) design of scale-invariant convolutional or pooling layers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b45">45]</ref>, and 4) study on the automated scale adaptive networks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">49]</ref>. Typically, Li et al. <ref type="bibr" target="#b16">[17]</ref> propose CSRNet that exploits dilated convolutional layers to enlarge receptive fields for boosting performance. Cao et al. <ref type="bibr" target="#b2">[3]</ref> propose SANet to aggregate multi-scale features for more accurate crowd count. These two approaches have achieved state-ofthe-art performance. Additionally, there also exist studies devoted to utilization of perspective maps <ref type="bibr" target="#b35">[35]</ref>, geometric constraints <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b51">51]</ref>, and region-of-interest (ROI) <ref type="bibr" target="#b19">[20]</ref> to improve the counting accuracy.</p><p>The aforementioned methods utilize the Euclidean distance, i.e. L 2 loss to optimize the model. Although these methods can obtain scale-invariant features, their performances are still unsatisfactory since the spatial awareness is largely ignored. Note that, SANet <ref type="bibr" target="#b2">[3]</ref> also tries to solve the problem of L 2 loss and adds local pattern consistency (L c loss) in the training phase. However, we find that L c still cannot learn the spatial context well. In our experiment, when integrating our MEP loss (L mep ) into SANet, we achieve significant performance improvement. Our proposed MEP loss could fully utilize the spatial awareness, which is a key factor for the task of crowd counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>In this section, we first review the problem of crowd counting and two loss functions (i.e., MESA loss and L 2 loss). Then we present the proposed SPANet and MEP loss in details. It is worth noting that our method can be directly applied to all CNN-based crowd counting networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Recent technologies define the crowd counting task as a density regression problem <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b52">52]</ref>. Given N images I = {I 1 , I 2 , ? ? ? , I N } as the training set, each image I i is annotated with a total of c i center points of pedestrians' heads P gt i = {P 1 , P 2 , ? ? ? , P ci }. Typically, the ground truth density map for each pixel p in image I i is defined as D gt,i ,</p><formula xml:id="formula_0">?p ? I i , D gt,i (p) = P ?P gt i N gt (p; ? = P, ? 2 ),<label>(1)</label></formula><p>where N gt is a Gaussian distribution. The number of people c i in image I i is equal to the sum of density values over all pixels as p?Ii D gt,i (p) = c i . With these training data, the aim of crowd counting task is to learn the predicted density map D pr towards the ground truth density map D gt . MESA loss. To make use of the spatial awareness in annotations (i.e., center head positions P gt ), the previous work <ref type="bibr" target="#b15">[16]</ref> has proposed the Maximum Excess over SubArrays (MESA) loss L mesa as follows, where B is the set of all potential rectangular subregions in image. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, MESA loss tries to find the box subregion whose predicted density map has the maximum difference from the ground truth. It can be treated as an upper bound for the count estimation of the entire image, as B could include the full image. Besides, this loss is directly related to the counting objective instead of the pixel-level density, and is only sensitive to the spatial layout of pedestrians. In the 1D case, Kolmogorov-Smirnov distance <ref type="bibr" target="#b23">[24]</ref> can be seen as a special case of L mesa . Despite the above merits, it is difficult to optimize the MESA loss due to the hard process of finding such subregion. One has to traverse all potential subregions to achieve this, which is obviously an impossible task in practical application. To solve it, previous approach <ref type="bibr" target="#b15">[16]</ref> converts the optimization of MESA loss to a convex quadratic program problem with limited constraints and utilizes Cutting-Plane optimization to obtain an approximate solution. However, since this method cannot be solved by the traditional gradient descent, MESA loss has not been exploited in any existing CNN-based crowd counting methods. L 2 loss. To facilitate the computation in deep frameworks, existing CNN-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b52">52]</ref> all directly use L 2 loss to minimize the difference between the estimated and ground truth density maps,</p><formula xml:id="formula_1">Lmesa D pr , D gt = 1 N N i=1 max B?B p?B D pr,i (p) ? p?B D gt,i (p) ,<label>(2)</label></formula><formula xml:id="formula_2">L 2 D pr , D gt = 1 2N N i=1 p?D pr,i D pr,i (p) ? D gt,i (p) 2 2 . (3)</formula><p>However, as discussed in Sec. 1, we reveal that L 2 loss can hardly retain the high-frequency variation in the density map, leading to the poor spatial awareness. Furthermore, it is also highly sensitive to typical noises in crowd counting, including the zero-mean noise, head size changes, and head occlusions. For example, existing methods always overestimate the density value in low-density areas and underestimate it in high-density regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Awareness Network</head><p>The proposed Spatial Awareness Network (SPANet) aims to leverage the spatial context for accurately predicting the density values. Instead of searching the mismatched <ref type="figure">Figure 3</ref>: The framework of our proposed SPatial Awareness Network (SPANet). The input images are first fed into the backbone network to extract feature representations and output the estimated density maps D pr . A K-branch architecture is devised. In each branch k, the network is optimized with the ranking objective by sampling two patches (one is sub-patch of the other) and outputs a new density mapD pr k . Then the two density maps are utilized to produce the subregion S k which has high discrepancy to the ground truth. The density values within the generated S k is erased in next branch to facilitate the latter optimization. In the end, K subregions from K branches are fused to form the final pixel-level subregion S, which is exploited to calculate the Maximum Excess over Pixels (MEP) loss. rectangular subregion as in MESA loss, which is the main obstacle for optimization, we try to find the pixel-level subregion S which has high discrepancy to the ground truth density map. Since there is not any annotation of such region, this problem is unsupervised and will still be significantly difficult to solve. Inspired by the recent weaklysupervised method <ref type="bibr" target="#b22">[23]</ref>, we exploit an obvious ranking relation to achieve this, i.e., one patch of a crowded scene image is guaranteed to contain the same number or fewer persons than the original image. By sampling a pair of patches (where one is the sub-patch of the other), the network is optimized with the ranking objective and outputs a new density map, which is in turn utilized to produce the subregion with high discrepancy, together with the previous one. We further devise a multi-branch architecture to leverage the full image by sampling multiple pairs of patches. Note that the whole SPANet could be end-to-end trained. <ref type="figure">Figure 3</ref> illustrates the framework of our proposed SPANet. Input images I are first fed into the backbone network to generate the predicted density maps D pr . The desired pixel-level subregion generation, i.e., S k , is conducted by branch k using a pair of patches sampled from density maps D pr . To leverage the full image for discrepancy detection, a multi-branch architecture with K branches is devised to produce multiple subregions by imitating the salience region detection <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b54">54]</ref>. Finally, K subregions (S 1 , S 2 , ..., S K ) are combined to produce the final S, which is then exploited to compute our proposed Maximum Excess over Pixels (MEP) loss. We will present these three sub-modules in details below.</p><p>Pixel-level Subregion Generation. The subregion S indicates the area with high density discrepancy to the ground truth. Unfortunately, directly subtracting the predicted D pr from the ground truth D gt would make the problem go round in circles -the bias is usually large enough to prevent it from providing accurate region. Consequently, we turn to find the region with high changes along with the network training. It is natural that one can pick two density maps of the same image from different iterations. However, the obtained area only reflects the region that is already "revised", which still seriously suffers from the poor spatial perception of the original L 2 loss. To this end, we exploit the weakly supervised ranking clues to produce the subregion. Instead of considering the pixel-level density, the ranking clue is directly related to the comparison of crowd counts.</p><p>In each branch k, two parallel image patches are first sampled. As the feature maps of deep convolutional layers already contain rich location information, we treat the sampling process as the mask pooling operation on the density map. The strategy of selecting patches will be described later. Without loss of generality, suppose the two masks M 1 k and M 2 k are the 2-dimensional matrix with 0 or 1 (1 indicates the patch area), and M 1 k is the sub-patch of M 2 k . The crowd counts C(M 1 k ) and C(M 2 k ) under the masks M 1 k and M 2 k could be obtained by integrating the values of density map over individual mask, which could be implemented as the mask pooling as follows,</p><formula xml:id="formula_3">C M 1 k = p?D pr k D pr k M 1 k , C M 2 k = p?D pr k D pr k M 2 k ,<label>(4)</label></formula><p>where is the element-wise product, and p indicates the pixel on density map D pr k . It is worth noting that we utilize the same predicted density map D pr k when calculating the counts for two masks, rather than generating individual maps at two consecutive iterations. The reason is that the density map D pr k is not restricted to be positive, thus pooling on the pair of patches could also provide the ranking information. We have conducted an experiment showing that the two schemes have similar results. Besides, directly pooling on the same map is more efficient than the other.</p><p>With the assumption that M 1 k is the sub-patch of M 2 k , the explicit constraint is that the number of people in M 1 k is fewer than that in M 2 k . Therefore, we employ a pairwise ranking hinge loss L r to model such relationship, which is formulated as</p><formula xml:id="formula_4">Lr C(M 1 k ), C(M 2 k ) = max 0, C(M 1 k ) ? C(M 2 k ) + ? ,<label>(5)</label></formula><p>where ? is a margin value that is set to the upper bound of the difference in the ground truth. The gradient of L r loss is calculated as</p><formula xml:id="formula_5">? Lr = ? ? ? 0, if C M 1 k ? C M 2 k + ? 0, ? C M 1 k ? ? C M 2 k , otherwise.<label>(6)</label></formula><p>Once the network parameters ? are updated with L r by back-propagation, the renewed density mapD pr k estimated by the network is computed b?</p><formula xml:id="formula_6">D pr k = Conv (I, ?) ,<label>(7)</label></formula><p>where I is the input image, and Conv(?) refers to a forward pass of the network. Given the updated density mapD pr k and the old one D pr k , the desired subregion S k is obtained by thresholding the difference D pr k between them, where D pr k = |D pr k ? D pr k |. To make it differentiable, we utilize a Sigmoid thresholding function, and S k is given by</p><formula xml:id="formula_7">S k = 1 1 + exp (?? ( D pr k ? ?)) ,<label>(8)</label></formula><p>where ? is a threshold matrix with all elements being ?. ? is the parameter to ensure that the value of S k is approximately equal to 1 when D pr k (p) &gt; ?, otherwise 0. Multi-branch Architecture. Note that in above section, only a pair of patches are sampled for generating the subregion. In principle, we hope that the full density map could be leveraged to provide more information. Instead of only sampling a small-large pair of patches, which may involve large bias error due to the large difference between two patches, we adopt a multi-branch architecture as shown in <ref type="figure">Figure 3</ref>. The bottom right corners of all patches are located at the same position, i.e., the bottom right corner of the density map. The area of patch is gradually enlarged along with the branches, until it reaches the size of full density map. Such design guarantees both the small bias error in each branch and the full utilization of training images.</p><p>To eliminate the influence of the detected subregion S k for better optimization in latter branches, we imitate the salience region detection <ref type="bibr" target="#b50">[50]</ref> to erase the density values within S k in next branch, which is formulated as</p><formula xml:id="formula_8">D pr k+1 = D pr k+1 (1 ? S k ),<label>(9)</label></formula><p>where 1 is the matrix with all elements being 1, and is the element-wise product.</p><p>Maximum Excess over Pixels (MEP) loss. In the end, K subregions (S 1 , S 2 , ..., S K ) are generated by the K branches. The final desired pixel-level subregion S is computed by simply combining them together as</p><formula xml:id="formula_9">S = K k=1 {S k } ,<label>(10)</label></formula><p>where indicates merging pixels with values close to 1 in all subregion masks {S k }, rather than the direct summation. In practice, we take the maximum value at each pixel position from all masks. The final output S is the mask that indicates the pixels which should be optimized. Based on that, our proposed MEP loss is then given by</p><formula xml:id="formula_10">Lmep D pr , D gt = 1 N N i=1 p?S D pr,i (p) ? p?S D gt,i (p) . (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Learning</head><p>Our SPANet could be easily integrated into existing crowd counting methods, which is equivalent to adding a pooling layer with different masks on the final convolutional layer. It is trained by sequentially optimizing the K times ranking loss, MEP loss, and the original loss of existing methods. When calculating the original loss, the mask pooling layer is removed. The overall training objective is formulated as</p><formula xml:id="formula_11">L global = K k=1 Lr + Lmep + L vanilla ,<label>(12)</label></formula><p>where L vanilla refers to the original loss of existing approach. In most cases, L vanilla is the L 2 loss. More details of the ground truth generation and data augmentation are described in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Settings</head><p>Networks. We evaluate our method by combining it with three networks, i.e., MCNN <ref type="bibr" target="#b52">[52]</ref>, CSRNet <ref type="bibr" target="#b16">[17]</ref>, and SANet <ref type="bibr" target="#b2">[3]</ref>. The implementations of MCNN 1 and CSR-Net 2 are from others, while SANet is implemented by us. In general, there are four main differences between them: (1) Different size of networks. Specifically, MCNN, SANet, and CSRNet are corresponding to small, medium, and large crowd counting networks.  Learning settings. For MCNN and SANet, the parameters are randomly initialized by a Gaussian distribution with mean of 0 and standard deviation of 0.01. Adam optimizer <ref type="bibr" target="#b13">[14]</ref> with a learning rate of 1e?5 is used to train the model. For CSRNet, the first ten convolutional layers are from pre-trained VGG-16 <ref type="bibr" target="#b37">[37]</ref>. The other layers are initialized in the same way as MCNN. Stochastic gradient descent (SGD) with a fixed learning rate of 1e?6 is applied during the training.</p><p>Datasets. We evaluate our method on four datasets, including ShanghaiTech <ref type="bibr" target="#b52">[52]</ref>, UCF CC 50 <ref type="bibr" target="#b10">[11]</ref>, World-Expo'10 <ref type="bibr" target="#b48">[48]</ref>, and UCSD <ref type="bibr" target="#b3">[4]</ref>. Typically, ShanghaiTech Part A is congested and noisy, while ShanghaiTech Part B is noisy but not highly congested. UCF CC 50 consists of extremely congested scenes with heavy background noises. WorldExpo'10 and UCSD contain sparse crowd scenes. The scenes in WorldExpo'10 are noisier than UCSD.</p><p>Evaluation details. MCNN and CSRNet are tested on the whole images, while SANet is tested on image patches. Following previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b52">52]</ref>, Mean Absolute Error (MAE) and Mean Square Error (MSE) are used to evaluate the performance by</p><formula xml:id="formula_12">M AE = 1 N N i=1 C i ? C gt i , M SE = 1 N N i=1 C i ? C gt i 2 ,<label>(13)</label></formula><p>where C i is the estimated crowd count and C gt i is the ground truth count of the i-th image. N is the number of test images. Additionally, PSNR (Peak Signal-to-Noise Ratio) <ref type="bibr" target="#b2">3</ref> and SSIM (Structural Similarity) <ref type="bibr" target="#b3">4</ref>  <ref type="bibr" target="#b44">[44]</ref> are utilized to 3 https://en.wikipedia.org/wiki/Peak signal-to-noise ratio 4 https://en.wikipedia.org/wiki/Structural similarity measure the quality of density maps. For fair comparison, similar to <ref type="bibr" target="#b16">[17]</ref>, bilinear interpolation is employed to resize estimated density maps to the same size as input images. <ref type="table" target="#tab_0">Table 1</ref> and 2 report the results of four challenging datasets. As a summary, our method significantly improves all baselines and outperforms the other state-of-theart methods. This result fully demonstrates the effectiveness of our SPANet, which could provide accurate density estimation on both dense and sparse crowd scenes, and can be applied to all CNN-based crowd counting networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with State-of-the-art</head><p>On ShanghaiTech dataset, our SPANet boosts MCNN, CSRNet, SANet with relative MAE improvements of 9.5%, 8.5%, 11.3% on Part A, and 27.7%, 20.8%, 22.7% on Part B. Noted that Part A is collected from the internet while Part B is from the busy streets and has more spatial constraints. Since our SPANet can fully utilize spatial awareness, it brings more improvements on Part B. On UCF CC 50, SPANet provides the relative MAE improvements of 22.5%, 7.6%, 10.0% for the three baselines. Noted that the improved MCNN is even comparable with other state-of-the-art methods. It clearly shows that SPANet can handle the extremely dense-crowd scenes. Similar to the above two datasets, SPANet also achieves significant improvements on UCSD and WorldExpo'10, verifying the effectiveness of our method on the sparse-crowd scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Sampling positions. We first evaluate the impact of different starting positions when sampling patches for mask pooling. The results are listed in <ref type="table" target="#tab_2">Table 3</ref>. We find that starting at the bottom is always better than the top, and the right is also better than the left. The possible reason is that it may be closely related to camera calibration. The results en-  Noted that the differences between these sampling schemes are quite small, which demonstrates the robustness of our method. Additionally, we also present the comparison of performing mask pooling on the same or different density maps in each branch, which is already discussed in Section 3.2 and Eq. (4). As shown in <ref type="table" target="#tab_2">Table 3</ref>, the results of two strategies are similar. Due to the efficiency problem, we directly pool patches from the same density map. Different losses/weights. We turn to evaluate the effect of different losses and weight schemes. As shown in <ref type="table" target="#tab_2">Table 3</ref>, adding the ranking loss only provides slight improvement, while the significant improvement comes from the MEP loss. Besides, there is no significant difference whether L 2 is used. It demonstrates that our MEP loss can effectively learn spatial awareness to boost crowd counting. We further conduct experiments on two weight schemes: the random weight and the grid search with step 0.1. As shown in <ref type="table" target="#tab_2">Table  3</ref>, our method is not sensitive to the weights. Even the grid search brings a very slight improvement. Number of branches. We measure the performance of SPANet with different branch numbers K. As illustrated in <ref type="figure">Figure 4</ref>, the performance first improves but then drops with the increasing number of K. This observation is not surprising. On one side, small K (e.g., K = 1) would involve large bias error due to the large difference between two patches. On the other side, large K (e.g., K = H 2 , where H is the height of estimated density map) implies that the difference of two patches in each branch is very small, which cannot provide enough discrepancy for subregion generation. In experiments, K is set to H 8 for MCNN/SANet and H 16 for CSRNet, which is determined via cross validation. Size of estimated density maps. We further validate the effect of the size of estimated density maps. We add deconvolutional layers on top of the MCNN to increase the size of the estimated density maps. Eventually, two variants of MCNN are obtained, whose estimated density maps are of 1/2 and the same size as the input images, respectively. As shown in <ref type="figure">Figure 4</ref>, the performance is improved along with the size increase of density maps. The results indicate that predicting high-resolution density maps could bring considerable improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Studies on Estimated Density Maps</head><p>We now evaluate the estimated density maps to verify whether our method can fully utilize spatial awareness. <ref type="table" target="#tab_3">Table 4</ref> summarizes the results. Our SPANet can significantly improve PSNR and SSIM across all baselines and datasets, which indicates that the quality of the generated density maps are significantly improved. To further verify that our method can indeed learn spatial awareness, we showcase the generated density maps of four examples from different methods in <ref type="figure">Figure 5</ref>. These four examples typically contain different crowd densities, occlusions, and scale changes. We can observe that the baseline models are always affected by the zero-mean noise, which leads to overestimation in low-density areas. In contrast, zero-mean noise is effectively suppressed in our SPANet. Besides, baseline models normally have an insufficient estimation for high-density areas, while ours can obtain a more accurate estimation for them. Noted that the ground truth itself is also generated with center points of pedestrians' heads, which inherently contains inaccurate information. It means that our method is still unable to produce the same density map to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Studies on Learning Curves</head><p>Finally, we study the learning curves to further evaluate our method. <ref type="figure">Figure 6</ref> shows the training and valida-  <ref type="bibr" target="#b52">[52]</ref>, CSRNet <ref type="bibr" target="#b16">[17]</ref> and SANet <ref type="bibr" target="#b2">[3]</ref> on ShanghaiTech Part A dataset <ref type="bibr" target="#b52">[52]</ref>. For better viewing, we smooth the learning curves by exponential moving average (EMA) with a smoothing factor ? = 0.1. Compared with original results, baselines integrated with our SPANet exhibit lower MAE on both training and testing set. Since the performance on the training and testing set generally denotes the fitting and generalization degree, this result demonstrates the promising capability on both sides. In addition, it also means that our method can significantly improve the stability during model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we present a novel deep architecture called SPatial Awareness Network (SPANet) for crowd counting, which is able to capture the spatial variations by finding the pixel-level subregion with high discrepancy to the ground truth. It could be integrated into all CNN-based methods and is end-to-end trainable. Experiments on four datasets and three various networks fully demonstrate that it can significantly improve all baselines and outperforms the stateof-the-art methods. It provides the elegant views of effectively using spatial awareness to improve crowd counting. In future work we will study how to preserve spatial awareness as much as possible in the ground truth generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Computation process of MESA loss. It is required to traverse all possible subregions and calculate the differences between their predicted density maps and the ground truth. Then the subregion with maximum difference is selected for optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 2 )</head><label>2</label><figDesc>Different architectures. MCNN and SANet are multi-column/multi-blob networks, while CSRNet is a single column network. In addition, SANet uses the Instance Normalization (IN) layer and the deconvolutional layer, while CSRNet utilizes the dilated convolutional layer. (3) Different size of density maps. Density maps of MCNN and CSRNet are 1/4 and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Comparisons of estimated density maps between baselines and our SPANet. '+' indicates combining SPANet with baselines. Learning Curves. Mean absolute error (MAE) on training and validation sets, vs. the number of training epochs of MCNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison with the state-of-the-art methods on ShanghaiTech<ref type="bibr" target="#b52">[52]</ref>, UCF CC 50<ref type="bibr" target="#b10">[11]</ref>, and UCSD<ref type="bibr" target="#b48">[48]</ref> datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">ShanghaiTech A</cell><cell cols="2">ShanghaiTech B</cell><cell cols="2">UCF CC 50</cell><cell>UCSD</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Venue &amp; Year</cell><cell>MAE ?</cell><cell>MSE ?</cell><cell cols="2">MAE ? MSE ?</cell><cell>MAE ?</cell><cell>MSE ?</cell><cell cols="2">MAE ? MSE ?</cell></row><row><cell>Idrees et al. [11]</cell><cell>CVPR</cell><cell>2013</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>419.5</cell><cell>541.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Zhang et al. [48]</cell><cell>CVPR</cell><cell>2015</cell><cell>181.8</cell><cell>277.7</cell><cell>32.0</cell><cell>49.8</cell><cell>467.0</cell><cell>498.5</cell><cell>1.60</cell><cell>3.31</cell></row><row><cell>CCNN [25]</cell><cell>ECCV</cell><cell>2016</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.51</cell><cell>-</cell></row><row><cell>Hydra-2s [25]</cell><cell>ECCV</cell><cell>2016</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>333.7</cell><cell>425.3</cell><cell>-</cell><cell>-</cell></row><row><cell>C-MTL [38]</cell><cell>AVSS</cell><cell>2017</cell><cell>101.3</cell><cell>152.4</cell><cell>20.0</cell><cell>31.1</cell><cell>322.8</cell><cell>397.9</cell><cell>-</cell><cell>-</cell></row><row><cell>SwitchCNN [32]</cell><cell>CVPR</cell><cell>2017</cell><cell>90.4</cell><cell>135.0</cell><cell>21.6</cell><cell>33.4</cell><cell>318.1</cell><cell>439.2</cell><cell>1.62</cell><cell>2.10</cell></row><row><cell>CP-CNN [39]</cell><cell>ICCV</cell><cell>2017</cell><cell>73.6</cell><cell>106.4</cell><cell>20.1</cell><cell>30.1</cell><cell>295.8</cell><cell>320.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Huang at al. [10]</cell><cell>TIP</cell><cell>2018</cell><cell>-</cell><cell>-</cell><cell>20.2</cell><cell>35.6</cell><cell>409.5</cell><cell>563.7</cell><cell>1.00</cell><cell>1.40</cell></row><row><cell>SaCNN [49]</cell><cell cols="2">WACV 2018</cell><cell>86.8</cell><cell>139.2</cell><cell>16.2</cell><cell>25.8</cell><cell>314.9</cell><cell>424.8</cell><cell>-</cell><cell>-</cell></row><row><cell>ACSCP [34]</cell><cell>CVPR</cell><cell>2018</cell><cell>75.7</cell><cell>102.7</cell><cell>17.2</cell><cell>27.4</cell><cell>291.0</cell><cell>404.6</cell><cell>-</cell><cell>-</cell></row><row><cell>IG-CNN [31]</cell><cell>CVPR</cell><cell>2018</cell><cell>72.5</cell><cell>118.2</cell><cell>13.6</cell><cell>21.1</cell><cell>291.4</cell><cell>349.4</cell><cell>-</cell><cell>-</cell></row><row><cell>Deep-NCL [36]</cell><cell>CVPR</cell><cell>2018</cell><cell>73.5</cell><cell>112.3</cell><cell>18.7</cell><cell>26.0</cell><cell>288.4</cell><cell>404.7</cell><cell>-</cell><cell>-</cell></row><row><cell>MCNN [52]</cell><cell>CVPR</cell><cell>2016</cell><cell>110.2</cell><cell>173.2</cell><cell>26.4</cell><cell>41.3</cell><cell>377.6</cell><cell>509.1</cell><cell>1.07</cell><cell>1.35</cell></row><row><cell>CSRNet [17]</cell><cell>CVPR</cell><cell>2018</cell><cell>68.2</cell><cell>115.0</cell><cell>10.6</cell><cell>16.0</cell><cell>266.1</cell><cell>397.5</cell><cell>1.16</cell><cell>1.47</cell></row><row><cell>SANet [3]</cell><cell>ECCV</cell><cell>2018</cell><cell>67.0</cell><cell>104.5</cell><cell>8.4</cell><cell>13.6</cell><cell>258.4</cell><cell>334.9</cell><cell>1.02</cell><cell>1.29</cell></row><row><cell>MCNN+SPANet</cell><cell>-</cell><cell>-</cell><cell>99.7</cell><cell>146.3</cell><cell>19.1</cell><cell>28.7</cell><cell>292.5</cell><cell>401.3</cell><cell>1.00</cell><cell>1.33</cell></row><row><cell>CSRNet+SPANet</cell><cell>-</cell><cell>-</cell><cell>62.4</cell><cell>99.5</cell><cell>8.4</cell><cell>13.2</cell><cell>245.8</cell><cell>333.1</cell><cell>1.12</cell><cell>1.42</cell></row><row><cell>SANet+SPANet</cell><cell>-</cell><cell>-</cell><cell>59.4</cell><cell>92.5</cell><cell>6.5</cell><cell>9.9</cell><cell>232.6</cell><cell>311.7</cell><cell>1.00</cell><cell>1.28</cell></row><row><cell cols="5">1/8 of original images, while SANet produces density maps</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">with the same size as input images. (4) Different testing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">scheme. SANet is tested on image patches, while CSRNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">and MCNN are tested on the whole images.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art methods on World-Expo'10 [4]  dataset. Only MAE is computed for each scene and then averaged to evaluate the overall performance. Ablation studies on ShanghaiTech Part A<ref type="bibr" target="#b52">[52]</ref>. The left shows the branch number K vs. MAE, and the right illustrates the size of estimated density maps vs. MAE, performed with MCNN.</figDesc><table><row><cell></cell><cell cols="3">Method</cell><cell>S1</cell><cell>S2</cell><cell></cell><cell cols="2">S3</cell><cell>S4</cell><cell>S5</cell><cell>Avg.</cell></row><row><cell></cell><cell cols="3">Zhang et al. [48]</cell><cell>9.8</cell><cell>14.1</cell><cell></cell><cell cols="2">14.3</cell><cell>22.2</cell><cell>3.7</cell><cell>12.9</cell></row><row><cell></cell><cell cols="3">Huang et al. [10]</cell><cell>4.1</cell><cell>21.7</cell><cell></cell><cell cols="2">11.9</cell><cell>11.0</cell><cell>3.5</cell><cell>10.5</cell></row><row><cell></cell><cell cols="3">Switch-CNN [32]</cell><cell>4.4</cell><cell>15.7</cell><cell></cell><cell cols="2">10.0</cell><cell>11.0</cell><cell>5.9</cell><cell>9.4</cell></row><row><cell></cell><cell cols="3">SaCNN [49]</cell><cell>2.6</cell><cell>13.5</cell><cell></cell><cell cols="2">10.6</cell><cell>12.5</cell><cell>3.3</cell><cell>8.5</cell></row><row><cell></cell><cell cols="3">CP-CNN [39]</cell><cell>2.9</cell><cell>14.7</cell><cell></cell><cell cols="2">10.5</cell><cell>10.4</cell><cell>5.8</cell><cell>8.9</cell></row><row><cell></cell><cell cols="3">MCNN [52]</cell><cell>3.4</cell><cell>20.6</cell><cell></cell><cell cols="2">12.9</cell><cell>13.0</cell><cell>8.1</cell><cell>11.6</cell></row><row><cell></cell><cell cols="3">CSRNet [17]</cell><cell>2.9</cell><cell>11.5</cell><cell></cell><cell cols="2">8.6</cell><cell>16.6</cell><cell>3.4</cell><cell>8.6</cell></row><row><cell></cell><cell cols="3">SANet [3]</cell><cell>2.6</cell><cell>13.2</cell><cell></cell><cell cols="2">9.0</cell><cell>13.3</cell><cell>3.0</cell><cell>8.2</cell></row><row><cell></cell><cell cols="3">MCNN+SPANet</cell><cell>3.4</cell><cell>14.9</cell><cell></cell><cell cols="2">15.1</cell><cell>12.8</cell><cell>4.5</cell><cell>10.1</cell></row><row><cell></cell><cell cols="3">CSRNet+SPANet</cell><cell>2.6</cell><cell>11.1</cell><cell></cell><cell cols="2">8.9</cell><cell>13.5</cell><cell>3.3</cell><cell>7.9</cell></row><row><cell></cell><cell cols="3">SANet+SPANet</cell><cell>2.3</cell><cell>12.3</cell><cell></cell><cell cols="2">7.9</cell><cell>12.9</cell><cell>3.2</cell><cell>7.7</cell></row><row><cell></cell><cell>120</cell><cell></cell><cell>CSRNet</cell><cell>SANet</cell><cell></cell><cell></cell><cell>102</cell></row><row><cell></cell><cell>110</cell><cell></cell><cell>MCNN</cell><cell></cell><cell></cell><cell></cell><cell>100</cell></row><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>98</cell></row><row><cell>MAE</cell><cell>60 70 80 90</cell><cell>1</cell><cell cols="2">H/16 The Number of Branches H/8 H/4</cell><cell>H/2</cell><cell>MAE</cell><cell>90 92 94 96</cell><cell>1/4 The Size of Estimated Density Maps 1/2 1</cell></row><row><cell cols="3">Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies of patch sampling strategy, mask pooling strategy, and losses on ShanghaiTech Part A dataset<ref type="bibr" target="#b52">[52]</ref>.</figDesc><table><row><cell>Configurations</cell><cell>MAE ?</cell><cell>MSE ?</cell></row><row><cell>Center point</cell><cell>101.2</cell><cell>153.3</cell></row><row><cell>Top left corner</cell><cell>101.5</cell><cell>153.7</cell></row><row><cell>Bottom left corner</cell><cell>100.7</cell><cell>149.2</cell></row><row><cell>Top right corner</cell><cell>100.5</cell><cell>149.4</cell></row><row><cell>Bottom right corner</cell><cell>99.7</cell><cell>146.3</cell></row><row><cell>Different density map</cell><cell>100.3</cell><cell>147.4</cell></row><row><cell>Same density map</cell><cell>99.7</cell><cell>146.3</cell></row><row><cell>L 2</cell><cell>110.2</cell><cell>173.2</cell></row><row><cell>Lr+Lmep</cell><cell>99.3</cell><cell>145.3</cell></row><row><cell>L 2 + Lr</cell><cell>107.2</cell><cell>164.5</cell></row><row><cell>L 2 + Lr + Lmep</cell><cell>99.7</cell><cell>146.3</cell></row><row><cell>Random</cell><cell>105.4</cell><cell>162.2</cell></row><row><cell>Grid Search</cell><cell>98.3</cell><cell>142.5</cell></row><row><cell cols="3">courage us to sample patches from the bottom right corner.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Density map quality comparison. Values on the left of '|' are from original baselines, while values on the right of '|' are results when integrating with the proposed SPANet.</figDesc><table><row><cell></cell><cell cols="2">MCNN</cell><cell cols="2">CSRNet</cell><cell cols="2">SANet</cell></row><row><cell>Dataset</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>PSNR ?</cell><cell>SSIM ?</cell></row><row><cell>ShanghaiTech-A [52]</cell><cell>21.42 | 22.18</cell><cell>0.52 | 0.66</cell><cell>23.79 | 24.88</cell><cell>0.76 | 0.85</cell><cell>23.36 | 25.33</cell><cell>0.78 | 0.85</cell></row><row><cell>ShanghaiTech-B [52]</cell><cell>23.43 | 26.19</cell><cell>0.78 | 0.85</cell><cell>27.02 | 29.50</cell><cell>0.89 | 0.92</cell><cell>27.44 | 29.17</cell><cell>0.89 | 0.91</cell></row><row><cell>UCF CC 50 [11]</cell><cell>14.44 | 18.25</cell><cell>0.37 | 0.51</cell><cell>18.76 | 20.17</cell><cell>0.52 | 0.78</cell><cell>18.35 | 20.01</cell><cell>0.51 | 0.76</cell></row><row><cell>UCSD [48]</cell><cell>17.43 | 18.52</cell><cell>0.75 | 0.83</cell><cell>20.02 | 21.80</cell><cell>0.86 | 0.89</cell><cell>21.33 | 22.20</cell><cell>0.84 | 0.90</cell></row><row><cell>WorldExpo'10 [4]</cell><cell>23.53 | 25.97</cell><cell>0.76 | 0.85</cell><cell>26.94 | 29.05</cell><cell>0.92 | 0.93</cell><cell>26.22 | 28.54</cell><cell>0.90 | 0.92</cell></row><row><cell cols="3">tion mean absolute error (MAE) at every epoch on Shang-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>haiTech Part A dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/svishwa/crowdcount-mcnn 2 https://github.com/leeyeehoo/CSRNet-pytorch/tree/master</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowdnet: A deep convolutional network for dense crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lokesh</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="640" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised bayesian detection of independent motion in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="594" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scale aggregation network for accurate and efficient crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinkun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="757" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang-Sheng John</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Counting people with low-level features and bayesian regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2160" to="2177" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving the learning of multicolumn convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Qi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia</title>
		<meeting>the 26th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiple component learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Stacked pooling: Improving crowd counting by boosting scale invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno>abs/1808.07456</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Body structure aware deep crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detecting humans in dense crowds using locally-consistent scale prior and global occlusion reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1986" to="1998" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crowd counting by adaptively fusing predictions from an image pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">89</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Neural Information Processing Systems</title>
		<meeting>Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimation of number of people in crowded scenes using perspective transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaw-Yeh</forename><surname>Sheng-Fuu Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decidenet: Counting varying density crowds through attention guided detection and density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adcrowdnet: An attention-injective deformable convolutional network for crowd understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hefeng</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1811.11968</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Geometric and physical constraints for head plane crowd density estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<idno>abs/1803.08805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Contextaware crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<idno>abs/1811.10452</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Leveraging unlabeled data for crowd counting by learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The kolmogorov-smirnov test for goodness of fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Massey</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">253</biblScope>
			<biblScope unit="page" from="68" to="78" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>O?oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto Javier L?pez-Sastre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">COUNT forest: Co-voting uncertain number of targets using random forest for crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet-Quoc</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuo</forename><surname>Kozakaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuzo</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Iterative crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed data fusion for real-time crowding estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tesei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="63" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Crowd counting using multiple local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Top-down feedback for crowd counting convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Artificial Intelligence</title>
		<meeting>Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7323" to="7330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Divide and grow: Capturing huge diversity in crowd images with incrementally growing CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><forename type="middle">N</forename><surname>Sajjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukundhan</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak Babu</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiv</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grad-Cam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Crowd counting via adversarial crossscale consistency pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5245" to="5254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Perspective-aware CNN for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijun</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/1807.01989</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Crowd counting with deep negative correlation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyan</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5382" to="5390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Advanced Video and Signal Based Surveillance</title>
		<meeting>International Conference on Advanced Video and Signal Based Surveillance</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generating high-quality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1879" to="1888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Padnet: Pan-density crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimei</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1811.02805</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to count with CNN boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3401" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">In defense of single-column networks for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
	<note>Xiantong Zhen, and Xianbin Cao</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adaptive scenario discovery for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno>abs/1812.02393</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-scale convolutional neural networks for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingke</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Image Processing</title>
		<meeting>International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="465" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Crowd counting via scale-adaptive convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaobo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Winter Conference on Applications of Computer Vision</title>
		<meeting>Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Attention to head locations for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youmei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<idno>abs/1806.10287</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Segmentation and tracking of multiple humans in crowded environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1198" to="1211" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
