<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
							<email>jiangjunjun@hit.edu.cnxuyang.wang@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Monocular Depth Estimation</term>
					<term>Adaptive Bins</term>
					<term>Multi-Scale Refinement</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular depth estimation is a fundamental task in computer vision and has drawn increasing attention. Recently, some methods reformulate it as a classification-regression task to boost the model performance, where continuous depth is estimated via a linear combination of predicted probability distributions and discrete bins. In this paper, we present a novel framework called BinsFormer, tailored for the classification-regression-based depth estimation. It mainly focuses on two crucial components in the specific task: 1) proper generation of adaptive bins and 2) sufficient interaction between probability distribution and bins predictions. To specify, we employ the Transformer decoder to generate bins, novelly viewing it as a direct set-to-set prediction problem. We further integrate a multi-scale decoder structure to achieve a comprehensive understanding of spatial geometry information and estimate depth maps in a coarse-to-fine manner. Moreover, an extra scene understanding query is proposed to improve the estimation accuracy, which turns out that models can implicitly learn useful information from an auxiliary environment classification task. Extensive experiments on the KITTI, NYU, and SUN RGB-D datasets demonstrate that Bins-Former surpasses state-of-the-art monocular depth estimation methods with prominent margins. Code and pretrained models will be made publicly available. 3</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monocular depth estimation is a fundamental yet challenging task in computer vision, which requires the algorithm to predict each pixel's depth within a single input RGB image. So far, there have been numerous mainstream methods formulating depth estimation as per-pixel regression, such as DAV <ref type="bibr" target="#b19">[20]</ref>, DPT <ref type="bibr" target="#b33">[34]</ref> and TransDepth <ref type="bibr" target="#b44">[45]</ref>  <ref type="figure" target="#fig_2">(Fig. 1a)</ref>, where a regression loss is applied to each pixel prediction. Per-pixel regression methods can neatly predict pixel-wise depth, thus   becoming a universal paradigm. Despite the proof of their great success, such methods still face problems of slow convergence and unsatisfied results <ref type="bibr" target="#b11">[12]</ref>. Another line of research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9]</ref> proposes to discretize continuous depth into several intervals and cast the depth network learning as a per-pixel classification problem <ref type="figure" target="#fig_2">(Fig. 1b</ref>). While this strategy significantly improves the model performance, it is worth noting that the discretization of depth values will result in poor visual quality with apparent sharp discontinuities.</p><p>To solve the issue, some methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref> reformulate depth estimation as a per-pixel classification-regression task <ref type="figure" target="#fig_2">(Fig. 1c)</ref>, learning probabilistic representations on each pixel and predicting the final depth value as a linear combination with bin centers. The bin centers are pre-defined in Uniform/Log-uniform space (UD/SID) or trained ones (for each dataset). They combine the best of both tasks and achieve an improving performance. On top of that, Adabins <ref type="bibr" target="#b2">[3]</ref> observes the extreme variation of depth distribution among changing scenes and further proposes the adaptive bins generation module to predict bins centers adaptively. While Adabins <ref type="bibr" target="#b2">[3]</ref> boosts depth estimation performance to a remarkable extent, several dilemmas still exist. 1) It directly applies bins prediction depending on the highest resolution feature map (the output of the last layer of the Decoder), leading to the difficulty of squaring up the global information and perceiving scenes. While it utilizes a Transformer block, the facade encoder-decoder with limited receptive fields leads to an inevitable loss of global information. 2) Bins and probabilistic representations are predicted based on the same single layer feature, which lacks sufficient interactions and leads to the global and fine-grained information blemishing each other. 3) The proposed chamfer loss applying distribution constraints on bins estimation introduces futile inductive bias. All these problems impede models from predicting more accurate estimation results.</p><p>In this paper, we propose a conceptually simple yet effective approach called BinsFormer, tailored for classification-regression-based monocular depth es-timation. It novelly views adaptive bins generation as a direct set prediction problem <ref type="bibr" target="#b3">[4]</ref>. We employ a separate Transformer decoder <ref type="bibr" target="#b9">[10]</ref> to compute a set of pairs, each consisting of a bins length and a bins embedding vector. The bins embedding vector is used to get the probabilistic representations via a dot product with the per-pixel representations obtained from an underlying fullyconvolutional decoder. Finally, BinsFormer predicts depth values as a linear combination of bins centers and probabilistic representations. Such disentangled decoder avoids bins embeddings and fine-grained per-pixel representations blemishing each other and combines the best of global and pixel-wise information. We further integrate a multi-scale decoder structure to comprehensively understand spatial geometry information and estimate depth maps in a coarse-to-fine manner. It enables sufficient interactions and aggregations of features via successive alternating cross-attention and self-attention mechanisms. To further improve the estimation accuracy, we equip an extra scene understanding query to the Transformer decoder, which aims to predict the classification of the input environment. It can benefit models to generate appropriate bins via auxiliary and implicit supervision.</p><p>We evaluate BinsFormer on three depth estimation datasets with various settings: NYU <ref type="bibr" target="#b34">[35]</ref> (indoor, depth range 0-10m), KITTI <ref type="bibr" target="#b14">[15]</ref> (outdoor, depth range 0-80m), and SUN-RGBD <ref type="bibr" target="#b35">[36]</ref> (indoor, depth range 0-10m, directly fine-tuning). Numerous experiments domestrate BinsFormer achieves the new state-of-the-art on all these datasets with Swin-Transformer <ref type="bibr" target="#b29">[30]</ref> backbone, outperforming other methods with large margins. Exhaustive ablation studies further validate the effectiveness of each proposed component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Monocular depth estimation plays a critical role in three-dimensional reconstruction and perception. There has been tremendous witnessed progress achieved by learning-based depth estimation methods in recent years. Eigen et al. <ref type="bibr" target="#b10">[11]</ref> groundbreakingly proposes a multi-scale deep network, consisting of a global network and a local network to predict the coarse depth and refine predictions, respectively. Motivated by <ref type="bibr" target="#b10">[11]</ref>, convolutional architectures have been intensively studied for depth estimation. For instance, CLIFFNet <ref type="bibr" target="#b40">[41]</ref> applies a multi-scale fusion convolutional framework to generate high-quality depth prediction. Recently, Transformer networks are gaining greater interest in the computer vision community <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b45">46]</ref>. Following the success of recent trends that apply the Transformer to solve computer vision tasks, TransDepth <ref type="bibr" target="#b44">[45]</ref> and DPT <ref type="bibr" target="#b33">[34]</ref> propose to replace convolution operations with Transformer layers, which further boosts model performance.</p><p>Though the above methods have significantly improved depth prediction accuracy, they suffer from relatively slow convergence and sub-optimal solutions since they regard monocular depth estimation as a regression task <ref type="bibr" target="#b11">[12]</ref>. Another line of research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9]</ref> proposes to discretize continuous depth into several intervals and cast the depth network learning as a per-pixel classification problem. DORN <ref type="bibr" target="#b11">[12]</ref> designs an effective ordinal classification depth estimation loss and develops an ASPP <ref type="bibr" target="#b4">[5]</ref> module to extract multi-level information. Based on <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref> softens the classification target during training and achieves improving performance. Moreover, some methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21]</ref> reformulate the problem as classificationregression to alleviate poor visual quality with apparent sharp depth discontinuities caused by discretization of depth values. Johnston et al. <ref type="bibr" target="#b20">[21]</ref> introduces the stereo DDV for monocular depth estimation. To further improve the model performance, Adabins <ref type="bibr" target="#b2">[3]</ref> proposes an adaptive bins strategy, which is crucial for accurate depth estimation.</p><p>In this paper, we further investigate the adaptive bins strategy and propose BinsFormer, tailored for classification-regression-based monocular depth estimation. We novelly treat the adaptive bins generation as a set prediction process <ref type="bibr" target="#b3">[4]</ref> and develop Transformer layers to resort to the problem, which is intuitively different from previous work <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45]</ref> that only utilize Transformer to strengthen the encoder capability. Furthermore, we melt an effective multi-scale refinement strategy into the Transformer-based decoder in a neat fashion. An extra scene understanding task further improves the model performance.</p><p>Indeed, various strategies have been explored to benefit monocular depth estimation, such as self-supervised learning <ref type="bibr" target="#b16">[17]</ref>, multi-task training <ref type="bibr" target="#b21">[22]</ref>, specific supervision losses <ref type="bibr" target="#b40">[41]</ref>, sparse ordinal <ref type="bibr" target="#b30">[31]</ref> or relative depth estimation <ref type="bibr" target="#b24">[25]</ref>. Our method focuses on the most basic framework design, which can be plugged into any other method as a strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we first present the overview of BinsFormer. Then, we introduce our instantiation of the adaptive bins generation strategy with the help of Transformer decoder <ref type="bibr" target="#b3">[4]</ref>. Finally, we present the auxiliary scene understanding task and the multi-scale prediction refinement strategies, which can be neatly melted into the framework and improve the depth estimation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>BinsFormer mainly consists of three essential components (see <ref type="figure" target="#fig_3">Fig. 2</ref>): the pixellevel module, the Transformer module, and the depth estimation module. Moreover, we propose the auxiliary scene classification and the multi-scale prediction refinement strategies to further boost model performance.</p><p>Given an input RGB image, the pixel-level module first extracts image features and decodes them into multi-scale immediate features F and the perpixel representations f p . Benefiting from the encoder-decoder framework with skip connections, BinsFormer can fully extract local information for fine-grained depth estimation. Then, queries in the Transformer module interact with F with the help of attention mechanisms. Independent MLPs are applied to project the query embeddings into bins predictions b and bins embeddings f b . Novelly viewing the bins generation as a set-to-set prediction problem and applying Transformer, BinsFormer can explore the global information and predict appropriate bins for integral depth estimation. Finally, the depth estimation module aggregates the best of the abovementioned modules and predicts final depth. It first calculates the probability distributions P and then combines them with bins centers c(b) via linear combinations.</p><p>On top of that, we equip an extra scene understanding query to the Transformer decoder, which aims to predict the classification of the input environment. Similarly, an MLP projects the query embedding to the classification result. The extra task can benefit models to generate appropriate bins via auxiliary and implicit supervision. Moreover, we further integrate a multi-scale decoder structure to comprehensively understand spatial geometry information and estimate depth maps in a coarse-to-fine manner. The Transformer queries progressively interact with multi-scale features F, enabling sufficient aggregations of features via successive attention modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BinsFormer</head><p>Per-pixel module takes an image of size H ?W as input. A backbone is applied to extract a set of feature maps. Then, a commonly used decoder gradually upsamples the features to generate the per-pixel representation</p><formula xml:id="formula_0">f p ? R H?W ?C , where C is the representation dimension. It produces S scale immediate feature maps F = {f i } S i=1</formula><p>as well, where f i indicates the feature map at scale i th . Note that any per-pixel depth estimation model fits the pixel-level module design including Transformer-based models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b2">3]</ref>. However, unlike previous methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45]</ref> that only adopt Transformer to replace convolutional opera-tions in models, BinsFormer seamlessly converts Transformer to solve the bins generation and leaves the per-pixel backbone untouched.</p><p>Transformer module applies the standard Transformer decoder <ref type="bibr" target="#b9">[10]</ref>, transforming N embeddings using multi-head self-and cross-attention mechanisms. Following <ref type="bibr" target="#b3">[4]</ref>, BinsFormer decodes the N bins in parallel at each decoder layer. The N input embeddings serve as bins queries to interact with image features F and are transformed into an output embedding Q ? R Cq?N by the decoder. Then, we apply a linear perceptron with softmax on top of the output embed-</p><formula xml:id="formula_1">dings Q to yield N bins length b = {b i } N i=1</formula><p>. Moreover, we utilize a 3-layer perceptron with ReLU activation function on Q to predict N bins embeddings f b ? R C?N to calculate the similarity with per-pixel representations f p in the depth prediction module.</p><p>Depth prediction module aggregates outputs from the per-pixel module and the Transformer module to predict depth. Given the predicted bins lengths b from the Transformer module, it first converts them to bins centers via a simple post-process following <ref type="bibr" target="#b2">[3]</ref>:</p><formula xml:id="formula_2">c(b i ) = d min + (d max ? d min ) ? ? b i 2 + i?1 j=1 b j ? ? ,<label>(1)</label></formula><p>where c(b i ) is center depth of the i th bins. d max and d min are the max and the min valid depth values of the dataset, respectively. Meanwhile, we obtain a similarity map via a dot product between the pixelwise representations f p from the per-pixel module and the bins embeddings f b from the Transformer module. We then convert it to a probability distribution map P ? R H?W ?N by a Softmax fuction. Finally, at each pixel, the final depth valued is calculated from a linear combination of the probability distribution at that pixel and the depth-bin-centers c(b) as follows:</p><formula xml:id="formula_3">d = N i=1 c(b i )p i .<label>(2)</label></formula><p>Compared to Adabins <ref type="bibr" target="#b2">[3]</ref>, we disentangle the bins generation and avoid bins embeddings and fine-grained per-pixel representations blemishing each other. This enables us to predict more accurate depth without large-area failures, as can be seen in <ref type="figure">Fig. 4</ref>.</p><p>After predicting final depth maps, we apply a scaled version of the Scale-Invariant loss (SI) introduced by Eigen et al. <ref type="bibr" target="#b10">[11]</ref>:  where g i = logd i ? log d i with the ground truth depth d i and predicted depthd i . T denotes the number of pixels having valid ground truth values. Following <ref type="bibr" target="#b2">[3]</ref>, we use ? = 0.85 and ? = 10 for all our experiments.</p><formula xml:id="formula_4">L reg = ? 1 T i g 2 i ? ? T 2 ( i g i ) 2 ,<label>(3)</label></formula><formula xml:id="formula_5">?*(+ $,) ? -)$ $,)</formula><p>Auxiliary scene classification is an auxiliary subtask to provide implicit guidance to the bins generation. Beyond the bins queries, we equip the Transformer decoder with an extra scene query, which is used to classify the scene environment. It can learn the global semantic information and transfer such knowledge through successive alternating self-attention to the bins queries. Similar to the bins embeddings, we adopt a 3-layer perceptron with ReLU activation function on the output embedding to yield the final classification result. During training, a simple CrossEntropy classification loss L cls is applied. Unlike Adabins <ref type="bibr" target="#b2">[3]</ref>, which applies chamfer loss to constrain the distribution of bins, our method avoids introducing such futile inductive bias. It means the supervision from the auxiliary classification subtask is implicit. The bins queries can adaptively absorb the global semantic information by self-attention with the scene query. This strategy only leads to a negligible overhead during the training process, compared with the computational pixel-wise chamfer loss.</p><p>Multi-scale prediction refinement is applied to obtain a global understanding of the image structure information and exploit a coarse-to-fine depth refinement. It is intuitively reasonable that the model can effectively square up the global information in low-resolution feature maps where the structure information is well reserved while high-frequency details are discarded. Similarly, since we combine the bins and the per-pixel representations to predict final depth, learning the fine-grained details in high-resolution feature maps is crucial for high-quality depth estimation. Hence, we propose the multi-scale prediction refinement strategy, which can be seamlessly adopted with the help of the Transformer module.</p><p>As shown in <ref type="figure" target="#fig_4">Fig 3,</ref> we feed one resolution of the multi-scale feature F to one Transformer decoder layer at a time. Each scale of the Transformer decoder contains L Transformer layers that consists of a cross-attention module, a self-attention module and a feed-forward network (FFN). During the forward propagation, queries at scale s first query the input image feature f s via a crossattention module, and then aggregate information among themselves through a self-attention module. Following the standard Transformer <ref type="bibr" target="#b39">[40]</ref>, a FFN is applied to improve the model capacity. Queries are zero initialized before being fed into the Transformer decoder and are associated with learnable positional embeddings. We also apply the depth estimation module at each Transformer layer to provide auxiliary supervison. Therefore, the total loss can be formulated as:</p><formula xml:id="formula_6">L total = S s=1 w s L l=1 L s,l reg + ?L s,l cls ,<label>(4)</label></formula><p>where ? = 1e ?3 is a hpyerparameter to balance the auxiliary classification loss and the depth estimation loss. w is a scale weight, which is set to { 1 2 s , 1 2 s?1 , ? ? ? , 1}. By simply reweight the multi-scale punishment to estimation results, we seamlessly integrate the multi-scale refinement strategy to the Transformer module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the performance of BinsFormer by comparing it against several baselines, starting by introducing datasets, evaluation metrics, and implementation details. Then, we present the comparison to the state-ofthe-art methods (containing a cross-dataset generalization evaluation), ablation studies, and uncertainty predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and evaluation metrics</head><p>Datasets: We assess the proposed method using KITTI <ref type="bibr" target="#b14">[15]</ref>, NYU-Depth-v2 <ref type="bibr" target="#b34">[35]</ref>, and SUN RGB-D <ref type="bibr" target="#b36">[37]</ref> datasets. KITTI is a dataset that provides stereo images and corresponding 3D laser scans of outdoor scenes captured by equipment mounted on a moving vehicle <ref type="bibr" target="#b14">[15]</ref>. Following the standard Eigen training/testing split <ref type="bibr" target="#b10">[11]</ref>, we use around 26K images from the left view for training and 697 frames for testing. When evaluation, we use the crop as defined by Garg et al. <ref type="bibr" target="#b13">[14]</ref> and upsample the prediction to the ground truth resolution. For the online KITTI depth prediction, we use the official benchmark split <ref type="bibr" target="#b38">[39]</ref>, which contains around 72K training data, 6K selected validation data, and 500 test data without the ground truth. NYU-Depth-v2 provides images and depth maps for different indoor scenes. We train our network on a 50K RGB-Depth pairs subset following previous works. We evaluate the results on the pre-defined center cropping by Eigen et al. <ref type="bibr" target="#b10">[11]</ref>. SUN RGB-D is an indoor dataset consisting of around 10K images with high scene diversity collected with four different sensors <ref type="bibr" target="#b36">[37]</ref>. We apply this dataset for generalization evaluation. Specifically, we cross-evaluate our NYU pre-trained models on the official test set of 5050 images without further fine-tuning. The depth upper bound is set to 10 meters. Notably, this dataset is only for evaluation. We do not train on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics:</head><p>We follow the standard evaluation protocol of the prior work <ref type="bibr" target="#b10">[11]</ref> to confirm the effectiveness of BinsFormer in experiments. For the NYU Depth V2, the KITTI Eigen split datasets, and the SUN RGB-D dataset, we utilize the accuracy under the threshold (? i &lt; 1.25 i , i = 1, 2, 3), mean absolute relative error (AbsRel), mean squared relative error (SqRel), root mean squared error (RMSE), root mean squared log error (RMSElog) and mean log10 error (log10) to evaluate our methods. In terms of the online KITTI depth prediction benchmark <ref type="bibr" target="#b38">[39]</ref>, we use the scale-invariant logarithmic error (SILog), percentage of AbsRel and SqRel (absErrorRel, sqErrorRel), and root mean squared error of the inverse depth (iRMSE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Training settings: BinsFormer is implemented with the Pytorch <ref type="bibr" target="#b31">[32]</ref> framework. We train the entire model with the batch size 16, learning rate 1e ? 4 for 38.4k iterations on a single node with 8 NVIDIA V100 32GB GPUs, which takes around 5 hours. We utilize the AdamW optimizer <ref type="bibr" target="#b23">[24]</ref> with (? 1 , ? 2 , wd) = (0.9, 0.999, 0.01), where wd is the weight decay. The linear learning rate warm-up strategy is applied for the first 30% iterations. The cosine annealing learning rate strategy is adopted for the learning rate decay. For the NYU-Depth-v2 dataset, we utilize the official 25 classes divided by folder names for the auxiliary scene understanding task. For KITTI, since the outdoor dataset is tough to classify, we omit the scene classification loss and only use ground truth depth to provide supervision.</p><p>Backbone: BinsFormer is compatible with any backbone architecture. In our work we use the standard convolution-based ResNet <ref type="bibr" target="#b17">[18]</ref> backbones <ref type="table">(ResNet-18</ref> and ResNet-50, respectively) and recently proposed Transformer-based Swin-Transformer <ref type="bibr" target="#b29">[30]</ref> backbones.</p><p>Pixel decoder: As for the pixel decoder in <ref type="figure" target="#fig_3">Fig. 2</ref>, any depth estimation decoder can be adopted (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2]</ref>). There are numerous depth estimation methods use modules like ASPP <ref type="bibr" target="#b4">[5]</ref> or CBAM <ref type="bibr" target="#b41">[42]</ref> to capture long-range correspondings. Since our Transformer module attends to all image representations, collecting both the global and local information to generate bins, we can omit the computional context aggregation in per-pixel module. Therefore, following <ref type="bibr" target="#b7">[8]</ref>, a light-weight pixel decoder is applied based on the popular FPN network <ref type="bibr" target="#b27">[28]</ref>.</p><p>Transformer decoder: We stack L = 3 Transformer layers for each scale of prediction refinement (i.e., 9 layers total) and 64 queries by default. The auxiliary loss is added to every intermediate Transformer decoder layer. Following <ref type="bibr" target="#b6">[7]</ref>, we adopt a simple deformable encoder <ref type="bibr" target="#b46">[47]</ref> to enhance the multi-scale image features. In our experiments, we observe that BinsFormer is competitive for depth estimation with a single decoder layer as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the state-of-the-art</head><p>This section compares the proposed approach with the current state-of-the-art monocular depth estimation methods.</p><p>KITTI : We evaluate on the Eigen split <ref type="bibr" target="#b10">[11]</ref> and Tab. 1 reports the results. BinsFormer significantly outperforms all the leading methods. Qualitative comparisons can be seen in the supplementary material. We then evaluate the proposed method on the online KITTI depth prediction benchmark server 4 and report the results in Tab. 2. While a saturation phenomenon persists in SILog, BinsFormer still achieves 6.1% improvement on this metric.</p><p>NYU-Depth-v2 : Tab. 3 lists the performance comparison results on the NYU-Depth-v2 dataset. While the performance of the state-of-the-art models tends to approach saturation, BinsFormer outperforms all the competitors with promi- In terms of classificationregression methods, we compare our bins generation method with pre-defined fixed UI/SID and Adabins <ref type="bibr" target="#b2">[3]</ref>. Results presented in Tab. 5 demonstrate the effectiveness of BinsFormer. We present bins predictions and probability distributions in <ref type="figure" target="#fig_5">Fig. 5</ref>, which indicates BinsFormer can adaptively estimate suitable bins for various dynamic scenes (e.g., for images containing large areas of distant pixels, predicted bins rather approaches the max depth).</p><p>Since we predict probability distribution maps for input images, it is possible to compute the measurement uncertainty <ref type="bibr" target="#b22">[23]</ref> for each ray by measuring the Maximum Likelihood Estimates (MLE) following <ref type="bibr" target="#b28">[29]</ref>. <ref type="figure" target="#fig_5">Fig. 5</ref> shows a trend <ref type="table">Table 5</ref>. Ablation study results on the NYU dataset. We compare BinsFormer with regression based methods and classification-regression based methods with different bins generalization strategies. Furthermore, we investigate the effectiveness of each component in BinsFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Cls   where uncertainty increases with distance, which has also been observed in unsupervised models that are capable of estimating uncertainty <ref type="bibr" target="#b20">[21]</ref>. Areas of fringes show very high uncertainty, likely attributed to the drastic variation of depth values and the lack of depth cues in these regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of bins:</head><p>To study the influence of the number of bins, we train our network for various values of N bins and measure the model performance.</p><p>Results are presented in Tab. 6. The enhancing performance gained by increasing the number of queries diminishes above N = 64. Hence, we use N = 64 for our final model. On top of that, we exemplify different areas that queries are responsible for sensing in <ref type="figure" target="#fig_5">Fig. 5</ref>. Since queries and predicted bins are one-to-one correspondences, there is a strong correlation between depth and interest area of queries.  <ref type="figure">6</ref>. Visualization of depth esitmation results and attention maps of the scene understanding query at the progressive multi-scale prediction refinement process. improve by adding more scale information. In detail, the performance increases significantly with the scale of the feature increasing until the number reaches three. We also provide immediate scale predictions in <ref type="figure">Fig. 6</ref>, which demonstrates predictions get sharper and more accurate with the help of the proposed multiscale refinement strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi</head><p>Auxiliary scene classification: Results in Tab. 5 demonstrate that the auxiliary scene understanding task can improve the model performance by introducing implicit supervision. Since the first query is responsible for classifying the environment, we visualize attention maps with the multi-scale features F to investigate the effectiveness of the auxiliary scene classification. As shown in <ref type="figure">Fig. 6</ref>, the classification query considers different areas of features at different scales and fully aggregates spatial information to provide hints for bins embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel classification-regression based monocular depth estimation method, called BinsFormer, that incorporates a Transformer module to prediction bins in a set-to-set manner, a per-pixel module to estimate high-resolution pixel-wise representations, and a depth estimation module to aggregate information to predict final depth maps. BinsFormer can adaptively generate bins and per-pixel probability distribution for accurate depth estimation. We propose an auxiliary scene understanding task and a multi-scale prediction refinement strategy that can be seamlessly integrated into the Transformer module. These two methods further boost model performance and only introduce negligible overhead. The performance of BinFormer achieves new state-of-the-art on two popular benchmark datasets. Moreover, the generalization ability of the method was further demonstrated in cross dataset experiments. <ref type="figure">Fig. 7</ref>. Network details of BinsFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Network Structure</head><p>More details of our network are shown in <ref type="figure">Fig 7.</ref> The outputs of FPN are fed into the Transformer module and interact with queries. MLPs project queries into bins embeddings, bins centers, and auxiliary scene classification results. The depth estimation module aggregates the information and predicts the final depth maps. It is also applied to each scale output to achieve multi-scale refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 More Qualitative Results</head><p>Here, we present more qualitative result comparisons to previous state-of-the-art methods, including BTS <ref type="bibr" target="#b25">[26]</ref>, Adabins <ref type="bibr" target="#b2">[3]</ref> and DORN <ref type="bibr" target="#b11">[12]</ref>. Detailed results on KITTI, KITTI online benchmark, SUN RGB-D are shown in <ref type="figure">Fig. 8, Fig. 9</ref> and <ref type="figure" target="#fig_2">Fig. 10</ref>, respectively. From the results, BinsFormer estimates better depth and recovers more details. Benefiting from the reasonable global bins generation, it predicts depth with an accurate overall scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Point Cloud Visualization</head><p>Monocular depth estimation aims to recover the 3D structures. To better see the 3D shape of the estimated depth map, we inv-project the image pixel back to the 3D space via the estimated depth maps. The point cloud visualizations on the test set of NYUv2 are shown in <ref type="figure" target="#fig_2">Fig. 11</ref>, where 3D structures can be recovered satisfyingly.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Regression vs. classification vs. classification-regression. (a) Depth estimation with per-pixel regression directly predicts continuous depth and applies regression loss. (b) Classification predicts probabilistic representations and is supervised by classification loss. Predictions are discrete. (c) Classification-regression combines the best of both, which first predicts probabilistic representations and then gets continuous predictions via a linear combination with bins centers. (d) Commonly used bins types. Adaptive means adaptively generating bins based on input images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>BinsFormer overview: We use a backbone and a pixel decoder to extract and upsample image features. A transformer decoder attends to multi-scale image features F and generate 1+N output embedding. The first one predicts enviroment classification and the other N ones independently predict N bins lengths b and N bins embeddings f b , respectively. Then the model predicts the probability distribution map P via a dot product between pixel representations fp and bins embeddings f b . Note, the dimensions for the dot are shown in gray. The final depth estimation is calculated by a linear combination between the probability distribution map P and post-processed N bins centers c(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Multi-scale prediction refinement: we propose to apply progressive interaction and aggregation among the embeddings and image features. At each scale of the Transformer decoder, one resolution of the multi-scale feature is fed into the Transformer layer. Multi-scale losses with gradually strengthened punishment weights are applied on each Transformer layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of different areas that queries are responsible for sensing, uncertainty maps, and probability distribution of selected points in images. We randomly select three points in the RGB images and plot their distributions in the Prob. plots, where Bins centers are presented by the small ticks upon the x-aris.Figure QueryN visualizes the similarity of N th query and the per-pixel representation fp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .Fig. 10 .Fig. 11 .</head><label>91011</label><figDesc>Qualitative results on KITTI online benchmark. Qualitative results on unseen SUN RGB-D dataset. Point cloud visualization on the test set of NYUv2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison of performances on the KITTI dataset. The reported numbers are from the corresponding original papers. Measurements are made for the depth range from 0m to 80m. Best / Second best results are marked bold / underlined. E-B5 are short for EfficientNet-B5<ref type="bibr" target="#b37">[38]</ref>. ?and ?represent the models are pre-trained by ImageNet-22K and auxiliary depth estimation dataset, respectively. Comparison of performances on the KITTI depth estimation benchmark test set. Reported numbers are from the official benchmark website.</figDesc><table><row><cell>Method</cell><cell>Ref</cell><cell>Backbone</cell><cell>?1?</cell><cell>?2?</cell><cell>?3?</cell><cell cols="4">REL ? Sq-rel ? RMS ? RMS log ?</cell></row><row><cell cols="2">Godard et al. [16] CVPR 2017</cell><cell>ResNet-50</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell><cell>0.114</cell><cell cols="2">0.898 4.935</cell><cell>0.206</cell></row><row><cell cols="2">Johnston et al. [21] CVPR 2020</cell><cell>ResNet-101</cell><cell>0.889</cell><cell>0.962</cell><cell>0.982</cell><cell>0.106</cell><cell cols="2">0.861 4.699</cell><cell>0.185</cell></row><row><cell>Gan et al. [13]</cell><cell>ECCV 2018</cell><cell>ResNet-101</cell><cell>0.890</cell><cell>0.964</cell><cell>0.985</cell><cell>0.098</cell><cell cols="2">0.666 3.933</cell><cell>0.173</cell></row><row><cell cols="2">DORN et al. [12] CVPR 2018</cell><cell>ResNet-101</cell><cell>0.932</cell><cell>0.984</cell><cell>0.994</cell><cell>0.072</cell><cell cols="2">0.307 2.727</cell><cell>0.120</cell></row><row><cell>Yin et al. [44]</cell><cell>ICCV 2019</cell><cell>ResNext-101</cell><cell>0.938</cell><cell>0.990</cell><cell>0.998</cell><cell>0.072</cell><cell>-</cell><cell>3.258</cell><cell>0.117</cell></row><row><cell>PGA-Net [43]</cell><cell>TPAMI 2020</cell><cell>ResNet-50</cell><cell>0.952</cell><cell>0.992</cell><cell>0.998</cell><cell>0.063</cell><cell cols="2">0.267 2.634</cell><cell>0.101</cell></row><row><cell>BTS [26]</cell><cell>Arxiv 2019</cell><cell>DenseNet-161</cell><cell>0.956</cell><cell>0.993</cell><cell>0.998</cell><cell>0.059</cell><cell cols="2">0.245 2.756</cell><cell>0.096</cell></row><row><cell cols="4">TransDepth [45] ICCV 2021 ResNet-50+ViT-B ? 0.956</cell><cell cols="3">0.994 0.999 0.064</cell><cell cols="2">0.252 2.755</cell><cell>0.098</cell></row><row><cell>DPT [34]</cell><cell cols="3">ICCV 2021 ResNet-50+ViT-B ? 0.959</cell><cell cols="3">0.995 0.999 0.062</cell><cell>-</cell><cell>2.573</cell><cell>0.092</cell></row><row><cell>AdaBins [3]</cell><cell cols="2">CVPR 2021 E-B5+mini-ViT</cell><cell>0.964</cell><cell cols="3">0.995 0.999 0.058</cell><cell cols="2">0.190 2.360</cell><cell>0.088</cell></row><row><cell></cell><cell></cell><cell>ResNet-18</cell><cell>0.954</cell><cell cols="3">0.994 0.999 0.065</cell><cell cols="2">0.230 2.574</cell><cell>0.099</cell></row><row><cell></cell><cell></cell><cell>ResNet-50</cell><cell>0.962</cell><cell cols="3">0.994 0.999 0.061</cell><cell cols="2">0.208 2.426</cell><cell>0.093</cell></row><row><cell>BinsFormer</cell><cell>Ours</cell><cell>Swin-Tiny Swin-Base</cell><cell>0.968 0.970</cell><cell cols="3">0.995 0.999 0.058 0.996 0.999 0.056</cell><cell cols="2">0.183 2.286 0.172 2.248</cell><cell>0.088 0.085</cell></row><row><cell></cell><cell></cell><cell>Swin-Base ?</cell><cell cols="4">0.974 0.997 0.999 0.053</cell><cell cols="2">0.156 2.141</cell><cell>0.080</cell></row><row><cell></cell><cell></cell><cell>Swin-Large ?</cell><cell cols="6">0.974 0.997 0.999 0.052 0.151 2.098</cell><cell>0.079</cell></row><row><cell cols="2">Method</cell><cell>SILog?</cell><cell cols="6">sqErrorRel? absErrorRel? iRMSE?</cell><cell></cell></row><row><cell cols="2">DORN [12]</cell><cell>11.77</cell><cell>2.23</cell><cell></cell><cell>8.78</cell><cell></cell><cell cols="2">12.98</cell><cell></cell></row><row><cell cols="2">BTS [26]</cell><cell>11.67</cell><cell>2.21</cell><cell></cell><cell>9.04</cell><cell></cell><cell cols="2">12.23</cell><cell></cell></row><row><cell cols="2">BANet [1]</cell><cell>11.55</cell><cell>2.31</cell><cell></cell><cell>9.34</cell><cell></cell><cell cols="2">12.17</cell><cell></cell></row><row><cell cols="2">PWA [27]</cell><cell>11.45</cell><cell>2.30</cell><cell></cell><cell>9.05</cell><cell></cell><cell cols="2">12.32</cell><cell></cell></row><row><cell cols="2">ViP-DeepLab [33]</cell><cell>10.80</cell><cell>2.19</cell><cell></cell><cell>8.94</cell><cell></cell><cell cols="2">11.77</cell><cell></cell></row><row><cell cols="2">BinsFormer</cell><cell>10.14</cell><cell>1.69</cell><cell></cell><cell>8.23</cell><cell></cell><cell cols="2">10.90</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Comparison of performances on the NYU-Depth-v2 dataset. The reported numbers are from the corresponding original papers. We provide results of BinsFormer based on various encoder to demonstrate the superior performance. Results of models trained on the NYU-Depth-v2 dataset and tested on the SUN RGB-D dataset<ref type="bibr" target="#b36">[37]</ref> without fine-tuning. We first evaluate each component of BinsFormer. We start with the per-pixel regression baseline. The Reg. Baseline uses the pixel-level module of BinsFormer and directly outputs per-pixel depth predictions (i.e., w/o Transformer and depth estimation module). For a fair comparison, we design the Reg. Baseline+, which adds the transformer module and query embedding MLP to the Reg. Baseline but predict depth in a regression manner (i.e., w/o depth estimation module). Thus, Reg. Baseline+ and BinsFormer differ only in the formulation: regression v.s. classification-regression.</figDesc><table><row><cell>Method</cell><cell>Encoder</cell><cell>?1?</cell><cell>?2?</cell><cell cols="2">?3? REL? RMS? log10?</cell></row><row><cell>DORN [12]</cell><cell>ResNet-101</cell><cell cols="4">0.828 0.965 0.992 0.115 0.509 0.051</cell></row><row><cell>Yin et al. [44]</cell><cell>ResNeXt-101</cell><cell cols="4">0.875 0.976 0.994 0.108 0.416 0.048</cell></row><row><cell>BTS [26]</cell><cell>DenseNet-161</cell><cell cols="4">0.885 0.978 0.994 0.110 0.392 0.047</cell></row><row><cell>DAV [20]</cell><cell>DRN-D-22</cell><cell cols="3">0.882 0.980 0.996 0.108 0.412</cell><cell>-</cell></row><row><cell cols="6">TransDepth [45] Res-50+ViT-B ? 0.900 0.983 0.996 0.106 0.365 0.045</cell></row><row><cell>DPT [34]</cell><cell cols="5">Res-50+ViT-B ? 0.904 0.988 0.998 0.110 0.357 0.045</cell></row><row><cell>AdaBins [3]</cell><cell cols="5">E-B5+Mini-ViT 0.903 0.984 0.997 0.103 0.364 0.044</cell></row><row><cell></cell><cell>ResNet-18</cell><cell cols="4">0.866 0.976 0.994 0.122 0.410 0.050</cell></row><row><cell></cell><cell>ResNet-50</cell><cell cols="4">0.882 0.978 0.995 0.111 0.390 0.047</cell></row><row><cell>BinsFormer</cell><cell>Swin-Tiny Swin-Base</cell><cell cols="4">0.890 0.983 0.996 0.113 0.379 0.047 0.902 0.984 0.996 0.104 0.362 0.044</cell></row><row><cell></cell><cell>Swin-Base ?</cell><cell cols="4">0.917 0.988 0.997 0.098 0.340 0.041</cell></row><row><cell></cell><cell>Swin-Large ?</cell><cell cols="4">0.925 0.989 0.997 0.094 0.330 0.040</cell></row><row><cell>Method</cell><cell>Encoder</cell><cell>?1?</cell><cell>?2?</cell><cell cols="2">?3? REL? RMS? log10?</cell></row><row><cell>Chen et al. [6]</cell><cell>SENet [19]</cell><cell cols="4">0.757 0.943 0.984 0.166 0.494 0.071</cell></row><row><cell cols="2">Yin et al. [44] ResNeXt-101</cell><cell cols="4">0.696 0.912 0.973 0.183 0.541 0.082</cell></row><row><cell>BTS [26]</cell><cell>DenseNet-161</cell><cell cols="4">0.740 0.933 0.980 0.172 0.515 0.075</cell></row><row><cell cols="6">Adabins [3] E-B5+Mini-ViT 0.771 0.944 0.983 0.159 0.476 0.068</cell></row><row><cell></cell><cell>ResNet-18</cell><cell cols="4">0.738 0.935 0.982 0.175 0.504 0.074</cell></row><row><cell>BinsFormer</cell><cell>Swin-Tiny</cell><cell cols="4">0.760 0.945 0.985 0.162 0.478 0.069</cell></row><row><cell></cell><cell>Swin-Large ?</cell><cell cols="4">0.805 0.963 0.990 0.143 0.421 0.061</cell></row><row><cell cols="6">nent margins in all metrics. It indicates the effectiveness of our proposed meth-</cell></row><row><cell cols="5">ods. Qualitative comparisons can be seen in Fig. 4.</cell></row><row><cell cols="6">SUN RGB-D : Following Adabins [3], we conduct a cross-dataset evaluation</cell></row><row><cell cols="6">by training our models on the NYU-Depth-v2 dataset and evaluating them on</cell></row><row><cell cols="6">the test set of the SUN RGB-D dataset without any fine-tuning. As shown</cell></row><row><cell cols="6">in Tab. 4, significant improvements in all the metrics indicate an outstanding</cell></row><row><cell cols="6">generalization performance of BinsFormer. Qualitative results are shown in the</cell></row><row><cell cols="2">supplementary material.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.4 Ablation studies</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BinsFormer:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.-Reg. Ada. Bins Bins Query Aux. Info. Multi-S.</figDesc><table><row><cell cols="7">RGB Query 8 Query 24 Query 50 Query 64 Uncert. Depth</cell><cell>Prob.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">?1? REL? RMS?</cell></row><row><cell>Reg. Baseline Reg. Baseline+ Fix UD Fix SID Adabins [3] BinsFormer</cell><cell>? ? ? ? ? ?</cell><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>? ? ?</cell><cell>?</cell><cell cols="2">0.852 0.130 0.422 0.870 0.123 0.403 0.851 0.130 0.424 0.825 0.145 0.453 0.850 0.136 0.434 0.878 0.116 0.397 0.882 0.115 0.388 0.890 0.113 0.379</cell></row><row><cell>RGB</cell><cell>BTS [26]</cell><cell></cell><cell cols="2">DPT [34] Adabins [3]</cell><cell>Ours</cell><cell></cell><cell>GT</cell></row><row><cell cols="7">Fig. 4. Qualitative comparison on the NYU-Depth-v2 dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation study on the NYU dataset: effect of number of bins (N) on performance. Similar to Adabins<ref type="bibr" target="#b2">[3]</ref>, we observe that performance starts to saturate as N increases above 64.</figDesc><table><row><cell cols="2"># of queries ?1?</cell><cell>?2?</cell><cell>?3? REL? RMS? log10?</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell>Not Converge</cell></row><row><cell>8</cell><cell cols="3">0.889 0.981 0.995 0.115 0.390 0.048</cell></row><row><cell>16</cell><cell cols="3">0.887 0.981 0.995 0.113 0.384 0.048</cell></row><row><cell>32</cell><cell cols="3">0.889 0.982 0.995 0.114 0.380 0.047</cell></row><row><cell>64</cell><cell cols="3">0.890 0.983 0.996 0.113 0.379 0.047</cell></row><row><cell>128</cell><cell cols="3">0.889 0.982 0.994 0.112 0.380 0.046</cell></row><row><cell>256</cell><cell cols="3">0.886 0.980 0.995 0.115 0.383 0.048</cell></row><row><cell>512</cell><cell cols="3">0.883 0.977 0.992 0.119 0.393 0.050</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>-scale strategy: We then study the effectiveness of the proposed multiscale strategy by changing the input features F of the Transformer module. Results are presented in Tab. 7. Interestingly, the performance does not always</figDesc><table><row><cell>RGB</cell><cell>Depth at scale 1-3</cell><cell>Att. maps of cls. query at scale 1-3</cell></row><row><cell>Fig.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Ablation study on the NYU dataset: performance of BinsFormer for different scales refinement.</figDesc><table><row><cell>f e</cell><cell cols="2"># layers ?1?</cell><cell>?2?</cell><cell>?3? REL? RMS? log10?</cell></row><row><cell>f4</cell><cell>3</cell><cell cols="3">0.882 0.980 0.995 0.115 0.388 0.048</cell></row><row><cell>f3, f4</cell><cell>6</cell><cell cols="3">0.888 0.982 0.995 0.113 0.380 0.047</cell></row><row><cell>f2, f3, f4</cell><cell>9</cell><cell cols="3">0.890 0.983 0.996 0.113 0.379 0.047</cell></row><row><cell>f1, f2, f3, f4</cell><cell>12</cell><cell cols="3">0.881 0.979 0.994 0.118 0.392 0.049</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_ prediction</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bidirectional attention network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M U</forename><surname>Vianney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00743</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High quality monocular depth estimation via transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11941</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4009" to="4018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Structure-aware residual pyramid network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06023</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Maskedattention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01527</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Soft labels for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4738" to="4747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with affinity, vertical pooling, and label enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="224" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Digging into selfsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="581" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4756" to="4765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-grained semantics-aware representation enhancement for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12642" to="12652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using relative depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Patch-wise attention network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1873" to="1881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural rgb (r) d sensing: Depth and uncertainty from a video camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10986" to="10995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Depth estimation from monocular images and sparse radar using deep ordinal regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3343" to="3347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vip-deeplab: Learning visual perception with depth-aware video panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3997" to="4008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
	<note>Sparsity invariant cnns</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cliffnet for monocular depth estimation with hierarchical embedding loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="316" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic graph attention network with conditional kernels for pixel-wise prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5684" to="5693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transformer-based dual relation graph for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-tosequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
