<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Object Permanence from Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Shamsian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofri</forename><surname>Kleinfeld</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA Research</orgName>
								<address>
									<settlement>Tel-Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Object Permanence from Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object Permanence allows people to reason about the location of non-visible objects, by understanding that they continue to exist even when not perceived directly. Object Permanence is critical for building a model of the world, since objects in natural visual scenes dynamically occlude and contain each-other. Intensive studies in developmental psychology suggest that object permanence is a challenging task that is learned through extensive experience.</p><p>Here we introduce the setup of learning Object Permanence from labeled videos. We explain why this learning problem should be dissected into four components, where objects are (1) visible, (2) occluded, (3) contained by another object and (4) carried by a containing object. The fourth subtask, where a target object is carried by a containing object, is particularly challenging because it requires a system to reason about a moving location of an invisible object. We then present a unified deep architecture that learns to predict object location under these four scenarios. We evaluate the architecture and system on a new dataset based on CATER, and find that it outperforms previous localization methods and various baselines. * equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding dynamic natural scenes is often challenged by objects that contain or occlude each other. To reason correctly about such visual scenes, systems need to develop a sense of Object Permanence (OP) <ref type="bibr" target="#b19">[20]</ref>. Namely, the understanding that objects continue to exist and preserve their physical characteristics, even if they are not perceived directly. For example, we want systems to learn that a pedestrian occluded by a truck may emerge from its other side, but that a person entering a car would "disappear" from the scene.</p><p>The concept of OP received substantial attention in the cognitive development literature. Piaget hypothesized that infants develop OP relatively late (at two years of age), suggesting that it is a challenging task that requires deep modelling of the world based on sensory-motor interaction with objects. Later evidence showed that children learn OP for occluded targets early <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Still, only at a later age do children develop understanding of objects that are contained by other objects <ref type="bibr" target="#b24">[25]</ref>. Based on these experiments we hypothesize that reasoning about the location of non-visible objects may be much harder when they are carried inside other moving objects.</p><p>Reasoning about the location of a target object in a video scene involves four different subtasks of increasing complexity <ref type="figure" target="#fig_0">(Figure 1</ref>). These four tasks are based on the state of the target object, depending if it is (1) visible, (2) occluded, (3) contained or (4) carried. The visible case is perhaps the simplest task, and corresponds to object detection, where one aims to localize an object that is visible. Detection was studied extensively and is viewed as a key component in computer vision systems. The second task, occlusion, is to detect a target object which becomes transiently invisible by a moving occluding object (e.g., bicycle behind a truck). Tracking objects under occlusion can be very challenging, especially with long-term occlusions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Third, in a containment scenario, a target object may be located inside another container object and become non-visible <ref type="bibr" target="#b27">[28]</ref> (e.g., a person enters a store). Finally, the fourth case of a carried object is arguably the most challenging task. It requires inferring the location of a non-visible object located inside a moving containing object (e.g., a person enters a taxi that leaves the scene). Among the challenging aspects of this task is the need to keep a representation of which object should be tracked at every time point and the need to "switch states" dynamically through time. This task received little attention in the computer vision community so far.</p><p>We argue that reasoning about the location of a non-visible object should address two distinct and fundamentally different cases: occlusion and containment. First, to localize an occluded object, an agent has to build an internal state that models how the object moves. For example, when we observe a person walking in the street, we can predict her ever-changing location even if occluded by a large bus. In this mode, our reasoning mechanism keeps attending to the person and keeps inferring her location from past data. Second, localizing contained objects is fundamentally different. It requires a reasoning mechanism that switches to attend to the containing object, which is visible. Here, even though the object of interest is not-visible, its location can be accurately inferred from the location of the visible containing object. We demonstrate below that incorporating these two reasoning mechanisms leads to more accurate localization in all four subtasks.</p><p>Specifically, we develop a unified approach for learning all four object localization subtasks in video. We design a deep architecture that learns to localize objects that may be visible, occluded, contained or carried. Our architecture consists of two reasoning modules designed to reason about (1) carried or contained targets, and (2) occluded or visible targets. The first reasoning component is explicitly designed to answer the question "Which object should be tracked now?". It does so by using an LSTM to weight the perceived locations of the objects in the scene. The second reasoning component leverages the information about which object should be tracked and previous known locations of the target to localize the target, even if it is occluded. Finally, we also introduce a dataset that is based on videos from CATER <ref type="bibr" target="#b7">[8]</ref>, enriched with new annotations about task type and about ground-truth location of all objects.</p><p>Our main novel contributions are: <ref type="bibr" target="#b0">(1)</ref> We conceptualize that localizing nonvisible objects requires two types of reasoning: about occluded objects and about carried ones. <ref type="bibr" target="#b1">(2)</ref> We define four subtypes of localization tasks and introduce annotations for the CATER dataset to facilitate evaluating each of these subtasks.</p><p>(3) We describe a new unified architecture for all four subtasks, which can capture the two types of reasoning, and we show empirically that it outperforms multiple strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relational Reasoning in Synthetic Video Datasets</head><p>Recently, several studies provided synthetic datasets to explore object interaction and reasoning. Many of these studies are based on CLEVR <ref type="bibr" target="#b11">[12]</ref>, a synthetic dataset designed for visual reasoning through visual question answering. CLEVRER <ref type="bibr" target="#b30">[31]</ref> extended CLEVR to video, focusing on the causal structures underlying object interactions. It demonstrated that visual reasoning models that thrive on perception based tasks often perform poorly in causal reasoning tasks.</p><p>Most relevant for our paper, CATER <ref type="bibr" target="#b7">[8]</ref> is a dataset for reasoning about object action and interactions in video. One of the three tasks defined in CATER, the snitch localization task, is closely related to the OP problem studied here. It is defined as localizing a target at the end of a video, where the target is usually visible. Our work refines their setup, learning to localize the target through the full video, and breaks down prediction into four types of localization tasks. As a result, we provide a fine-grained insight about the architectures and reasoning that is required for solving the complex localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Architectures for Video Reasoning</head><p>Several recent papers studied the effectiveness of CNN-based architectures for video action recognition. Many approaches use 3D convolutions for spatiotem-poral feature learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref> and separate the spatial and temporal modalities by adding optical flow as a second stream <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>. These models are computationally expensive because 3D convolution kernels may be costly to compute. As a result, they may limited to sequence length to 20-30 frames <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>. In <ref type="bibr" target="#b32">[33]</ref> it was proposed to sparsely sample video frames to capture temporal relations in action recognition datasets. However, sparse sampling may be insufficient for long occlusion and containment sequences, which is the core of our OP focus.</p><p>Another strategy for temporal aggregation is to use recurrent architectures like LSTM <ref type="bibr" target="#b9">[10]</ref>, connecting the underlying CNN output along the temporal dimension <ref type="bibr" target="#b31">[32]</ref>. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b22">23]</ref> combined LSTM with spatial attention, learning to attend to those parts of the video frame that are relevant for the task as the video progresses. In Section 6 we experiment with a spatial attention module, which learns to dynamically focus on relevant objects.</p><p>Tracking with Object Occlusion. A large body of work has been devoted to tracking objects <ref type="bibr" target="#b17">[18]</ref>. For objects under complex occlusion like carrying, early work studied tracking using classical techniques and without deep learning methods. For instance, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref> used the idea of object permanence to track objects through long-term occlusions. They located objects using adaptive appearance models, spatial distributions and inter-occlusion relationships. In contrast, the approach presented in this paper focuses on a single deep differentiable model to learn motion reasoning end-to-end. <ref type="bibr" target="#b8">[9]</ref> succeeds to track occluded targets by learning how their movement is coupled with the movement of other visible objects. Unfortunately, the dataset studied here, CATER <ref type="bibr" target="#b7">[8]</ref>, has weak objectobject motion coupling by design. Specifically, when measuring the correlation between the movement of the target and other object (as in <ref type="bibr" target="#b8">[9]</ref>), we found that the correlation in 94% of the videos was not statistically significant.</p><p>More recently, models based on Siamese neural network achieved SOTA results in object tracking <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34]</ref>. Despite the power of these architectures, tracking highly-occluded objects is still challenging <ref type="bibr" target="#b17">[18]</ref>. The tracker of <ref type="bibr" target="#b33">[34]</ref>, DaSiamRPN, extends the region-proposal sub-network of <ref type="bibr" target="#b14">[15]</ref>. It was designed for long-term tracking and handles full occlusion or out-of-view scenarios. DaSi-amRPN was used as a baseline for the snitch localization task in CATER <ref type="bibr" target="#b7">[8]</ref>, and we evaluated its performance for the OP problem in Section 6.</p><p>Containment. Few recent studies explored the idea of containment relations. <ref type="bibr" target="#b15">[16]</ref> recovered incomplete object trajectories by reasoning about containment relations. <ref type="bibr" target="#b27">[28]</ref> proposed an unsupervised model to categorize spatial relations, including containment between objects. The containment setup defined in these studies differs from the one defined here in that the contained object is always at least partially visible <ref type="bibr" target="#b27">[28]</ref>, or the containment does not involve carrying <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Learning Setup: Reasoning about Non-Visible Objects</head><p>We next formally define the OP task and learning setup. We are given a set of videos v 1 , ..., v N where each frame x i t in video v i is accompanied by the bounding box position B i t of the target object as its label. The goal is to predict for each frame a bounding boxB i t of the target object that is closest (in terms of L 1 distance) to the ground-truth bounding box B i t . We define four localization tasks: (1) Localizing a visible object, which we define as an object that is at least partially visible. (2) Localizing an occluded object, which we define as an object that is fully occluded by another object.</p><p>(3) Localizing an object contained by another object, thus also completely non visible. (4) Localizing an object that is carried along the surface by a containing object. Thus in this case the target is moving while being completely nonvisible. Together, these four tasks form a localization task that we call objectpermanence localization task, or OP.</p><p>In Section 7.2, we also study a semi-supervised learning setup, where at training time the location B i t of the target is provided only in frames when it is visible. This would correspond to the case of a child learning object permanence without explicit feedback about where an object is located when it is hidden.</p><p>It is instructive to note how the task we address here differs from the tasks of relation or reaction recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref>. In these tasks, models are trained to output an explicit label that captures the name of the interaction or relation (e.g., "behind", "carry"). In our task, the model aims to predict the location of the target (a regression problem), but it is not trained to name it explicitly (occluded, contained). While it is possible that the model creates some implicit representation describing the visibility type, this is not mandated by the loss or the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>We describe a deep network architecture designed to address the four localization subtasks of the OP task. We refer to the architecture as OPNet. It contains three modules, that account for the perception and inference computations which facilitate OP (see <ref type="figure" target="#fig_1">Figure 2</ref>).</p><p>Perception and detection module <ref type="figure" target="#fig_1">(Figure 2a</ref>): A perception module, responsible for detecting and tracking visible objects. We incorporated a Faster R-CNN <ref type="bibr" target="#b20">[21]</ref> object detection model, fine-tuned on frames from our dataset, as the perception component of our model. After pre-training, we used the detector to output the bounding boxes together with identifiers of all objects in any given frame. Specifically, we represent a frame using a K ? 5 matrix. Each row in the matrix represents an object using 5 values: four values of the bounding box (x 1 , y 1 , x 2 , y 2 ) and one visibility bit, which indicates whether the object is visible or not. As the video progresses, we assign a unique row to each newly identified object. If an object is not detected in a given frame, its corresponding information (assigned row) is set to zero. In practice, K = 15 was the maximal number of objects in a single video in our dataset. Notably, the videos in the dataset we used do not contain two identical objects, but we found that the detector sometimes mistakes one object for another.</p><p>"Who to track?" module ( <ref type="figure" target="#fig_1">Figure 2c</ref>): responsible for understanding which object is currently covering the target. This component consists of a single LSTM layer with a hidden dimension of 256 neurons and a linear projection matrix. After applying the LSTM to the object bounding boxes, we project its output to K neurons, each representing a distinct object in the frame. Finally we apply a softmax layer, resulting in a distribution over the objects in the frame. This distribution can be viewed as an attention mask focusing on the object that covers the target in this frame. Importantly, we do not provide explicit supervision to this attention mask (e.g., by explicitly "telling the model" during training what is the correct attention mask). Rather, our only objective is the location of the target. The output of this module is 5 numbers per frame. It is computed as the the weighted average over the K ? 5 outputs of the previous stage, weighted by the attention mask.</p><p>"Where is it" module ( <ref type="figure" target="#fig_1">Figure 2b</ref>): learns to predict the location of occluded targets. This final component consists of a second LSTM and a projection matrix. Using the output of the previous component, this component is responsible for predicting the target localization. It takes the output of the previous step (5 values per frame), feeds it into the LSTM and projects its output to four units, representing the predicted bounding box of the target for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The LA-CATER Dataset</head><p>To train models and evaluate their performance on the four OP subtasks defined above, we introduce a new set of annotations to the CATER dataset <ref type="bibr" target="#b7">[8]</ref>. We refer to these as Localization Annotations (LA-CATER).</p><p>The CATER dataset consists of 5,500 videos generated programmatically using the Blender 3D engine. Each video is 10-second long (300 frames) and contains 5 to 10 objects. Each object is characterized by its shape (cube, sphere, cylinder and inverted cone), size (small, medium, large), material (shiny metal and matte rubber) and color (eight colors). Every video contains a golden small sphere referred to as "the snitch", that is used as the target object which needs to be localized.</p><p>For the purpose of this study, we generated videos following a similar configuration to the one used by CATER, but we computed additional annotations during video generation. Specifically, we augmented the CATER dataset with ground-truth bounding boxes locations of all objects. These annotations were programmatically extracted from the Blender engine, by projecting the internal 3D coordinates of objects are to the 2D pixel space.</p><p>We further annotated videos with detailed frame-level annotations. Each frame was labeled with one of four classes: visible, fully occluded, contained (i.e., covered, static and non-visible) and carried (i.e., covered, moving and nonvisible). This classification of frames matches the four localization subtasks of the OP problem. To compute these annotations, we computed the line-of sight from the camera position to determine if a target is occluded by another object, or occluding it.</p><p>LA-CATER includes a total number of 14K videos split into train, dev and test datasets. See <ref type="table">Table 1</ref> for a classification of video frames to each one of the localization subtasks across the dataset splits. Further details about dataset preparation are provided in appendix C. Occluded and carried target frames make up less than 8% of the frames, but they present the most challenging prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We describe our experimental setup, compared methods and evaluation metrics. Implementation details are given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baselines and Model Variants</head><p>We compare our proposed OPNet with six other architectures designed to solve the OP tasks. Since we are not aware of previously published unified architectures designed to solve all OP tasks at once, we used existing models as components in our baselines. All baseline models receive the predictions of the object detector (perception) component as their input.</p><p>(A) Programmed Models. We evaluated two programmed models. These models are "hard-coded" rather than learned. They are designed to reflect models that programmatically solve the reasoning task.</p><p>-(1) Detector + Tracker. Using the detected location of the target, this method initiates a DaSiamRPN tracker <ref type="bibr" target="#b33">[34]</ref> to track the target. Whenever the target is no longer visible, the tracker is re-initiated to track the object located in the last known location of the target. -(2) Detector + Heuristic. When the target is not detected, the model switches from tracking the target to tracking the object located closest to last known location of the target. The model also employs an heuristic logic to adjust between the sizes of the current tracked object and the original target.</p><p>(B) Learned Models. We evaluated four learned baselines with an increasing level of representation complexity.</p><p>- by introducing a much complex representations for objects in frame. We utilized a transformer encoder <ref type="bibr" target="#b28">[29]</ref> after up-sampling the input representations, employing self attention between all the objects in a frame. We used a transformer encoder with 2 layers and 2 attention heads, yielding a single vector containing the target attended values. These attended values, which corresponds to each other object in the frame, are then fed into the LSTM. -(7) LSTM + MLP. This model <ref type="figure" target="#fig_1">(Figure 2)</ref> ablates the second LSTM module (c) in the model presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Metric</head><p>We evaluate model performance at a given frame t by comparing the predicted target localization and the ground truth (GT) target localization. We use two metrics as follows. First, the intersection over union (IoU) metric.</p><formula xml:id="formula_0">IoU t = B GT t ? B p t B GT t ? B p t ,<label>(1)</label></formula><p>where B p t denotes the predicted bounding box for frame t and B GT t denotes the ground truth bounding box for frame t.</p><p>Second, we evaluate models using the mean average precision (MAP) metric. MAP is computed by employing an indicator function on each frame, determining whether the IoU value is greater than a predefined threshold, then averaging across frames in a single video and all the videos in the dataset.</p><formula xml:id="formula_1">AP = 1 n n t=1 1 t , where 1 t = 1 IoU t &gt; IoU threshold 0 otherwise (2) M AP = 1 N N v=1 AP v .<label>(3)</label></formula><p>These per-frame metrics allow us to quantify the performance on each of the four OP subtasks separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We start with comparing OPNet with the baselines presented in Section 6.1. We then provide more insights into the performance of the models by repeating the evaluations with "Perfect Perception" in Section 7.  <ref type="table">Table 2</ref>. Mean IoU performance of various models on the LA-CATER test data. "?" denotes the standard error of the mean (SEM). OPNet performs consistently well across all subtasks. Also, on the contained and carried frames OPNet is significantly better than the other methods.</p><p>We first compare OPNet and the baselines presented in Section 6.1. <ref type="table">Table 2</ref> shows IoU for all models in all four sub-tasks and <ref type="figure" target="#fig_2">Figure 3</ref> presents the MAP accuracy of the models across different IoU thresholds.</p><p>It can be seen in <ref type="table">Table 2</ref> that OPNet performs consistently well across all subtasks and outperforms all other models overall. On the visible and occluded frames performance is similar to other baselines. But on the contained and carried frames, OPNet is significantly better than the other methods. This is likely due to OPNet's explicit modeling of the object to be tracked. <ref type="table">Table 2</ref> also reports results for two variants of OPNet: OPNet (LSTM+MLP) and OPNet (LSTM+LSTM). The former is missing the second module ("Where is it" in <ref type="figure" target="#fig_2">Figure 3</ref>) which is meant to handle occlusion and indeed under-performs for occlusion frames (the "occluded" and "contained" subtasks). This highlights the importance of using the two LSTM modules in <ref type="figure" target="#fig_2">Figure 3</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> provides interesting insight into the behavior of the programmed models (namely Detector + Tracker and Detector + Heuristic). It can be seen that these models perform well when the IoU threshold is low. This reflects the fact that they have a good coarse estimate of where the target is, but fail to provide more accurate localization. On the other hand our OPNet model does well for accurate localization, presumably due to its learned "Where is it" module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Reasoning with Perfect Perception</head><p>The OPNet model contains an initial "Perception" module that analyzes the frame pixels to get bounding boxes. Errors in this component will naturally propagate to the rest of the model and adversely affect results. Here we analyze the effect of the perception module by replacing it with ground truth bounding boxes and visibility bits. See Appendix D for details on extracting ground-truth annotations. In this setup all errors reflect failure in the reasoning components of the models.  <ref type="table">Table 3</ref>. Mean IoU performance with the Perfect Perception setup. "?" denotes the standard error of the mean (S.E.M.). Results are similar in nature to those with imperfect, detector-based, perception ( <ref type="table">Table 2</ref>). All models improve when using the groundtruth perception information. The subtask that improves the most with OPNet is the carried task. <ref type="table">Table 3</ref> provides the IoU performance and <ref type="figure" target="#fig_4">Figure 4</ref> the MAP for all compared methods on all four subtasks. The results are similar to the previous results. When compared to the previous section (imperfect, detector-based, perception), the overall trend is the same, but all models improve when using the ground truth perception information. Interestingly, the subtask that improves the most from using ground truth boxes is the carried task. This makes sense, since it is the hardest subtask and the one that most relies on having the correct object locations per frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Learning only from Visible Frames</head><p>We now examine a learning setup in which localization supervision is available only for frames where the target object is visible. This setup corresponds more naturally to the process by which people learn object permanence. For instance, imagine a child learning to track a carried (non visible) object for the first time and receiving a surprising feedback only when the object reappears in the scene.</p><p>In absence of any supervision when the target is non-visible, incorporating an extra auxiliary loss is needed to account for these frames. Towards this end, we incorporated an auxiliary consistency loss that minimizes the change between predictions in consecutive frames. L consistency = 1 n n t=1 b t ? b t?1 2 . The total loss is defined as an interpolation between the localization loss and the consistency loss, balancing their different scales: L = ? ? L localization + ? ? L consistency Details on choosing the values of ? and ? are provided in the supplementary. <ref type="table">Table 4</ref> shows the mean IoU for this setup (compare with <ref type="table">Table 2</ref>). The baselines perform well when the target is visible, fully occluded or contained without movement. This phenomenon goes hand-in-hand with the inductive bias of the consistency loss. Usually, to solve these subtasks, a model only needs to predict the last known target location. This explains why the OPNet (LSTM+MLP) model performs so poorly in this setup.</p><p>We note that the performance of non-OPNet models on the carried task is similar to that obtained using full supervision (see <ref type="table">Table 2</ref>, Section 7) . This suggests that these models fail to use the supervision for the "carried" task, and further reinforces the observation that localizing carried object is highly challenging.  <ref type="table">Table 4</ref>. IoU performance for the only visible supervision setting. "?" denote the standard error of the mean (S.E.M.). The models perform well when the target is visible, fully occluded or contained without movement, but not when the target is carried.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Comparison with CATER Data</head><p>The original CATER paper <ref type="bibr" target="#b7">[8]</ref> considered the "snitch localization" task, aiming to localize the snitch at the last frame of the video, and formalized as a classification problem. The x-y plane was divided with a 6-by-6 grid, and the goal was to predict the correct cell of that grid.</p><p>Here we report the performance of OPNet and relevant baselines evaluated on the exact setup as in <ref type="bibr" target="#b7">[8]</ref>, to facilitate comparison between our models and the results reported there. <ref type="table">Table 5</ref> shows the accuracy and L 1 distance metrics for this evaluation. OPNet significantly improves over all baselines from <ref type="bibr" target="#b7">[8]</ref>. It cuts down the classification error from 40% error down to 24%, and the l 1 distance from 1.2 to 0.54.  <ref type="table">Table 5</ref>. Classification accuracy on the CATER dataset using the metrics of <ref type="bibr" target="#b7">[8]</ref>. OPNet significantly improves over all baselines for the "snitch localization task".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Qualitative Examples</head><p>To gain insight into the successes and failures of our model, we now analyze specific examples. We provide two sets of examples to illustrate: (1) Comparison between baselines and variants over the same set of videos; <ref type="bibr" target="#b1">(2)</ref> Wins and losses of our approach. Model Comparison. We show two videos comparing OPNet with baselines and other variants. In both videos, four competing methods are applied to the same video scene. We recommend playing videos at a slow speed. (top right) successfully tracks the object containing the target, adjusting well to the target size. See <ref type="figure">Figure 5(a)</ref>. -The second model comparison video (https://youtu.be/KoxbhgalazU ) shows a visual scene analyzed by four methods. In this video, the target is being occluded by multiple objects, including full occlusion, which makes it challenging to track. The Tracker, Heuristic and OPNet MLP models occasionally drift from the target when it is fully occluded by a large object. OPNet (ours) successfully localizes the target throughout the video. See <ref type="figure">Figure 5</ref>(b).</p><p>Wins and Losses of OPNet. We provide interesting examples of OPNet success and failures, adding insights into the behaviour and limitations of the OPNet model.</p><p>-The video https://youtu.be/FnturB2Blw8 provides a "win" example. It demonstrates the power of OPNet and its "who to track" reasoning component. In the video, the model handles phases of recursive containment ("babushka"), which involve "carrying". It suggests that OPNet learns an implicit representation of the object actions (pick-up, slide, contain etc.) even though it was not explicitly trained to do so. See <ref type="figure">Figure 6</ref> (top row) -The video https://youtu.be/qkdQSHLrGqI illustrates a failure of our model.</p><p>It shows an example where OPNet fails to switch between tracked objects when the target is "carried". The model accidentally switches to a wrong cone object (the yellow cone) that already contains another object, not the target. Interestingly, OPNet properly identifies when the yellow cone is picked up and switches to track the blue ball that was contained by the yellow cone. It suggests that OPNet has implicitly learned the "meaning" of actions performed by objects, without being explicitly trained to do so. See <ref type="figure">Figure 6</ref> (bottom row)</p><p>Further insight may be provided by comparing the attention mask of the OPNet "Who to Track" module and the ground-truth mask of the containing or carrying object. Figure 7 compares these masks for success and failure cases. It can be seen that OPNet nicely tracks the correct object for most of the frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We considered the problem of localizing one target object in a highly dynamic scenes where the object can be occluded, contained or even carried away by another object. We name this task object permanence, following the cognitive concept of a target object that is physically present in a scene but is occluded and carried in various ways. We presented an architecture called OPNet, whose components correspond to the natural perceptual and reasoning stages of solving OP. Specifically, it has a module that learns to switch attention to another object if it infers that the object contains or carries th target. Our empirical evaluation shows that these components are needed for improving accuracy in this task.</p><p>Our results highlight a remaining gap between perfect perception and a pixelbased detector. It is expected that this gap may be even wider when applying OP to more complex natural videos in an open-world setting. It will be interesting to further improve detection architectures in order to reduce this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Erorr Analysis across the Video Corpus</head><p>Videos in our dataset vary substantially in terms of what OP tasks they involve. This has a large effect over localization accuracy, because it is much harder to localize a carried target than a visible one. To gain more insight into the performance of the leading models, we compare the localization IoU on a videoby-video basis. <ref type="figure" target="#fig_0">Figure S1</ref> depicts per-video IoU of OPNet and two other strong baselines. Each point corresponds to one video and the color reflects the type of frames in that video. <ref type="figure" target="#fig_0">Figure S1</ref>  Similarly, <ref type="figure" target="#fig_0">Figure S1</ref>(b) compares OPNet with the OPNet (LSTM + MLP) baseline, which contains only the first reasoning component (see Our Approach section). It shows that OPNet outperforms the baseline on videos including a high number of occlusion frames (colored in green). It also emphasizes that for most videos, OPNet is superior, as illustrated by the great number of points in the lower half of the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We trained OPNet and baseline variants using L 1 loss optimized using Adam optimizer with ? 1 = 0.9, ? 2 = 0.999, ? = 1e ? 08, and using a batch size of 16. We initialized the learning rate to 0.001 and employed a learning rate decay policy, which reduced the learning rate by a factor of 0.8 every 3 epochs without loss improvement. We tuned all hyperparameters using the validation set. We experimented with using a higher initial learning rate of 1e ? 2, but it turned out to be too noisy for the relatively small loss induced by the L 1 loss. We also tried lower learning rate (1e ? 4), but it did not converge to a good minimum.</p><p>The model was trained for 160 epochs, which we verified via manual inspection to be sufficient for convergence of all models. Early stopping was based on the validation-set mean IoU.</p><p>For comparisons with CATER <ref type="bibr" target="#b7">[8]</ref> ( <ref type="table">Table 5</ref> of the main paper), we used the accuracy values reported in their paper.</p><p>For the learning only from visible frames setup (Section 7.2 and <ref type="table">Table 4</ref> of the main paper) we used the values ? = 1 and ? = 0.5. We used these values to normalize the different scales of L localization and L consistency . We verified via manual inspection that (1) for the first 60-70 epochs the loss component L localization is significantly greater than the loss component L consistency . Thus, in this phase the model improves its prediction when the target is visible; (2) After 60-70 epochs the two loss components have the same scale. Thus, in this phase the model improves its prediction also when the target is not-visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C LA-CATER Dataset Preparation</head><p>Our new LA-CATER dataset augments the CATER dataset <ref type="bibr" target="#b7">[8]</ref> with groundtruth locations of all objects and with detailed frame level annotations. Also, instead of using the videos released by CATER we generated new videos using their configuration, and expanded their code to add ground-truth locations and frame-level annotations.</p><p>We now describe how we classify frames into the four corresponding OP subtasks. The CATER dataset annotates each frame with the actions occurring for each object in that frame. These actions are defined as follows:</p><p>-Slide. Object changes its position by sliding on the XY-plane. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Inferring object location in rich dynamic scenes involves four different tasks, and two different types of reasoning. (a) The target, a red ball, is fully visible. (b) The target is fully-or partially occluded by the static cube. (c) The target is located inside the cube and fully covered. (d) The non-visible target is located inside another moving object; its location changes even though it is not directly visible</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of Object Permanence network (OPNet) consists of three components. (a) Perception module for detection. (b) Reasoning module for inferring which object to track in case the target is carried or contained. (c) A second reasoning module for occluded or visible targets, and for refining the exact location of the predicted target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 3 )</head><label>3</label><figDesc>OPNet. The proposed model, as presented in Section 4. -(4) Baseline LSTM. This model uses a single unidirectional LSTM layer with a hidden state of 512 neurons, operating on the temporal (frames) dimension. The input to the LSTM is the concatenation of the objects input representations. It is the simplest learned baseline as the input representation is not transformed non-linearly before being fed to the LSTM. -(5) Non-Linear + LSTM. This model augments the previous model and increases the complexity of the scene representation. The input representations are upsampled using a linear layer followed by a ReLU activation, resulting in a 256-dimensional vector representation for each object in the frame. These high-dimensional objects representations are concatenated and fed into the LSTM. -(6) Transformer + LSTM. This model augments the previous baselines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Mean average precision (MAP) as a function of IoU thresholds. The two programmed models, Detector+Tracker (blue) and Detector+Heuristic (orange) perform well when the IoU threshold is low, providing a good coarse estimate of target location. OPNet performs well on all subtasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Mean average precision (MAP) as a function of IoU thresholds for reasoning with Perfect Perception (Section 7.1). The most notable performance gain of OPNet (pink and brown curves) was with carried targets (subtask d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .Fig. 6 .-Fig. 7 .</head><label>567</label><figDesc>Screenshots from the model comparison video files. Blue boxes denote the ground truth location. Yellow boxes denote the predicted location. OPNet (ours) is at the bottom right panel. (a) The target is contained and then carried by the blue cone and is captured successfully by OPNet. (b) The target is occluded by the red cone and purple ball. These occlusions confuse all baselines, while OPNet localizes the target accurately. Examples of a success case (top row) and a failure case (bottom row) for localizing a carried object. The blue box marks the ground-truth location. The yellow box marks the predicted location. Top (a) The target object is visible; (b-c) The target becomes covered and carried by the orange cone; (d-e) The big golden cone covers and carries the orange cone, illustrating recursive containment. The target object is not visible, but OPNet successfully tracks it. Bottom (c-d) OPNet accidentally switches to the wrong cone object (the yellow cone instead of the brown cone); (e) OPNet correctly finds when the yellow cone is picked up and switches to track the blue ball that was underneath. The first model comparison video (https://youtu.be/TZgoxoKcGrE ) shows one visual scene analyzed by four methods. OPNet (ours) successfully localizes the target throughout the video. When the target is "carried", the Transformer model (bottom left) fails to switch and instead of tracking the carrying object it keeps predicting the last seen location of the target. The Tracker model (top left) switches to a wrong object. The Heuristic model Switching attention across objects.In each pair of panels, each row traces the probability assigned to an object along the video in the ground truth (left) and predicted attention (right). (a) The system successfully switches attention from object 1 (target) when it is contained by object 6 and then carried by object 3. (b) After a successful switch from the object 1 to 10, the system incorrectly witches to object 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) shows how OPNet outperforms Transformer on videos including carried frames (colored in orange). Clearly, videos with carried frames are clustered in the lower half of the figure, where OPNet is superior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. S1 .</head><label>S1</label><figDesc>Sample-by-samples comparison of OPNet with two strong baselines. Each point represents the IoU of a video from the test set, achieved by OPNet and a baseline. (a) Videos with more than 7% carried frames are colored in orange. (b) Videos with more than 7% occlusion frames are colored in green. Points in the lower part corresponds to videos in which OPNet is superior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>-</head><label></label><figDesc>Pick-Place. Object is picked up in the air along the Z-axis, moved to a new location and placed down. -Contain. A special action performed by cones only, in which cone execute Pick-Place action and positioned on top of another object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>?0.13 53.62 ?0.58 39.98 ?0.38 34.45 ?0.40 71.23 ?0.51 Detector + Heuristic 90.06 ?0.14 47.03 ?0.73 55.36 ?0.53 55.87 ?0.59 76.91 ?0.43 Baseline LSTM 81.60 ?0.19 59.80 ?0.61 49.18 ?0.64 21.53 ?0.40 67.20 ?0.53 Non-Linear + LSTM 88.25 ?0.14 70.14 ?0.62 55.66 ?0.67 24.58 ?0.44 73.53 ?0.51 Transformer + LSTM 90.82 ?0.14 80.40 ?0.61 70.71 ?0.78 28.25 ?0.45 80.27 ?0.50 OPNet (LSTM + MLP) 88.11 ?0.16 55.32 ?0.85 65.18 ?0.89 57.59 ?0.85 78.85 ?0.52 OPNet (LSTM + LSTM) 88.89 ?0.16 78.83 ?0.56 76.79 ?0.62 56.04 ?0.77 81.94 ?0.41</figDesc><table><row><cell>Mean IoU? SEM</cell><cell>Visible</cell><cell>Occluded</cell><cell>Contained</cell><cell>Carried</cell><cell>Overall</cell></row><row><cell>Detector + Tracker</cell><cell>90.27</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1. Section 7.2 describes a semi-supervised setting of training with visible frames only. Finally, in Section 7.3 we compare OPNet with the models presented in the CATER paper on the original CATER data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>TRACKER 90.27 ?0.13 53.62 ?0.58 39.98 ?0.38 34.45 ?0.40 71.23 ?0.51 DETECTOR + HEURISTIC 95.59 ?0.34 30.40 ?0.81 59.81 ?0.47 59.33 ?0.50 81.24 ?0.49 BASELINE LSTM 75.22 ?0.31 50.52 ?0.75 45.10 ?0.62 19.12 ?0.36 61.41 ?0.53 NON-LINEAR + LSTM 88.63 ?0.25 65.73 ?0.82 58.77 ?0.70 23.89 ?0.41 74.53 ?0.54 TRANSFORMER + LSTM 93.99 ?0.24 81.31 ?0.88 75.75 ?0.85 28.01 ?0.44 83.78 ?0.55 OPNet (LSTM + MLP) 88.11 ?0.16 19.39 ?0.60 77.40 ?0.68 78.25 ?0.65 83.84 ?0.48 OPNet (LSTM + LSTM) 88.78 ?0.25 67.79 ?0.69 83.47 ?0.47 76.42 ?0.66 85.44 ?0.38</figDesc><table><row><cell>Mean IoU ? SEM</cell><cell>Visible</cell><cell>Occluded</cell><cell>Contained</cell><cell>Carried</cell><cell>Overall</cell></row><row><cell>DETECTOR +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>?0.16 80.39 ?0.54 68.35 ?0.76 27.39 ?0.45 78.09 ?0.49 Non Linear + LSTM 89.30 ?0.15 82.49 ?0.45 67.25 ?0.75 27.34 ?0.45 78.15 ?0.49 Transformer + LSTM 88.33 ?0.15 83.74 ?0.44 69.93 ?0.77 27.65 ?0.54 78.43 ?0.49 OPNet (LSTM + MLP) 88.45 ?0.17 48.03 ?0.82 10.95 ?0.51 7.28 ?0.30 61.18 ?0.69 OPNet (LSTM + LSTM) 88.95 ?0.16 81.84 ?0.48 69.01 ?0.76 27.50 ?0.45 78.50 ?0.49</figDesc><table><row><cell>Mean IoU</cell><cell>Visible</cell><cell>Occluded</cell><cell>Contained</cell><cell>Carried</cell><cell>Overall</cell></row><row><cell>Baseline LSTM</cell><cell>88.61</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This study was funded by a grant to GC from the Israel Science Foundation (ISF 737/2018), and by an equipment grant to GC and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18). AG received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation program (grant ERC HOLI 819080).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>-Contained Frames. We classify a frame as Contained when the target is contained by a cone. Explicitly, a frame is classified as Contained when it is annotated with the "contain" action in CATER, with a cone marked as the containing object and the target marked as the contained object. A frame with recursive containment, namely, a containing cone is itself contained by another cone, is also considered to be a contained frame. Frames are marked as contained from the moment the target is covered and until the containing object is picked up as part of pick-place action. -Carried Frames. We mark a frame as Carried when the target is contained by a cone (its action is marked in CATER as contained) and slides along with it. Frames are marked as carried from the beginning of the slide action until the end of the slide action. Thus, only frames corresponding to the slide action are marked as carried. -Occluded Frames. For frame t, we define the occlusion rate (OR) of object</p><p>x by object y as</p><p>Where Area x t is the area of object x in frame t. We define the distance from camera (DC) of object x</p><p>denote the 3D coordinates location of object x and the camera in frame t respectively. We define an indicator for a fully occluded (FO) object:</p><p>We then mark frame t as Occluded when the target is fully occluded by another object. e.g F O target t = 1 -Visible Frames. Finally, we define frame as Visible when the target is not Contained, Carried or Occluded. Thus, the target needs to be only partially visible to be considered as visible. For instance, the target is still considered visible when it is 20% occluded (e.g ? y s.t OR x t (y) = 0.2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Annotating Frames in Perfect Perception</head><p>For the perfect-perception setup, we extend the definition of fully occluded (FO) objects from Eq S3. We define an object to be partially occluded (PO) with respect to the rate p as follows: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2.5-month-old infants&apos; reasoning about when objects should and should not be occluded</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baillargeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="116" to="157" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object permanence in young infants: Further evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baillargeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child development</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1227" to="1246" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5374" to="5383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Siamese cascaded region proposal networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02155</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video captioning with attentionbased lstm and semantic consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2045" to="2055" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04744</idno>
		<title level="m">Cater: A diagnostic dataset for compositional actions and temporal reasoning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tracking the invisible: Learning where the object might be</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cattin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1285" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tracking multiple objects through occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1051" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking vot2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tracking occluded objects and recovering incomplete trajectories by reasoning about containment relations and human actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Marvasti-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ghanei-Yakhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<title level="m">Deep learning for visual tracking: A comprehensive survey. arXiv pp</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1912</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiple objects tracking in the presence of longterm occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Papadourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="835" to="846" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piaget</surname></persName>
		</author>
		<title level="m">The construction of reality in the child</title>
		<imprint>
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1745" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<title level="m">Action recognition using visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The significance of event information for 6-to 16-month-old infants&apos; perception of containment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smitsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Dejonckheere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>De Wit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">207</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An end-to-end spatio-temporal attention model for human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-first AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A model for discovering containmentrelations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dorfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page" from="67" to="81" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<title level="m">Clevrer: Collision events for video representation and reasoning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Temporal relational reasoning in videos. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Objects are represented by a 5-coordinate vector, containing 4 bounding box coordinates in (x 1 , y 1 , x 2 , y 2 ) format and an additional visibility bit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visible objects are represented by their ground-truth bounding boxes and a turned-on visibility bit. Non-visible objects are represented by a four-zeros bounding box coordinates and a turned-off visibility bit</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
