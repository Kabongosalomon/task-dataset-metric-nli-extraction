<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frequency Dynamic Convolution: Frequency-Adaptive Pattern Recognition for Sound Event Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonuk</forename><surname>Nam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Hu</forename><surname>Kim</surname></persName>
							<email>seonghu.kim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeong-Yun</forename><surname>Ko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Hwa</forename><surname>Park</surname></persName>
							<email>yhpark@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Frequency Dynamic Convolution: Frequency-Adaptive Pattern Recognition for Sound Event Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: frequency dynamic convolution</term>
					<term>sound event de- tection</term>
					<term>frequency-dependent patterns</term>
					<term>physics-informed learn- ing</term>
					<term>dynamic convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2D convolution is widely used in sound event detection (SED)   to recognize two dimensional time-frequency patterns of sound events. However, 2D convolution enforces translation equivariance on sound events along both time and frequency axis while frequency is not shift-invariant dimension. In order to improve physical consistency of 2D convolution on SED, we propose frequency dynamic convolution which applies kernel that adapts to frequency components of input. Frequency dynamic convolution outperforms the baseline by 6.3% in DESED validation dataset in terms of polyphonic sound detection score (PSDS). It also significantly outperforms other pre-existing contentadaptive methods on SED. In addition, by comparing class-wise F1 scores of baseline and frequency dynamic convolution, we showed that frequency dynamic convolution is especially more effective for detection of non-stationary sound events with intricate time-frequency patterns. From this result, we verified that frequency dynamic convolution is superior in recognizing frequency-dependent patterns.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sound event detection (SED), which aims to recognize sound event class and corresponding timestamps (onset and offset) from audio signals, has been rapidly growing since success of deep learning (DL) methods in various pattern recognition fields <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. SED has been adopting various DL methods from speech processing tasks such as automatic speech recognition (ASR) and speaker verification which are also based on audio signal processing <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9]</ref>. However, there is no guarantee that DL methods from other domains are flawlessly compatible with SED. While transformer is prevalently used in natural language processing (NLP) and ASR <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>, it does not necessarily perform better than pre-existing convolutional recurrent neural network (CRNN) on SED <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>. Furthermore, conformer <ref type="bibr" target="#b7">[7]</ref> which achieved state-of-the-art performance in ASR failed to show stable performance in SED <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. Considering the similarity between ASR and SED that both tasks take audio data as input and yield sequential output, conformer appears to be a reasonable choice for SED as well but it turned out not to be. This emphasizes that DL methods qualified in other similar domains have to be thoroughly examined before applying to SED.</p><p>2D convolution has been widely used in speech and audio domain DL tasks to recognize 2D time-frequency patterns. However, 2D Convolutions is proposed to recognize 2D image data thus not exactly compatible with audio data. Recently, there have been several attempts to make 2D convolution to reflect domain knowledge of audio data rather than image data. For speaker recognition tasks, inspired by domain knowledge on speech rapidly changing along time due to variation of phonemes, temporally adaptive 2D convolution methods are proposed <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref>.</p><p>Similarly, we apply domain knowledge on SED that sound events exhibit frequency-dependent patterns. On SED, 2D convolution imposes translation equivariance along both time and frequency axis. It is because 2D convolution is designed for 2D image data which is shift-invariant on both dimensions as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (a). However, translation equivariance should not be imposed along frequency axis because 2D audio data is not shift-invariant along frequency axis as shown in <ref type="figure" target="#fig_0">Figure  1</ref> (b). A time-frequency pattern would sound different when it is shifted along the frequency axis. Therefore, it would be more physically consistent to release translation equivariance along frequency axis from 2D convolution in order to account for frequency-dependence of sound events. In addition, SED was shown to be highly frequency-dependent in previous work, FilterAugment <ref type="bibr" target="#b19">[19]</ref>. Therefore, instead of simply applying preexisting content-adaptive methods such as dynamic convolution using input-adaptive kernel <ref type="bibr" target="#b20">[20]</ref> and temporal dynamic convolution using time-adaptive kernel <ref type="bibr" target="#b17">[17]</ref>, we propose a domain knowledge inspired method named frequency dynamic convolution. The main contributions of this work are as follows:</p><p>1. We propose frequency dynamic convolution that applies frequency-adaptive kernel in order to release translation equivariance of 2D convolution along frequency axis, for physical consistency with the time-frequency patterns in sound events.</p><p>2. Proposed frequency dynamic convolution outperforms not only baseline (by 6.3%), but also other pre-existing content-adaptive methods proposed for other tasks (dynamic convolution and temporal dynamic convolution).</p><p>3. By class-wise performance comparison with the baseline, we showed that frequency dynamic convolution is especially effective on non-stationary sound events, proving that frequency dynamic convolution is superior on frequency-dependent pattern recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2203.15296v2 [eess.AS] 4 Jul 2022</head><p>The official implementation code is available on GitHub 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Motivation</head><p>Dynamic convolution was proposed to enhance representation capability of vanilla convolution by adapting convolution kernel to given input <ref type="bibr" target="#b20">[20]</ref>. Dynamic convolution first extracts attention weights from input of convolution, and then performs weighted sum of basis kernels with attention weights to obtain kernel optimal for the input. Similarly, temporal dynamic convolution is proposed for speaker verification to apply kernel optimal for each time frame. It uses kernel that adapts to input's time frame in order to consider various phonemes composing spoken speech along time axis <ref type="bibr" target="#b17">[17]</ref>. Likewise, we were motivated to make use of the domain knowledge on SED that sound event patterns are frequencydependent: the same time-frequency pattern sounds different on different frequency regions. Within time-frequency domain, certain pattern sounds the same when it is shifted along time axis because it has the same frequency components despite it happen at different time. On the other hand, it would sound different when it is shifted along frequency axis because the frequency component which composes acoustic characteristics of the sound event changes. Such characteristics of time-frequency pattern is shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b). Small shift in frequency axis could be perceived as just minor pitch shift, but large shift would make it difficult for us to recognize the information of original sound pattern. In addition, previous work proved that frequency-dependency is a critical issue in SED. Filter-Augment, a data augmentation method which applies different weights on random frequency regions proved that regularizing SED model over wider frequency regions enhances SED performance by a large margin, 6.5% in terms of polyphonic sound detection score (PSDS) <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b21">21]</ref>. This is because sound events exhibit distinctive patterns over various frequency regions. Such insights inspired us to develop a frequency-dependent pattern recognition method for SED.</p><p>Vast majority of SED models based on CRNN architecture <ref type="bibr" target="#b22">[22]</ref> uses 2D convolution which enforces translation equivariance on two dimensions of input. Computer vision domain utilizes translation equivariance of 2D convolution to recognize image pattern regardless of its relative position within the image <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>. Likewise, DL based audio tasks use 2D convolution on time-frequency patterns enforcing translation equivariance along both time and frequency axis. While translation equivariance is helpful on SED along time axis, it might not along frequency axis. 2D convolution introduces inconsistency between its frequency equivariance and the frequency-dependence of sound events. Thus we should maintain translation equivariance of 2D convolution on time dimension while loosening it on frequency dimension to improve model's physical consistency with sound events' time-frequency patterns and to improve SED performance. Frequency dynamic convolution is proposed to solve this problem using dynamic kernel that adapts to the input's frequency component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Frequency Dynamic Convolution</head><p>Frequency dynamic convolution uses frequency-adaptive kernel in order to enforce frequency-dependency on 2D convolution thus to improve physical consistency of SED model with sound events' time-frequency patterns. The operation is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. It first extracts frequency-adaptive attention weights from input by applying average pooling over time axis followed by two 1D convolution layers along channel axis. Instead of using fully-connected (FC) layers as dynamic convolution did <ref type="bibr" target="#b20">[20]</ref>, we applied 1D convolution in order to consider adjacent frequency components as well. Between two 1D convolution layers, batch normalization and ReLU are applied. 1D convolution layers compress channel dimension into the number of basis kernels. Then, softmax is applied to make frequency-adaptive attention weights range between zero and one and make sum of the weights for different basis kernel equal to one. Temperature of 31 was applied on the softmax to ensure uniform learning of basis kernels and stable training <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b20">20]</ref>. Then frequency-adaptive convolution kernel is obtained by weighted sum of basis kernels using frequencyadaptive attention weights, where basis kernels are trainable parameters as well. Obtained frequency-adaptive kernel is used for frequency dynamic convolution operation just as normal 2D convolution.</p><p>Note that the procedure described in the preceding paragraph is for better explanation and presentation of the concept of frequency dynamic convolution, and the programmed algorithm in the official implementation code for frequency dynamic convolution takes slightly different procedure to reduce computational cost as in <ref type="bibr" target="#b17">[17]</ref>. While the programmed algorithm extracts frequency-adaptive attention weights in the same way, it applies convolution kernels differently. In the actual algorithm, outputs by each basis kernel are obtained first as in <ref type="formula">(1)</ref>, then weighted sum is applied as in (2) as follows:</p><formula xml:id="formula_0">yi(t, f ) = Wi * x(t, f ) + bi (1) y(t, f, x) = K i=1 ?i(f, x)yi(t, f )<label>(2)</label></formula><p>where t is time, f is frequency, x and y are input and output of one frequency dynamic convolution layer, Wi and bi are weight and bias of ith basis kernel, yi is output from ith basis kernel and ?i(f, x) is frequency-adaptive attention weight for ith basis kernel, and K is number of basis kernels. This procedure is equivalent to the procedure illustrated in preceding paragraph with fewer computation.  <ref type="bibr" target="#b20">[20]</ref>, temporal dynamic convolution <ref type="bibr" target="#b17">[17]</ref> and frequency dynamic convolution on DESED real validation dataset.  <ref type="bibr" target="#b2">[3]</ref> which consists of synthesized strongly labeled data, real weakly labeled data and real unlabeled data. Baseline is based on CRNN architecture <ref type="bibr" target="#b22">[22]</ref>. Attention pooling module is added at the last FC layer for joint training of weakly labeled data, and mean teacher method is applied for consistency training with unlabeled data for semi-supervised learning <ref type="bibr" target="#b25">[25]</ref>. Frame shift, mixup <ref type="bibr" target="#b26">[26]</ref>, time masking <ref type="bibr" target="#b6">[6]</ref> and FilterAugment <ref type="bibr" target="#b19">[19]</ref> are applied for data augmentation. Baseline is the model using optimal step type FilterAugment from <ref type="bibr" target="#b19">[19]</ref> with minor updates including seed of 21, mixup ratio of 1.0, and median filter that differs by classes. More details regarding the baseline are available in the GitHub repository and <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Metrics</head><p>SED models were optimized to maximize sum of PSDS1 and PSDS2, which is the ranking score used in detection and classification of acoustic scenes and events (DCASE) 2021 and 2022 challenge task4 <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b4">5]</ref>. PSDS1 favors SED system with accurate timestamps, while PSDS2 favors SED system with less cross triggers. Collar-based F1 score and intersection-based F1 score with threshold of 0.5 are listed for the reference with labels "CB-F1" and "IB-F1" in the tables <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">28]</ref>. Metric values listed in the tables are based on the best performance of each metric among 32 trained models, from student model and teacher model <ref type="bibr" target="#b25">[25]</ref> of 16 separate training runs. As dynamic convolutions are relatively unstable thus shows larger performance deviation, more training runs are operated compared to the previous work <ref type="bibr" target="#b19">[19]</ref>. In addition, time taken to train models for 200 epochs using one NVIDIA RTX Titan are listed in <ref type="table" target="#tab_0">Table 1</ref> to compare training time for dynamic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dynamic Convolutions on SED</head><p>We compared the performances of baseline with dynamic convolution <ref type="bibr" target="#b20">[20]</ref>, temporal dynamic convolution <ref type="bibr" target="#b17">[17]</ref>, and proposed frequency dynamic convolution, abbreviated as DY-CRNN, TDY-CRNN and FDY-CRNN respectively. For all dynamic convolution models, dynamic convolution layers replaced all convolution layers except the first layer from the baseline model <ref type="bibr" target="#b20">[20]</ref>, with four basis kernel and temperature of 31.</p><p>From the results in <ref type="table" target="#tab_0">Table 1</ref>, FDY-CRNN significantly outperforms the rests. Dynamic convolution applies kernel that adapts to input as whole. Temporal dynamic convolution applies kernel that adapts to each time frame of input. Fre- quency dynamic convolution applies kernel that adapts to each frequency bin of input. Since kernel that adapts to each frequency bin of input significantly outperformed other contentadaptive kernels on SED, we can conclude that SED is a highly frequency-dependent task.</p><p>Considering that temporal dynamic convolution outperformed dynamic convolution on text-independent speaker verification by extracting speaker information from rapidly timevarying phonemes <ref type="bibr" target="#b17">[17]</ref>, it appears to be advantageous on SED as well because adapting kernels on time frames could help frame-wise predictions by SED. However, TDY-CRNN failed to outperform DY-CRNN and even marginally outperformed the baseline. It is because time-dependency is more critical on speaker verification due to the characteristics of speech data composed of short and rapidly changing phoneme sequences. Although time-dependency matters on SED too where sound events also vary along time axis, we should note that CRNN architecture for SED already process sequential information over time using recurrent neural network (RNN) layers. Therefore, TDY-CRNN is less effective than DY-CRNN is on SED, in terms of both performance and training time. On the contrary, FDY-CRNN that applies frequency-adaptive kernels performs much better because CRNN architecture lacks function to consider dependence on frequency regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Number of Basis Kernels in FDY-CRNN</head><p>Number of basis kernels K directly affect the model's representation capability and computational cost. Larger the K, stronger the expressiveness of trained SED model. However, too large K not only increase computational cost but also might cause overfitting of the model or undertraining of basis kernels. <ref type="table" target="#tab_2">Table 2</ref> shows SED performance of FDY-CRNN on different number of basis kernels. From the table, PSDS values are better on K=4 just as <ref type="bibr" target="#b20">[20]</ref>, but F1 scores are better on K=5. As PSDS are more comprehensive metric that does not depend on calibrating threshold value while CB-F1 and IB-F1 depend on threshold value <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">28]</ref>, we chose optimal model based on PSDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Class-wise Performance Comparison between Baseline and FDY-CRNN</head><p>Class-wise performance of baseline and FDY-CRNN are compared for more detailed analysis on how frequency dynamic convolution affects SED performance. We chose representative models with following performances: baseline with PSDS1 of 0.412, PSDS2 of 0.634 and CB-F1 of 0.515 and FDY-CRNN with PSDS1 of 0.432, PSDS2 of 0.643 and CB-F1 of 0.532. Since PSDS is a comprehensive measure that takes into account of influences between different classes, class-wise performance is compared using CB-F1 instead.</p><p>It can be observed that baseline performs better on blender, frying, and vacuum cleaner in <ref type="figure">Figure 3</ref>. These quasi-stationary sound events are almost stationary over time <ref type="bibr" target="#b29">[29]</ref>, thus exhibit simple time-frequency patterns as shown in <ref type="figure" target="#fig_2">Figure 4 (a)</ref>, an example of vacuum cleaner sound log Mel spectrogram. Blender <ref type="figure">Figure 3</ref>: Comparison of SED performance of baseline and FDY-CRNN in terms of class-wise collar-based F1 score. For labels, alm, bld, cat, dsh, dog, shv, fry, wtr, spch and vcm refer to alarm/bell ringing, blender, cat, dishes, dog, electric shaver/toothbrush, frying, running water, speech and vacuum cleaner respectively. and vacuum cleaner sounds are mostly caused by running motors deriving dominant periodic mechanical noise <ref type="bibr" target="#b30">[30]</ref>. These sound events may involve other minor non-stationary noises such as blender cutting hard chunk or vacuum cleaner's head hitting or rolling over other objects. Nonetheless, the motor sound is loud enough to dominate other noises thus these sound events can be considered quasi-stationary. Frying sound is caused by evaporation of water molecules on the surface of food being fried. Such evaporation occurs randomly and continuously just as raindrops randomly falling on ground, which is a classic example of random noise. Thus frying sound can be classified as random noise which is quasi-stationary as well <ref type="bibr" target="#b29">[29]</ref>. Quasi-stationary sound events hardly change over time, thus result in horizontal patterns on log Mel spectrogram as shown in <ref type="figure" target="#fig_2">Figure 4</ref> (a). Because such horizontal patterns are simple and similar on different frequency regions, the advantage of frequency dynamic convolution that it applies different kernels on different frequency regions is less evident for detecting quasi-stationary sound events.</p><p>On the other hand, <ref type="figure">Figure 3</ref> shows that FDY-CRNN performs better on the other sound event classes: alarm/bell ringing, cat, dishes, dog, electric shaver/toothbrush, running water and speech. These classes are non-stationary sound events those keep changing along time axis, thus result in intricate time-frequency patterns as shown in <ref type="figure" target="#fig_2">Figure 4 (b)</ref>, an example of speech sound log Mel spectrogram. Alarm/bell ringing and dishes involve transient and abrupt short sounds. Cat, dog, and speech involve constantly changing pitches, with impulsive sounds such as stops and transient turbulent sound such as fricatives <ref type="bibr" target="#b31">[31]</ref>. Electric shaver/toothbrush could be viewed as quasistationary sound like blender and vacuum cleaner because they are run by motors as well. However, their motor sound is not loud enough to dominate other impulsive noise they make while brushing teeth or shaving beard. Thus these sound events are rather non-stationary. Running water might appear as random noise like frying sound, as water that keeps running alone involves turbulent sound as it hits other surfaces <ref type="bibr" target="#b29">[29]</ref>. But running water in domestic environments involves interactions with people, as people would not just let it flow for no reason in their home. Human interaction keeps intervening the sound of running water, thus it is considered as a non-stationary sound event. Non-stationary sound events keeps changing its frequency components over time, resulting in more intricate patterns on various frequency regions of log Mel spectrogram as shown in From above discussions, it could be inferred that frequency dynamic convolution has greatly improved SED performance by enhancing recognition of diverse and intricate patterns that non-stationary sound events exhibit, by applying frequencyadaptive kernels. This result again proves the premise on this work that frequency dynamic convolution effectively recognizes frequency-dependent patterns of sound events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Frequency dynamic convolution is proposed to recognize frequency-dependent patterns of sound event data for SED. Conventional 2D convolution imposes translation equivariance along both time and frequency axis, but it is physically inconsistent with frequency-dependent patterns of sound events. Thus frequency dynamic convolution is designed to release translation equivariance along frequency axis by applying frequencyadaptive kernels and enforce physical consistency of the model with time-frequency patterns in sound events. Experiments on DESED dataset showed that frequency dynamic convolution is superior to not only baseline but also dynamic convolution and temporal dynamic convolution. In addition, comparison of class-wise F1 scores between baseline and FDY-CRNN showed that frequency dynamic convolution is especially helpful in detection of non-stationary sound events, proving effectivity of frequency dynamic convolution on frequency-dependent patterns. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of shift-invariance of (a) location in 2D image data, (b) time and frequency in 2D audio data (log Mel spectrogram).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>1 https://github.com/frednam93/FDY-SED An illustration of frequency dynamic convolution operation. x and y are input and output of frequency dynamic convolution layer. T , F and Cin are input dimension size of time, frequency and channel, and T , F and Cout are output dimension size of time, frequency and channel. K is number of basis kernels, Wi and bi are weight and bias of ith basis kernel and ?i(f ) is frequency-adaptive attention weight for ith basis kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Fig-Log Mel spectrogram examples of quasi-stationary and non-stationary sound events: (a) vacuum cleaner sound event as an example of quasi-stationary sound, (b) speech sound event as an example of non-stationary sound. ure 4 (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>This work was supported by "Human Resources Program in Energy Technology" of the Korea Institute of Energy Technology Evaluation and Planning (KETEP), granted financial resource from the Ministry of Trade, Industry &amp; Energy, Republic of Korea. (No. 20204030200050), and also supported by Korea Institute of Marine Science and Technology Promotion(KIMST) grant funded by the year 2022 Finances of Korea Ministry of Oceans and Fisheries (MOF) (Development of Technology for Localization of Core Equipment in the Marine Fisheries Industry, 20210623).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Training time and SED performance of baseline, dynamic convolution</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FDY-SED performance with different number of basis kernels, K.</figDesc><table><row><cell>K</cell><cell>PSDS1 ?</cell><cell>PSDS2 ?</cell><cell>CB-F1 ?</cell><cell>IB-F1 ?</cell></row><row><cell>2</cell><cell>0.446</cell><cell>0.666</cell><cell>0.532</cell><cell>0.751</cell></row><row><cell>3</cell><cell>0.449</cell><cell>0.668</cell><cell>0.531</cell><cell>0.754</cell></row><row><cell>4</cell><cell>0.452</cell><cell>0.670</cell><cell>0.533</cell><cell>0.753</cell></row><row><cell>5</cell><cell>0.445</cell><cell>0.665</cell><cell>0.540</cell><cell>0.755</cell></row><row><cell>6</cell><cell>0.440</cell><cell>0.659</cell><cell>0.537</cell><cell>0.752</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<title level="m">Computational Analysis of Sound Scenes and Events</title>
		<imprint>
			<publisher>Springer Publishing Company</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="71" to="77" />
		</imprint>
	</monogr>
	<note>1st ed</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Metrics for polyphonic sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Parag</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Detection and Classification of Acoustic Scenes and Events</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Heavily augmented sound event detection utilizing weak predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dcase 2021 challenge task4: Sound event detection and separation in domestic environmentse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dcase</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<ptr target="http://dcase.community/challenge2021/task-sound-event-detection-and-separation-in-domestic-environments" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conformer: Convolution-augmented Transformer for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning based cough detection camera using enhanced features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive convolutional neural network for text-independent speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="641" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CHT+NSYSU Sound Event Detection System With Multiscale Channel Attention And Multiple Consistency Training For DCASE 2021 Task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sound event detection based on self-supervised learning of wav2vec 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolution-augmented conformer for sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional network with conformer for semi-supervised sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Integrating advantages of recurrent and transformer structures for sound event detection in multiple scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhiyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DCASE2021 Challenge, Tech. Rep</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal dynamic convolutional neural network for text-independent speaker verification and phonemetic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Decomposed temporal dynamic cnn: Efficient time-adaptive network for text-independent speaker verification explained with speaker activation map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15277</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Filteraugment: An acoustic environmental data augmentation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A framework for the robust evaluation of sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tuveri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Azcarreta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krstulovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional recurrent neural networks for polyphonic sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1291" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Physics-informed machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Physics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="422" to="440" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semisupervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<ptr target="https://github.com/DCASE-REPO/DESEDtask" />
		<title level="m">Dcase2021 task4 baseline. GitHub</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving sound event detection metrics: Insights from dcase 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Azcarreta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tuveri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krstulovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="631" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bendat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piersol</surname></persName>
		</author>
		<title level="m">Random Data: Analysis and Measurement Procedures</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="8" to="12" />
		</imprint>
	</monogr>
	<note>4th ed.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Engineering Vibrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Inman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="172" to="177" />
		</imprint>
	</monogr>
	<note>4th ed. Pearson, 2013</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schafer</surname></persName>
		</author>
		<title level="m">Theory and Applications of Digital Speech Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="123" />
		</imprint>
	</monogr>
	<note>1st ed. Pearson</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
