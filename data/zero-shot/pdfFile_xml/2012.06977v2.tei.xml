<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MVFNet: Multi-View Fusion Network for Efficient Video Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
							<email>wuwenhao01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
							<email>hedongliang01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
							<email>lintianwei01@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
							<email>lifu@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<email>ganchuang1990@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
							<email>dingerrui@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Vision Technology (VIS)</orgName>
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MVFNet: Multi-View Fusion Network for Efficient Video Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventionally, spatiotemporal modeling network and its complexity are the two most concentrated research topics in video action recognition. Existing state-of-the-art methods have achieved excellent accuracy regardless of the complexity meanwhile efficient spatiotemporal modeling solutions are slightly inferior in performance. In this paper, we attempt to acquire both efficiency and effectiveness simultaneously. First of all, besides traditionally treating H ?W ?T video frames as space-time signal (viewing from the Height-Width spatial plane), we propose to also model video from the other two Height-Time and Width-Time planes, to capture the dynamics of video thoroughly. Secondly, our model is designed based on 2D CNN backbones and model complexity is well kept in mind by design. Specifically, we introduce a novel multi-view fusion (MVF) module to exploit video dynamics using separable convolution for efficiency. It is a plug-and-play module and can be inserted into off-theshelf 2D CNNs to form a simple yet effective model called MVFNet. Moreover, MVFNet can be thought of as a generalized video modeling framework and it can specialize to be existing methods such as C2D, SlowOnly, and TSM under different settings. Extensive experiments are conducted on popular benchmarks (i.e., Something-Something V1 &amp; V2, Kinetics, UCF-101, and HMDB-51) to show its superiority. The proposed MVFNet can achieve state-of-the-art performance but maintain 2D CNN's complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rapid development of the Internet and mobile devices, video data has exploded over the past years. Huge video information has far exceeded the processing capacity of the conventional manual system and attracted increased research interest in video understanding. Video recognition, as a fundamental task in video analytics, has become one of the most active research topics. It becomes increasingly demanding in a wide range of applications such as video surveillance, video retrieval, and personalized recommendation. Hence, recognition accuracy and inference complexity are of equal importance for the large scale applications.</p><p>Recently, significant progress has been achieved in video action recognition following the deep convolutional network paradigm <ref type="bibr" target="#b21">(Simonyan and Zisserman 2014a;</ref><ref type="bibr" target="#b29">Wang et al. 2016;</ref><ref type="bibr" target="#b0">Carreira and Zisserman 2017;</ref><ref type="bibr" target="#b30">Wang et al. 2018b;</ref><ref type="bibr" target="#b17">Lin, Gan, and Han 2019;</ref><ref type="bibr" target="#b4">Feichtenhofer et al. 2019)</ref>. Full-3D CNNs such as C3D <ref type="bibr" target="#b25">(Tran et al. 2015)</ref> and I3D (Carreira and Zisserman 2017), are intuitive spatial-temporal networks which are natural extension over their 2D counterparts to directly tackle 3D volumetric video data. Especially, the good performance achieved in I3D <ref type="bibr" target="#b0">(Carreira and Zisserman 2017)</ref> at the cost of thousands of GFLOPs 1 . In these methods, spatial and temporal features are jointly learned in an unconstrained way. However, the huge number of model parameters and computational burdens can largely limit the practicality of these methods. Then, some works <ref type="bibr" target="#b20">(Qiu, Yao, and Mei 2017;</ref><ref type="bibr" target="#b34">Xie et al. 2018;</ref><ref type="bibr" target="#b27">Tran et al. 2018</ref>) try to factorize 3D convolutional kernel into spatial (e.g., 1?3?3) and temporal part (e.g., 3?1?1) separately to reduce the cost. In practice, however, compared with their 2D counterparts, the increased computational overhead is still not negligible. The recent state-of-the-art model TSM <ref type="bibr" target="#b17">(Lin, Gan, and Han 2019)</ref>, which achieves a good trade-off between performance and complexity, shifts feature along the temporal dimension instead of temporal convolution to model temporal dynamics in videos. TSM approximates spatiotemporal modeling done in 3D CNN while introduces zero FLOPs for the 2D CNN backbone. This inspires us to focus on designing efficient 2D CNN based architectures to learn more representative features for action recognition.</p><p>In this paper, we seek to design action recognition models to acquire both performance and efficiency. First of all, we propose to model dynamics in H ? W ? T video signal from multiple viewpoints for performance improvement and we introduce an efficient spatiotemporal module, termed as Multi-View Fusion Module (MVF Module). MVF is a novel plug-and-play module and can turn an existing 2D CNN into a powerful spatiotemporal feature extractor with minimal overhead. Specifically, the MVF module adopts three independent 1D channel-wise convolutions over the T , H and W dimensions respectively to capture multi-view information. To make this module more efficient, we decompose the feature maps into two parts, one acts as inputs of the three 1D channel-wise convolutions for multi-view spatiotemporal modeling and the other is directly concatenated with the outputs of the first part for making the original activation still accessible. In practice, we integrate the MVF module into the standard ResNet block to construct our MVF block. Then the final video architecture MVFNet is constructed by stacking multiple blocks. Interestingly, MVFNet can be regarded as the generalized spatiotemporal model and several existing methods such as C2D, SlowOnly <ref type="bibr" target="#b4">(Feichtenhofer et al. 2019)</ref>, and TSM <ref type="bibr" target="#b17">(Lin, Gan, and Han 2019)</ref> can be specialized from MVFNet with different settings.</p><p>Extensive experimental results on multiple well-known datasets, including Kinetics-400 <ref type="bibr" target="#b11">(Kay et al. 2017</ref>), Something-Something V1 and V2 <ref type="bibr" target="#b5">(Goyal et al. 2017</ref>), UCF-101 <ref type="bibr" target="#b23">(Soomro, Zamir, and Shah 2012)</ref> and HMDB-51 <ref type="bibr" target="#b13">(Kuehne et al. 2011)</ref> show the superiority of our solution. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, MVFNet achieves excellent performance with quite limited overhead on Something-Something V1 and it is superior compared with existing state-of-the-art frameworks. The same conclusion can be drawn on other datasets. Codes and models are available 2 . Overall, our major contributions are summarized as follows:</p><p>? Instead of only temporal modeling, we propose to exploit dynamic inside the three dimensional video signal from multiple viewpoints. A novel MVF module is designed to better exploit spatiotemporal dynamics.</p><p>? The MVF module works in a plug-and-play way and can be integrated easily with existing 2D CNN backbones. Our MVFNet is a generalized video modeling network and it can specialize to become recent state-of-the-arts.</p><p>? Extensive experiments on five public benchmark datasets demonstrate that the proposed MVFNet outperforms the state-of-the-art methods with computational cost (GFLOPs) comparable to 2D CNN.</p><p>2 Related Work 2D CNNs were extensively applied to conduct video recognition. Over the past years, inspired by the great success of deep convolution frameworks in image recognition <ref type="bibr" target="#b8">(He et al. 2016;</ref><ref type="bibr" target="#b22">Simonyan and Zisserman 2014b;</ref><ref type="bibr" target="#b9">Ioffe and Szegedy 2015)</ref>, many methods have been proposed to explore the application of deep convolutional architectures on action recognition in videos. Among these methods, the Two-Stream architecture is a popular extension of 2D CNNs to handle video <ref type="bibr" target="#b21">(Simonyan and Zisserman 2014a;</ref><ref type="bibr" target="#b36">Zhang et al. 2016)</ref>, which can learn video representations respectively from RGB and optical flows or motion vector. To further boost performance, TSN ) proposed a sparse temporal sampling strategy for the two-stream structure. TRN <ref type="bibr" target="#b37">(Zhou et al. 2018)</ref> proposed by focusing on the multi-scale temporal relations among sampled frames. More recently, TSM <ref type="bibr" target="#b17">(Lin, Gan, and Han 2019)</ref>, STM <ref type="bibr" target="#b10">(Jiang et al. 2019)</ref>, GST <ref type="bibr" target="#b19">(Luo and Yuille 2019)</ref>, GSM (Sudhakaran, Escalera, and Lanz 2020), TEI <ref type="bibr" target="#b18">(Liu et al. 2020)</ref>, TEA <ref type="bibr" target="#b16">(Li et al. 2020b</ref>) perform efficient temporal modeling. 3D CNNs and (2+1)D CNN variants based approach is another typical branch. C3D <ref type="bibr" target="#b25">(Tran et al. 2015)</ref> is the first work in this line which directly learn spatiotemporal features from the video clip with 3D convolution. However, C3D has a huge number of parameters which makes it harder to train and more prone to over-fitting than 2D counterparts. To alleviate such problems, I3D <ref type="bibr" target="#b0">(Carreira and Zisserman 2017)</ref> proposed to inflate the ImageNet pre-trained 2D convolution into 3D convolution for initialization. Following the I3D paradigm for spatiotemporal modeling, S3D <ref type="bibr" target="#b34">(Xie et al. 2018)</ref>, P3D <ref type="bibr" target="#b20">(Qiu, Yao, and Mei 2017)</ref>, R(2+1)D <ref type="bibr" target="#b27">(Tran et al. 2018)</ref> and StNet ) are proposed to reduce computation overhead of 3D convolution while remaining the spatiotemporal modeling property. These (2+1)D CNN variants decompose 3D convolution into 2D spatial convolution followed by 1D temporal convolution on either per convolution operation basis or per 3D convolution network block basis. There exist several other networks that merge 2D and 3D information in CNN blocks to enhance the feature abstraction capability and resort to shallower backbones for efficiencies, such as ECO <ref type="bibr" target="#b38">(Zolfaghari, Singh, and Brox 2018)</ref> and ARTNet <ref type="bibr" target="#b28">(Wang et al. 2018a</ref>). More recently, SlowFast <ref type="bibr" target="#b4">(Feichtenhofer et al. 2019)</ref> explored the potential of different temporal speeds with two different 3D CNN architectures (e.g., Slow-Only and Fast-Only) to mimic twostream fusion of 3D CNNs. In fact, our MVFNet can easily replace the Slow path in SlowFast.</p><p>The most closely related work to ours is CoST ), which also learns features from multiple views for action recognition. However, there are substantial differences between the CoST and the proposed MVF module. CoST learns collaborative spatiotemporal features through weight sharing among three regular 2D 3?3 convolutions on T -H, T -W , and H-W planes and replaces the middle 3?3 convolution of ResNet block with CoST operation. CoST targets on performance and the three 2D 3?3 convolutions are applied to the whole input feature map. On the contrary, we learn independent features via three different 1D channelwise convolutions over T , H, and W dimensions and col- laborative learning does not work for our module. Besides of channel-wise 1D convolution, our MVF module performs multi-view modeling using part of the whole input feature map rather than the whole input feature map as done in CoST. Compared with CoST, our MVFNet achieves superior performance with much smaller computation cost.</p><p>There is also active research on dynamic inference <ref type="bibr" target="#b32">(Wu et al. 2020)</ref>, adaptive frame sampling techniques <ref type="bibr">(Wu et al. 2019b,a;</ref><ref type="bibr" target="#b12">Korbar, Tran, and Torresani 2019)</ref> and selfsupervised video representation learning <ref type="bibr" target="#b1">(Chen et al. 2021;</ref><ref type="bibr" target="#b6">Han, Xie, and Zisserman 2020)</ref>, which we think can be complementary to our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, first we present the technical details of our novel multi-view fusion (MVF) module which encodes spatiotemporal features effectively and efficiently with minimal overhead. Then we describe details of the MVFNet building block and how to insert this block into the off-the-shelf architecture of 2D CNNs. Finally, we present that MVF is a generalization of several existing methods such as C2D, SlowOnly <ref type="bibr" target="#b4">(Feichtenhofer et al. 2019</ref>) and TSM <ref type="bibr" target="#b17">(Lin, Gan, and Han 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-View Fusion Module</head><p>As shown in <ref type="figure" target="#fig_1">Fig 2,</ref> we compare the proposed method to several common competitive methods. C2D is a simple 2D baseline architecture without any temporal interaction before the last global average pooling. A natural generalization of C2D is to turn it into a 3D convolutional counterpart. As 3D convolutions are computationally intensive, SlowOnly (the Slow path in SlowFast <ref type="bibr" target="#b4">(Feichtenhofer et al. 2019)</ref>) inflated the first 1?1 kernel in a residual block to 3?1?1 instead of inflating the 3?3 kernel in a residual block to 3?3?3, and achieved promising performance. To model temporal structure efficiently, TSM (Lin, Gan, and Han 2019) involves some hand-crafted designs of shifting part of channels at each timestamp forward and backward along the temporal dimension, thus introduces zero FLOPs compared with C2D. An overview of our proposed multiview fusion module is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(e), the input feature map channels are split into two parts, one for making part of the information in the original activation accessible, and the other for multi-view spatiotemporal modeling. In our module, multi-view modeling is performed via channel-wise convolution along the temporal, horizontal and vertical dimensions respectively, then the outputs of the three convolutions is element-wise added. Finally, the two part of features are concatenated together to combine the original activation and the multi-view modeling activation.</p><p>Formally, let tensor X ? R C?T ?H?W denote the input feature maps where C is the number of input channels and T , H, W are the temporal and spatial dimensions. Then let X 1 , X 2 be the two splits of X along the channel dimension where X 1 ? R ?C?T ?H?W , X 2 ? R (1??)C?T ?H?W and ? denoted the proportion of input channels for multi-view spatiotemporal features.</p><formula xml:id="formula_0">O T = i K T c,i ? X 1 c,t+i,h,w ,<label>(1)</label></formula><formula xml:id="formula_1">O H = i K H c,i ? X 1 c,t,h+i,w ,<label>(2)</label></formula><formula xml:id="formula_2">O W = i K W c,i ? X 1 c,t,h,w+i ,<label>(3)</label></formula><p>where c, t, x, y is the index of different dimensions of channel, time, height and width. K T , K H , K W is the channelwise convolutional kernels used for modeling H-W , W -T and H-T views respectively. In this paper, the kernel size of three channel-wise convolution is 3?1?1, 1?3?1 and 1?1?3. Then, the feature maps O T , O H , O W from three different views are fused by a weighted summation as</p><formula xml:id="formula_3">O 1 = ?(? T ? O T + ? H ? O H + ? W ? O W ),<label>(4)</label></formula><p>where ? is activation function, O 1 ? R ?C?T ?H?W is the activated feature maps, ? T , ? H , ? W represents the weight value of the corresponding view. In this paper, we simply set</p><formula xml:id="formula_4">? T = ? H = ? W = 1. Finally, we get the MVF module output Y ? R C?T ?H?W as Y = Concat(X 2 , O 1 ),<label>(5)</label></formula><p>where Concat represents concatenation operation along dimension of channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network Architecture</head><p>We here describe how to instantiate the Multi-View Fusion Network (MVFNet). The proposed MVF module is a flexible and efficient spatiotemporal module, which can be easily plugged into the existing 2D CNNs with a strong ability to learning the spatioltemporal features in videos. Therefore, our method is able to not only use the pre-trained Ima-geNet model for initialization to have faster training, but also bring very limited extra computation cost compared with 3D CNNs or (2+1)D CNNs. In practice, our MVFNet obviously improves the performance on different types of large-scale video datasets: scene-related one such as Kinetics-400, and temporal-related one such as Something-Something. We observed that many recent state-of-the-art action recognition methods <ref type="bibr" target="#b17">(Lin, Gan, and Han 2019;</ref><ref type="bibr" target="#b4">Feichtenhofer et al. 2019;</ref><ref type="bibr" target="#b14">Li et al. 2019;</ref><ref type="bibr" target="#b18">Liu et al. 2020</ref>) usually use ResNet as the backbone network due to its simple and modular structure. For a fair comparison with these stateof-the-arts, in our experiments, we instantiate the MVFNet using ResNet as the backbone. Specifically, we integrate the proposed module into standard ResNet block to construct our MVF block then form our MVFNet. The overall design of the MVF block is illustrated in <ref type="figure" target="#fig_1">Fig. 2(d)</ref>, MVF module is inserted before the first convolution of ResNet Block.</p><p>Unless specified, in our experiments we choose the 2D ResNet-50 as our backbone for its trade-off between the accuracy and speed. Following recent common practice <ref type="bibr" target="#b17">(Lin, Gan, and Han 2019)</ref>, there are no temporal downsampling operations in this instantiation. The ResNet backbone is initialized from ImageNet pre-trained weights and our MVF modules do not need any specialized initialization strategy, simple random gaussian initialization work pretty well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relation to Existing Methods</head><p>Here we discuss the connection between MVF and other methods shown in <ref type="figure" target="#fig_1">Fig. 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We evaluate our method on three large-scale video recognition benchmarks, including Kinetics-400 (K400) <ref type="bibr" target="#b11">(Kay et al. 2017)</ref>, Something-Something (Sth-Sth) V1&amp;V2 <ref type="bibr" target="#b5">(Goyal et al. 2017)</ref>, and other two small-scale datasets, UCF-101 <ref type="bibr" target="#b23">(Soomro, Zamir, and Shah 2012)</ref> and HMDB-51 <ref type="bibr" target="#b13">(Kuehne et al. 2011)</ref>. Kinetics-400 contains 400 human action categories and provides around 240k training videos and 20k validation videos. For Something-Something datasets, the actions therein mainly include object-object and human-object interactions, which require strong temporal relation to well categorizing them. V1 includes about 110k videos and V2 includes 220k video clips for 174 finegrained classes. To elaborately study the effectiveness of our method on these two types of datasets, we use Kinetics-400 and Sth-Sth V1 for the ablation experiments. Moreover, transfer learning experiments on the UCF-101 and HMDB-51, which are much smaller than Kinetics and sth-sth, is carried out to show the transfer capability of our solution.</p><p>We report top-1 and top-5 accuracy (%) for Kinetics and top-1 accuracy (%) for Something-Something V1&amp;V2. For UCF-101 and HMDB-51, we follow the original evaluation scheme using mean class accuracy. Also, we report the computational cost (in FLOPs) as well as the number of model parameters to depict model complexity. In this paper, we only use the RGB frames of these datasets for experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Training. We utilize 2D ResNet <ref type="bibr" target="#b8">(He et al. 2016</ref>) as our backbone and train the model in an end-to-end manner. We use random initialization for our MVF module. Following the similar practice in <ref type="bibr" target="#b30">(Wang et al. 2018b</ref>) on Kinetics-400, we sample frames from a set of consecutive 64 frames per video. For Something-Something V1 &amp; V2, we observe that the duration of most videos normally has less than 64 frames, thus we employ the similar uniform sampling strategy to TSN ) to train our model. In our experiments, we sample 4, 8 or 16 frames as a clip. The size of the short side of these frames is fixed to 256 and then random scaling is utilized for data augmentation. Finally, we resize the cropped regions to 224?224 for network training.</p><p>On the Kinetics-400 dataset, the learning rate is 0.01 and will be reduced by a factor of 10 at 90 and 130 epochs (150 epochs in total) respectively. For Something-Something V1 &amp; V2 dataset, our model is trained for 50 epochs starting with a learning rate 0.01 and reducing it by a factor of 10 at 30, 40 and 45 epochs. For these large-scale datasets, our models are initialized by pre-trained models on Ima-geNet <ref type="bibr" target="#b2">(Deng et al. 2009</ref>). For UCF-101 and HMDB-51, we followed the common practice to fine-tune from Kinetics pre-trained weights and start training with a learning rate of 0.01 for 25 epochs. The learning rate is decayed by a factor 10 every 10 epochs. For all of our experiments, we utilize SGD with momentum 0.9 and weight decay of 1e-4 to train our models on 8 GPUs. Each GPU processes a mini-batch of 8 video clips by default. When changing to a larger batch  Inference. Following the widely used practice in <ref type="bibr" target="#b17">(Lin, Gan, and Han 2019;</ref><ref type="bibr" target="#b18">Liu et al. 2020)</ref>, two ways for inference are considered to trade-off accuracy and speed. (a) For high accuracy, we follow the common setting in <ref type="bibr" target="#b30">(Wang et al. 2018b;</ref><ref type="bibr" target="#b4">Feichtenhofer et al. 2019)</ref> to uniformly sample multiple clips from a video along its temporal axis. We sample 10 clips for Kinetics-400 and 2 clips for others. For each clip, we resize the short side to 256 pixels and take 3 spatial crops in each frame. Finally, we average the softmax probabilities of all clips as the final prediction. (b) For efficiency, we only use 1 clip per video and a central region of size 224?224 is cropped for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>To comprehensively evaluate our proposed MVF module, in this section we provide ablation studies on both Kinetics-400 and Something-Something V1 datasets which represent the two types of datasets. <ref type="table" target="#tab_2">Table 1</ref> shows a series of ablations. Accordingly, the effectiveness of each component in our framework is analyzed as follows.</p><p>Paramter Choice. As shown in <ref type="table" target="#tab_2">Table 1a</ref>, we compare networks with different proportion of input channels (? = 0, 1/8, 1/4, 1/2, 1) for multi-view spatiotemporal feature. Here we add MVF blocks into res 4?5 for efficiency. Especially, when ? = 0, MVFNet becomes exactly C2D. Our approach achieves considerable absolute improvement over C2D baseline on both datasets (+33.36% for Sth-Sth v1, +2.34% for Kinetics-400), which demonstrates the effectiveness of the MVF blocks. For Kinetics-400, we observe that the change in ? = 1/8, 1/4, 1/2 appeared to have little impact on performance thus we choose ? = 1/8 for efficiency in the following experiments. As for Something-Something V1, our method with ? = 1/2 achieves the highest Top-1 accuracy compared with the other settings, so 1/2 is adopted in the following experiments.</p><p>The Number of MVF Blocks. We denote the conv2 x to conv5 x of ResNet architecture as res 2 to res 5 . To figure out how many MVF blocks can obtain a good tradeoff, we gradually add MVF blocks from res 5 to res 2 in ResNet-50. According to the results in <ref type="table" target="#tab_2">Table 1a</ref>, we set ? to 1/2 for Something-something V1 and 1/8 for Kinetics-400. As shown in <ref type="table" target="#tab_2">Table 1b</ref>, on Kinetics-400, the improvement brought by MVF blocks on res 4?5 , res 3?5 or res 2?5 is comparable. res 2?5 outperforms res 4?5 by 0.01% but 7 extra MVF-blocks are needed. On Something-something V1, MVF-blocks added to res 4?5 achieves the highest top-1 accuracy. Thus we use MVF block in stages of res 4?5 in the following experiments for efficiency.</p><p>Different Views. We also evaluate impacts of different views in MVF module. As illustrated in <ref type="figure" target="#fig_1">Fig. 2(e)</ref>, we can get different multi-view fusion by controlling ? T , ? H and ? W . For example, we simply set ? T =? H =? W =1 to get view of T -H-W . Also, typical T -H, T -W , T views can be easily obtained. From <ref type="table" target="#tab_2">Table 1c</ref>, we can see that T -H-W outperforms T by 1.35% and 0.49% on Something-Something V1 and Kinetics-400, respectively. Moreover, we also try the collaborative feature learning in CoST ) by sharing the he convolution kernels among different views. However, with weight sharing among different views, accuracy gets degraded by 3.2% and 0.4% on two datasets, respectively.</p><p>Comparison with Other Temproal Modules. Here we make a comparison with methods described in Sec. 3.1 under the same setting of backbone and inputs. We list FLOPs and the number of parameter for all models in <ref type="table" target="#tab_2">Table 1d</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Backbone</head><p>Frames?Crops?Clips GFLOPs Top-1 Top-5 I3D  Inception <ref type="formula" target="#formula_0">V1</ref>    <ref type="table" target="#tab_2">Table 1d</ref> shows the superiority of our MVFNet is quite impressive.</p><p>Deeper Backbone. In <ref type="table" target="#tab_2">Table 1e</ref> we compare various instantiations of MVFNet models on Kinetics-400. Thus far, all experiments used ResNet-50 as the backbone, we further study the performance of our MVFNet with a deeper backbone (i.e., ResNet-101 (R-101)). For models involving R-101, we use the same hyper-parameter settings as R-50 above. As expected, using advanced backbones is complementary to our method. Comparing with the R-50 counterparts, our MVFNet gets additional improvement on R-101. We also investigate generalization of our models on longer input videos. Our models work well on longer sequences and all models have better results with longer inputs.</p><p>Different Backbones. We further study the performance of MVFNet with MobileNet-V2 which is much smaller than the ResNet backbone. As shown in <ref type="table" target="#tab_2">Table 1f</ref>, comparing with the 4frame-C2D, our 4frame-MVFNet achieves a better accuracy (67.5% vs. 64.4%) with the same MobileNet-V2 backbone on Kinetics-400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-arts</head><p>Results on Kinetics-400. We make a comprehensive comparison in <ref type="table" target="#tab_4">Table 2</ref>, where our MVFNet outperforms the recent SOTA approaches on Kinetics-400. Here we only list the models using RGB as inputs to perform comparisons. Compared with 2D CNN based models, when utilizing 8 frames as input, our MVFNet outperforms these current state-of-the-art efficient methods (e.g., TSM, TEINet and TEA) with a clear margin (76.0% vs. 74.1%/74.9%/75.0%). Compared with computationally expensive models, MVFNet-R50 with 8-frame outperforms NL I3D-R50 with 32-frame (76.0% vs. 74.9%) but only uses 2.1? less GFLOPs, and MVFNet-R50 with 16-frame uses 4.2? less GFLOPs than NL I3D-R50 with 128-frame but achieves a better accuracy (77.0% vs. 76.5%). Moreover, 8-frame MVFNet-R101 achieves a competitive accuracy (77.4% vs. 77.2%, 77.4%, 77.7%) when using 3?, 5.7?, 6.7? less GFLOPs than SlowOnly, NL I3D and SmallBig respectively. To mimics two-steam fusion with two temporal rates as done in SlowFast, we perform score fusion over 16-frame MVFNet-R101 and 8-frame MVFNet-R101. Our MVFNet En obtains better performance than SlowFast   <ref type="table" target="#tab_6">Table 3</ref>. These methods can be divided into two categories as shown in the two parts of Table 3. The upper part presents the 3D CNN based methods including S3D-G, ECO and I3D+GCN models. When compared with these methods, the obtained result shows substantial improvements and the #FLOPs of our model is much smaller than these models. The lower part is 2D CNN based methods. Compared with these lightweight models which also target at improving efficiency, our solution yields a superior accuracy of 52.6% on Something-Something V1 validation set and 65.2% on V2 with 16 frames as input. For readers' reference, here we also report the results of ensemble the models using 16 frames and 8 frames as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Transfer Learning on UCF-101 &amp; HMDB-51</head><p>We also evaluate the performance of our method on UCF-101 and HMDB-51 to show the generalization ability of MVFNet on smaller datasets. We finetune our MVFNet with 16 frames as inputs on these two datasets using model pretrained on Kinetics-400 and report the mean class accuracy over three splits. From <ref type="table">Table 4</ref>, we see that our model shows a pretty transfer capability and the mean class accuracy is 96.6% and 75.7% on UCF-101 and HMDB-51, respectively. As shown in the middle part of <ref type="table">Table 4</ref>, our MVFNet outperforms these 2D CNN based lightweight models. When comparing with the state-of-the-art models based on 3D convolu-  <ref type="table">Table 4</ref>: Mean class accuracy on UCF-101 and HMDB-51 achieved by different methods which are transferred from their Kinetics models with RGB modality (over 3 splits).</p><p>tions such as I3D, R(2+1)D and S3D, our proposed MVFNet also obtains comparable or better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented the Multi-View Fusion (MVF) module to better exploit spatiotemporal dynamics in videos. The MVF module works in a plug-and-play way and can be easily integrated into standard ResNet block to form a MVF block for constructing our MVFNet. We conducted a series of empirical studies to verify the effectiveness of MVFNet for video action recognition. Without any 3D convolution or pre-calculation of optical flow, the experimental results show that our method achieves the new state-of-the-art results on both temporal information dependent and spatial information dominated datasets with computational cost comparable to 2D CNN. In the future, we think the learnable ? to weight views will further boost the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>MVF achieves state-of-the-art performance on Something-Something V1 dataset and get better accuracycomputation trade-off than I3D, ECO and TSM.Figure credit: TSM (Lin, Gan, and Han 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of various residual blocks for video action recognition. (a) shows a simple 2D ResNet-block. (b) shows the I3D 3?1?1 type block, which decouples the spatial and temporal filters by inflating the first 1?1 kernel to 3?1?1. (c) shows a TSM block, which shifts partial feature maps along the temporal dimension for efficient temporal modeling. (d) shows our MVF block which integrate the MVF module into standard ResNet block. (e) depict the architecture of the MVF module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Details are described in the following: ? With ?=0, MVF module degenerates to C2D which only focus on learning the spatial feature representation. ? With ?=1 and ? H =? W =0, MVF module collapses to SlowOnly (Feichtenhofer et al. 2019) with depth-wise separable convolution. Put simply, the first 1?1?1 convolution of ResNet block perform as a point-wise convolution, then the 3?1?1 channel-wise convolution followed by the 1?1?1 point-wise convolution naturally form the depth-wise separable convolution. ? With ?=1/4 and ? H =? W =0, MVF module could be viewed as a learnable TSM (Lin, Gan, and Han 2019).</figDesc><table><row><cell>More specifically, in fact TSM could be viewed as</cell></row><row><cell>a channel-wise 3?1?1 convolution, where convolution</cell></row><row><cell>kernel is fixed as [0, 0, 1] for forward shift, and [1, 0,</cell></row><row><cell>0] for backward shift.</cell></row></table><note>Above all, we conclude that MVFNet can be regarded as the generalized spatiotemporal model and above approaches can be specialized from MVFNet with different settings.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Top-5 FLOPs #F Top-1 Top-5 FLOPs ?=0 8 17.12 43.46 32.88G 4 71.87 90.02 16.44G ?=1/8 8 49.74 78.09 32.90G 4 74.21 91.34 16.45G ?=1/4 8 49.24 77.91 32.92G 4 74.18 91.46 16.46G ?=1/2 8 50.48 79.14 32.96G 4 74.21 91.42 16.48G ?=1 8 49.73 77.94 33.04G 4 73.75 91.40 16.52G (a) Parameter choices of ?. Backbone: R-50.</figDesc><table><row><cell>Setting</cell><cell cols="4">Sth-sth v1 #F Top-1 Stages Kinetics-400</cell><cell>Blocks</cell><cell cols="3">Sth-sth v1, ?=1/2 #F Top-1 Top-5 FLOPs #F Top-1 Top-5 FLOPs Kinetics-400, ?=1/8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>None</cell><cell>0</cell><cell cols="3">8 17.12 43.46 32.88G 4 71.87 90.02 16.44G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>res{5}</cell><cell>3</cell><cell cols="3">8 46.02 75.60 32.90G 4 73.46 91.09 16.44G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>res{4,5}</cell><cell>9</cell><cell cols="3">8 50.48 79.14 32.96G 4 74.21 91.34 16.45G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>res{3,4,5}</cell><cell>13</cell><cell cols="3">8 49.72 78.82 33.04G 4 74.08 91.51 16.46G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">res{2,3,4,5} 16</cell><cell cols="3">8 49.95 77.96 33.12G 4 74.22 91.56 16.47G</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) The number of MVF Blocks inserted into R-50.</cell></row><row><cell>Views T</cell><cell>Sth v1 K400 #F Top-1 #F Top-1 8 49.13 4 73.72</cell><cell>Method C2D</cell><cell cols="2">Sth v1 K400 FLOPs Params Top-1 Top-1 17.1 71.4 32.9G 24.3M</cell><cell>R-50</cell><cell>#F Top-1 FLOPs 4 74.21 16.45G 8 75.99 32.90G</cell><cell>Mb-V2</cell><cell>Model Top-1 FLOPs C2D 64.4 1.25G MVF 67.5 1.25G</cell></row><row><cell>T-H T-W</cell><cell>8 49.22 4 74.01 8 49.31 4 73.88</cell><cell cols="3">TSM SlowOnly -47.2 74.1 32.9G 24.3M 74.9 41.9G 32.4M</cell><cell></cell><cell>16 77.04 65.81G 4 75.98 31.36G</cell><cell>R-50</cell><cell>C2D 71.9 16.44G MVF 74.2 16.48G</cell></row><row><cell>T-H-W</cell><cell>8 50.48 4 74.21</cell><cell>CoST  *</cell><cell>-</cell><cell>-45.8G 24.3M</cell><cell>R-101</cell><cell>8 77.46 62.72G</cell><cell></cell></row><row><cell cols="2">T-H-W (S) 8 47.21 4 73.81</cell><cell cols="3">MVFNet 50.5 76.0 32.9G 24.3M</cell><cell></cell><cell>16 78.42 125.45G</cell><cell></cell></row><row><cell cols="2">(c) Study on the different views</cell><cell cols="3">(d) Study on the effectiveness of</cell><cell cols="2">(e) Advanced backbones</cell><cell cols="2">(f) Different backbones for</cell></row><row><cell cols="2">of MVF module. Backbone: R-</cell><cell cols="3">MVFNet. Backbone: R-50, 8f input. *</cell><cell cols="2">for MVFNet on Kinetics-</cell><cell cols="2">MVFNet on Kinetics-400.</cell></row><row><cell cols="2">50. S denotes weight sharing.</cell><cell cols="3">indicates our implementation.</cell><cell>400.</cell><cell></cell><cell cols="2">Mb-V2 denotes MobileNet-V2.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation studies on Something-Something V1 and Kinetics-400. We show top-1 and top-5 classification accuracy (%), as well as computational complexity measured in FLOPs (floating-point operations) for a single clip input of spatial size 224 2 . #F indicates the number of frames sampled from each video clip. We follow the common setting to sample multiple clips per video (10 for Kinetics-400, 2 for Something-Something V1).</figDesc><table /><note>size b of each GPU for higher gpu memory usage, we use linear scaling initial learning rate (0.01?b/8).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art models on Kinetics-400. Similar to<ref type="bibr" target="#b4">(Feichtenhofer et al. 2019)</ref>, we report the inference cost by computing the GFLOPs (of a single view) ? the number of views (temporal clips with spatial crops). N/A denotes the numbers are not available for us.</figDesc><table><row><cell>our MVFNet is more lightweight (32.9G vs. 41.9G) than the</cell></row><row><cell>SlowOnly branch of SlowFast (Feichtenhofer et al. 2019)</cell></row><row><cell>and achieves a better accuracy than it (76.0% vs. 74.9%).</cell></row><row><cell>Also, our MVFNet outperforms the TSM (Lin, Gan, and</cell></row><row><cell>Han 2019) with a large margin on both datasets (Sth V1:</cell></row><row><cell>50.5% vs. 47.2%, K400: 76.0% vs. 74.1%) while remain-</cell></row><row><cell>ing similar computational cost. For a fair comparison with</cell></row><row><cell>CoST (Li et al. 2019), we implement the architecture based</cell></row><row><cell>on our baseline backbones by adding the CoST module</cell></row><row><cell>into res4 and res5. Feeding 8-frame clips, as expected, our</cell></row><row><cell>MVFNet-R50 is more lightweight than CoST-R50 (32.9G</cell></row><row><cell>vs. 46G).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance and FLOPs consumptions of our method on the Something-Something V1 and V2 datasets compared with the state-of-the-art methods.(79.1% vs. 78.9%), using less GFLOPs (188G vs. 213G). TPN-I3D-101 achieves top-1 accuracy of 78.9% with 32frame clips as input, its complexity is as high as 374 GFLOPs. Our MVFNet En obtains better performance than TPN-R101, using 2?less GFLOPs(188G vs. 374G).Results on Something-Something V1 &amp; V2. The Something-Something datasets are more complicated than Kinetics. The comparison of our solution against existing state-of-the-arts are list in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">GFLOPs is short for float-point operations in Giga and is widely used to measure model complexity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/whwu95/MVFNet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">X3D: Expanding Architectures for Efficient Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The&quot; Something Something&quot; Video Database for Learning and Evaluating Visual Common Sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Self-supervised Co-Training for Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>In Neurips</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">StNet: Local and global spatial-temporal modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6232" to="6242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collaborative spatiotemporal feature learning for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7872" to="7881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Small-BigNet: Integrating Core and Contextual Views for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1092" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TEA: Temporal Excitation and Aggregation for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TSM: Temporal Shift Module for Efficient Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TEINet: Towards an Efficient Architecture for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11669" to="11676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grouped spatial-temporal aggregation for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5512" to="5521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In Neurips</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gate-Shift Networks for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1102" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Appearance-and-Relation Networks for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-Local Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic Inference: A New Approach Toward Efficient Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="676" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaframe: Adaptive frame selection for fast video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1278" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ECO: Efficient Convolutional Network for Online Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
