<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Failure: Training Debiased Classifier from Biased Classifier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Nam</surname></persName>
							<email>junhyun.nam@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyuntak</forename><surname>Cha</surname></persName>
							<email>hyuntak.cha@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of AI</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsoo</forename><surname>Ahn</surname></persName>
							<email>sungsoo.ahn@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
							<email>jaeho-lee@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
							<email>jinwoos@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of AI</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Failure: Training Debiased Classifier from Biased Classifier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks often learn to make predictions that overly rely on spurious correlation existing in the dataset, which causes the model to be biased. While previous work tackles this issue by using explicit labeling on the spuriously correlated attributes or presuming a particular bias type, we instead utilize a cheaper, yet generic form of human knowledge, which can be widely applicable to various types of bias. We first observe that neural networks learn to rely on the spurious correlation only when it is "easier" to learn than the desired knowledge, and such reliance is most prominent during the early phase of training. Based on the observations, we propose a failure-based debiasing scheme by training a pair of neural networks simultaneously. Our main idea is twofold; (a) we intentionally train the first network to be biased by repeatedly amplifying its "prejudice", and (b) we debias the training of the second network by focusing on samples that go against the prejudice of the biased network in (a). Extensive experiments demonstrate that our method significantly improves the training of network against various types of biases in both synthetic and real-world datasets. Surprisingly, our framework even occasionally outperforms the debiasing methods requiring explicit supervision of the spuriously correlated attributes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When trained on carefully curated datasets, deep neural networks achieve state-of-the-art performances on many tasks in artificial intelligence, including image classification <ref type="bibr" target="#b10">[11]</ref>, object detection <ref type="bibr" target="#b7">[8]</ref>, and speech recognition <ref type="bibr" target="#b8">[9]</ref>. On the other hand, neural networks often dramatically fail when trained on a highly biased dataset, by learning the unintended decision rule that works well only on the dataset being trained on. For instance, it is widely known that object classification datasets suffer from such misleading correlations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>. As an example, suppose that the "boat" is the only object category appearing in the images with the "water" background. When trained on a dataset with such bias, neural networks often learn to make predictions using the unintended decision rule based on the background of images, whereas a learner intended to learn the decision rule based on the object in the images.</p><p>To train a debiased model that captures the "intended correlation" from such biased datasets, recent approaches focus on how to utilize various types of human supervision effectively. One of the most popular forms of such supervision is an explicit label that indicates the misleadingly correlated attribute <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref>. For instance, Kim et al. <ref type="bibr" target="#b13">[14]</ref>, Li and Vasconcelos <ref type="bibr" target="#b17">[18]</ref> consider training a model to classify digits (instead of misleadingly correlated color) from the Colored MNIST dataset, under the setup where RGB values for coloring digits are given as side information. Another line of research focuses on developing algorithms tailored to a domain-specific type of bias in the target dataset, whose existence and characteristics are diagnosed by human experts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref>. For instance, <ref type="bibr">34th</ref> Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.</p><p>Geirhos et al. <ref type="bibr" target="#b6">[7]</ref> diagnose that ImageNet-trained classifiers are biased toward texture instead of a presumably more human-aligned notion of shapes, and use this takeaway to construct an augmented dataset to train shape-oriented classifiers.</p><p>Acquiring human supervision on the bias, however, is often a dauntingly laborious and expensive task. Gathering explicit labels of such misleadingly correlated attributes requires manual labeling by the workers that have a clear understanding of the underlying bias. Collecting expert knowledge of a human-perceived bias (e.g., texture bias) takes even more effort, as it requires a careful ablation study on the classifiers trained on biased datasets, e.g., Geirhos et al. <ref type="bibr" target="#b6">[7]</ref> synthesize data via style transfer <ref type="bibr" target="#b5">[6]</ref> to discover the existence of texture bias. Hence, an approach to train a debiased classifier without relying on such expensive supervision is warranted.</p><p>Contribution. In this paper, we propose a failure-based debiasing scheme, coined Learning from Failure (LfF). Our scheme does not require expensive supervision on the bias, such as explicit labels of misleadingly correlated attributes, or bias-tailored training technique. Instead, our method utilizes a cheaper form of human knowledge, leveraging the following intriguing observations on neural networks that are being trained on biased datasets.</p><p>We first observe that a biased dataset does not necessarily lead the model to learn the unintended decision rule; the bias negatively affects the model only when the bias attribute is "easier" to learn than the target attribute (Section 2.2). For the bias that negatively affects the model, we also observe that samples aligned with the bias show distinct loss trajectories in the training phase compared to samples conflicting with the bias. To be more specific, the classifier learns to fit samples aligned with the bias during the early stage of training and learns samples conflicting with the bias later (Section 2.3). The latter observation lines up with recent findings on training dynamics of deep neural networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17]</ref>; networks tend to defer learning hard concepts, e.g., samples with random labels, to the later phase of training.</p><p>Based on the findings, we propose the following debiasing scheme, LfF. We simultaneously train two neural networks, one to be biased and the other to be debiased. Specifically, we train a "biased" neural network by amplifying its early-stage predictions. Here, we employ generalized cross entropy loss <ref type="bibr" target="#b27">[28]</ref> for the biased model to focus on easy samples, which are expected to be samples aligned with bias. In parallel, we train a "debiased" neural network by focusing on samples that the biased model struggles to learn, which are expected to be samples conflicting with the bias. To this end, we re-weight training samples using the relative difficulty score based on the loss of the biased model and the debiased model (Section 3).</p><p>We show the effectiveness of LfF on various biased datasets, including Colored MNIST <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref> with color bias, Corrupted CIFAR-10 <ref type="bibr" target="#b11">[12]</ref> with texture bias, and CelebA <ref type="bibr" target="#b18">[19]</ref> with gender bias. In addition, we newly construct a real-world dataset, coined biased action recognition (BAR), to resolve the lack of realistic evaluation benchmark for debiasing schemes. In all of the experiments, our method succeeds in training a debiased classifier. In particular, our method improves the accuracy of the unbiased evaluation set by 35.34% ? 63.39%, 17.93% ? 31.66%, for the Colored MNIST and Corrupted CIFAR-10 1 datasets, respectively, even when 99.5% of the training samples are bias-aligned (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A closer look at training deep neural networks on biased datasets</head><p>In this section, we describe two empirical observations on training the neural networks with a biased dataset. These observations serve as a key intuition for designing and understanding our debiasing algorithm. We first provide a formal description of biased datasets in Section 2.1. Then we provide our empirical observations in Section 2.2 and Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Setup</head><p>Consider a dataset D where each input x can be represented by a set of (possibly latent) attributes {a 1 , . . . , a k } for a i ? A i that describes the input. The goal is to train a predictor f that belongs to a set of intended decision rules F t , consisting of decision rules that correctly predict the target attribute y = a t ? A t . We say that a dataset D is biased, if (a) there exists another attribute a b = y that is highly correlated to the target attribute y (i.e., H(y|a b ) ? 0), and (b) one can settle an unintended decision rule g b / ? F t that correctly classifies a b . We denote such an attribute a b by a bias attribute.  In biased datasets with a bias attribute a b , we say that a sample is bias-aligned whenever it can be correctly classified by the unintended decision rule g b , and bias-conflicting whenever it cannot be correctly classified by g b .</p><p>Throughout the paper, we consider two types of evaluation datasets: the unbiased and bias-conflicting evaluation sets. We construct the unbiased evaluation set in a way that the target and bias attributes are uncorrelated. To this end, the unbiased evaluation set is constructed to have the same number of samples for every possible value of (a t , a b ). We simply construct the bias-conflicting evaluation set by excluding bias-aligned samples from the unbiased evaluation set.</p><p>Here, we illustrate the examples of biased datasets using the datasets considered for the experiment in Section 2.2 and 2.3, i.e., Colored MNIST and Corrupted CIFAR-10.</p><p>Colored MNIST. We inject color with random perturbation into the MNIST dataset <ref type="bibr" target="#b15">[16]</ref> designed for digit classification, resulting in a dataset with two attributes: Digit and Color. In the case of (a t , a b ) = (Digit, Color), a set of intended decision rules F t consists of decision rules that correctly classify images based on the Digit of the images. Here, a decision rule based on other attributes, e.g. Color, is considered as an unintended decision rule. <ref type="figure">Figure 1</ref> Corrupted CIFAR-10. This dataset is generated by corrupting the CIFAR-10 dataset <ref type="bibr" target="#b14">[15]</ref> designed for object classification, following the protocols proposed by Hendrycks and Dietterich <ref type="bibr" target="#b11">[12]</ref>. The resulting dataset consists of two attributes, i.e., category of the Object and type of Corruption used. Similar to the Colored MNIST dataset, this results in two possible choices for the target and bias attribute. We use two sets of protocols for corruption to build two datasets, namely the Corrupted CIFAR-10 1 and the Corrupted CIFAR-10 2 datasets. See <ref type="figure">Figure 1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) for corruption-biased examples.</head><p>A detailed description of the datasets is provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Not all biases are malignant</head><p>Our first observation is that training a classifier with a biased dataset does not necessarily lead to learning an unintended decision rule. Instead, the bias in the dataset negatively affects the prediction only if the bias is easier to be captured by the learned classifier. In <ref type="table" target="#tab_0">Table 1</ref>, we report the accuracy of the classifiers for the unbiased evaluation set. We first note the existence of benign bias, i.e., there are cases where the classifier has not been affected by the bias. Particularly, when there is a degradation of accuracy with a certain choice of the target and bias attribute, the degradation does not occur with the choice made in reversed order. For each dataset, the left and the right plots corresponds to training on the malignant bias and the benign bias. Our choice of (target, bias) attribute for the malignant bias is provided in brackets. For the benign bias, we choose the target and the bias attribute in reversed order.</p><p>From such an observation, we now define two types of bias: malignant and benign. For a biased dataset D with a target attribute y and a bias attribute a b , we say this bias is malignant if a model trained on D suffers performance degradation on unbiased evaluation set compared to one trained on another dataset which is not biased. In contrast, we say bias is benign if a model trained on D does not suffer such performance degradation. We interpret this observation as follows: the bias attribute inducing malignant bias is "easier" to learn than the target attribute, e.g., Color is easier to learn than Digit. To be specific, the classifier establishes its decision rule by relying on either (a) the intended correlation from the target or (b) the spurious correlation from the bias attribute. If (a) is harder to leverage than (b), the bias becomes malignant since the classifier learns unintended correlation. Otherwise, the classifier learns the correct correlation, hence the bias becomes benign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Malignant bias is learned first</head><p>Next, we observe that the loss dynamics of bias-aligned samples and bias-conflicting samples during the training phase are in stark contrast, given that the bias is malignant. The loss of bias-aligned samples quickly declines to zero, but the loss of bias-conflicting samples first increases and starts decreasing after the loss of bias-aligned samples reaches near zero.</p><p>In <ref type="figure" target="#fig_2">Figure 2</ref>, we observe a significant difference between training dynamics on the malignant and the benign bias, consistently over the considered datasets. In the case of training under malignant bias, the training loss of the bias-conflicting samples is higher than the loss of the bias-aligned samples, and the gap is more significant during the early stages. In contrast, they are almost indistinguishable when trained under a benign bias.</p><p>We make an interesting connection between our observation and the observation made by Arpit et al. <ref type="bibr" target="#b0">[1]</ref> on training neural networks on datasets with noisy labels. Arpit et al. <ref type="bibr" target="#b0">[1]</ref> analyzed the training dynamics of the neural network on noisy datasets. They empirically demonstrate the preference of neural networks for easy concepts, e.g., common patterns in samples with correct labels, over the hard concepts, e.g., random patterns in samples with incorrect labels. One can interpret our results similarly; since the malignant bias attributes are easier to learn than the original task, the neural network tends to memorize it first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Debiasing by learning from failure (LfF)</head><p>Based on our findings in Section 2, we propose a debiasing algorithm, coined Learning from Failure Training a biased model. We first describe how we train the biased model. i.e., the model following the unintended decision rule. From our observation in Section 2, we intentionally strengthen the prediction of the model from the early stage of training to make it follow the unintended decision rule. To this end, we use the following generalized cross entropy (GCE) <ref type="bibr" target="#b27">[28]</ref> loss to amplify the bias </p><formula xml:id="formula_0">f B (x; ? B ) and f D (x; ? D ). 3: for t = 1, ? ? ? , T do 4: Draw a mini-batch B = {(x (b) , y (b) )} B b=1 from D 5: Update f B (x; ? B ) by ? B ? ? B ? ?? ? B (x,y)?B GCE(f B (x), y). 6: Update f D (x; ? D ) by ? D ? ? D ? ?? ? D (x,y)?B W(x) ? CE(f D (x)</formula><p>, y). 7: end for of the neural network:</p><formula xml:id="formula_1">GCE(p(x; ?), y) = 1 ? p y (x; ?) q q</formula><p>where p(x; ?) and p y (x; ?) are softmax output of the neural network and its probability assigned to the target attribute of y, respectively. Here, q ? (0, 1] is a hyperparameter that controls the degree of amplification. For example, when lim q?0 1?p q q = ? log p, GCE becomes equivalent to standard cross entropy (CE) loss. Compared to the CE loss, the gradient of the GCE loss up-weights the gradient of the CE loss for the samples with a high probability p y of predicting the correct target attribute as follows:</p><formula xml:id="formula_2">?GCE(p, y) ?? = p q y ?CE(p, y) ?? ,</formula><p>Therefore, GCE loss trains a model to be biased by emphasizing the "easier" samples with the strong agreement between softmax output of the neural network and the target, which amplifies the "prejudice" of the neural network compare to the network trained with CE.</p><p>Training a debiased model. While we train a biased model as described earlier, we also train a debiased model simultaneously with the samples using the CE loss re-weighted by the following relative difficulty score:   </p><formula xml:id="formula_3">W(x) = CE(f B (x), y) CE(f B (x), y) + CE(f D (x), y) , where f B (x), f D (x) are</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we demonstrate the effectiveness of our LfF algorithm proposed in Section 3, and introduce our newly constructed biased action recognition dataset, coined BAR. All experimental results in this section support that LfF successfully trains a debiased classifier, with the knowledge that the bias attribute is learned earlier than the target attribute (instead of explicit supervision for the bias present in the dataset). The classifiers trained by LfF consistently outperforms the vanilla classifiers (trained without any debiasing procedure) on both the unbiased and bias-conflicting evaluation set.</p><p>For the experiments in this section, we use MLP with three hidden layers, ResNet-20, and ResNet-18 <ref type="bibr" target="#b10">[11]</ref> for the Colored MNIST, Corrupted CIFAR-10, and {CelebA, BAR} datasets, respectively. All results reported in this section are averaged over three independent trials. We provide a detailed description of datasets we considered in Appendix A, B and experimental details in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Controlled experiments</head><p>Baselines. For controlled experiments, we compare the performance of our algorithm with other debiasing algorithms, either presuming a particular bias type or requiring access to additional labels  containing information about bias attributes. We consider HEX proposed by Wang et al. <ref type="bibr" target="#b24">[25]</ref>, which attempts to remove texture bias using the bias-specific knowledge. We also consider REPAIR and Group DRO proposed by Li and Vasconcelos <ref type="bibr" target="#b17">[18]</ref> and Sagawa et al. <ref type="bibr" target="#b23">[24]</ref>, requiring explicit labeling of the bias attributes. We provide a detailed description of the employed baselines in Appendix D.</p><p>Ratio of the bias-aligned samples. In the first set of experiments, we vary the ratio of bias-aligned samples in the training dataset by selecting from {95.0%, 98.0%, 99.0%, 99.5%}. We experiment on Colored MNIST, and Corrupted CIFAR-10 1,2 datasets with (a t , a b ) = (Digit, Color), and (a t , a b ) = (Object, Corruption), respectively.</p><p>In <ref type="table" target="#tab_2">Table 2</ref>, 3, we report the accuracy evaluated on unbiased samples and bias-conflicting samples. We observe that the proposed method significantly outperforms the baseline on all levels of ratio for bias-aligned samples. Most notably, LfF achieves 41.37% accuracy on the unbiased evaluation set for the Corrupted CIFAR-10 1 dataset with 99% bias-aligned samples, while the vanilla model only achieves 22.72%. In addition, we report the accuracy of unbiased evaluation set and bias-conflicting samples with varying levels of difficulty for the bias attributes in Appendix E.</p><p>Detailed analysis of failure-based debiasing. We further analyze the specific details of our algorithm. To be precise, we first investigate the accuracies of the vanilla model and the biased, debiased models f B , f D trained by our algorithm for the bias-aligned and bias-conflicting samples. In <ref type="figure">Figure 4</ref>, we plot the training curves of each model. We observe both the vanilla model and the biased model easily achieve 100% accuracy for the bias-aligned samples. On the other hand, the vanilla model shows 50% accuracy for the bias-conflicting samples, and the biased model performs close to random guessing. From this result, we can say that our intentionally biased model only exploits the bias attribute without learning the target attribute.</p><p>To compare our debiased model to the vanilla model, we start by observing the accuracy gap between the bias-aligned and bias-conflicting samples. As described before, while the vanilla model easily achieves 100% accuracy for the bias-aligned samples, it cannot achieve similar performance for the bias-conflicting samples. In contrast, our debiased model shows consistent performance (about 80%) for both the bias-aligned and bias-conflicting samples. As a result, it indicates that our debiased model successfully learns the intended target attribute, while the predictions of the vanilla model heavily rely on the unintended bias attribute.</p><p>Contribution of GCE loss. We also test a variant of our algorithm, where we train the biased model with standard cross entropy instead of GCE. As observed in <ref type="figure">Figure 4</ref>, the model trained with standard cross entropy, denoted by Vanilla, not only exploits the bias attribute but also partially learns the target attribute. Therefore, one can expect that using such a CE-trained model as the biased model can hurt debiasing ability of our algorithm. In <ref type="table" target="#tab_4">Table 4</ref>, we report the test performance of our debiased  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real-world experiments</head><p>CelebA. The CelebA dataset <ref type="bibr" target="#b18">[19]</ref> is a multi-attribute dataset for face recognition, equipped with 40 types of attributes for each image. Among 40 attributes, we find that Gender and HairColor attributes have a high correlation. Moreover, we observe the attribute Gender is used as a cue for predicting the attribute HairColor. Therefore, we use HairColor as the target and Gender as the bias attribute. Similarly, we do the same thing for HeavyMakeup as the target and Gender as the bias attribute.</p><p>In <ref type="table" target="#tab_5">Table 5</ref>, we observe our algorithm consistently outperforms the vanilla model while the vanilla model suffers from gender bias existing in the real-world dataset. The accuracy gap between the vanilla model and our model is larger for the bias-conflicting samples, which indicates the vanilla model fails to learn the intended target attribute, instead exploits the biased statistic of the dataset. Notably, our model is comparable to Group DRO, which requires explicit labeling for the bias attribute (unlike ours), and even outperforms on the unbiased evaluation set when the target attribute is HeavyMakeup. This is because Group DRO aims to maximize the worst-case group accuracy, not overall unbiased accuracy.</p><p>Biased action recognition dataset. To verify effectiveness of our proposed scheme in a realistic setting, we construct a place-biased action recognition (BAR) dataset with training and evaluation set. We settle six typical action-place pairs by inspecting imSitu dataset <ref type="bibr" target="#b26">[27]</ref>, which provides action and place labels. We assign images describing these six typical action-place pairs to the training set and, otherwise, the evaluation set of BAR. BAR is publicly available 1 , and a detailed description of BAR is in Appendix B.</p><p>BAR aims to resolve the lack of realistic evaluation benchmark for debiasing schemes. Since previous debiasing schemes have tackled certain types of bias, they have assumed the correlation between the target and bias attribute to be tangible, which indeed is not available in the case of real-world settings. Such an assumption also makes it hard to verify whether one's scheme can be applied to a wide range of realistic settings. BAR instead offers evaluation set which is constructed based on the intuition that "a majority of samples that do not match typical action-place pairs are bias-conflicting." To demonstrate, the evaluation set consists of samples not matching the settled six typical pairs. In the end, we can use this evaluation set, which would be similar to a set of bias-conflicting samples to verify our algorithm's effectiveness. <ref type="table" target="#tab_6">Table 6</ref> illustrates the test accuracy on the BAR evaluation set. LfF outperforms the vanilla classifier for all action classes. This indicates that LfF encourages the model to train on relatively hard training samples, thereby leading to debiasing the model. In addition, LfF outperforms ReBias <ref type="bibr" target="#b1">[2]</ref>, which is also free from explicit labeling on the bias attribute, for most action classes except Diving. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Debiasing without explicit supervision. In a real-world scenario, bias presented in the dataset often hard to be easily characterized in the form of labels. Even if one can characterize the bias, acquiring explicit supervision for the bias is still an expensive task that requires manual labeling by human labelers, having a clear understanding of the bias. To address this issue, there have been several works to resolve dataset bias, without explicit supervision on the bias. Geirhos et al. <ref type="bibr" target="#b6">[7]</ref> observe the presence of texture bias in the ImageNet-trained classifiers, and train shape-oriented classifier with augmented data using style transfer <ref type="bibr" target="#b5">[6]</ref>. Wang et al. <ref type="bibr" target="#b24">[25]</ref> also aim to remove texture bias, using a hand-crafted module to extract bias and remove captured bias by domain adversarial loss and subspace projection. More recently, Bahng et al. <ref type="bibr" target="#b1">[2]</ref> utilize the small-capacity model to capture bias and force debiased model to learn independent feature from the biased model.</p><p>Debiasing from the biased model. To train a debiased model, recent works utilize an intentionally biased model to debias another model. These works mainly focused on removing well-known dataset bias that can easily be characterized. Cadene et al. <ref type="bibr" target="#b2">[3]</ref> use a question-only model to reduce question bias in a visual question answering (VQA) model. Clark et al. <ref type="bibr" target="#b4">[5]</ref> construct bias-only models for the task having prior knowledge of existing biases, including VQA, reading comprehension, and natural language inference (NLI). Concurrently, He et al. <ref type="bibr" target="#b9">[10]</ref> also train a biased model that only uses features known to relate to dataset bias in NLI. While these works are limited to biases existing in the NLP domain, Bahng et al. <ref type="bibr" target="#b1">[2]</ref> capture local texture bias in image classification and static bias in video action recognition task using small-capacity models.</p><p>Previous works mentioned above have leveraged expert knowledge, used as a substitute for explicit supervision, for a particular type of human-perceived bias. We, in a more straightforward approach, consider general properties of bias from observations on training dynamics of bias-aligned and bias-conflicting samples. Although our method also uses a certain form of human knowledge that whether existing bias in the dataset follows our observation, this is a yes/no type of knowledge, which has advantages in its affordability and applicability. We also assume the existence of bias-conflicting samples on which the debiased model should focus, which indeed is the case in real-life application scenarios. In the end, we propose a simple yet widely applicable debiasing scheme free from the choice for form and amount of supervision on the bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose a debiasing scheme, coined Learning from Failure (LfF), for training neural networks in the biased dataset. Our framework is based on an important observation on relationship between the training of neural networks and the "easiness" of biased attribute. Through extensive experiments, LfF shows successful results on the debiased training of neural networks. We expect our achievements may shed light on the nature of debiasing neural networks with minimal human supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Mitigating the potential risk caused by biased datasets is a timely subject, especially with the widespread use of AI systems in our daily lives. Since the world is biased by nature, biased models are often deployed without perceiving their discriminative behavior, thereby leading to invoking the potential risk. For instance, a facial recognition software in digital cameras turned out to "over-predict" Asians as blinking when it was trained on Caucasian faces <ref type="bibr" target="#b19">[20]</ref>. Disregarding such potential risks would further result in critical social issues <ref type="bibr" target="#b12">[13]</ref>, such as racism, gender discrimination, filter bubbles <ref type="bibr" target="#b21">[22]</ref>, and social polarization.</p><p>We propose the debiasing scheme to mitigate the aforementioned potential risks. A common approach to reducing the risks is to develop schemes that specifically tackle a bias of interest, e.g., gender, race, etc. However, underexplored biases might exist in the dataset, but bias-specific schemes would not be able to address these other biases. We thus recommend leveraging the general behaviors of neural networks trained on biased datasets, which can be applied for debiasing in diverse applications. Now we discuss potential benefits and limitations of the proposed scheme. The underexplored types of bias can be discovered by using a set of samples that are hard for the model to learn. Using this approach can increase awareness of underexplored biases. This awareness can be specifically important for groups that would be potentially affected. We acknowledge that assessing the reduction of potential risks by the proposed scheme can be a challenge without specifically identifying the biases. Still, we anticipate that our approach opens a potential to analyze and interpret underexplored types of bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Controlled experiments</head><p>We conduct the experiments on biased datasets with the same number of categories for the target and bias attributes, i.e., |A t | = |A b |. Furthermore, the empirical distribution p train of the training dataset is biased to satisfy the following equation:</p><formula xml:id="formula_4">p train (a b |a t ) = ? ? ? p align if g(a t ) = a b , p conflict |A b | ? 1 otherwise, p align &gt; p conflict , a b ?A b p train (a b |a t ) = 1.</formula><p>Here, a t and a b are the target and bias attribute, respectively. The function g : A t ? A b is a bijection between the target and bias attribute that assigns the bias attribute to each value of the target attribute. p align , p conflict are the ratio of bias-aligned and bias-conflicting samples, respectively. In what follows, we describe instance-specific details on the datasets considered in the experiments.</p><p>Colored MNIST. The MNIST dataset <ref type="bibr" target="#b15">[16]</ref> consists of grayscale digit images. We modify the original MNIST dataset to have two attributes: Digit and Color. Note that similar modification has been proposed by Kim et al. <ref type="bibr" target="#b13">[14]</ref>, Li and Vasconcelos <ref type="bibr" target="#b17">[18]</ref>, Bahng et al. <ref type="bibr" target="#b1">[2]</ref>. To define the Color attribute, we first choose ten distinct RGB values by drawing them uniformly at random. We use these ten RGB values throughout all the experiments for the Colored MNIST dataset. Then we generate ten Color distributions by assigning chosen RGB values to each Color distribution as its mean. Each Color distribution is a 3-dimensional Gaussian distribution having the assigned RGB value as its mean with predefined covariance ? 2 I. We pair Digit a t and Color distribution a b to make a correlation between two attributes, Digit and Color. Each bias-aligned sample has a Digit colored by RGB value sampled from paired Color distribution, and each bias-conflicting sample has a Digit colored by RGB value sampled from the other (nine) Color distributions. We control the ratio of bias-aligned samples among {99.5%, 99.0%, 98.0%, 95.0%}. The level of difficulty for the bias attribute is defined by the variance (? 2 ) of the Color distribution. We vary the standard deviation (?) of the Color distributions among {0.05, 0.02, 0.01, 0.005}. We use 60,000 training samples and 10,000 test samples.</p><p>Corrupted CIFAR-10. This dataset is generated by corrupting the CIFAR-10 dataset <ref type="bibr" target="#b14">[15]</ref> designed for object classification, following the protocols proposed by Hendrycks and Dietterich <ref type="bibr" target="#b11">[12]</ref>. The resulting dataset consists of two attributes, i.e., category of the Object and type of Corruption used. We use two sets of protocols for Corruption to build two datasets, namely the Corrupted CIFAR-10 1 and the Corrupted CIFAR-10 2 datasets. In particular, the Corrupted CIFAR-10 1,2 datasets use the following types of Corruption, respectively: {Snow, Frost, Fog, Brightness, Contrast, Spatter, Elastic, JPEG, Pixelate, Saturate} and {GaussianNoise, ShotNoise, ImpulseNoise, SpeckleNoise, GaussianBlur, DefocusBlur, GlassBlur, MotionBlur, ZoomBlur, Original}, respectively. In order to introduce the varying levels of difficulty, we control the "severity" of Corruption, which was predefined by Hendrycks and Dietterich <ref type="bibr" target="#b11">[12]</ref>. As the Corruption gets more severe, the images are likely to lose their characteristics and become less distinguishable. We use 50,000 training samples and 10,000 test samples for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Real-world experiments</head><p>CelebA. The CelebA dataset <ref type="bibr" target="#b18">[19]</ref> is a multi-attribute dataset for face recognition, equipped with 40 types of attributes for each image. Among 40 attributes, we use the BlondHair attribute (denoted by HairColor in the main text) following Sagawa et al. <ref type="bibr" target="#b23">[24]</ref>, and additionally consider HeavyMakeup attribute as the target attributes. For both of the cases, we use Male attribute (denoted by Gender in the main text) as the bias attribute. The dataset consists of 202,599 face images, and we use the official train-val split for training and test (162,770 for training, <ref type="bibr" target="#b18">19</ref>,867 for test). To evaluate the unbiased accuracy with an imbalanced evaluation set, we evaluate accuracy for each value of (a t , a b ), and compute average accuracy over all (a t , a b ) pairs. B Biased action recognition dataset 6-class action recognition dataset. Biased Action Recognition (BAR) dataset is a real-world image dataset categorized as six action classes which are biased to distinct places. We carefully settle these six action classes by inspecting imSitu <ref type="bibr" target="#b26">[27]</ref>, which provides still action images from Google Image Search with action and place labels. In detail, we choose action classes where images for each of these candidate actions share common place characteristics. At the same time, the place characteristics of action class candidates should be distinct in order to classify the action only from place attributes. In the end, we settle the six typical action-place pairs as (Climbing, RockWall), (Diving, Underwater), (Fishing, WaterSurface), (Racing, APavedTrack), (Throwing, PlayingField), and (Vaulting, Sky).</p><p>The source of dataset. We construct BAR with images from various sources: imSitu <ref type="bibr" target="#b26">[27]</ref>, Stanford 40 Actions <ref type="bibr" target="#b25">[26]</ref>, and Google Image Search. In the case of imSitu <ref type="bibr" target="#b26">[27]</ref>, we merge several action classes where the images have a similar gesture for constructing a single action class of BAR dataset, e.g., {hurling, pitching, flinging} for constructing throwing, and {carting, skidding} for constructing racing.</p><p>Construction process. BAR consists of training and evaluation sets; images describing the typical six action-place pairs belong to the training set and otherwise, the evaluation set. Before splitting images into these two sets, we exclude inappropriate images: illustrations, clip-arts, images with solid color background, and different gestures with the target gesture of the settled six action-place pairs. Since our sanitized images do not have explicit place labels, we split images into two sets by workers on Amazon Mechanical Turk. We designed the reasoning process to help workers answer the given questions. To be more specific, workers were asked to answer three binary questions. We split images into 'invalid', 'training', and 'evaluation' set based on workers' responses through binary questions. Workers were also asked to draw a bounding box where they considered it a clue to determine the place in order to help workers filter out images without an explicit clue. Here is the list of binary questions for each action class:    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental details</head><p>Architecture details. For the Colored MNIST dataset, we use the multi-layered perceptron consisting of three hidden layers where each hidden layer consists of 100 hidden units. For the Corrupted CIFAR-10 dataset, we use the ResNet-20 proposed by He et al. <ref type="bibr" target="#b10">[11]</ref>. For CelebA and BAR, we employ the Pytorch torchvision implementation of the ResNet-18 model, starting from pretrained weights.</p><p>Training details. We use Adam optimizer throughout all the experiments in the paper. We use a learning rate of 0.001 and a batch size of 256 for the Colored MNIST and Corrupted CIFAR-10 datasets. We use a learning rate of 0.0001 and a batch size of 256 for the CelebA and BAR dataset. Samples were augmented with random crop and horizontal flip transformations for the Corrupted CIFAR-10 and BAR dataset, and horizontal flip transformation for CelebA. For the Corrupted CIFAR-10 dataset, we take 32 ? 32 random crops from image padded by 4 pixels on each side. For the BAR dataset, we take 224 ? 224 random crops using torchvision.transforms.RandomResizedCrop in Pytorch. We do not use data augmentation schemes for training the neural network on the Colored MNIST dataset. We train the networks for 100, 200, 50, and 90 epochs for Colored MNIST, Corrupted CIFAR-10, CelebA and BAR, respectively. The GCE hyperparamter q = 0.7 is simply taken from the original paper <ref type="bibr" target="#b27">[28]</ref>. For stable training of LfF, we use an exponential moving average of loss for computing relative difficulty score instead of loss at each training epoch, with a fixed exponential decay hyperparameter 0.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Baselines</head><p>(1) HEX <ref type="bibr" target="#b24">[25]</ref> attempts to mitigate texture bias when the texture related domain identifier is not available. By utilizing gray-level co-occurrence matrix (GLCM), neural gray-level co-occurrence Matrix (NGLCM) can capture superficial statistics on the images, and HEX projects the model's representation orthogonal to the captured texture bias. Since our interest in debiasing is similar to that of HEX in terms of a method without explicit supervision on the bias, we use HEX as a baseline to compare debiasing performance in the case of controlled experiments.</p><p>(2) REPAIR <ref type="bibr" target="#b17">[18]</ref> "re-weights" training samples to have minimal mutual information between the bias-relevant labels and the intermediate representations of the target classifier. We test all four variants of REPAIR: REPAIR-T, REPAIR-R, REPAIR-C, and REPAIR-S, corresponding to the re-weighting schemes based on thresholding, ranking, per-class ranking, and sampling, respectively. We report the best result among four variants of REPAIR. We use RGB values for coloring digits and classes of corruption as representations inducing bias for the Colored MNIST and Corrupted CIFAR-10 datasets, respectively.</p><p>(3) Group DRO <ref type="bibr" target="#b23">[24]</ref> aims to minimize "worst-case" training loss over a set of pre-defined groups. Note that one requires additional labels of the bias attribute to define groups to apply group DRO for our problem of interest. With the label of bias attribute, we define |A t | ? |A b | groups, one for each value of (a t , a b ). Sagawa et al. <ref type="bibr" target="#b23">[24]</ref> expect that models that learn the spurious correlation between a t and a b in the training data would do poorly on groups for which the correlation does not hold, and hence do worse on the worst-group.   Comparison to other combination rule. There have been several works that utilize intentionally biased models to debias another model. RUBi proposed by Cadene et al. <ref type="bibr" target="#b2">[3]</ref> masks original prediction with the mask obtained from the prediction of the biased model. LearnedMixin proposed by Clark et al. <ref type="bibr" target="#b4">[5]</ref> uses an ensemble of logits of two models. DRiFt proposed by He et al. <ref type="bibr" target="#b9">[10]</ref> learns residual of the pretrained biased model to obtain the debiased model. As an effort to keep the usage of human knowledge minimal, we designed our combination rule without any hyperparameter. In <ref type="table" target="#tab_0">Table 10</ref>, we constructed an ablation study on LfF with a combination rule replaced by that of RUBi. While our method equipped with the RUBi combination rule slightly improves accuracy over the vanilla model, it is far behind other resampling/reweighting based methods like REPAIR and Group DRO. In conclusion, resampling/reweighting based methods are generally effective method than manipulating the predictions or logits directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional experiments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 Figure 1 :</head><label>11</label><figDesc>Illustration of bias-aligned samples for Colored MNIST, and Corrupted CIFAR-10 1 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) illustrates examples of bias-aligned samples, which can be correctly classified by an unintended decision rule based on Color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) Colored MNIST, (Digit, Color) (b) Corrupted CIFAR-10 1 , (Object, Corruption) Illustration of neural network training on the biased datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(LfF), for training neural networks on a biased dataset. At a high level, our algorithm simultaneously trains a pair of neural networks (f B , f D ) as follows: (a) intentionally training a model f B to be biased and (b) training a debiased model f D by focusing on the training samples that the biased model struggles to learn. In the rest of this section, we provide details on each component of our debiasing algorithm. We offer a full description of our debiasing scheme in Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Algorithm 1</head><label>31</label><figDesc>Illustration of training two models (f D , f B ) to be debiased and biased, respectively. The biased model optimizes generalized cross entropy (L GCE ) loss to amplify bias. The debiased model trains with weighted cross entropy loss leveraging relative difficulty. It results in larger weights to bias-conflicting samples while training the debiased model. Learning from Failure 1: Input: ? B , ? D , training set D, learning rate ?, number of iterations T 2: Initialize two networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 Figure 4 :</head><label>14</label><figDesc>(a) Bias-{aligned, conflicting} Colored MNIST (b) Bias-{aligned, conflicting} Corrupted CIFAR-10 Learning curves of vanilla, biased, and debiased model for Colored MNIST and Corrupted CIFAR-10 1 . For each dataset, the left and the right plots correspond to curves for the bias-aligned samples and the bias-conflicting samples, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of BAR images of six typical action-place pairs settled as (Climbing, RockWall), (Diving, Underwater), (Fishing, WaterSurface), (Racing, APavedTrack), (Throwing, PlayingField), and (Vaulting, Sky). The images with red border lines belong to BAR evaluation set, and others belong to BAR training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of BAR reasoning process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Overview of web pages for workers to validate and split images of BAR. Workers are asked to answer three binary questions and drawing a bounding box task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy of the unbiased evaluation set with varying choice of (target, bias) attribute pair for the Colored MNIST and Corrupted CIFAR-10 1,2 datasets. Accuracy and Accuracy * denotes the performance of the model trained on the biased and unbiased training set, respectively.</figDesc><table><row><cell>Dataset</cell><cell>Target</cell><cell>Bias</cell><cell cols="2">Accuracy Accuracy  *  Relative drop</cell></row><row><cell>Colored MNIST</cell><cell>Color Digit</cell><cell>Digit Color</cell><cell>99.97?0.04 100.0?0.00 50.34?0.16 96.41?0.07</cell><cell>-0.03% -47.79%</cell></row><row><cell cols="2">Corrupted CIFAR-10 1 Corruption Object</cell><cell cols="2">Object Corruption 22.72?0.87 80.00?0.01 98.34?0.26 99.62?0.03</cell><cell>-1.28% -71.60%</cell></row><row><cell cols="2">Corrupted CIFAR-10 2 Corruption Object</cell><cell cols="2">Object Corruption 21.07?0.29 79.65?0.11 98.64?0.20 99.80?0.01</cell><cell>-1.16% -73.56%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy evaluated on unbiased samples for the Colored MNIST and Corrupted CIFAR-10 1,2 datasets with varying ratio of bias-aligned samples. We denote bias supervision type by (no supervision), (bias-tailored supervision), and (explicit bias supervision). Best performing results are marked in bold.</figDesc><table><row><cell>Dataset</cell><cell>Ratio (%)</cell><cell>Vanilla</cell><cell>Ours</cell><cell>HEX</cell><cell>REPAIR</cell><cell>Group DRO</cell></row><row><cell></cell><cell>95.0</cell><cell cols="2">77.63?0.44 85.39?0.94</cell><cell cols="2">70.44?1.41 82.51?0.59</cell><cell>84.50?0.46</cell></row><row><cell>Colored</cell><cell>98.0</cell><cell cols="2">62.29?1.47 80.48?0.45</cell><cell cols="2">62.03?0.24 72.86?1.47</cell><cell>76.30?1.53</cell></row><row><cell>MNIST</cell><cell>99.0</cell><cell cols="2">50.34?0.16 74.01?2.21</cell><cell cols="2">51.99?1.09 67.28?1.69</cell><cell>71.33?1.76</cell></row><row><cell></cell><cell>99.5</cell><cell cols="2">35.34?0.13 63.39?1.97</cell><cell cols="2">41.38?1.31 56.40?3.74</cell><cell>59.67?2.73</cell></row><row><cell></cell><cell>95.0</cell><cell cols="2">45.24?0.22 59.95?0.16</cell><cell cols="2">21.74?0.27 48.74?0.71</cell><cell>53.15?0.53</cell></row><row><cell>Corrupted</cell><cell>98.0</cell><cell cols="2">30.21?0.82 49.43?0.78</cell><cell cols="2">17.81?0.29 37.89?0.22</cell><cell>40.19?0.23</cell></row><row><cell>CIFAR-10 1</cell><cell>99.0</cell><cell cols="2">22.72?0.87 41.37?2.34</cell><cell cols="2">16.62?0.80 32.42?0.35</cell><cell>32.11?0.83</cell></row><row><cell></cell><cell>99.5</cell><cell cols="2">17.93?0.66 31.66?1.18</cell><cell cols="2">15.39?0.13 26.26?1.06</cell><cell>29.26?0.11</cell></row><row><cell></cell><cell>95.0</cell><cell cols="2">41.27?0.98 58.57?1.18</cell><cell cols="2">19.25?0.81 54.05?1.01</cell><cell>57.92?0.31</cell></row><row><cell>Corrupted</cell><cell>98.0</cell><cell cols="2">28.29?0.62 48.75?1.68</cell><cell cols="2">15.55?0.84 44.22?0.84</cell><cell>46.12?1.11</cell></row><row><cell>CIFAR-10 2</cell><cell>99.0</cell><cell cols="2">20.71?0.29 41.29?2.08</cell><cell cols="2">14.42?0.51 38.40?0.26</cell><cell>39.57?1.04</cell></row><row><cell></cell><cell>99.5</cell><cell cols="2">17.37?0.31 34.11?2.39</cell><cell cols="2">13.63?0.42 31.03?0.42</cell><cell>34.25?0.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy evaluated on bias-conflicting samples for the Colored MNIST and Corrupted CIFAR-10 1,2 datasets with varying ratio of bias-aligned samples. We denote bias supervision type by (no supervision), (bias-tailored supervision), and (explicit bias supervision). Best performing results are marked in bold.</figDesc><table><row><cell>Dataset</cell><cell>Ratio (%)</cell><cell>Vanilla</cell><cell>Ours</cell><cell>HEX</cell><cell>REPAIR</cell><cell>Group DRO</cell></row><row><cell></cell><cell>95.0</cell><cell cols="2">75.17?0.51 85.77?0.66</cell><cell cols="2">67.75?1.49 83.26?0.42</cell><cell>83.11?0.41</cell></row><row><cell>Colored</cell><cell>98.0</cell><cell cols="2">58.13?1.63 80.67?0.56</cell><cell cols="2">58.80?0.28 73.42?1.42</cell><cell>74.28?1.93</cell></row><row><cell>MNIST</cell><cell>99.0</cell><cell cols="2">44.83?0.18 74.19?1.94</cell><cell cols="2">46.96?1.20 68.26?1.52</cell><cell>69.58?1.66</cell></row><row><cell></cell><cell>99.5</cell><cell cols="2">28.15?1.44 63.49?1.94</cell><cell cols="2">35.05?1.46 57.27?3.92</cell><cell>57.07?3.60</cell></row><row><cell></cell><cell>95.0</cell><cell cols="2">39.42?0.20 59.62?0.03</cell><cell cols="2">14.09?0.31 49.99?0.92</cell><cell>49.00?0.45</cell></row><row><cell>Corrupted</cell><cell>98.0</cell><cell cols="2">22.65?0.95 48.69?0.70</cell><cell cols="2">9.34?0.41 38.94?0.20</cell><cell>35.10?0.49</cell></row><row><cell>CIFAR-10 1</cell><cell>99.0</cell><cell cols="2">14.24?1.03 39.55?2.56</cell><cell cols="2">8.37?0.56 33.05?0.36</cell><cell>28.04?1.18</cell></row><row><cell></cell><cell>99.5</cell><cell cols="2">10.50?0.71 28.61?1.25</cell><cell cols="2">6.38?0.08 26.52?0.94</cell><cell>24.40?0.28</cell></row><row><cell></cell><cell>95.0</cell><cell cols="2">34.97?1.06 58.64?1.04</cell><cell cols="2">10.79?0.90 54.46?1.02</cell><cell>54.60?0.11</cell></row><row><cell>Corrupted</cell><cell>98.0</cell><cell cols="2">20.52?0.73 48.99?1.61</cell><cell cols="2">6.60?7.23 44.63?0.75</cell><cell>42.71?1.24</cell></row><row><cell>CIFAR-10 2</cell><cell>99.0</cell><cell cols="2">12.11?0.29 40.84?2.06</cell><cell cols="2">5.11?0.59 38.81?0.20</cell><cell>37.07?1.02</cell></row><row><cell></cell><cell>99.5</cell><cell cols="2">10.01?0.01 32.03?2.51</cell><cell cols="2">4.22?0.43 31.45?0.28</cell><cell>30.92?0.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy evaluated on the unbiased samples and bias-conflicting samples of the Colored MNIST and Corrupted CIFAR-10 1,2 dataset for ablation of the proposed algorithm. We denote our algorithm using vanilla model as biased model by Ours ? . Best performing results are marked in bold.</figDesc><table><row><cell>Dataset</cell><cell>Vanilla</cell><cell>Unbiased Ours  ?</cell><cell>Ours</cell><cell>Vanilla</cell><cell>Bias-conflicting Ours</cell></row></table><note>? Ours Colored MNIST 50.34?0.16 49.90?1.67 74.01?2.21 44.83?0.18 44.44?1.83 74.19?1.94 Corrupted CIFAR-10 1 22.72?0.87 25.15?0.63 41.37?2.34 14.24?1.03 17.21?0.69 39.55?2.56 Corrupted CIFAR-10 2 20.71?0.29 22.90?0.47 41.29?2.08 12.11?0.29 14.89?0.66 40.84?2.06</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Accuracy evaluated on the unbiased samples and bias-conflicting samples for the CelebA dataset with Gender as the bias attribute.</figDesc><table><row><cell>Target attribute</cell><cell>Vanilla</cell><cell>Unbiased Ours</cell><cell>Group DRO</cell><cell>Vanilla</cell><cell cols="2">Bias-conflicting Ours Group DRO</cell></row><row><cell>HairColor</cell><cell cols="2">70.25?0.35 84.24?0.37</cell><cell>85.43?0.53</cell><cell cols="2">52.52?0.19 81.24?1.38</cell><cell>83.40?0.67</cell></row><row><cell>HeavyMakeup</cell><cell cols="2">62.00?0.02 66.20?1.21</cell><cell>64.88?0.42</cell><cell cols="2">33.75?0.28 45.48?4.33</cell><cell>50.24?0.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Class-wise action recognition accuracy on BAR evaluation set. Best performing results are marked in bold.</figDesc><table><row><cell>Action</cell><cell>Climbing</cell><cell>Diving</cell><cell>Fishing</cell><cell>Racing</cell><cell>Throwing Vaulting</cell><cell>Average</cell></row><row><cell>Vanilla</cell><cell cols="5">59.05?17.48 16.56?1.58 62.69?3.64 77.27?2.62 28.62?2.95 66.92?7.25</cell><cell>51.85?5.92</cell></row><row><cell>ReBias</cell><cell cols="5">77.78?8.32 51.57?4.54 54.76?2.38 80.56?2.19 28.63?2.71 65.14?7.21</cell><cell>59.74?1.49</cell></row><row><cell>Ours</cell><cell cols="5">79.36?4.79 34.59?2.26 75.39?3.63 83.08?1.90 33.72?0.68 71.75?3.32</cell><cell>62.98?2.76</cell></row></table><note>model trained with the CE-trained biased model instead of the GCE-trained model, denoted as Ours ? . As expected, using the CE-trained model to compute relative difficulty does not help debiasing model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Does the picture clearly describe Fishing and include person?, Then, does the picture contain the surface of a body of a water?, Draw a box around the surface of a body of a water (including a person) on the image. If the water region does not more than 90% of the image's background, click 'Cannot find clue'. Does the picture clearly capture the Throwing / Pelting moment and include person?, Then, can you see a type of playing field (baseball mound, football pitch, etc.) where the person is throwing something on?, Draw a box around the playing field (baseball mound, football pitch, etc.) on the image.Finally, we use 2,595 images to construct BAR dataset. All image sizes are over 400px width and 300px height. The BAR training and evaluation sets are publicly available on https://anonymous. 4open.science/r/c9025a07-2784-47fb-8ba1-77b06c3509fe/.</figDesc><table><row><cell>? Climbing: Does the picture clearly describe Climbing and include</cell></row><row><cell>person?, Then, is the person rock climbing?, Draw a box around the</cell></row><row><cell>natural rock wall (including a person) on the image. If it is not natural</cell></row><row><cell>rock wall, click 'Cannot find clue'.</cell></row><row><cell>? Diving: Does the picture clearly describe Scuba Diving /</cell></row><row><cell>Diving jump / Diving and include person?, Then, does the pic-</cell></row><row><cell>ture include a body of water or the surface of a body of water?, Draw</cell></row><row><cell>a box around a body of water or the surface of a body of water</cell></row><row><cell>(including a person) on the image.</cell></row><row><cell>? Fishing: ? Racing: Does the picture clearly describe Auto racing /</cell></row><row><cell>Motorcycle racing / Cart racing ?, Then, is the racing held</cell></row><row><cell>on a paved track?, Draw a box around a paved track (including a</cell></row><row><cell>vehicle) on the image.</cell></row><row><cell>? Throwing:</cell></row></table><note>? Vaulting: Does the picture clearly capture the Pole Vaulting and include person?, Then, does the picture contain the sky as back- ground? Draw a box around the sky region (including a person) on the image. If the sky region does not more than 90% of the image's background, click 'Cannot find clue'.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Per-class count of BAR dataset.</figDesc><table><row><cell>Action</cell><cell cols="6">Climbing Diving Fishing Racing Throwing Vaulting</cell><cell>Total</cell></row><row><cell>Training</cell><cell>326</cell><cell>520</cell><cell>163</cell><cell>336</cell><cell>317</cell><cell>279</cell><cell>1941</cell></row><row><cell>Evaluation</cell><cell>105</cell><cell>159</cell><cell>42</cell><cell>132</cell><cell>85</cell><cell>131</cell><cell>654</cell></row><row><cell></cell><cell cols="2">(a) Describe action?</cell><cell></cell><cell></cell><cell cols="2">(b) Match typical pairs?</cell><cell></cell></row><row><cell cols="3">(c) Draw a bounding box.</cell><cell></cell><cell></cell><cell cols="2">(d) Finish</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Accuracy evaluated on the unbiased samples for the Colored MNIST and Corrupted CIFAR-10 1,2 datasets with varying difficulty of the bias attributes. We denote bias supervision type by (no supervision), (bias-tailored supervision), and (explicit bias supervision). Best performing results are marked in bold.</figDesc><table><row><cell>Dataset</cell><cell>Difficulty</cell><cell>Vanilla</cell><cell>Ours</cell><cell>HEX</cell><cell>REPAIR</cell><cell>Group DRO</cell></row><row><cell></cell><cell>1</cell><cell cols="2">50.97?0.59 75.91?1.25</cell><cell cols="2">51.38?0.59 69.60?0.97</cell><cell>70.34?1.98</cell></row><row><cell>Colored</cell><cell>2</cell><cell cols="2">50.92?1.16 74.05?2.21</cell><cell cols="2">51.38?0.59 64.14?0.38</cell><cell>70.80?1.82</cell></row><row><cell>MNIST</cell><cell>3</cell><cell cols="2">49.66?0.42 72.50?1.79</cell><cell cols="2">52.88?1.24 69.20?2.03</cell><cell>71.03?2.24</cell></row><row><cell></cell><cell>4</cell><cell cols="2">50.34?0.16 74.01?2.21</cell><cell cols="2">51.99?1.09 67.28?1.69</cell><cell>71.33?1.76</cell></row><row><cell></cell><cell>1</cell><cell cols="2">35.37?0.58 52.12?1.99</cell><cell cols="2">23.92?0.80 37.73?0.73</cell><cell>49.62?1.49</cell></row><row><cell>Corrupted</cell><cell>2</cell><cell cols="2">29.30?3.11 47.19?2.26</cell><cell cols="2">21.23?0.38 36.22?0.88</cell><cell>44.54?1.70</cell></row><row><cell>CIFAR-10 1</cell><cell>3</cell><cell cols="2">26.44?0.98 44.12?1.53</cell><cell cols="2">18.66?1.16 34.59?1.88</cell><cell>38.43?1.44</cell></row><row><cell></cell><cell>4</cell><cell cols="2">22.72?0.87 41.37?2.34</cell><cell cols="2">16.62?0.80 32.42?0.35</cell><cell>32.11?0.83</cell></row><row><cell></cell><cell>1</cell><cell cols="2">32.00?0.87 46.89?3.02</cell><cell cols="2">20.12?0.44 41.00?0.39</cell><cell>44.85?0.04</cell></row><row><cell>Corrupted</cell><cell>2</cell><cell cols="2">27.62?1.31 43.56?2.10</cell><cell cols="2">16.82?0.38 39.57?0.61</cell><cell>43.21?1.54</cell></row><row><cell>CIFAR-10 2</cell><cell>3</cell><cell cols="2">22.14?0.03 41.46?0.30</cell><cell cols="2">15.22?0.47 38.16?0.52</cell><cell>42.12?0.52</cell></row><row><cell></cell><cell>4</cell><cell cols="2">20.71?0.29 41.29?2.08</cell><cell cols="2">14.42?0.51 38.40?0.26</cell><cell>39.57?1.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Accuracy evaluated on the bias-conflicting samples for the Colored MNIST and Corrupted CIFAR-10 1,2 datasets with varying difficulty of the bias attributes. We denote bias supervision type by (no supervision), (bias-tailored supervision), and (explicit bias supervision). Best performing results are marked in bold.</figDesc><table><row><cell>Dataset</cell><cell>Difficulty</cell><cell>Vanilla</cell><cell>Ours</cell><cell>HEX</cell><cell>REPAIR</cell><cell>Group DRO</cell></row><row><cell>Colored</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MNIST</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Accuray evaluated on the unbiased samples and bias-conflicting samples for the Colored MNIST datasets with varying ratio of bias-aligned samples. 63?0.44 85.39?0.94 78.22?0.34 75.17?0.51 85.77?0.66 75.84?0.36 98.0 62.29?1.47 80.48?0.45 64.92?0.78 58.13?1.63 80.67?0.56 61.04?0.83 99.0 50.34?0.16 74.01?2.21 52.41?0.42 44.83?0.18 74.19?1.94 46.85?0.46 99.5 35.34?0.13 63.39?1.97 36.42?0.37 28.15?1.44 63.49?1.94 29.36?0.43</figDesc><table><row><cell>Dataset Ratio (%)</cell><cell>Vanilla</cell><cell>Unbiased Ours</cell><cell>RUBi</cell><cell>Vanilla</cell><cell>Bias-conflicting Ours</cell><cell>RUBi</cell></row><row><cell>95.0</cell><cell>77.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Colored</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MNIST</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/alinlab/BAR</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A.</p><p>In <ref type="table">Table 8</ref>, 9, we again observe our algorithm to consistently outperform the baseline algorithm by a large margin, regardless of the difficulty for the biased attribute. Furthermore, we observe that both baseline and LfF trained classifiers get more biased as the difficulty of the bias attribute increase in general, which also validates our claims made in Section 2. Notably, LfF even outperforms the baseline methods that utilizes explicit label on the bias attribute in most cases.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning de-biased representations with biased representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reducing unimodal biases for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Why can&apos;t i dance in the mall? learning to mitigate scene bias in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Messou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenettrained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unlearn dataset bias in natural language inference by fitting the residual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving fairness in machine learning systems: What do industry practitioners need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Holstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dud?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05239</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning not to learn: Training deep neural networks with biased data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Repair: Removing representation bias by dataset resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09635</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Insights on representational similarity in neural networks with canonical correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Filter Bubble: What the Internet Is Hiding from You. Penguin Group , The</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pariser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot; explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning robust representations by projecting superficial statistics out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Situation recognition: Visual semantic role labeling for image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object recognition with and without objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno>32?0.45 68.03?1.11 50.54?0.88 67.70?1.02 68.77?1.26 2 45.54?0.65 75.56?1.22 46.84?0.44 63.71?0.29 69.28?1.13 3 45.48?1.29 74.29?1.78 47.88?1.37 70.05?2.10 68.68?1.26 4 44.83?0.18 74.19?1.94 46.96?1.20 68.26?1.52 69.58?1.66</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="0151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">We introduce four levels of difficulty for the Colored MNIST and Corrupted CIFAR-10 1,2 datasets with the target and the biased attribute chosen as (Digit, Color) and (Object, Corruption), respectively. For the Colored MNIST dataset, we vary the standard deviation of the Gaussian noise for perturbing the RGB values of injected color. In cases of Corrupted CIFAR-10, we control the &quot;severity&quot; of Corruption</title>
	</analytic>
	<monogr>
		<title level="m">addition to varying ratio of bias-conflicting samples, we vary the level of</title>
		<imprint/>
	</monogr>
	<note>difficulty&quot; for the biased attributes by controlling how much the target attribute is easy to distinguish from the given image. Based on our observations in Section 2, the difficulty of bias is lower, the more likely the classifier is to suffer from bias. which was predefined by Hendrycks and Dietterich [12]. We provide a detailed description of difficulty of the bias attribute in</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
