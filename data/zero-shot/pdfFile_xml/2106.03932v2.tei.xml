<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>K?p?kl?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Taseska</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Successful active speaker detection requires a threestage pipeline: (i) audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation modeling between a reference speaker and the background speakers within each frame, and (iii) temporal modeling for the reference speaker. Each stage of this pipeline plays an important role for the final performance of the created architecture. Based on a series of controlled experiments, this work presents several practical guidelines for audio-visual active speaker detection. Correspondingly, we present a new architecture called ASDNet, which achieves a new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5% outperforming the second best with a large margin of 4.7%. Our code and pretrained models are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fusion of audio and video modalities has been shown to provide promising solutions to long-standing challenging problems. These include among others, speaker diarization <ref type="bibr" target="#b16">[16]</ref>, biometrics <ref type="bibr" target="#b6">[7]</ref>, and action recognition <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b38">38]</ref>. Similar to other tasks, Audiovisual Active Speaker Detection (AV-ASD) has also long been studied in literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. A particularly challenging flavor of this problem is AV-ASD in the wild, where speech is to be detected and assigned to one of possibly multiple active speakers at each instant in time. Clearly, fusing the complementary discriminative information from audio and video modalities is crucial: visual-only approaches can easily be mistaken by other face/mouth motions such as eating, yawning or emotional expressions. Audio-only approaches, although able to perform source clustering and separation <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b46">46]</ref>, aren't sufficiently robust to count the number of speakers and assign speech to the correct source. This is especially challenging with a single microphone input in acoustically adverse conditions, typically encountered in practice. <ref type="bibr" target="#b0">1</ref> https://github.com/okankop/ASDNet <ref type="figure">Figure 1</ref>. Audio-visual active speaker detection pipeline. The task is to determine if the reference speaker at frame t is speaking or not-speaking. The pipeline starts with audio-visual encoding of each speaker in the clip. Secondly, inter-speaker relation modeling is applied within each frame. Finally, temporal modeling is used to capture long-term relationships in natural conversations. Examples are from AVA-ActiveSpeaker dataset <ref type="bibr" target="#b42">[42]</ref>.</p><p>Recently, the AVA-ActiveSpeaker dataset <ref type="bibr" target="#b42">[42]</ref> provided the first large-scale standard benchmark for audio-visual active speaker detection in the wild. Recent research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">32]</ref> indicates that active speaker detection in the wild requires (i) integration of audio-visual information for each speaker, (ii) contextual information that captures inter-speaker relationships, and (iii) temporal modeling to exploit long term relationships in natural conversation. In this paper, we con-solidate this three-stage pipeline for audio visual speaker detection, illustrated in <ref type="figure">Fig. 1</ref>, and study the importance of each stage in detail.</p><p>Contributions. We propose a novel three-stage pipeline for audio-visual active speaker detection in the wild. Our architecture, named ASDNet, sets a new state-of-the-art result on AVA-ActiveSpeaker dataset with a 93.5% mAP, and outperforms the second best method <ref type="bibr" target="#b32">[32]</ref> with a large margin of 4.7% mAP (Section 4.5). As part of ASDNet, we propose (1) architectures for the audio and video backbones of the audio-visual encoder (Section 3.2), that haven't been previously explored for active speaker detection;</p><p>(2) a simple, yet effective inter-speaker relation modeling mechanism (Section 3.3);</p><p>(3) In addition, we provide detailed ablation study and guidelines for tuning all components of ASDNet. The study includes comparison to the state of the art for the two novel components mentioned above, as well as evaluation of various Recurrent Neural Network (RNN) architectures for temporal modeling (Section 4.2.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We present the related work in two parts: (i) audio-visual feature extraction in various applications, and (ii) contributions that address active speaker detection in the wild and its challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Audio-visual feature extraction</head><p>Audio. A common approach to extract features in speech and audio research in different applications, is to use Convolutional Neural Networks (CNNs) and RNNs with log-Mel or Short-Time Fourier Transform (STFT) spectrograms as inputs <ref type="bibr" target="#b12">[13]</ref>. The popularity of these fixed transforms is due to their success in traditional speech and audio processing and the fact that they extract relevant information from first principles. Furthermore, the image-like configuration of the spectrograms allows employing network architectures well-known from computer vision applications. Particularly, in AV-ASD, this allows to use similar audio and video backbone architectures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">32]</ref>.</p><p>Based on the interpretation of CNNs as a data-driven filterbank, researchers have applied CNNs directly on the audio waveforms to capture discriminative information for the task at hand <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">30]</ref>. Such an approach in the context of AV-ASD has been used for an audio backbone in <ref type="bibr" target="#b1">[2]</ref>. However, these approaches need much more data and computational resources that the ones exploiting spectrograms. With the goal to exploit the best from both worlds, researchers have come up with learnable, but yet constrained transformations of raw audio data. Examples include Harmonic CNNs used for music tagging, and the SincNet architecture proposed in <ref type="bibr" target="#b41">[41]</ref>. The latter was successfully used in several audio applications <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36]</ref>. To our best knowledge, this promising architecture hasn't been used in the context of AV-ASD.</p><p>Video. Active speaker detection using only video modality can be viewed as action recognition task. Prior to CNNs, action recognition research was dominated by hand-crafted features <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b49">49]</ref>, combined with Fisher Vector representations <ref type="bibr" target="#b39">[39]</ref> or Bag-of-Features histograms <ref type="bibr" target="#b7">[8]</ref>. Ever since AlexNet <ref type="bibr" target="#b26">[26]</ref> won the ImageNet Challenge <ref type="bibr" target="#b43">[43]</ref>, handcrafted features were mostly abandoned in favor of features extracted by CNNs. This trend extended to video analysis tasks as well, including action recognition. Initially, due to the absence of a large-scale video dataset, architectures for action recognition could benefit from pretraining on the very-large ImageNet dataset <ref type="bibr" target="#b10">[11]</ref>. The first intuitive approach was to treat video frames as multi channel input to 2D-CNNs <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b45">45]</ref>. Other approaches include extraction of frame-level features with a 2D-CNN, followed by a spatiotemporal modeling mechanism <ref type="bibr" target="#b23">[23]</ref>.</p><p>With the availability of large-scale video datasets such as Kinetics <ref type="bibr" target="#b2">[3]</ref>, Moments-in-Time <ref type="bibr" target="#b37">[37]</ref>, and Jester <ref type="bibr" target="#b34">[34]</ref>, 2D-CNNs were replaced by 3D-CNNs, to better capture temporal information and motion patterns within video frames. A 3D-CNN architecture was first proposed in <ref type="bibr" target="#b20">[20]</ref> by Ji et al.</p><p>Since then, many 3D-CNN architectures for video recognition tasks followed, such as C3D <ref type="bibr" target="#b47">[47]</ref>, I3D <ref type="bibr" target="#b2">[3]</ref>, P3D <ref type="bibr" target="#b40">[40]</ref>, R(2+1)D <ref type="bibr" target="#b48">[48]</ref>, SlowFast <ref type="bibr" target="#b14">[14]</ref>, etc. In <ref type="bibr" target="#b17">[17]</ref>, the effect of dataset size on performance is investigated for several 3D-CNN architectures. Inflated versions of popular resourceefficient 2D-CNN architectures are analyzed for video classification tasks in <ref type="bibr" target="#b24">[24]</ref>. In this work, we explore variants of 3D-CNNs for the AV-ASD task.</p><p>Fusion. The extracted modality-specific features can be combined at data level <ref type="bibr" target="#b25">[25]</ref>, feature level <ref type="bibr" target="#b35">[35]</ref> or decision level <ref type="bibr" target="#b45">[45]</ref>. The fusion that we apply in this work can be considered as feature level fusion, since we keep processing fused features at inter-speaker relation modeling and temporal modeling mechanisms afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Active speaker detection in the wild</head><p>Audio-visual active speaker detection is a specific case of source separation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">50]</ref>, where audio and visual signals are leveraged jointly to assign a speech segment to its speaker. For this task, initial approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> use datasets collected in controlled environments. With the availability of AVA-ActiveSpeaker dataset <ref type="bibr" target="#b42">[42]</ref>, the research community was able to shift towards active speaker detection in the wild.</p><p>Audio-visual feature extraction is the first step in topperforming frameworks for active speaker detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b54">54]</ref>. A two-backbone approach has established it- self as a standard architecture due to its versatility <ref type="bibr" target="#b45">[45]</ref>. With a good audio-visual feature extraction and RNN-based temporal modeling, the authors in <ref type="bibr" target="#b5">[6]</ref> achieved competitive performance on the AVA-ActiveSpeaker dataset. Temporal modeling constitutes an integral part of recent active speaker detection pipelines <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b54">54]</ref>. Often neglected is the context information that can be obtained by modeling inter-speaker relationships. Researchers have only recently proposed methods to exploit the context information <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Drawing inspiration from the insights in recent research, we seek to establish a general pipeline that incorporates audio-visual encoding, inter-speaker (context) modeling, and temporal modeling. By designing an appropriate architecture for each component, we are able to exceed the stateof-the art performance on the AVA-ActiveSpeaker dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation and Overview</head><p>Let K denote the total number of speakers in a given clip. The data available to the active speaker detection system at time t is a set X t = {X t,1 , X t,2 , . . . X t,K , x t }, where X t,k ? R n?3?d h ?dw is a tensor of face crops corresponding to the k-th speaker. The height and width of the face crops are denoted by d h and d w , 3 is the RGB channels and n is the number of consecutive face crops centering time instant t. The vector x t contains the samples of the audio track corresponding to the duration of the video input. Given the input data, the objective is to produce a binary vector z t , where z t [k] = 1 if the k-th speaker is detected as speaking at time frame t, and z t [k] = 0 otherwise.</p><p>A high-level overview of our pipeline that maps the raw data X t to the predictions z t is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. Next, in Sec. 3.2-3.4, we zoom in on the design of the three pipeline components. In Sec. 3.5, we discuss the training strategy that enables an end-to-end inference: from face crops and an audio waveform, to a prediction speaking or not speaking for each speaker in the video clip. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Audio-Visual Encoder Architecture</head><p>Our audio-visual encoder is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. The stack of face thumbnails X t,k consists of n frames, X t? n 2 ,k , . . . , X t,k , . . . , X t+ n 2 ?1,k , and the size of the audio input vector x t is determined by the number of video frames, the video frame rate, and the audio signal sampling rate. The encoder produces an embedding vector by concatenating the modality-specific embeddings</p><formula xml:id="formula_0">v t,k = f v (X t,k ; w v ), a t = f a (x t ; w a ),<label>(1)</label></formula><p>where f v and f a are neural networks with trainable parameters w v and w a , respectively. The concatenated features v t,k a t are fed into a fully connected layer to get final predictions. To train the audiovisual encoder, we apply cross-entropy loss between the predictions and ground-truth labels. To ensure that consistent discriminative features are extracted from both modalities, we apply auxiliary classification networks after each backbone, following previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">42]</ref>. The auxiliary networks are also trained with cross-entropy loss. The final loss becomes L f inal = L av + L a + L v . After training is completed, supervision heads are discarded and only the audio-visual backbone is used to extract features v t,k and a t for all speakers and time instants.</p><p>While the described high-level architecture is similar to that of existing audio-visual encoders <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">42]</ref>, our contribution lies in the choice and design of the video and audio backbones, discussed next.</p><p>Video backbone. Movements of mouth and facial muscles are indicative of active speaking. Hence, to fully exploit the available video data, it is important to accurately model motion patterns. To this end, we propose using a 3D-CNN as the visual encoder function f v , in contrast to the state-of-the-art approacches that apply 2D-CNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b54">54]</ref>. As part of our study, we experimented with various resource-efficient and high-performance 3D-CNN architectures <ref type="bibr" target="#b24">[24]</ref> and found 3D-ResNeXt-101 to be the best performing candidate for our video backbone. Further insights from our investigation are discussed in Section 4.1.</p><p>Audio backbone. For the audio encoding backbone, the majority of existing AV-ASD approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b54">54]</ref> extract Mel Frequency Cepstral Coefficients (MFCC) from the raw signal, and use the MFCCs as input to 2D-CNNs. In contrast, we propose using an audio backbone architecture that directly operates on raw audio signal via sinc convolutions <ref type="bibr" target="#b41">[41]</ref>. In this manner, the system doesn't require a dedicated filterbank and directly exploits all available audio information. This is not the case in existing approaches, where phase information is often discarded after the filterbanks. After sinc convolutions, we apply log-compression, i.e., y = log(abs(x) + 1). This non-linearity has been effective in other raw audio processing tasks as well <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b53">53]</ref>. The features extracted by the sinc-convolutions are used as input to Depthwise Separable Convolutional (DSConv) blocks with Leaky-ReLU nonlinearity <ref type="bibr" target="#b52">[52]</ref>. Our full audio encoder architecture, referred to as SincDSNet, is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Features after the global average pooling are extracted as the audio features a t . The advantage of the proposed raw-audio backbone over existing feature-based backbones is experimentally demonstrated in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inter-Speaker Relation Modeling (ISRM)</head><p>The audio-visual encoder extracts features for each individual speaker separately -the features for speaker k do not contain visual information from the remaining speakers in the frame. However, features belonging to background speakers contain complementary information that improves the system performance, as shown in <ref type="bibr" target="#b0">[1]</ref>. In this paper, we propose a method to aggregate information from the back- ground speakers efficiently.</p><p>Consider a reference speaker k and m background speakers in the scene at time t. The output of the audiovisual encoder for the reference speaker is [v t,k , a t ]. To incorporate information from background speakers, we propose to extract an additional feature vector b t,k using a single-layer perceptron, as illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>. The input to the MLP are the concatenated audio-visual embeddings from all background speakers at time t. Note that the number m is fixed from the system's perspective: if there are less than m background speakers at time t, the encoder features are populated with zero vectors. If there are more than m speakers, only m are randomly selected. In this manner, the input dimension of the MLP is fixed. The final feature vector [v t,k , a t , b t,k ] is fed to the temporal model. An experimental study of the proposed ISRM, and comparison to the approach in <ref type="bibr" target="#b0">[1]</ref> is provided in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Temporal Modeling</head><p>Speaking is a coherent action in time: if a person is speaking at previous or future time instants, it is likely that the person is speaking at the current time instant. This is also valid for remaining silent action. Therefore, temporal modeling is crucial for accurate active speaker detection.</p><p>We experimented with several RNN-based temporal modeling architectures: Long Short-Term Memory (LSTM) <ref type="bibr" target="#b19">[19]</ref>, Gated Recurrent Unit (GRU) <ref type="bibr" target="#b4">[5]</ref>, Simple Recurrent Unit (SRU) <ref type="bibr" target="#b31">[31]</ref> and their bidirectional versions. For the uni-directional methods, the reference frame is at the end of the input, while for the bidirectional methods it is at center of the input. The hidden state vector of the recurrent block at the reference frame is fed to a fully connected layer to produce a binary output z t [k] ? {0, 1} (i.e. active speaker or not). In case speakers' features are not available for the selected time window, similar to <ref type="bibr" target="#b0">[1]</ref> we apply same padding to the beginning or to the end. Out of all methods, Bidirectional-GRU performs best and becomes our final choice in temporal modeling stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training Details</head><p>Training Audio-Visual Encoding Backbones. We train our audio-visual encoder using ADAM optimizer <ref type="bibr" target="#b22">[22]</ref> for 70 epochs. Batch size is selected as highest possible number that fits to a single NVIDIA Titan XP GPU for different backbones. However, gradients are accumulated reaching to effective batch size of 192 before doing backward propagation. The learning rate is initialized with 3 ? 10 ?4 and dropped by a factor of 10 at every 30 epochs. For video input, we apply random cropping, random horizontal flipping and color transformations as data augmentation at the training time. Finally, video input is reshaped to the resolution of 160?160. The audio signals are sampled at 16 kHz. 3D-CNNs are pretrained on Kinetics <ref type="bibr" target="#b2">[3]</ref>, 2D-CNNs are pretrained with ImageNet <ref type="bibr" target="#b10">[11]</ref>, and SincDSNet is trained from scratch. Once the training is finished, prediction heads are discarded and the features v t,k ? R 512 and a t ? R 160 are used to train the ISRM and the temporal model.</p><p>Training ISRM and Temporal Modeling. We used ADAM optimizer with cross-entropy loss to train the ISRM and the temporal model. We train for 10 epochs, with batch size of 256. The learning rate is initialized with 3 ? 10 ?6 and dropped by 10 at 5th epoch. The MLP in the ISRM extracts the feature b t,k ? R 128 independent from the number of background speakers. For the temporal model, we used two recurrent layers with hidden state dimension of 128, which experimentally proved to be optimal for our system. Our final architecture ASDNet is implemented in Py-Torch and all experiments are performed using a single NVIDIA Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset. The AVA-ActiveSpeaker dataset <ref type="bibr" target="#b42">[42]</ref>  tracks, and each face crop is annotated with speaking or notspeaking label. This results in 38.5 hours of face tracks with the corresponding audio signal. The number of speakers in the videos is time-varying, and a significant portion of face crops has resolution less than 100 pixels, making the dataset considerably challenging.</p><p>Evaluation Metric. We use the official ActivityNet evaluation tool that computes mean average precision (mAP). Unless stated otherwise, we use AVA-ActiveSpeaker validation set for our evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Audio-Visual Encoder Evaluation</head><p>In this section, we investigate the advantage of the proposed audio and video backbones, compared to backbones used in state-of-the-art active speaker detection systems. The encoder architecture is of utmost importance: the overall performance of the AV-ASD pipeline can only be as good as the extracted features. For these experiments, ISRM and temporal modeling are not used.</p><p>Which encoder architectures should be used? Following recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b54">54]</ref>, we take 2D-ResNet-18 architecture as the audio and video backbones of a baseline encoder. Inputs to the video backbone are stacked face crops, and inputs to the audio backbone are MFCCs, corresponding to a length of eight frames. This baseline achieves 79.0 mAP as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>To demonstrate the benefit of applying 3D convolution kernels, we keep the baseline audio backbone and replace 2D-ResNet-18 by 3D-ResNet-18. This change alone brings improvement of 4.9 mAP over the baseline. The improvement is achieved solely due to the ability of the 3D convolution kernels to capture motion patterns in the video data.</p><p>Similarly, to evaluate the benefit of SincDSNet as the proposed audio backbone, we keep the baseline video back- bone and replace the 'MFCC + 2D-ResNet-18' audio backbone by SincDSNet. This change brings improvement of 1.8 mAP over the baseline, thanks to the partially learnable feature extraction by SincDSNet, operating on the raw audio data. Importantly, SincDSNet has 75 times less parameters than 2D-ResNet-18 and requires less floating point operations (FLOPs), as shown in <ref type="table">Table 2</ref>.</p><p>Finally, our audio-visual encoder that uses both 3D-ResNet-18 and SincDSNet as backbones, achieves 7.1 mAP improvement over the baseline.</p><p>Can we use resource-efficient video encoders? One can attribute the performance boost achieved by 3D-ResNet-18 backbone to its increased number of parameters and FLOPs. Therefore, we have used several resource efficient 3D CNNs <ref type="bibr" target="#b24">[24]</ref> as video backbone. We report their performance at the bottom of <ref type="table">Table 3</ref>. Notably, all 3D CNN architectures achieve better performance than 2D-ResNet-18. For isntance, although 3D-MobileNetV2 1.0x contains much smaller number of parameters (approx. 7x less) and less FLOPs compared to 2D-ResNet-18, it achieves around 4 mAP better performance.</p><p>We have also experimented with deeper and computationally more expensive 3D-ResNeXt-101 architecture to check how much performance can be increased. 3D-ResNeXt-101 shows 0.6 mAP improvement over 3D-ResNet-18 when 8-frames input is used.</p><p>How does clip length affect performance? Although we used 8-frames clips to train our audio-visual backbones, longer clips would provide larger temporal context. In <ref type="table">Table 3</ref>, we compare clip lengths of 8-frames, 16-frames and 32-frames for the best performing 3D-ResNeXt-101 and 3D-ResNet-18 video backbones. To maintain similar complexity, we removed the initial temporal downsampling for 8-frames input, and inserted an additional temporal downsampling to the initial convolution layer for 32-frames input. Applying 16-frames clip length brings a performance improvement of 1.4 mAP and 2.2 mAP over 8-frames clip</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background features mAP</head><p>only reference frame 93.4 neighbouring window of 9 frames 93.5 <ref type="table">Table 6</ref>. Performance comparison when background speakers' features at different number of frames are leveraged. length for 3D-ResNet-18 and 3D-ResNeXt-101, respectively. Using 32-frames clip length does not show same performance improvement over using 16-frames. We suspect that inserting additional temporal downsampling hinders backbones ability to capture motion patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inter-Speaker Relation Modeling Evaluation</head><p>In this section, we investigate the performance of the proposed ISRM and compare it to an existing approach <ref type="bibr" target="#b0">[1]</ref> for context modelling. These experiments include the full ASDNet pipeline (encoder, ISRM, and a temporal model), where the temporal model, if present, is a Bidirectional-GRU with sequence length of 64.</p><p>How many background speakers to use for ISRM? We experimented with different number of background speakers for ISRM, and the results are reported in <ref type="table">Table 4</ref>. In general, increasing the number of background speakers features increases the performance. ISRM increases the performance by 0.8 mAP compared to the case where only reference speaker's features are used with temporal modeling (0 background speaker case). In the rest of our experiments, we use three background speakers in the ISRM module.</p><p>How does our ISRM compare to existing approaches? In <ref type="table">Table 5</ref>, we provide a comparison of our ISRM approach to the NonLocal [51] approach proposed in <ref type="bibr" target="#b0">[1]</ref>. NonLocal captures relationships between all the speakers within clip, whereas our ISRM approach captures relationships between speakers only within reference frame. When used alone, after the audio-visual backbones, neither NonLocal nor our ISRM approach bring significant performance improvement (NonLocal even degrades the performance). However, ISRM contributes additional 0.8 mAP compared to a system that uses only temporal modeling.</p><p>Can ISRM benefit from neighbouring frames? At ISRM, we do not have to use background speakers' features at only reference frame. Neighbouring frames relative to the reference frame can also provide useful information for ISRM. Therefore we have used background speakers' features at neighbouring window of 9 frames, which shows a modest 0.1 mAP improvement as reported in <ref type="table">Table 6</ref>. For the rest of the paper, we use 9 neighbouring frames at ISRM.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Temporal Modeling Evaluation</head><p>Which RNN architectures are most suitable? <ref type="table" target="#tab_3">Table 7</ref> shows the performance comparison of different RNN blocks used for temporal modeling. All one-directional methods takes 32-frames features as input and last output is used as input to final fc layer (reference frame is placed to the last of input sequence). For bidirectional methods, we have used 64-frames features as input and center output is used as input to final fc layer (reference frame is placed at the center of input sequence). Compared to their bidirectional versions, one-directional methods perform around 0.7 mAP worse. Out of all methods, bidirectional-GRU achieves the best performance.</p><p>What should be the length of the input sequence? We have experimented with different sequence lengths and reported results in <ref type="table" target="#tab_4">Table 8</ref>. In general, using larger sequence length does not hurt the final performance. However, after sequence length 64, the performance converges to 93.5 mAP.</p><p>(a) (b) <ref type="figure">Figure 6</ref>. The network predictions for speaking probabilities of each speaker (a) after only audio-visual encoding (b) after temporal modeling and ISRM are also applied. Ground truths of speaking and not-speaking classes are denoted with green and red rectangles, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Component-wise Analysis</head><p>How does each component contribute to the performance?</p><p>We investigated the contribution of each component to the final performance in <ref type="table" target="#tab_5">Table 9</ref>. We highlight several findings: (i) Without ISRM and temporal modeling, suitable backbones alone achieve 88.9 mAP, which is better than any other state-of-the-art approach; (ii) ISRM and temporal modeling improve the performance by 0.7 mAP and 3.7 mAP when they are applied alone, respectively, showing the importance of both stages in the pipeline; (iii) In rows 6 and 7 in <ref type="table" target="#tab_5">Table 9</ref>, we investigated the importance of ISRM stage by evaluating the performance without using reference speakers video features. Accordingly, even without looking reference speaker's face, information acquired from background speakers and audio enables our architecture to achieve around 68 mAP. This shows that ISRM is an indispensable part of our pipeline; (iv) When ISRM and temporal modeling are applied together, our architecture achieves the best performance with 93.5 mAP. The contribution of temporal modeling and ISRM stages is visually illustrated in <ref type="figure">Fig. 6</ref>. With only audio-visual encoding, each speaker is analyzed independently and predictions for speaking probabilities are made without contextual and long-term temporal information in <ref type="figure">Fig. 6 (a)</ref>. After applying temporal modeling and ISRM stages, the ASDNet predictions of speaking probabilities for notspeaking speakers drop and speaking speaker increases considerable as shown in <ref type="figure">Fig. 6 (b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mAP validation set</head><p>ASDNet (ours) 93.5 Causal ASDNet (ours) 90.6 MAAS-TAN <ref type="bibr" target="#b32">[32]</ref> 88.8 Chung et al. <ref type="bibr" target="#b5">[6]</ref> 87.8 ASC <ref type="bibr" target="#b0">[1]</ref> 87.1 Zhang et al. <ref type="bibr" target="#b54">[54]</ref> 84.0 Sharma et al. <ref type="bibr" target="#b44">[44]</ref> 82.0 Roth et al. <ref type="bibr" target="#b42">[42]</ref> 79.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>test set</head><p>ASDNet (ours) 91.7 Chung et al. <ref type="bibr" target="#b5">[6]</ref> 87.8 ASC <ref type="bibr" target="#b0">[1]</ref> 86.7 Zhang et al. <ref type="bibr" target="#b54">[54]</ref> 83.5 Roth et al. <ref type="bibr" target="#b42">[42]</ref> 82.1 <ref type="table" target="#tab_0">Table 11</ref>. Comparison with state-of-the-art methods on the AVA-ActiveSpeaker dataset. mAP results are calculated with the official evaluation tool as explained in <ref type="bibr" target="#b42">[42]</ref>.</p><p>How does the clip length affect performance? Increased encoder clip length (16-frames instead of 8-frames using 3D-ResNeXt-101 video backbone) improves the performance by 2.2 mAP if ISRM and temporal modeling are not applied. However, in the complete pipeline this improvement reflects to a marginal 0.1 mAP improvement in the final performance, which is shown in <ref type="table" target="#tab_0">Table 10</ref>. This shows that increased encoder clip length shifts the improvement that could have been provided by temporal modeling to the encoder. This might not be desirable if complexity is important at the design of the architecture since doubling encoder clip length means doubling the complexity.</p><p>Can ISRM be placed after temporal modeling? If necessary, the order of ISRM and temporal modeling can be changed, which results in only a 0.1 mAP performance degradation.</p><p>Can we make the full pipeline causal? The complete pipeline can be made causal by placing the reference frame to the last place of the input for encoder and temporal modeling stages; and by not using neighbouring frames background speakers' features at ISRM. So that, no future information is used for the active speaker detection of the current frame. Causal pipeline achieves 90.6 mAP, which is still better than any state-of-the-art approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the State-of-the-art</head><p>How does ASDNet compare to state-of-the-art methods?</p><p>We compare the performance of ASDNet with several stateof-the-art methods in <ref type="table" target="#tab_0">Table 11</ref>. For the final ASDNet, we used 16-frames clips at the audio-visual encoding stage, 3 background speakers with 9 neighbouring window at the ISRM stage, and bidirectional-GRU with 64-frames se- quence length at the temporal modeling stage. ASDNet outperforms the second best approach by 4.7 mAP on the validation set, and by 3.9 mAP on the test set of AVA-ActiveSpeaker dataset.</p><p>How does number of faces affect the performance? Increased number of faces makes the active speaker detection task more challenging and the performance of ISRM becomes more critical. ASDNet outperforms all other stateof-the-art methods for all different face numbers as shown in <ref type="table" target="#tab_0">Table 12</ref>. Superiority of ASDNet becomes more significant as number of faces increases.</p><p>How does face size affect the performance? Performance comparison for face size, which is set as small for [0, 64), medium for [64, 128), and large for [128, ?) pixels, is shown in <ref type="table" target="#tab_0">Table 13</ref>. ASDNet outperforms all other stateof-the-art methods for all different face sizes. Superiority of ASDNet becomes more significant for smaller faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we scrutinized the task of Audio-Visual Active Speaker Detection and proposed a three-stage architecture, called ASDNet. With the proposed audio-visual encoder and the inter-speaker relation modelling mechanism, ASDNet outperforms the previous state-of-the-art with significant 4.7 mAP and 3.9 mAP on the validation and test set of AVA-ActiveSpeaker dataset, respectively. To make the final design and hyperparameter choices for ASDNet, we followed insights from carefully designed experiments each targeted a specific aspect of the system. Each of these experiments was discussed in the paper. We believe that these insights can be useful for other complex audio-visual tasks as well that require context and temporal modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the three-stage pipeline in ASDNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Audio-visual encoder architecture. Visual input X t,k and audio input xt are fed to the respective backbones to produce features v t,k and at. A concatenated feature vector v t,k at is fed to a fully connected layer which produces a prediction if speaker k is speaking at time t. Prediction heads are removed after training and are not part of the global picture inFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Audio encoder utilizing Sinc Convolutions (SincConv) and Depthwise Separable Convolutions (DSConv). The convolution parameters, c, k, s corrspond to the number of output channels, kernel size, and stride, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Inter-speaker relation modeling architecture. For reference speaker k at time instant t, we extract background features b t,k by passing the concatenated features of background speakers through one layer MLP. Extracted features are then concatenated to reference speakers video features and audio features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of different audio and video backbones. Input length of 8-frames is used for all evaluations.</figDesc><table><row><cell cols="3">Audio Backbone Video Backbone mAP</cell></row><row><cell>2D-ResNet-18</cell><cell>2D-ResNet-18</cell><cell>79.0</cell></row><row><cell>2D-ResNet-18</cell><cell>3D-ResNet-18</cell><cell>83.9</cell></row><row><cell>SincDSNet</cell><cell>2D-ResNet-18</cell><cell>80.8</cell></row><row><cell>SincDSNet</cell><cell>3D-ResNet-18</cell><cell>86.1</cell></row><row><cell cols="3">Audio Backbone Params MFLOP</cell></row><row><cell>SincDSNet</cell><cell>0.15M</cell><cell>13.8</cell></row><row><cell>2D-ResNet-18</cell><cell>11.2M</cell><cell>19.2</cell></row></table><note>Table 2. Complexity comparison of different audio backbones.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>is the first audio-visual active speaker dataset collected in the wild. It contains 262 15-minute videos from Hollywood movies, recorded at 25-30 fps, 120 of which are used for training, 33 for validation, and 109 for testing. The videos consist of 3.65 million human-labeled frames, where face crops belonging to the same speaker are aggregated to create face</figDesc><table><row><cell></cell><cell>Video Backbone</cell><cell cols="3">Params GFLOP mAP</cell></row><row><cell>32-f</cell><cell>3D-ResNeXt-101 3D-ResNet-18</cell><cell>48.6M 33.2M</cell><cell>13.2 10.3</cell><cell>88.9 87.4</cell></row><row><cell>16-f</cell><cell>3D-ResNeXt-101 3D-ResNet-18</cell><cell>48.6M 33.2M</cell><cell>14.1 11.2</cell><cell>88.9 87.5</cell></row><row><cell></cell><cell>3D-ResNeXt-101</cell><cell>48.6M</cell><cell>13.2</cell><cell>86.7</cell></row><row><cell></cell><cell>3D-ResNet-18</cell><cell>33.2M</cell><cell>10.3</cell><cell>86.1</cell></row><row><cell></cell><cell>2D-ResNet-18</cell><cell>11.2M</cell><cell>0.9</cell><cell>80.8</cell></row><row><cell>8-f</cell><cell>3D-MobileNetV1 2.0x</cell><cell>13.9M</cell><cell>0.6</cell><cell>81.6</cell></row><row><cell></cell><cell>3D-MobileNetV2 1.0x</cell><cell>2.1M</cell><cell>0.7</cell><cell>85.1</cell></row><row><cell></cell><cell>3D-ShuffleNetV1 2.0x</cell><cell>4.6M</cell><cell>0.7</cell><cell>85.0</cell></row><row><cell></cell><cell>3D-ShuffleNetV2 2.0x</cell><cell>3.9M</cell><cell>0.6</cell><cell>84.2</cell></row><row><cell cols="5">Table 3. Comparison of video backbones for different clip lengths.</cell></row><row><cell cols="5">SincDSNet is used at the audio backbone, and face crop resolution</cell></row><row><cell cols="2">is 160 ? 160.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>mAP 92.6 93.1 93.4 93.4 93.4 93.3 Performance of inter-speaker relation modelling for different number of background speakers. Comparison of inter-speakers relation modeling methods.</figDesc><table><row><cell># Speakers</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Method</cell><cell></cell><cell cols="4">Temporal Model mAP</cell><cell></cell></row><row><cell cols="2">NonLocal [1]</cell><cell></cell><cell></cell><cell></cell><cell>87.2</cell><cell></cell></row><row><cell cols="2">NonLocal [1]</cell><cell></cell><cell>?</cell><cell></cell><cell>92.8</cell><cell></cell></row><row><cell cols="2">ISRM (ours)</cell><cell></cell><cell></cell><cell></cell><cell>89.0</cell><cell></cell></row><row><cell cols="2">ISRM (ours)</cell><cell></cell><cell>?</cell><cell></cell><cell>93.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Performance comparison of temporal modeling methods.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Sequence Length mAP</cell></row><row><cell cols="2">Bidirectional-GRU</cell><cell></cell><cell>64</cell><cell></cell><cell>93.5</cell></row><row><cell cols="2">Bidirectional-LSTM</cell><cell></cell><cell>64</cell><cell></cell><cell>93.4</cell></row><row><cell cols="2">Bidirectional-SRU</cell><cell></cell><cell>64</cell><cell></cell><cell>93.2</cell></row><row><cell>GRU</cell><cell></cell><cell></cell><cell>32</cell><cell></cell><cell>92.8</cell></row><row><cell>LSTM</cell><cell></cell><cell></cell><cell>32</cell><cell></cell><cell>92.7</cell></row><row><cell>SRU</cell><cell></cell><cell></cell><cell>32</cell><cell></cell><cell>92.7</cell></row><row><cell>Seq. Length</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell></row><row><cell cols="6">mAP 92.0 92.8 93.3 93.5 93.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>Performance comparison of using different sequence lengths at the training of Bidirectional-GRU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .</head><label>9</label><figDesc>Contribution of each component to the final performance.</figDesc><table><row><cell>#</cell><cell>Speaker Video Feat.</cell><cell>Audio Feat.</cell><cell>ISRM Feat.</cell><cell cols="2">Temporal Modeling</cell><cell>mAP</cell></row><row><cell>1</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78.8</cell></row><row><cell>2</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>49.3</cell></row><row><cell>3</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>88.9</cell></row><row><cell>4</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>92.6</cell></row><row><cell>5</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>89.6</cell></row><row><cell>6</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>64.5</cell></row><row><cell>7</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>67.8</cell></row><row><cell>8</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>93.5</cell></row><row><cell></cell><cell>Encoder Clip Length</cell><cell cols="3">ISRM and Temporal Modeling</cell><cell>mAP</cell></row><row><cell></cell><cell>8-frames</cell><cell></cell><cell>?</cell><cell></cell><cell>86.7</cell></row><row><cell></cell><cell>16-frames</cell><cell></cell><cell>?</cell><cell></cell><cell>88.9</cell></row><row><cell></cell><cell>8-frames</cell><cell></cell><cell>?</cell><cell></cell><cell>93.4</cell></row><row><cell></cell><cell>16-frames</cell><cell></cell><cell>?</cell><cell></cell><cell>93.5</cell></row><row><cell cols="7">Table 10. Effect of encoder clip length on the final performance.</cell></row><row><cell cols="7">SincDSNet and 3D-ResNeXt-101 are used for audio and video</cell></row><row><cell cols="3">backbones, respectively.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 12 .</head><label>12</label><figDesc>Performance comparison by number of visible faces on each frame.</figDesc><table><row><cell>Method</cell><cell cols="3">Number of Faces</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell cols="2">ASDNet (Ours) 95.7</cell><cell>92.4</cell><cell>83.7</cell></row><row><cell>MAAS [32]</cell><cell>93.3</cell><cell>85.8</cell><cell>68.2</cell></row><row><cell>ASC [1]</cell><cell>91.8</cell><cell>83.8</cell><cell>67.6</cell></row><row><cell>Baseline [42]</cell><cell>87.9</cell><cell>71.6</cell><cell>54.4</cell></row><row><cell>Method</cell><cell></cell><cell>Face Size</cell><cell></cell></row><row><cell></cell><cell>S</cell><cell>M</cell><cell>L</cell></row><row><cell cols="2">ASDNet (Ours) 74.3</cell><cell>89.8</cell><cell>96.3</cell></row><row><cell>MAAS [32]</cell><cell>55.2</cell><cell>79.4</cell><cell>93.0</cell></row><row><cell>ASC [1]</cell><cell>56.2</cell><cell>79.0</cell><cell>92.2</cell></row><row><cell>Baseline [42]</cell><cell>44.9</cell><cell>68.3</cell><cell>86.4</cell></row></table><note>Table 13. Performance comparison by face size.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active speakers in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Juan Le?n Alc?zar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="12465" to="12474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An end-to-end multimodal voice activity detection using wavenet encoder and residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ariav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="274" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Who&apos;s speaking? audio-supervised classification of active speakers in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayeh</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Van Hamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</title>
		<meeting>the 2015 ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="87" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Naver at activitynet challenge 2019-task b active speaker detection (ava)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Joon Son</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10555</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1086" to="1090" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jutta</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on statistical learning in computer vision, ECCV</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Look who&apos;s talking: Speaker detection using video and audio correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No. 00TH8532)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1589" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio-visual segmentation and &quot;the cocktail party effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end learning for music audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6964" to="6968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<publisher>Avinatan Hassidim, William T. Freeman,</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Listen to look: Action recognition by previewing audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10457" to="10467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Audio-visual speaker diarization based on spatiotemporal bayesian fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sileye</forename><surname>Israel D Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1086" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Comparative analysis of cnn-based spatiotemporal reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>K?p?kl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05165</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Resource efficient 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Kopuklu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neslihan</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Gunduz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Motion fused frames: Data level fusion strategy for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Kopuklu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neslihan</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2103" to="2111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lightweight end-to-end speech recognition from raw audio data using sinc-convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>K?rzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Lindae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palle</forename><surname>Klewitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Raw waveform-based audio classification using sample-level cnn architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongpil</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taejun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00866</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simple recurrent units for highly parallelizable recurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Maas: Multi-modal assignation for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Le?n-Alc?zar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">Caba</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03682</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multichannel speech enhancement by raw waveform-mapping using fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Le</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sze-Wei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You-Jin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1888" to="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The jester dataset: A large-scale video dataset of human gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multimodal gesture recognition based on the resc3d network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiguang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3047" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Small-footprint keyword spotting on raw audio data with sinc-convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mittermaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>K?rzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Waschneck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7454" to="7458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="502" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Speech2action: Cross-modal supervision for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10317" to="10326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with sincnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ava active speaker: An audio-visual dataset for active speaker detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liat</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharadh</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4492" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Crossmodal learning for audio-visual speech event localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Somandepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04358</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Waveu-net: A multi-scale neural network for end-to-end audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Society for Music Information Retrieval Conference</title>
		<meeting>the 19th International Society for Music Information Retrieval Conference<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-23" />
			<biblScope unit="page" from="334" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Voicefilter: Targeted voice separation by speaker-conditioned spectrogram masking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Muckenhirn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2728" to="2732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning filterbanks from raw speech for phone recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schaiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on acoustics, speech and signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5509" to="5513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Multi-task learning for audio-visual active speaker detection. 2, 3, 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
