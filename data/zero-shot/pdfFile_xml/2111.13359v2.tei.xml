<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Collaborative Graph Machines for Table Structure Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<email>billbliu@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqiang</forename><surname>Jiang</surname></persName>
							<email>dqiangjiang@tencent.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinsong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><forename type="middle">Ren</forename><surname>Tencent</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtu</forename><surname>Lab</surname></persName>
						</author>
						<title level="a" type="main">Neural Collaborative Graph Machines for Table Structure Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, table structure recognition has achieved impressive progress with the help of deep graph models. Most of them exploit single visual cues of tabular elements or simply combine visual cues with other modalities via early fusion to reason their graph relationships. However, neither early fusion nor individually reasoning in terms of multiple modalities can be appropriate for all varieties of table structures with great diversity. Instead, different modalities are expected to collaborate with each other in different patterns for different table cases. In the community, the importance of intra-inter modality interactions for table structure reasoning is still unexplored. In this paper, we define it as heterogeneous table structure recognition (Hetero-TSR) problem. With the aim of filling this gap, we present a novel Neural Collaborative Graph Machines (NCGM) equipped with stacked collaborative blocks, which alternatively extracts intra-modality context and models inter-modality interactions in a hierarchical way. It can represent the intrainter modality relationships of tabular elements more robustly, which significantly improves the recognition performance. We also show that the proposed NCGM can modulate collaborative pattern of different modalities conditioned on the context of intra-modality cues, which is vital for diversified table cases. Experimental results on benchmarks demonstrate our proposed NCGM achieves state-ofthe-art performance and beats other contemporary methods by a large margin especially under challenging scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Table structure recognition (TSR) aims to recognize the table internal structure to the machine readable data mainly presented in two formats: logical structure <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b57">50]</ref> and physical structure <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">24,</ref><ref type="bibr" target="#b32">26,</ref><ref type="bibr" target="#b37">31,</ref><ref type="bibr" target="#b40">34,</ref><ref type="bibr" target="#b41">35,</ref><ref type="bibr">38,</ref><ref type="bibr" target="#b46">39,</ref><ref type="bibr" target="#b51">44,</ref><ref type="bibr" target="#b56">49]</ref>. More concretely, logical structure only focuses on whether two table elements belong to the same row, column or cells (i.e., logical relationships), while the physical one contains * Equal contribution. ? Contact person.   The multiple modalities are modeled on their intra-modality relationships which are then fused for final results prediction. Due to lack of collaboration, for a distorted table case, previous methods cannot well extract the row relations (connected by blue lines) for an anchor element (yellow) with some true relation lost (green dotted line). (c) Our proposed NCGM. Different modalities are built into graphs with collaboration, which well accommodate the distorted table case. not only logical relationships but also physical coordinates of cell boxes. The recognized tabular structure is essential to many downstream applications <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref>. Although many previous algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">24,</ref><ref type="bibr" target="#b32">26,</ref><ref type="bibr" target="#b40">34,</ref><ref type="bibr" target="#b41">35,</ref><ref type="bibr">38,</ref><ref type="bibr" target="#b46">39,</ref><ref type="bibr" target="#b51">44,</ref><ref type="bibr" target="#b56">49,</ref><ref type="bibr" target="#b57">50]</ref> have achieved impressive progress in the community, TSR is still a challenging task due to two factors of complicated tables. The interior factor is complex table structure where spanning cell occupies at least two columns or rows, while exterior one is table distortion incurred by capture device.</p><p>Intuitively, table elements (text segment bounding boxes or table cells) commonly have inherent relationships and natural graph structure. Therefore, recent methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">34,</ref><ref type="bibr">38]</ref> attempt to attack the problem via constructing visual cues of table elements as graphs and applying the deep graph model, such as Graph Convolutional Networks (GCN) <ref type="bibr" target="#b18">[19]</ref> to reason their relationships. To introduce richer table information, several methods <ref type="bibr" target="#b30">[24,</ref><ref type="bibr" target="#b40">34,</ref><ref type="bibr">38]</ref> con-catenate the visual features with other modalities of features, such as geometry features, as a whole input to the graph model, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>  <ref type="bibr">(a)</ref>. Nevertheless, the relational inductive biases of different modalities would be highly discrepant, which makes naively early-fused modalities unable to deal with all table structures of great diversity. Besides, the intra-modality relationships would negatively affect each other when reasoning specific table structures. For example, the coordinates of table would dominate when recognizing a regular table, but they would become unreliable when processing distorted table cases. Instead, another alternative way is to individually model intra-modality relationships between table elements and combine them by a late-fusion strategy ( <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>). Unfortunately, the disentangled reasoning in terms of intra-modality interactions would introduce the curtailment of inter-modality interactions. This dilemma leads to the following question: can different modalities collaborate with each other rather than interfering under different table scenarios? We define this practical problem as heterogeneous table structure recognition (Hetero-TSR), which still lacks investigation.</p><p>In this work, we propose a novel Neural Collaborative Graph Machines (NCGM) tailored for this problem, as illustrated in <ref type="figure" target="#fig_1">Fig. 1 (c)</ref>. Concretely, we adopt text segment bounding boxes as table elements in our method and extract their multi-modality feature embeddings from appearance, geometry and content dimensionality separately. To obtain the corresponding graph context and explore their interactions, we go beyond the standard attention model and propose a basic collaborative block with two successive modules, i.e., Ego Context Extractor (ECE) and Cross Context Synthesizer (CCS). Among, ECE plays a role that dynamically generates graph context for the samples of each modality while the subsequent CCS is in charge of fusing and modulating inter-modality interactive information for different table cases. We stack this elemental block multiple times. Through this way, the intra-modality context generation and inter-modality collaboration can be conducted alternatively in a hierarchical way, which enables intra-inter modality interactions to be generated constantly from the low layer to the top one. In other words, the lowlevel contextual information in multiple modalities and the high-level one can collaborate with each other throughout the whole network, which is similar to the human perception process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">30]</ref>. The yielded collaborative graph embeddings enable our method to achieve better performance compared to other TSR methods, especially under more challenging scenarios, as clearly validated by extensive experimental results. To sum up, our contributions are in the four folds:</p><p>? We investigate the importance of collaboration between different modalities in TSR and propose the Hetero-TSR problem. To our best knowledge, we are the first to research the collaborative patterns between modality interaction for predicting table structure. ? We coin a novel NCGM tailored for Hetero-TSR problem, which consists of collaborative blocks alternatively conducting intra-modality context extraction and inter-modality collaboration in a hierarchical way. ? Experimental results on public benchmarks demonstrate that our method significantly outperforms the state-of-the-arts. ? We release a synthesizing method to augment existing benchmarks to more challenging ones. Under more challenging scenarios, our method can achieve at most 11% improvement than the second best method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Table Structure Recognition</head><p>Before the flourishing of deep learning, traditional table structure recognition methods rely on pre-defined rules and hand-crafted features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">45]</ref>. With the development of deep learning, table structure recognition methods have recently advanced substantially on performance, which can be classified into three categories: boundary extraction-based <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">26,</ref><ref type="bibr" target="#b37">31,</ref><ref type="bibr" target="#b46">39,</ref><ref type="bibr" target="#b51">44]</ref>, generative modelbased <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b57">50]</ref>, and graph-based <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">24,</ref><ref type="bibr" target="#b40">34,</ref><ref type="bibr">38]</ref> methods.</p><p>Boundary extraction-based methods. To extract cell boundaries, DeepDeSRT <ref type="bibr" target="#b46">[39]</ref> and TableNet <ref type="bibr" target="#b37">[31]</ref> are proposed by utilizing semantic segmentation. Besides, another technique <ref type="bibr" target="#b16">[17]</ref> exploits bi-directional GRUs to establish row and column boundaries in a context driven manner. However, these methods are struggled when identifying cells spanning multiple rows and columns. SPLERGE <ref type="bibr" target="#b51">[44]</ref> splits the table into grid elements in which adjacent ones are merged to restore spanning cells, whereas it still suffers from boundary ambiguity problem. To tackle this issue, the hierarchical GTE <ref type="bibr" target="#b56">[49]</ref> leverages clustering algorithm for cell structure recognition. Cycle-CenterNet <ref type="bibr" target="#b32">[26]</ref> exploits the cycle-pairing module to simultaneously detect and group tabular cells into structured tables, which focuses on the precision of cell boundary of the wired table in the wild. In the similar spirit, LGPMA <ref type="bibr" target="#b41">[35]</ref> applies soft pyramid mask learning mechanism on both the local and global feature maps. Nevertheless, the subsequently heuristic structure recovery pipeline cannot achieve decent performance in complex scenarios.</p><p>Generative model-based methods. The method <ref type="bibr" target="#b21">[22]</ref> utilizes the encoder-decoder framework, which generates an HTML tag sequence that represents the arrangement of rows and columns as well as the type of table cells. Moreover, another generative algorithm <ref type="bibr" target="#b57">[50]</ref>, termed EDD, consists of an encoder, a structure decoder and a cell decoder. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Row</head><p>Col.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table Image</head><p>Feature Extraction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collaborative Block 1</head><p>Collaborative Block 2 Collaborative Block L</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-modality Stream</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-modality Stream</head><p>Appearance <ref type="figure" target="#fig_9">Figure 2</ref>. The architecture of our proposed method. Best viewed in color.</p><formula xml:id="formula_0">Geometry Content F A F G F C C A (1) C G (1) C C (1) M A (1) M G (1) Init M C (1) M A (0) M G (0) M C (0) M A (2) M G (2) M C (2) C A (2) C G (2) C C (2) M A (l-1) M G (l-1) M C (l-1) C A (l-1) C G (l-1) C C (l-1) M A (l) M G (l) M C (l)</formula><p>The encoder captures visual features of input table images, while the structure decoder reconstructs table structure and helps the cell decoder to recognize cell content.</p><p>Graph-based methods. GraphTSR <ref type="bibr" target="#b1">[2]</ref> employs graph attention blocks to learn the vertex and edge representations in the latent space, and classifies edges as horizontal, vertical or unrelated. The method <ref type="bibr" target="#b40">[34]</ref> introduces DGCNN to predict the relationship between words represented by the appearance and geometry features. Also based on DGCNN, TabStruct-Net <ref type="bibr">[38]</ref> proposes an end-to-end network training cell detection and structure recognition networks in a joint manner. Besides, FLAG-Net <ref type="bibr" target="#b30">[24]</ref> leverages the modulatable dense and sparse context of table elements. However, the above graph-based works are mostly designed for the interaction between table elements but lack the cues of the collaborative pattern of different modalities. In contrast to these works, our proposed NCGM leverages modality interaction to boost the multimodal representation for complex scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transformer-based Multimodal Fusion</head><p>Transformer <ref type="bibr" target="#b53">[46]</ref> architecture not only achieves significant performance gains in NLP community <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">25,</ref><ref type="bibr" target="#b42">36,</ref><ref type="bibr" target="#b50">43]</ref>, but also gives birth to several pre-training methods [23, <ref type="bibr" target="#b33">27,</ref><ref type="bibr" target="#b55">48]</ref> fusing various modalities for multimodal tasks.</p><p>Multiple embeddings fusion. VL-BERT <ref type="bibr" target="#b49">[42]</ref> inheriting from BERT <ref type="bibr" target="#b5">[6]</ref> introduces additional visual feature embeddings for visual-linguistic representations. LayoutLM <ref type="bibr" target="#b55">[48]</ref> is a document understanding pre-trained model, which jointly models the interactions between text and layout information across scanned document images. However, the above algorithms simply take early-fused multiple embeddings as inputs, which may ignore the interactions between different modalities and result in discretization error and important details missing.</p><p>Co-attentional fusion. To better utilize visual-linguistic representations, ViLBERT <ref type="bibr" target="#b33">[27]</ref> processes both visual and textual inputs in separate streams that interact through coattentional transformer layers. Moreover, SelfDoc [23] establishes the contextualization over a block of content via cross-modal learning to manipulate visual features and textual features. Nevertheless, these previous co-attention based methods can only handle two modalities. By comparison, our proposed NCGM focuses on modality collaboration rather than simple fusion. Further, NCGM can not only process the interaction among more than two individual modalities, but also alternatively conduct intramodality context extraction and inter-modality collaboration, which exploits more useful information provided by different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>The overview of the proposed Neural Collaborative Graph Machines (NCGM) is shown in <ref type="figure" target="#fig_9">Fig. 2</ref>. It mainly consists of collaborative blocks, which have two successive Multi-head Attention-based <ref type="bibr" target="#b53">[46]</ref> modules, i.e., Ego Context Extractor (ECE) and the Cross Context Synthesizer (CCS). First, three modalities of feature embeddings (F ? ? F G , F A , F C ) in terms of table elements are extracted, i.e., geometry, appearance and content embeddings. In each collaborative block, the extracted feature embeddings are built as context graphs which are separately applied by the ECE to shape "intra-modality stream". Afterwards, the CCS selectively fuses individual contextual information from different modalities as inter-modality interactions maintained in "inter-modality stream". Note, we set M ? (0) = F ? as the initial input of CCS. The block is stacked L layers to implement the intra-inter modality collaboration in a hierarchical way. To predict the final table structure, the output collaborative graph embeddings from the l-th layer of inter-modality stream are sampled as pairs for cells, rows and columns classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extraction</head><p>In this component, a set of multi-modality features in terms of table elements are extracted from table image, including geometry embeddings F G ? R N ?d , appearance embeddings F A ? R N ?d and content embeddings F C ? R N ?d . N denotes the number of text segment bounding boxes. A more detailed description is given in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Collaborative Block</head><p>Ego Context Extractor. Now we elaborate on how to extract contextual interactions within each modality of table elements with the help of the Ego Context Extractor (ECE). Specifically, each extracted modality of features input to the ECE is constructed as individual directed graph G ? = {V, E} ? G G , G A , G C . In each decoupled modality of graph, corresponding embedding of each text segment bounding box is regarded as node X = {x 1 , x 2 , ..., x N } ? V which is connected to each other by edges E ? V ? V. In the similar spirit with works <ref type="bibr" target="#b40">[34,</ref><ref type="bibr">38]</ref>, we adopt the following asymmetric edge function h ? (x i , x j ) = x i (x i ? x j ) to combine graph edge features to each node, which can be denoted as H ? ? ? R (N ?(N ?1)/2)?d . In the constructed graphs, each node can be either an anchor or one of context of others. In previous works using DGCNN <ref type="bibr" target="#b40">[34,</ref><ref type="bibr">38]</ref>, only local context of each node is selected by k-Nearest Neighbors algorithm (KNN) to be aggregated into node feature. However, the local context is not versatile for representing relationships of all modalities. Besides, the DGCNN-based methods apply CNN to perform local context aggregation. For graph representation, CNN with strong inductive bias (e.g., local behavior) may not be the optimal choice. To tackle the above problems, our proposed ECE instead aggregates information of fully-connected graph for all three modalities via Multi-head Attention (MHA) <ref type="bibr" target="#b53">[46]</ref> module, which has been verified that it makes few assumptions about inputs and can learn to combine local behavior and global information based on input content <ref type="bibr" target="#b3">[4]</ref>. More concretely, l-th ECE takes intra-modality features C ? (l-1) as queries Q and the graph edge combined features H ? ? as keys K and values V as illustrated in <ref type="figure" target="#fig_2">Fig. 3(a)</ref>. Note, for the first layer, we input F ? as C ? (0) . However, the main limitation of using MHA is that the amount of input K and V can be very large ( N ? (N ? 1)/2 in our case), which is infeasible to be trained. Given Q ? R N ?dq , K ? R M ?d k , V ? R M ?dv and M = N ? (N ? 1)/2, the time complexity of the attention operation is O(N M ) and the output is in N ? d v dimensionalities, of which the number is only relevant to that of Q. Therefore, we can extend the MHA to a more memory-efficient Compressed MHA (CMHA) by introducing memory compression module which is utilized to reduce image pixel numbers in <ref type="bibr" target="#b54">[47]</ref>, as depicted in <ref type="figure" target="#fig_2">Fig. 3</ref> <ref type="bibr">(b)</ref>. In detail, the compression operation can be implemented as:</p><formula xml:id="formula_1">V K Q Add &amp; Norm FFN Add &amp; Norm Multi-head Attention X 1 Y (c) Cross Context Synthesizer X 2 Memory Compression U U U C~( l-1) Graph building (a) Ego Context Extractor M G (l) M C (l) M A (l) M G (l-1) M C (l-1) M A (l-1) C G (l) C A (l) CMHA CMHA CMHA CMHA (b) Compressed Multi-head Attention Q K,V Q K,V Q K,V Q K,V P " C~( l) C C (l) C A (l) C G (l) C C (l)</formula><formula xml:id="formula_2">M C(H) = N orm(Reshape(x, )W h ),<label>(1)</label></formula><p>where Reshape(H, ) denotes the operation of reshaping input x ? R M ?d to x ? R M ?d/ , and ? [0, 1] is the compression ratio. Through this way, the complexity can be quadratically reduced from O(N M ) to O(N M ). In default, we set = N/M , where N is the number of queries Q. And N orm(?) is the layer normalization. Additionally, we also equip the CMHA with residual connections in our method to make the query information flow unimpeded, which can be defined as:</p><formula xml:id="formula_3">Y = Add&amp;N orm(F F N ( P), P),<label>(2)</label></formula><formula xml:id="formula_4">P = Add&amp;N orm(Q, P), (3) P = M HA(Q, M C(K), M C(V)),<label>(4)</label></formula><p>where "F F N (?)" is the feed-forward layer and "Add&amp;N orm(?)" denotes element-wise addition and layer normalization, which is similar to the work <ref type="bibr" target="#b53">[46]</ref>. Conclusively, the contextual graph information is baked into graph node as C ? ? C G , C A , C C within each modality through the CMHA in our ECE module.</p><p>Cross Context Synthesizer. Once heterogeneous context graph embeddings are obtained, our goals are to fuse them together in a collaborative way and to learn the collaborative patterns between different modalities. Also based on the CMHA, we design the Cross Context Synthesizer (CCS), as is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>(c). In detail, the CCS has three parallel CMHA modules, and each of them takes one modality as queries while the other two are jointly regarded as keys and values. Take the first branch in <ref type="figure" target="#fig_2">Fig. 3</ref>(c) for example, the CMHA takes "content" modality of context graph embeddings as Q, and the respective outputs of ECE for "geometry" and "appearance" are input as K and V. In <ref type="figure" target="#fig_2">Fig. 3</ref>(c), " U " denotes the union of two modality sets. For the similar purpose in ECE process, we also follow the similar rule to compress the number of "memory" to N which equals to that of Q. Essentially, the query modality explores helpful information from another two modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Table Structure Prediction</head><p>At the l-th layer of collaborative block, the outputs of CCS are to further fused as collaborative graph embeddings, which are denoted as E = {e 1 , e 2 , ..., e N } ? R N ?de . Based on the embedings E, our method constructs the i-th and j-th samples as pairs and concatenate them along channel axis as vectors U = {u 1,1 , u 1,2 , ..., u i,j , ..., u N,N } ? R N 2 ?2de . Then three groups of FC layers are separately applied for predicting binary-class relations of U, i.e., whether the pair of i-th and j-th sample is belong to the same row, column or cell, as illustrated in <ref type="figure" target="#fig_9">Fig. 2</ref>. Each FC group consists of three FC layers with 256 dimensions and a 2dimension FC with softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training Strategy</head><p>We train our proposed NCGM in an end-to-end way. The whole loss function is defined as L = L cell + L col + L row , where L cell , L col or L row represents cell, column and row relationship losses. For each of them, we adopt the multitask loss L ? = ? 1 L class + ? 2 L con to satisfy both the contrastive objective and to predict belonging classes of the output embedding pairs. L con and L class are contrastive loss and binary classification loss functions respectively. A more detailed description is given in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Protocol</head><p>Datasets. We perform extensive experiments on various benchmark datasets. Among, ICDAR-2013 <ref type="bibr" target="#b8">[9]</ref>, ICDAR-2019 <ref type="bibr" target="#b6">[7]</ref>, WTW <ref type="bibr" target="#b32">[26]</ref>, UNLV <ref type="bibr" target="#b47">[40]</ref>, SciTSR <ref type="bibr" target="#b1">[2]</ref> and SciTSR-COMP <ref type="bibr" target="#b1">[2]</ref> are employed for physical structure recognition, while TableBank <ref type="bibr" target="#b21">[22]</ref> and PubTabNet <ref type="bibr" target="#b57">[50]</ref> are adopted for evaluating logical structure recognition performance. It should be noted that there is no training set in ICDAR-2013 and UNLV datasets, so we extend the two datasets to the partial versions, which is similar to TabStruct-Net <ref type="bibr">[38]</ref>. A more detailed description about public benchmarks is given in supplementary material.</p><p>To further investigate the capacity of our proposed method under more challenging scenarios, we expand "SciTSR-COMP" dataset to "SciTSR-COMP-A" by applying two kinds of distortion algorithms. A more detailed description is given in supplementary material.  <ref type="bibr" target="#b30">[24]</ref> and the OCR results of Tesseract <ref type="bibr" target="#b48">[41]</ref> as inputs in Setup-A. Evaluation protocol. We employ precision, recall and F1-score <ref type="bibr" target="#b7">[8]</ref> as protocol to evaluate the performance of our model for recognizing table physical structure including vertical and horizontal relations. For the recognition of table logical structure, BLEU score <ref type="bibr" target="#b38">[32]</ref> used in <ref type="bibr" target="#b21">[22]</ref> and Tree-Edit-Distance-based Similarity (TEDS) proposed in <ref type="bibr" target="#b57">[50]</ref> are exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The framework is built on Pytorch <ref type="bibr" target="#b39">[33]</ref>. We scale the input table images to a fixed size 512 ? 512 to introduce scale invariance. In default, the layer number of collaborative blocks is set to 3 and the hidden size d is set to 64. Further, we set h = 8, d m = 64, d k = d v = 8 for both Ego Context Extractor (ECE) and Cross Context Synthesizer (CCS) of each collaborative block. During training, the learning rate is initialized as 1e?4 and divided by 10 when the loss stops decreasing. For the training loss, we empirically set all weight parameters ? 1 = ? 2 = 1. For all experiments, the models are pre-trained on SciTSR for 10 epochs, and then fine-tuned on different benchmarks for 50 epochs, which is conducted on the platform with one Nvidia Tesla V100 GPU and 32 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-arts</head><p>Results of physical structure recognition. As is shown in Tab. 1, our NCGM outperforms most of previous methods on different datasets for physical structure recognition. Compared with the strong baseline FLAG-Net <ref type="bibr" target="#b30">[24]</ref>, NCGM increases average F1-score on all datasets by round 2% under both Setup-A settings and Setup-B settings. When processing table images with complex distortions ("SciTSR-COMP-A"), it is worth mentioning that our NCGM can achieve about 11% and 12% higher F1-scores under Setup-A and Setup-B than the second-best FLAG-Net <ref type="bibr" target="#b30">[24]</ref> without using distorted images as training data. If taking distorted data as training set, the performance of NCGM still can surpass it round 7% and 9% under both settings respectively. We also visualize row and column physical relationships of distorted table in <ref type="figure" target="#fig_4">Fig. 4</ref>. Note, the different color blocks in it merely visualize the belonging relationship rather than dividing the entire box. Taking right column of <ref type="figure" target="#fig_4">Fig. 4</ref> for example, "POS tagging information" is one whole text segment bounding box. In logical, one can observe that "POS tagging information" box spans across five columns of word bounding boxes below it in column dimension. Therefore, the five columns attribute their respective colors to the "POS tagging information" box. By comparison, our method correctly recognizes both relationships while the FLAG-Net performs unsatisfactorily under distorted table scenes.</p><p>Results of logical structure recognition. In order to evaluate our model on logical structure recognition task benchmarks, i.e., TableBank and PubTabNet, we perform lightweight post-processing (see supplementary material) on the NCGM's output results of row/column relationships to convert them to the HTML representation. Tab. 2 presents that our method achieves significant improvement compared with other methods for logical structure recognition task.</p><p>Computational complexity. A more detailed description is given in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ICDAR-2013-P</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Train  <ref type="table">Table 1</ref>. Comparison results of physical structure recognition on ICDAR-2013-P, ICDAR-2019, WTW, UNLV-P, SciTSR, SciTSR-COMP and SciTSR-COMP-A dataset. "-P" means partial dataset and "-A" represents augmented dataset by distortion. "P", "R" and "F1" stand for "Precision", "Recall" and "F1-score" respectively. "TabStr." and "C-CTRNet" denote "TabStruct-Net" and "Cycle-CenterNet" individually.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this subsection, we perform several analytic experiments under Setup-B settings on SciTSR-COMP benchmark to investigate the contributions of intra-modality and inter-modality interactions in our proposed NCGM.</p><p>Effect of intra-modality interactions. For intramodality interactions, Tab. 3 compares the effectiveness of various extractors, including DGCNN <ref type="bibr" target="#b40">[34]</ref> and Transformer <ref type="bibr" target="#b53">[46]</ref>, with ECE in our method. "Mixed" means all modality features are early-fused by concatenation as the input and "Individual" denotes each modality is input into context extractor separately. Tab. 3 shows ECE can achieve the best performance when taking either mixed features or individual features as input while "Transformer" performs the worst. For "DGCNN", it only aggregates information from top K similar nodes of each node instead of all ones. Compared with "DGCNN", although "Transformer" can deploy the global information of nodes, it ignores the directed edge effects between nodes. Encouragingly, our CMHA-based ECE can not only consider the directed relationships between nodes, but also extract the context information from all nodes. Additionally, we can also observe that individual features can yield better results than the mixed ones, which proves that decoupling the individual modality from each other is indeed a more preferable way to solve the Hetero-TSR problem.  <ref type="table">Table 3</ref>. Ablation studies of NCGM on SciTSR-COMP dataset. "Intra." and "Inter." stand for intra-modality interactions and intermodality interactions respectively. "Mix." and "Ind." are short for "Mixed" and "Individual". "DG." and "Tr." denote "DGCNN" and "Transformer". "Con." represents "Concatenation".</p><p>Effect of inter-modality interactions. We compare the proposed CCS with the "Concatenation" operation of multimodal features in Tab. 3. It can be observed that CCS improves the accuracy of predicting adjacency relationship compared with directly late-fused multiple model features via concatenation. This confirms the benefits of CCS that enables one modality to positively collaborate with the others, and can capture the complex implicit modality relationships. Moreover, it also proves that the CCS module combined with ECE can further boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Further Analysis on Collaborative Block</head><p>What does ECE learn from the intra-modality? As suggested by recent works <ref type="bibr" target="#b34">[28,</ref><ref type="bibr" target="#b35">29,</ref><ref type="bibr" target="#b43">37]</ref> on interpreting attention mechanism, separate attention heads may learn to look for various relationships between inputs and introducing more sparsity and diversity for attention may improve performance and interpretability. To explore the intra-modality interactions learned by ECE in collaborative block, we in <ref type="figure" target="#fig_6">Fig. 6</ref> visualize the multi-head attention maps from last blocks of ECE. For comparison, we also visualize the multi-head self-attention maps from the last blocks of "Transformer-Mixed" <ref type="bibr" target="#b53">[46]</ref> and KNN (K = 5) selection heatmaps of all layers in DGCNN <ref type="bibr" target="#b40">[34]</ref>, where a lighter color indicates a closer relationship. The KNN results of DGCNN show that the feature aggregation of one node only pays attention to the top K similar features of other nodes instead of all the nodes, and relies on the choice of K. The attention maps of Transformer-Mixed present equilibrium status, which lacks sparsity and diversity. Comparatively, our "ECE-Mixed" taking mixed features presents more diversified attention maps in eight heads, which indicates ECE can more effectively capture context information. Moreover, the attention maps generated by "ECE-Individual" show different intriguing focus patterns for different features. Specifically, ECE prefers to extract interactions for appearance and geometry features in global scope while content features bring more local focus patterns. How do different modalities collaborate with each other? To investigate the working pattern of CCS, we adopt Jensen-Shannon Divergence [5] (see supplementary materials) to measure the average diversity of attention map in CCS when the model also takes input table image shown in <ref type="figure" target="#fig_6">Fig. 6</ref>. As shown in <ref type="figure" target="#fig_5">Fig. 5</ref>, solid lines (? w/ CCS) represent the diversity distributions when one modality features are regarded as queries and others as keys/values. After removing CCS, diversity of attention weights in ECE for each modality is also presented by dashed lines (? w/o CCS). For those with CCS, the higher value indicates the query modality is in a closer collaboration with the other modalities. Particularly, appearance modality has the strongest collaborative relationship with others while geometric one requires the least collaboration. By comparison, the diversities of attention weights in ECE also follow a similar trend, but with lower values on average.   The more collaborative blocks, the better performance? To further explore the effect of the collaborative block number on the NCGM performance, we conduct a set of experiments setting block numbers from 1 to 9, respectively. It can be seen from <ref type="figure" target="#fig_8">Fig. 7</ref> that it is a trade-off problem. Small block number can render faster convergence to the model. As the number increases, the performance keeps improving until block number increases to 5, but the convergence speed of the network keeps slowing down. In particular, we observe that the F1-score decreases sharply when NCGM with more than 7 blocks is trained over round 50 epochs, which indicates more blocks are easier to cause model training collapse problem. Based on the above observation, we set it to 3 as default number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Limitation</head><p>We present a novel graph-based method for heterogeneous table structure recognition through learning intrainter modality collaboration. Extensive experiments on public benchmarks demonstrate its superiority over stateof-the-art methods, especially under challenge scenarios. There still exist two limitations that can be improved in future work. The first one is the inevitable problem of computational complexity increase when introducing multiple modalities and decoupled processing. The second one lies in the fact that NCGM with deeper blocks is easier to suffer from the training collapse problem. We may introduce more inductive bias into the attention model to tackle it.  <ref type="figure">(x, y)</ref> represents the center point of the box while height h and width w correspond to its short side and long side respectively. Then a d-dimension Fully-Connected (FC) layer is applied on the above vectors to obtain the geometry embeddings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><formula xml:id="formula_5">F G = {g 1 , g 2 , ..., g N } ? R N ?d .</formula><p>Appearance embedding. We employ ResNet18-based CNN <ref type="bibr" target="#b11">[12]</ref> as backbone to extract whole table image feature. In detail, the backbone consists of conv1 to conv2 2 of ResNet18 followed by three convolutional layers of size 3 ? 3 ? 64. Hereafter, the output of backbone is applied by the RoI Align <ref type="bibr" target="#b10">[11]</ref> in terms of text segment bounding boxes. After passing a FC layer with d dimensions, appearance embeddings</p><formula xml:id="formula_6">F A = {f 1 , f 2 , ..., f N } ? R N ?d are obtained.</formula><p>Content embedding. First, we embed corresponding text of each text segment bounding box in distributional space via word2vec <ref type="bibr" target="#b2">[3]</ref>. Then, one convolutional layer with 7 ? 1 ? d kernel size and 1 stride is applied to model text sequential feature as content feature embeddings F C = {t 1 , t 2 , ..., t N } ? R N ?d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Ablation Study of Mutil-modalities</head><p>As shown in Tab. 4, we observe that among the three modalities, "G" plays a dominant role, followed by "A", and finally "C". The proposed model leveraging all three modalities can achieve impressive progress under all evaluation metrics. In addition, we also explore the attention weights of individual modality. That is, the attention weights of "A" and "G" tend to be grid-like, indicating that the model focuses on the spatial position of the row or column in global range. And the attention weights of "C" are inclined to emphasize on local successive segment bounding boxes. To sum up, the inductive biases of different modalities are of large disparency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-head Attention</head><p>We build the core collaborative block of our method upon Multi-head Attention (MHA) <ref type="bibr" target="#b53">[46]</ref> module. Here, we briefly introduce it as preliminary knowledge. Given</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Modality</head><p>Setup  <ref type="table">Table 4</ref>. Ablation studies of multi-modalities on SciTSR-COMP dataset. "A", "G" and "C" stand for "appearance", "geometry" and "content" modality respectively. queries Q, keys K and values V, MHA is defined as:</p><formula xml:id="formula_7">M ultiHead(Q, K, V) = Concat(H 1 , H 2 , ..., H h )W * , H i = Attention(QW Q i , KW K i , VW V i ), i ? {1, 2, ..., h}, Attention(Q, K, V) = sof tmax( QK ? d k )V, where d k is the dimension of keys while h is the head num- ber. W Q i ? R dm?d k , W K i ? R dm?d k , W V i ? R dm?dv and W *</formula><p>i ? R hdv?dm are projection matrices separately. Essentially, the attention process can be regarded as "memory accessing" procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Design of Loss Function</head><p>The binary classification loss is widely applied in previous graph-based works of table structure recognition (TSR). Particularly, we train our proposed Neural Collaborative Graph Machines (NCGM) in an end-to-end way to satisfy both the contrastive objective and to predict belonging classes of the output embedding pairs. Given a pair of collaborative graph embeddings ({e <ref type="bibr">(a)</ref> , e (b) }) and corresponding concatenated vector u <ref type="bibr">(a,b)</ref> , we define the multi-task loss function as: , L class = ?log(P (z = c|u <ref type="bibr">(a,b)</ref> )),</p><formula xml:id="formula_8">L = L cell + L col + L row , L ? = ? 1 L class + ? 2 L con ,</formula><formula xml:id="formula_9">P (z = c|u (a,b) ) = exp(S c u (a,b) ) k exp(S k u (a,b) ) , c ? {0, 1},</formula><p>where L ? represents L cell , L col or L row , corresponding to cell, column and row relationship loss. L con is contrastive loss in which e + (b) and e ? (b) are the positive and negative pair of e <ref type="bibr">(a)</ref> respectively. The margin parameter ? is set to 1. Correspondingly, L class is the standard softmax loss in terms of u <ref type="bibr">(a,b)</ref> . z is the predicted class for the input pairs, and S is the weight matrix used in the softmax function, and S c and S k represent the c-th and k-th column of it, respectively. c = 1 denotes the concatenated pairs belong to the same cell/column/row, and otherwise c = 0. They are combined by weight parameters ? 1 and ? 2 . Considering memory efficiency, we also introduce Monte Carlo sampling for constructing collaborative graph embedding pairs in the training phase, which is similar to <ref type="bibr" target="#b40">[34]</ref>. For inference, the sampling is not performed and we construct all collaborative graph embeddings as pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Forward Process</head><p>For clarity, the detailed forward process of NCGM is shown in Alg. 1. Note, the symbol with superscript "?" denotes it is derived from "appearance", "geometry" or "content" modality. And the symbol with subscript "?" represents it belongs to one of "cell", "column" or "row" relationships. The sample size S of Monte Carlo sampling is set to 10 in the training phase.  <ref type="table">Table 5</ref>. Ablation studies of losses on SciTSR-COMP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Ablation Study of Loss</head><p>Lcon and L class are contrastive loss and binary classification loss respectively.</p><p>We also perform experiments to evaluate the effect of different loss functions. For the sake of fairness, all models with different loss settings are trained with the same backbone model and training data. As shown in Tab. 5, we observe that the model trained by binary classification loss L class outperforms the one trained by contrastive loss L con , while the combination of L class and L con can achieve better performance than either of the two. We attribute this to the extra regularization provided by contrastive loss, that makes the model pay more attention to hard negative pairs. As a consequence, our method can learn more discriminative representations of row, column or cell relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Post-processing</head><p>For a fair comparison with other methods, we perform post-processing on the results of our method. As opposed to pre-processing, post-processing aims to convert the adja- </p><formula xml:id="formula_10">V)) return Y /* Ego Context Extractor. */ Function ECE(C ? (l-1) ): Q ? C ? (l -1) K ? V ? H ? ? ? h ? (x i , x j ) C ? (l) ? CM HA(Q, K, V) return C ? (l) /* Cross Context Synthesizer. */ Function CCS(M C (l-1) , C A (l) , C G (l) ): Q ? M C (l-1) K ? V ? C A (l) U C G (l) M C (l) ? CM HA(Q, K, V) return M C (l)</formula><p>Function Main:</p><p>F ? ? Extract appearance, geometry and content features from T. cency matrix containing relationships to spanning information either in "XML" format for evaluating physical structure recognition or "HTML" format for evaluating logical structure recognition respectively, which is shown in <ref type="figure" target="#fig_13">Fig. 8</ref>.</p><formula xml:id="formula_11">/* Initialization. */ C ? (0) ? M ? (0) ? F ? /* Generate collaborative embeddings by NCGM. */ for l = 1, 2, 3 do C A (l) ? ECE(C A (l-1) ) C G (l) ? ECE(C G (l-1) ) C C (l) ? ECE(C C (l-1) ) M A (l) ? CCS(M A (l-1) , C G (l) , C C (l) ) M G (l) ? CCS(M G (l-1) , C A (l) , C C (l) ) M C (l) ? CCS(M C (l-1) , C A (l) , C G (l) ) E ? M A (3) + M G (3) + M C<label>(</label></formula><p>Post-process for physical structure recognition. We also take the row relationship for example. First of all, all boxes are sorted by their y coordinates of top left points to generate their indexes (represented in blue). For each box v i , the row belonging list is generated according to row adjacency matrix. Afterwards, the spanning information in "XML" format can be obtained. Here, we define the table box row index according to the boundaries of boxes, as illustrated by the red numbers in <ref type="figure" target="#fig_13">Fig. 8</ref>. In detail, boxes belonging to the same row belonging list are assigned with the same starting-row and ending-row indexes. Similarly, we can also obtain the spanning results from column adjacency matrix. Finally, an XML file is created with the extracted spanning information along with bounding box coordinates and contents.</p><p>Post-process for logical structure recognition. As for the datasets (i.e., TableBank <ref type="bibr" target="#b21">[22]</ref> and PubTabNet <ref type="bibr" target="#b57">[50]</ref>) in which GTs are in the form of HTML sequences, the evaluation protocol put more emphasis on correctly recognizing the logical structure of tables. We can also convert the adjacency matrix of relationship to HTML tag sequences according to the belonging list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Datasets for Experiments</head><p>We perform large-scale experiments on various benchmark datasets as summarized in Tab. 6. Among, ICDAR-2013 <ref type="bibr" target="#b8">[9]</ref>, ICDAR-2019 <ref type="bibr" target="#b6">[7]</ref>, UNLV <ref type="bibr" target="#b47">[40]</ref>, WTW <ref type="bibr" target="#b32">[26]</ref>, Sc-iTSR <ref type="bibr" target="#b1">[2]</ref> and SciTSR-COMP <ref type="bibr" target="#b1">[2]</ref> are employed for physical structure recognition, while TableBank <ref type="bibr" target="#b21">[22]</ref> and PubTab-Net <ref type="bibr" target="#b57">[50]</ref> are adopted for evaluating logical structure recognition performance.</p><p>In particular, it should be noted that there exists no training set in ICDAR-2013 <ref type="bibr" target="#b8">[9]</ref> and UNLV <ref type="bibr" target="#b47">[40]</ref> datasets, so we extend the two datasets to the partial versions (i.e., ICDAR-2013-P and UNLV-P). Concretely, we randomly split each dataset into five folds, of which four folds for training and the left one for testing. The random splits are performed ten rounds for computing averaged performance, which is similar to TabStruct-Net <ref type="bibr">[38]</ref>.</p><p>For more clarity, we also count the number of text segment bounding boxes and tables in every table image for different datasets in Tab. 6 ("-" means no training set provided).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Train</head><p>Test Image Content C-Box T-Box  <ref type="table">Table 6</ref>. Statistics of the datasets our experiments performed on. "Amt" and "Avg" denote "Amount" and "Average" separately. "-P" means partial dataset and "-A" represents augmented dataset by distortion. "IC13", "IC19", "Sci." and "Sci.-C" are short for "ICDAR-2013", "ICDAR-2019", "SciTSR" and "SciTSR-COMP" individually. "C-Box" and "T-Box" stand for "cell bounding boxes" and "text segment bounding boxes" respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Processing on Inconsistent Annotation Levels</head><p>Pre-process for bounding boxes. One major challenge of performing comparisons on different datasets lies in the inconsistency of annotation levels on the bounding boxes. As shown in Tab. 6, ICDAR-2019 <ref type="bibr" target="#b6">[7]</ref>, UNLV <ref type="bibr" target="#b47">[40]</ref> and WTW <ref type="bibr" target="#b32">[26]</ref> datasets have ground truth (GT) bounding boxes of cell, while ICDAR-2013 <ref type="bibr" target="#b8">[9]</ref> and SciTSR <ref type="bibr" target="#b1">[2]</ref> datasets take text segment bounding boxes as GT annotations. In our method, we regard text segment bounding boxes as table elements. Therefore, we do some processing to eliminate the inconsistency in annotation levels.</p><p>In detail, we convert the cell bounding boxes to the text segment ones according to OCR results in the training stage. For the text-segment-level datasets (i.e., ICDAR-2013 <ref type="bibr" target="#b8">[9]</ref> and SciTSR <ref type="bibr" target="#b1">[2]</ref>), we consider the original boxes and text contents as model input directly, which are extracted by parsing GT files. To unify the input format, for the cell-level datasets (i.e., ICDAR-2019 <ref type="bibr" target="#b6">[7]</ref>, UNLV <ref type="bibr" target="#b47">[40]</ref> and WTW <ref type="bibr" target="#b32">[26]</ref>), the text-segment-level boxes with contents are generated by the OCR results of Tesseract <ref type="bibr" target="#b48">[41]</ref>. Note that an original cell-level box may contain more than one text-segment-level boxes, which have the common row and column spanning information (i.e., starting-row, startingcolumn, ending-row and ending-column indexes) of the corresponding cell-level box. During the testing time, however, we still keep the original cell-level or text-segment-level boxes as GTs instead of the pre-processed ones in Setup-B, which ensures consistency while comparing our method against previously published ones. Especially, we take the result boxes of detection in FLAG-Net <ref type="bibr" target="#b30">[24]</ref> and the OCR results of Tesseract <ref type="bibr" target="#b48">[41]</ref> as inputs for fair comparison in Setup-A.   Pre-process for relationships. In order to provide the uniform GT of adjacency relationships (GT ? in Alg. 1) for the model's training phase, we convert the spanning information of table's rows and columns in various formats into the adjacency matrices of cell, row and column, which represent three adjacency relationships for the table elements. Take the row adjacency matrix for example, if the i-th and j-th boxes belong to the same row relationship, the value located at (i, j) in adjacency matrix is assigned to 1, otherwise to 0. In this way, we can construct the row adjacency matrix to represent the relationship of row. The adjacency matrices of cell and column are also generated in the similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Synthesizing Method</head><p>To further investigate the capacity of TSR methods under more challenging scenes, we augment existing datasets with the following two kinds of image distortion algorithms to simulate distractors brought by capture device, which are visualized in <ref type="figure">Fig. 9</ref>. Distortion 1. The first disortion is based on perspective transformation algorithm, which projects the table image to a new view plane according to the mapping matrix, as is shown in <ref type="figure">Fig. 9(b)</ref>.</p><formula xml:id="formula_12">Distortion 2.</formula><p>For the second kind of distortion, we employ a algorithm based on the quadratic B?zier curve <ref type="bibr" target="#b15">[16]</ref> to augment the datasets, which can be defined as:</p><formula xml:id="formula_13">B 2 (t) = (1 ? t) 2 P 0 + 2t(1 ? t)P 1 + t 2 P 2 , t ? [0, 1],</formula><p>where P 0 , P 1 and P 2 denote three control points of the B?zier curve. Concretely, for each row of the image, we generate a quadratic B?zier curve applied on it to implement pixellevel distortion. There are three main steps to determine the control points of quadratic B?zier curve. As shown in <ref type="figure" target="#fig_1">Fig. 10</ref>, we first randomly initialize the axis line l (the red line) and the offset b. Next, each row of the image is regarded as l 0 , and its starting point is deemed as the control point P 0 while ending point as P 2 . Besides, the control point P 1 is located at a position offset from M (the intersection point between l 0 and l) by b. Through this way, the quadratic B?zier curves are determined by the control points, which are applied on each row of image pixels to perform distortion. It is worth mentioning that the blank pixels generated in the distortion process are interpolated by neighbouring pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Computational Complexity</head><p>To further compare the computational complexity of existing various methods of table structure recognition, we summarize the model sizes and the inference operations of different models in Tab. 7. Since LGMPA <ref type="bibr" target="#b41">[35]</ref> and Cycle-CenterNet <ref type="bibr" target="#b32">[26]</ref> recover table structure based on heuristic rules after detecting cells, which is infeasible to perform the comparison between them and our method, we do not report them in Tab. 7. In particular, note that TabStruct-Net <ref type="bibr">[38]</ref> and FLAG-Net <ref type="bibr" target="#b30">[24]</ref> are only tested for structure recognition, so we do not count the parameters and operations of cell detection for a fair comparison.</p><p>Although the parameters and FLOPs of NCGM are larger than FLAG-Net <ref type="bibr" target="#b30">[24]</ref>, the performance of our method increases average F1-score by a large margin especially under challenging scenarios (e.g., WTW and SciTSR-COMP-A). The reasons for increasing computational complexity is probably because of the individual operations on multiple modalities in our method. Compared with TabStruct-Net [38], NCGM can achieve better performance with less parameters and similar computational budgets. Moreover, the model size and FLOPs of GraphTSR <ref type="bibr" target="#b1">[2]</ref> are the smallest among the compared methods, but it only utilizes the box coordinates as input to recognize table structure, which cannot achieve comparable performance than other methods. We consider to optimize the computational complexity and size of model without performance degradation in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Setup-B #Param FLOPs GraphTSR <ref type="bibr" target="#b1">[2]</ref> 7. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Jensen-Shannon Divergence</head><p>We in this work introduce the Jensen-Shannon Divergence [5] to measure the average diversity of attention maps in CCS, which is defined as:</p><formula xml:id="formula_14">JSD = H( 1 n n i=1 P i ) ? 1 n n i=1 H(P i ),</formula><p>where P i is the vector of attention weights assigned by one head to i-th node in the graph, and H is the Shannon entropy. The trends of attention diversity variance in different blocks for different modalities with and without CCS are all shown in <ref type="figure" target="#fig_1">Fig. 11</ref>. <ref type="figure" target="#fig_1">Fig. 12</ref> demonstrates more qualitative results of structure recognition on benchmark datasets. The figures show the generalization ability of our proposed NCGM which is able to correctly recognize various types of table structures. Especially for more challenging cases, <ref type="figure" target="#fig_1">Fig. 12</ref>(f)-(g) verify that our method can not only handle regular tables but also robustly recognize distorted ones, which is more applicable in realistic scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Qualitative Results</head><p>We also show the failure cases of our method in <ref type="figure" target="#fig_1">Fig. 13</ref>. As one can see, the table that impairs the performance of our algorithm is the nested table, which contains severe misalignment of row and column. To put it in another way, it is ambiguous to judge whether certain boxes belong to the same row or column. The ambiguity also incurs inadaptability of existing evaluation protocols in either logical or physical format requiring the rigid alignment of box boundary in row or column relationships. In the future work, we will investigate this problem and attempt to attack it by introducing more robust representation of the nested table structure, such as tree structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Broader Impact</head><p>Table elements have natural graph structure. Learning collaborative patterns from graph data of multiple modalities offers many potential applications and opportunities as graph data in multiple modalities naturally co-occur and have implicit relationships. Our model can be applied in many specific verticals ranging from financial area to medical area including large-scale heterogeneous table data, such as financial documents, medical examination reports and etc. And we focus on the impact our model might have on them. A model that is capable of dealing with largescale multi-modality data is extremely significant for table information registration and data analysis. With the development of smart phones, a large amount of table images are captured by mobile cameras in realistic application. Different from regular table images obtained by scanner or parsing PDF metadata, those captured by mobile device contain more distractors (e.g., distortion). Table structure recognition (TSR) algorithm plays as the front-end role that converts input table image to machine readable data, which is vital to the whole document processing system. However, most of existing TSR methods are merely designed for regular tables and cannot generate satisfactory results from table cases with more challenging distractors. Thanks to the more effective capture of inter-intra modality interaction, our model tailored for Hetero-TSR can yield more precise results, especially under more challenging scenarios, which is demonstrated by extensive experiments. In other words, our model can not only greatly save labor costs and improve document processing efficiency, but show more extensibility in application scenarios. Besides, we provide a successful attempt in the direction of investigating the collaborative patterns with and between modalities. We encourage researchers to build graph embedding models based on NCGM for other graph-based tasks we can expect to be particularly beneficial. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of motivation of the proposed NCGM. (a) Early fusion-based method. The multiple modalities of table elements are fused before modeling their relationships. (b) Late fusion-based method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The proposed Ego Context Extractor and Cross Context Synthesizer Modules in collaborative block. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Sample result of FLAG-Net on SciTSR-COMP-A dataset. (b) Sample result of NCGM on SciTSR-COMP-A dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of physical relationships of distorted table between FLAG-Net and NCGM. The first and second column indicate the predictions of rows and columns respectively. The boxes belonging to the same relationships are filled in the same colors. The boundaries of the text segment boxes with misrecognized relationships are marked in red lines. Our NCGM shows better tolerance for the challenging scenarios compared with FLAG-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Diversities of attention maps for different modalities with or without CCS. Solid lines (? w/ CCS) represent the diversity distributions of attention in CCS when one modality features are regarded as queries and others as keys/values. Dashed lines (? w/o CCS) present diversity of attention weights in ECE for each modality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of the heat-maps generated by DGCNN and multi-head attention maps from the Transformer and ECE. Y-axis (red) and X-axis (blue) are "probes" and "candidates" respectively. For ECE, probes are graph node features and candidates are edge combined features. For Transformer and DGCNN, probes and candidates are both non-graph features. The heat-maps of DGCNN show a local hard selection way in terms of context. And Transformer yields attention maps lacking sparsity and diversity. In contrast, ECE-Mixed presents more diversified attention maps and ECE-Individual extracts interactions in global or local pattern conditioned on different features. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>The relationship between block number of NCGM and F1-score on SciTSR-COMP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>L 2 +</head><label>2</label><figDesc>con = e (a) ? e + (b) 2 max 0, ? ? e (a) ? e ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Algorithm 1</head><label>1</label><figDesc>NCGM pseudo code. Input: T, GT ? ; // T denotes input table elements. GT ? (GT ? ? {GT cell , GT row , GT col }) represents the Ground Truth of different relationships. Output: F pred ? /* Extract features by Compressed Multi-head Attention. */ Function CMHA(Q, K, V): Y ? M HA(Q, M C(K), M C(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>3 )/*??</head><label>3</label><figDesc>Construct pairs. */ U ? P airing(E) if train then /* Monte Carlo sampling. S is the sample size. */ U S ; GT S ? ? Sampling([U; GT ? ] , S) /* Separately compute cell/col/row loss. */ L ? ? Loss(U S , GT S ? ) Backward. else /* Separately predict cell/col/row relationships. */ F pred Classif y ? (U) return</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 .</head><label>8</label><figDesc>Post-processing of our proposed NCGM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>2 Figure 9 .</head><label>29</label><figDesc>Images from SciTSR-COMP dataset applied by distortion algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 .</head><label>10</label><figDesc>Determination of control points in B?zier curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 .</head><label>11</label><figDesc>Diversities of attention maps for different modalities with or without CCS in different blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 .</head><label>13</label><figDesc>Failure cases of NCGM on table with more complex structure. image-based table detection and recognition. In Proceedings of The 12th Language Resources and Evaluation Con-ference, pages 1918-1925, 2020. 1, 2, 5, 6, 11 [23] Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Han-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table image</head><label>image</label><figDesc></figDesc><table><row><cell>Relation graph</cell></row><row><cell>with table elements</cell></row><row><cell>Predicted connections</cell></row><row><cell>Missing connections</cell></row><row><cell>Collaborative graph</cell></row><row><cell>Table image</cell></row><row><cell>with table elements</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Evaluation settings. Several existing works are only applicable to table images alone, while others utilize additional information including text segment/cell bounding boxes or text contents. To compare in a unified protocol, we follow two different experimental setups in [38]: (a) Setup-A where only table image is taken as input without additional information and (b) Setup-B where table image along with additional features such as cell/text segment bounding boxes and text contents. For a fair comparison, we also incorporate the result boxes of detection in FLAG-Net</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>UNLV-P 84.9 82.8 83.9 99.2 99.4 99.3 FLAG-Net [24] Sci. + UNLV-P 89.2 87.3 88.2 98.9 99.5 99.2 NCGM Sci. + UNLV-P 88.9 88.2 88.5 99.8 99.8 99.8</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>P</cell><cell cols="2">Setup-A R F1 P</cell><cell>Setup-B R F1</cell></row><row><cell cols="2">DGCNN [34] Sci. + IC13-P</cell><cell>-</cell><cell>-</cell><cell cols="2">-98.6 99.0 98.8</cell></row><row><cell>TabStr. [38]</cell><cell cols="5">Sci. + IC13-P 93.0 90.8 91.9 99.1 99.3 99.2</cell></row><row><cell>GTE [49]</cell><cell cols="4">Pub. + IC13-P 94.4 92.7 93.5 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">LGPMA [35] Sci. + IC13-P 96.7 99.1 97.9 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">C-CTRNet [26] WTW + IC19 95.5 88.3 91.7 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">FLAG-Net [24] Sci. + IC13-P 97.9 99.3 98.6 99.2 99.5 99.3</cell></row><row><cell>NCGM</cell><cell cols="5">Sci. + IC13-P 98.4 99.3 98.8 99.3 99.9 99.6</cell></row><row><cell></cell><cell cols="3">ICDAR-2019</cell><cell></cell></row><row><cell cols="2">DGCNN [34] Sci. + IC19</cell><cell cols="3">80.3 77.8 79.0 -</cell><cell>-</cell><cell>-</cell></row><row><cell>TabStr. [38]</cell><cell>Sci. + IC19</cell><cell cols="4">82.2 78.7 80.4 97.5 95.8 96.6</cell></row><row><cell cols="2">C-CTRNet [26] WTW</cell><cell>-</cell><cell cols="2">-80.8 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">FLAG-Net [24] Sci. + IC19</cell><cell cols="4">85.2 83.8 84.5 96.1 96.3 96.2</cell></row><row><cell>NCGM</cell><cell>Sci. + IC19</cell><cell cols="4">84.6 86.1 85.3 98.9 98.8 98.8</cell></row><row><cell></cell><cell cols="2">WTW</cell><cell></cell><cell></cell></row><row><cell cols="2">C-CTRNet [26] WTW</cell><cell cols="3">93.3 91.5 92.4 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">FLAG-Net [24] WTW</cell><cell cols="4">91.6 89.5 90.5 93.2 91.7 92.4</cell></row><row><cell>NCGM</cell><cell>WTW</cell><cell cols="4">93.7 94.6 94.1 95.8 96.4 96.1</cell></row><row><cell></cell><cell cols="3">UNLV-P</cell><cell></cell></row><row><cell cols="3">DGCNN [34] Sci. + UNLV-P -</cell><cell>-</cell><cell cols="2">-92.1 89.8 90.9</cell></row><row><cell>TabStr. [38]</cell><cell cols="2">Sci. + SciTSR</cell><cell></cell><cell></cell></row><row><cell cols="2">DGCNN [34] Sci.</cell><cell>-</cell><cell>-</cell><cell cols="2">-97.0 98.1 97.6</cell></row><row><cell>TabStr. [38]</cell><cell>Sci.</cell><cell cols="4">92.7 91.3 92.0 98.9 99.3 99.1</cell></row><row><cell cols="2">LGPMA [35] Sci.</cell><cell cols="3">98.2 99.3 98.8 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">FLAG-Net [24] Sci.</cell><cell cols="4">99.7 99.3 99.5 99.8 99.5 99.6</cell></row><row><cell>NCGM</cell><cell>Sci.</cell><cell cols="4">99.7 99.6 99.6 99.7 99.8 99.7</cell></row><row><cell></cell><cell cols="3">SciTSR-COMP</cell><cell></cell></row><row><cell cols="2">DGCNN [34] Sci.</cell><cell>-</cell><cell>-</cell><cell cols="2">-96.3 97.4 96.9</cell></row><row><cell>TabStr. [38]</cell><cell>Sci.</cell><cell cols="4">90.9 88.2 89.5 98.1 98.7 98.4</cell></row><row><cell cols="2">LGPMA [35] Sci.</cell><cell cols="3">97.3 98.7 98.0 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">FLAG-Net [24] Sci.</cell><cell cols="4">98.4 98.6 98.5 98.6 99.0 98.8</cell></row><row><cell>NCGM</cell><cell>Sci.</cell><cell cols="4">98.7 98.9 98.8 98.8 99.3 99.0</cell></row><row><cell></cell><cell cols="3">SciTSR-COMP-A</cell><cell></cell></row><row><cell cols="2">FLAG-Net [24] Sci.</cell><cell cols="4">70.7 66.2 68.4 83.3 81.0 82.1</cell></row><row><cell cols="6">FLAG-Net [24] Sci. + Sci.-C-A 82.5 83.0 82.7 88.8 87.5 88.1</cell></row><row><cell>NCGM</cell><cell>Sci.</cell><cell cols="4">79.6 78.9 79.2 93.3 94.8 94.0</cell></row><row><cell>NCGM</cell><cell cols="5">Sci. + Sci.-C-A 88.4 90.7 89.5 97.2 97.5 97.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>TableBank</cell><cell></cell></row><row><cell>Method</cell><cell>Train Dataset</cell><cell>Setup-A BLEU</cell></row><row><cell>Image-to-Text [22]</cell><cell>TableBank</cell><cell>73.8</cell></row><row><cell>TabStruct-Net [38]</cell><cell>SciTSR</cell><cell>91.6</cell></row><row><cell>FLAG-Net [24]</cell><cell>SciTSR</cell><cell>93.9</cell></row><row><cell>NCGM</cell><cell>SciTSR</cell><cell>94.6</cell></row><row><cell></cell><cell>PubTabNet</cell><cell></cell></row><row><cell>Method</cell><cell>Train Dataset</cell><cell>Setup-A TEDS</cell></row><row><cell>EDD [50]</cell><cell>PubTabNet</cell><cell>88.3</cell></row><row><cell>TabStruct-Net [38]</cell><cell>SciTSR</cell><cell>90.1</cell></row><row><cell>GTE [49]</cell><cell>PubTabNet</cell><cell>93.0</cell></row><row><cell>LGPMA [35]</cell><cell>PubTabNet</cell><cell>94.6</cell></row><row><cell>FLAG-Net [24]</cell><cell>SciTSR</cell><cell>95.1</cell></row><row><cell>NCGM</cell><cell>SciTSR</cell><cell>95.4</cell></row></table><note>. Comparison results of logical structure recognition on TableBank and PubTabNet datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>and H are the width and height of the table image.</figDesc><table><row><cell>A. Feature Extraction</cell><cell></cell></row><row><cell>A.1. Multi-modality Features</cell><cell></cell></row><row><cell cols="2">Geometry embedding. We derive the geometry feature</cell></row><row><cell>of each text segment bounding box as x W , y H , w W , h H where W</cell><cell>,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table (</head><label>(</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Box</cell><cell>Table</cell><cell>Box</cell></row><row><cell></cell><cell>Amt)</cell><cell>(Avg)</cell><cell>(Amt)</cell><cell>(Avg)</cell></row><row><cell>IC13</cell><cell>-</cell><cell>-</cell><cell>158</cell><cell>93</cell></row><row><cell>IC13-P</cell><cell>124</cell><cell>92</cell><cell>34</cell><cell>96</cell></row><row><cell>IC19</cell><cell cols="4">600 314 150 359</cell></row><row><cell>UNLV</cell><cell>-</cell><cell>-</cell><cell>558</cell><cell>77</cell></row><row><cell>UNLV-P</cell><cell>446</cell><cell>84</cell><cell>112</cell><cell>43</cell></row><row><cell>WTW</cell><cell cols="4">10970 101 3611 96</cell></row><row><cell>Sci.</cell><cell cols="4">12000 47 3000 48</cell></row><row><cell>Sci.-C</cell><cell cols="2">12000 47</cell><cell>716</cell><cell>74</cell></row><row><cell cols="5">Sci.-C-A 24000 47 1432 74</cell></row><row><cell cols="5">TableBank 145K 50 1000 49</cell></row><row><cell cols="5">PubTabNet 339K 72 114K 74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table image with table elements (word-level boxes and text contents).</head><label></label><figDesc></figDesc><table><row><cell>0 3</cell><cell>1 4</cell><cell>2</cell><cell>5</cell><cell>0 1 2 Row indexes</cell><cell>Parsing adjacency matrix</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[0, 1, 2]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>XML files for</cell><cell>[3, 4, 5]</cell><cell>HTML sequences for</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>physical structure</cell><cell>Row belonging list</cell><cell>logical structure</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">"Product" : starting-row:0, ending-row:1</cell><cell>&lt;tr&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">"Roast coffee" : starting-row:0, ending-row:1</cell><cell>&lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">"Instant Coffee" : starting-row:0, ending-row:1</cell><cell>&lt;td&gt;&lt;/td&gt; &lt;/tr&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">"Turnover in ?" : starting-row:1, ending-row:2</cell><cell>&lt;tr&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">"7,581" : starting-row:1, ending-row:2</cell><cell>&lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt;</cell></row><row><cell>Indexes of table elements</cell><cell></cell><cell></cell><cell></cell><cell cols="2">"2,517" : starting-row:1, ending-row:2</cell><cell>&lt;td&gt;&lt;/td&gt; &lt;/tr&gt;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 .</head><label>7</label><figDesc>Computational complexity comparison of different methods. #Param denotes the number of parameters (M), while FLOPs are the numbers of FLoating point OPerations (G). The number of input table's text segment bounding boxes is 42.</figDesc><table><row><cell></cell><cell>0e-4</cell><cell>1.8e-4</cell></row><row><cell>DGCNN [34]</cell><cell>0.8</cell><cell>4.1</cell></row><row><cell>TabStruct-Net [38]</cell><cell>4.7</cell><cell>11.9</cell></row><row><cell>FLAG-Net [24]</cell><cell>1.9</cell><cell>3.3</cell></row><row><cell>NCGM</cell><cell>3.1</cell><cell>12.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cognitive psychology and its implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John R Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Macmillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Da</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxuan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04729</idno>
		<title level="m">Complicated table structure recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<title level="m">Word2vec. Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Learning Representations-ICLR 2020, number CONF</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gon?alo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niculae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00015</idno>
		<title level="m">and Andr? FT Martins. Adaptively sparse transformers</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Icdar 2019 competition on table detection and recognition (ctdar)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangcai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>D?jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinqin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kleber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A methodology for evaluating algorithms for table understanding in pdf documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>G?bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermelinda</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Orsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM symposium on Document engineering</title>
		<meeting>the 2012 ACM symposium on Document engineering</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Icdar 2013 table competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>G?bel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamir</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermelinda</forename><surname>Oro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Orsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recognition of tables using table grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnamoorthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval</title>
		<meeting>the Fourth Annual Symposium on Document Analysis and Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="261" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A method for table structure analysis using dp matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Hirayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3rd International Conference on Document Analysis and Recognition</title>
		<meeting>3rd International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="583" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Table structure recognition based on textblock arrangement and ruled line position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Itonori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR&apos;93)</title>
		<meeting>2nd International Conference on Document Analysis and Recognition (ICDAR&apos;93)</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="765" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tables as semi-structured knowledge for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Quadratic bezier curves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">12</biblScope>
			<pubPlace>Davis</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Table structure extraction with bi-directional gated recurrent unit networks</title>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<editor>Saqib Ali Khan, Syed Muhammad Daniyal Khalid, Muhammad Ali Shahzad, and Faisal Shafait</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The t-recs table recognition and analysis system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kieninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Document Analysis Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tablebank</surname></persName>
		</author>
		<title level="m">Table benchmark for</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">Sample result of NCGM on ICDAR-2013 dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">Sample result of NCGM on ICDAR-2019 dataset. (c) Sample result of NCGM on UNLV dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">Sample result of NCGM on SciTSR dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<title level="m">Sample result of NCGM on SciTSR-COMP dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">Sample result of NCGM on SciTSR-COMP-A (Distortion 1) dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<title level="m">Sample result of NCGM on SciTSR-COMP-A (Distortion 2) dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Sample TSR output of NCGM on table images of various datasets. The first, second and last column indicate the predictions of cells, rows and columns respectively</title>
		<imprint/>
	</monogr>
	<note>Figure 12</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Selfdoc: Self-supervised document representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Dong Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfu</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5652" to="5660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Show, read and reason: Table structure recognition with flexible context aggregator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parsing table structures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujiao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongpan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparse and constrained attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Ft</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="370" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Extracting syntactic trees from transformer encoder self-attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mare?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Rosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="347" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruno A Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4700" to="4719" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tablenet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shubham Singh Paliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monika</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lovekesh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="128" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking table recognition using graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Shah Rukh Qasim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaisheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lgpma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06224</idno>
		<title level="m">Complicated table structure recognition with local and global pyramid mask alignment</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An analysis of encoder representations in transformer-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP. The Association for Computational Linguistics</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP. The Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Table structure recognition using top-down and bottom-up cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajoy</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deepdesrt: Deep learning for detection and structure recognition of tables in document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Agne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR international conference on document analysis and recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1162" to="1167" />
		</imprint>
	</monogr>
	<note>Andreas Dengel, and Sheraz Ahmed</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An open approach towards the benchmarking of table structure recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kieninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IAPR International Workshop on Document Analysis Systems</title>
		<meeting>the 9th IAPR International Workshop on Document Analysis Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An overview of the tesseract ocr engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth international conference on document analysis and recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>ICDAR 2007</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ernie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Enhanced representation through knowledge integration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep splitting and merging for table structure decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Extracting tabular information from text files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Tupaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwa</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Alam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>Medford, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>EECS Department, Tufts University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Layoutlm: Pre-training of text and layout for document image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1192" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy Xin Ru</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Image-based table recognition: data, model, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elaheh</forename><surname>Shafieibavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Jimeno</forename><surname>Yepes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10683</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
