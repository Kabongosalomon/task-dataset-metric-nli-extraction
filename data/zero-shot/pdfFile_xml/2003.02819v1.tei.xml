<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Does label smoothing mitigate label noise?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-06">March 6, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
							<email>mlukasik@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
							<email>bsrinadh@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
							<email>adityakmenon@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
							<email>sanjivk@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">New</forename><surname>York</surname></persName>
						</author>
						<title level="a" type="main">Does label smoothing mitigate label noise?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-06">March 6, 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label smoothing is commonly used in training deep learning models, wherein one-hot training labels are mixed with uniform label vectors. Empirically, smoothing has been shown to improve both predictive performance and model calibration. In this paper, we study whether label smoothing is also effective as a means of coping with label noise. While label smoothing apparently amplifies this problem -being equivalent to injecting symmetric noise to the labels -we show how it relates to a general family of loss-correction techniques from the label noise literature. Building on this connection, we show that label smoothing is competitive with loss-correction under label noise. Further, we show that when distilling models from noisy data, label smoothing of the teacher is beneficial; this is in contrast to recent findings for noise-free problems, and sheds further light on settings where label smoothing is beneficial.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Label smoothing is commonly used to improve the performance of deep learning models <ref type="bibr" target="#b24">[Szegedy et al., 2016</ref><ref type="bibr">, Chorowski and Jaitly, 2017</ref><ref type="bibr" target="#b28">, Vaswani et al., 2017</ref><ref type="bibr" target="#b36">, Zoph et al., 2018</ref><ref type="bibr" target="#b19">, Real et al., 2018</ref><ref type="bibr" target="#b10">, Huang et al., 2019</ref><ref type="bibr" target="#b12">, Li et al., 2020</ref>. Rather than standard training with one-hot training labels, label smoothing prescribes using smoothed labels by mixing in a uniform label vector. This procedure is generally understood as a means of regularisation <ref type="bibr" target="#b24">[Szegedy et al., 2016</ref><ref type="bibr" target="#b33">, Zhang et al., 2018</ref> that improves generalization and model calibration <ref type="bibr" target="#b18">[Pereyra et al., 2017</ref><ref type="bibr" target="#b14">, M?ller et al., 2019</ref>.</p><p>How does label smoothing affect the robustness of deep networks? Such robustness is desirable when learning from data subject to label noise <ref type="bibr" target="#b1">[Angluin and Laird, 1988]</ref>. Modern deep networks can perfectly fit such noisy labels <ref type="bibr" target="#b32">[Zhang et al., 2017]</ref>. Can label smoothing address this problem? Interestingly, there are two competing intuitions. On the one hand, smoothing might mitigate the problem, as it prevents overconfidence on any one example. On the other hand, smoothing might accentuate the problem, as it is equivalent to injecting uniform noise into all labels <ref type="bibr" target="#b30">[Xie et al., 2016]</ref>.</p><p>Which of these intuitions is borne out in practice? A systematic study of this question is, to our knowledge, lacking. Indeed, label smoothing is conspicuously absent in most treatments of the noisy label problem <ref type="bibr" target="#b16">[Patrini et al., 2016</ref><ref type="bibr" target="#b7">, Han et al., 2018b</ref><ref type="bibr">, Charoenphakdee et al., 2019</ref><ref type="bibr" target="#b25">, Thulasidasan et al., 2019</ref><ref type="bibr" target="#b0">, Amid et al., 2019</ref>. Intriguingly, however, a cursory inspection at popular loss correction techniques in this literature <ref type="bibr" target="#b15">[Natarajan et al., 2013</ref><ref type="bibr" target="#b17">, Patrini et al., 2017</ref><ref type="bibr" target="#b26">, van Rooyen and Williamson, 2018</ref>] reveals a strong similarity to label smoothing (see <ref type="bibr">?3)</ref>. But what is the precise relationship between these methods, and does it imply label smoothing is a viable denoising technique?</p><p>In this paper, we address these questions by first connecting label smoothing to existing label noise techniques. At first glance, this connection indicates that smoothing has an opposite effect to one such loss-correction technique. However, we empirically show that smoothing is competitive with such techniques in denoising, and that it improves performance of distillation <ref type="bibr" target="#b9">[Hinton et al., 2015]</ref> under label noise. We then explain its denoising ability by analysing smoothing as a regulariser. In sum, our contributions are: (i) we present a novel connection of label smoothing to loss correction techniques from the label noise literature <ref type="bibr" target="#b15">[Natarajan et al., 2013</ref><ref type="bibr" target="#b17">, Patrini et al., 2017</ref>. (ii) we empirically demonstrate that label smoothing significantly improves performance under label noise, which we explain by relating smoothing to 2 regularisation. (iii) we show that when distilling from noisy labels, smoothing the teacher improves the student;</p><p>this is in marked contrast to recent findings in noise-free settings.</p><p>Contributions (i) and (ii) establish that label smoothing can be beneficial under noise, and also highlight that a regularisation view can complement a loss view, the latter being more popular in the noise literature <ref type="bibr" target="#b17">[Patrini et al., 2017]</ref>. Contribution (iii) continues a line of exploration initiated in <ref type="bibr" target="#b14">M?ller et al. [2019]</ref> as to the relationship between teacher accuracy and student performance. While <ref type="bibr" target="#b14">M?ller et al. [2019]</ref> established that label smoothing can harm distillation, we show an opposite picture in noisy settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and notation</head><p>We present some background on (noisy) multiclass classification, label smoothing, and knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multiclass classification</head><p>In multiclass classification, we seek to classify instances X into one of L labels Y = [L] . = {1, 2, . . . , L}. More precisely, suppose instances and labels are drawn from a distribution P. Let : [L] ? R L ? R + be a loss function, where (y, f ) is the penalty for predicting scores f ? R L given true label y ? [L].</p><p>We seek a predictor f : X ? R L minimising the risk of f , i.e., its expected loss under P:</p><formula xml:id="formula_0">R(f ) . = E (x,y) [ (y, f (x))] = E x p * (x) T (f (x)) , where p * (x) . = P(y | x) y?[L]</formula><p>is the class-probability distribution, and (f ) . = (y, f ) y? <ref type="bibr">[L]</ref> . Canonically, is the softmax cross-entropy, (y, f )</p><formula xml:id="formula_1">. = ?f y + log y ?[L] e f y .</formula><p>Given a finite training sample S = {(x n , y n )} N n=1 ? P N , one can minimise the empirical risk</p><formula xml:id="formula_2">R(f ; S) . = 1 N N n=1</formula><p>(y n , f (x n )).</p><p>In label smoothing <ref type="bibr" target="#b24">[Szegedy et al., 2016]</ref>, one mixes the training labels with a uniform mixture over all possible labels: for ? ? [0, 1], this corresponds to minimisin?</p><formula xml:id="formula_3">R(f ; S) = 1 N N n=1? T n (f (x n )),<label>(1)</label></formula><formula xml:id="formula_4">where (? n ) i . = (1 ? ?) ? i = y + ? L .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning under label noise</head><p>The label noise problem is the setting where one observes samples from some distributionP with P(y | x) = P(y | x); i.e., the observed labels are not reflective of the ground truth <ref type="bibr" target="#b1">[Angluin and</ref><ref type="bibr">Laird, 1988, Scott et al., 2013]</ref>. Our goal is to nonetheless minimise the risk on the (unobserved) P. This poses a challenge to deep neural networks, which can fit completely arbitrary labels <ref type="bibr" target="#b32">[Zhang et al., 2017]</ref>.</p><p>A common means of coping with noise is to posit a noise model, and design robust procedures under this model. One simple model is class-conditional noise <ref type="bibr">[Blum and Mitchell, 1998</ref><ref type="bibr" target="#b22">, Scott et al., 2013</ref><ref type="bibr" target="#b15">, Natarajan et al., 2013</ref>, wherein there is a row-stochastic noise transition matrix T ? [0, 1] L?L such that for each (x, y) ? P, label y may be flipped to y with probability T y,y . Formally, if p * y (x)</p><p>. =P(y | x) and p * y (x) . = P(y | x) are the noisy and clean class-probabilities respectively, we havep</p><formula xml:id="formula_5">* (x) = T T p * (x).<label>(2)</label></formula><p>The symmetric noise model further assumes that there is a constant flip probability ? ? 0, 1 ? 1 L of changing the label uniformly to one of the other classes <ref type="bibr" target="#b13">[Long and</ref><ref type="bibr">Servedio, 2010, van Rooyen et al., 2015]</ref>, i.e., for ?</p><formula xml:id="formula_6">. = L L?1 ? ?, T = (1 ? ?) ? I + ? L ? J<label>(3)</label></formula><p>where I denotes the identity and J the all-ones matrix.</p><p>While there are several approaches to coping with noise, our interest will be in the family of loss correction techniques: assuming one has knowledge (or estimates) of the noise-transition matrix T, such techniques yield consistent risk minimisers with respect to P. <ref type="bibr" target="#b17">[Patrini et al., 2017]</ref> proposed two such techniques, termed backward and forward correction, which respectively involve the losses</p><formula xml:id="formula_7">? (f ) = T ?1 (f ) (4) ? (f ) = (Tf ).<label>(5)</label></formula><p>Observe that for a given label y, Backward correction was inspired by techniques in <ref type="bibr" target="#b15">Natarajan et al. [2013]</ref>, <ref type="bibr">Cid-Sueiro et al. [2014]</ref>, <ref type="bibr" target="#b26">van Rooyen and Williamson [2018]</ref>, and results in an unbiased estimate of the risk with respect to P. Recent works have studied robust estimation of the T matrix from noisy data alone <ref type="bibr" target="#b17">[Patrini et al., 2017</ref><ref type="bibr" target="#b7">, Han et al., 2018b</ref><ref type="bibr" target="#b29">, Xia et al., 2019</ref>. Forward correction was inspired by techniques in <ref type="bibr" target="#b20">Reed et al. [2014]</ref>, <ref type="bibr" target="#b23">Sukhbaatar et al. [2015]</ref>, and does not result in an unbiased risk estimate. However, it preserves the Bayes-optimal minimiser, and is empirically effective <ref type="bibr" target="#b17">[Patrini et al., 2017]</ref>.</p><formula xml:id="formula_8">? (y, f ) = y ?[L] T ?1 yy ? (y , f (x))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Knowledge distillation</head><p>Knowledge distillation Bucil? et al. <ref type="bibr">[2006]</ref>, <ref type="bibr" target="#b9">Hinton et al. [2015]</ref> refers to the following recipe: given a training sample S ? P N , one trains a teacher model using a loss function suitable for estimating class-probabilities, e.g., the softmax cross-entropy. This produces a class-probability estimator p t : X ? ? L , where ? denotes the simplex. One then uses {(x n , p t (x n ))} N n=1 to train a student model, e.g., using cross entropy <ref type="bibr" target="#b9">Hinton et al. [2015]</ref> or square loss <ref type="bibr" target="#b21">Sanh et al. [2019]</ref> as an objective.</p><p>The key advantage of distillation is that the resulting student has improved performance compared to simply training the student on labels in S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Label smoothing meets loss correction</head><p>We now relate label smoothing to loss correction techniques for label noise via a label smearing framework. <ref type="table">Table 1</ref>: Comparison of different label smearing methods. Here, I denotes the identity and J the all-ones matrix. For backward correction, the theoretical optimal choice of ? = L L?1 ? ?, where ? is the level of symmetric label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smearing matrix</head><formula xml:id="formula_9">Standard I Label smoothing (1 ? ?) ? I + ? L ? J Backward correction 1 1?? ? I ? ? (1??)?L ? J</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Label smearing for loss functions</head><p>Suppose we have some base loss of interest, e.g., the softmax cross-entropy. Recall that we summarise the loss via the vector (f ) . = (y, f ) y? <ref type="bibr">[L]</ref> . The loss on an example (x, y) is (y, f (x)) = e T y (f (x)) for one-hot vector e y .</p><p>Consider now the following generalisation, which we term label smearing: given a matrix M ? R L?L , we compute SM (f ) .</p><p>= M (f ).</p><p>On an example (x, y), the smeared loss is given by</p><formula xml:id="formula_10">e T y SM (f (x)) = M yy ? (y, f (x)) + y =y M yy ? (y , f (x)).</formula><p>Compared to the standard loss, we now potentially involve all possible labels, scaled appropriately by the matrix M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Special cases of label smearing</head><p>The label smearing framework captures many interesting approaches as special cases (see <ref type="table">Table 1</ref>):</p><p>? Standard training. Suppose that M = I, for identity matrix I. This trivially corresponds to standard training.</p><p>? Label smoothing. Suppose that M = (1 ? ?) ? I + ? L ? J, where J is the all-ones matrix, and ? ? [0, 1] is a tuning parameter. This corresponds to mixing the true label with a uniform distribution over all the classes, which is precisely label smoothing per (1).</p><p>? Backward correction. Suppose that M = T ?1 , where T is a class-conditional noise transition matrix. This corresponds to the backward correction procedure of <ref type="bibr" target="#b17">Patrini et al. [2017]</ref>. Here, the entries of M may be negative; indeed, for symmetric noise,</p><formula xml:id="formula_11">M = 1 1?? ? I ? ? L ? J where ? . = L L?1 ? ?.</formula><p>While this poses optimisation problems, recent works have studied means of correcting this <ref type="bibr" target="#b11">[Kiryo et al., 2017</ref><ref type="bibr" target="#b6">, Han et al., 2018a</ref>.</p><p>The above techniques have been developed with different motivations. By casting them in a common framework, we can elucidate some of their shared properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Statistical consistency of label smearing</head><p>Recall that our fundamental goal is to devise a procedure that can approximately minimise the population risk R(f ). Given this, it behooves us to understand the effect of label smearing on this risk. As we shall explicate, label smearing:</p><p>(i) is equivalent to fitting to a modified distribution. (ii) preserves classification consistency for suitable M.</p><formula xml:id="formula_12">?4 ?2 2 4 6 8 ?2 2 4 f (f ) ? = 0.0 ? = 0.1 ? = 0.2 (a) Label smoothing. ?4 ?2 2 4 6 8 10 ?2 2 4 f (f ) ? = 0.0 ? = 0.1 ? = 0.2 (b) Backward correction. ?4 ?2 2 4 6 8 10 ?2 2 4 f (f ) ? = 0.0 ? = 0.1 ? = 0.2 (c) Forward correction.</formula><p>For (i), observe that the smeared loss has corresponding risk</p><formula xml:id="formula_13">R sm (f ) = E x p * (x) T SM (f (x)) = E x p * (x) T M (f (x)) .</formula><p>Consequently, minimising a smeared loss is equivalent to minimising the original loss on a smeared distribution with class-probabilities p SM (x) = M T p * (x).</p><p>For example, under label smoothing, we fit to the class-probabilities M T p * (x) = (1 ? ?) ? p * (x) + ? L . This corresponds to a scaling and translation of the original. This trivially preserves the label with maximal probability, provided ? &lt; 1. Smoothing is thus consistent for classification, i.e., minimising its risk also minimises the classification risk <ref type="bibr" target="#b34">[Zhang, 2004a</ref><ref type="bibr">,b, Bartlett et al., 2006</ref>]. Now consider backward correction with M = T ?1 . Suppose this is applied to a distribution with class-conditional label noise governed by transition matrix T. Then, we will fit to probabilities M Tp * (x) = (T T ) ?1p * (x). By (2), these will exactly equal the clean probabilities p * (x); i.e., backward correction will effectively denoise the labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">How does label smoothing relate to loss correction?</head><p>Following <ref type="table">Table 1</ref>, one cannot help but notice a strong similarity between label smoothing and backward correction for symmetric noise. Both methods combine an identity matrix with an all-ones matrix; the striking difference, however, is that this combination is via addition in one, but subtraction in the other. This results in losses with very different forms:</p><formula xml:id="formula_14">LS (y, f ) ? (y, f ) + ? (1 ? ?) ? L ? y (y , f ) (6) ? (y, f ) ? (y, f ) ? ? L y (y , f ).</formula><p>Fundamentally, the effect of the two techniques is different: smoothing aims to minimise the average per-class loss 1 L y (y , f ), while backward correction seeks to maximise this. <ref type="figure" target="#fig_1">Figure 1</ref> visualises the effect on the losses when L = 2, and is the logistic loss. Intriguingly, the smoothed loss is seen to penalise confident predictions. On the other hand, backward correction allows one to compensate for overly confident negative predictions by allowing for a negative loss on positive samples that are correctly predicted.</p><p>Label smoothing also relates to forward correction: recall that here, we compute the loss ? (f ) = (Tf ). Compared to label smoothing, forward correction thus performs smoothing of the logits. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, the effect is that the loss becomes bounded for all predictions.</p><p>At this stage, we return to our original motivating question: can label smoothing mitigate label noise? The above would seem to indicate otherwise: backward correction guarantees an unbiased risk estimate, and yet we have seen smoothing constructs a fundamentally different loss. In the next section, we assess whether this is borne out empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Effect of label smoothing on label noise</head><p>We now present experimental observations of the effects of label smoothing under label noise. We then provide insights into why smoothing can successfully denoise labels, by viewing smoothing as a form of shrinkage regularisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Denoising effects of label smoothing</head><p>We begin by empirically answering the question: can label smoothing successfully mitigate label noise? To study this, we employ smoothing in settings where the training data is artificially injected with symmetric label noise. This follows the convention in the label noise literature <ref type="bibr" target="#b17">[Patrini et al., 2017</ref><ref type="bibr" target="#b6">, Han et al., 2018a</ref><ref type="bibr">, Charoenphakdee et al., 2019</ref>.</p><p>Specifically, we consider the CIFAR-10, CIFAR-100 and ImageNet datasets, and add symmetric label noise at level ? * = 20% to the training (but not the test) set; i.e., we replace the training label with a uniformly chosen label 20% of the time. On CIFAR-10 and CIFAR-100 we train two different models on this noisy data, ResNet-32 and ResNet-56, with similar hyperparameters as <ref type="bibr" target="#b14">M?ller et al. [2019]</ref>. Each experiment is repeated five times, and we report the mean and standard deviation of the clean test accuracy. On ImageNet we train ResNet-v2-50 with <ref type="bibr">LARS You et al. [2017]</ref>. We describe the detailed experimental setup in Appendix B.</p><p>As loss functions, our baseline is training with the softmax cross-entropy on the noisy labels. We then employ label smoothing (1) (LS) for various values of ?, as well as forward (FC) and backward (BC) correction (4), (5) assuming symmetric noise for various values of ?. We remark here that in the label noise literature, it is customary to estimate ?, with theoretical optimal value ? * = L L?1 ? ? * ; however, we shall here simply treat this as a tuning parameter akin to the smoothing ?, whose effect we shall study.</p><p>We now analyse the results along several dimensions.</p><p>Accuracy: In <ref type="figure" target="#fig_2">Figure 2</ref>, we plot the test accuracies of all methods on CIFAR-10 and CIFAR-100. Our first finding is that label smoothing significantly improves accuracy over the baseline. We observe similar denoising effects on ImageNet in <ref type="table" target="#tab_1">Table 2</ref>. This confirms that empirically, label smoothing is effective in dealing with label noise.</p><p>Our second finding is that, surprisingly, choosing ? ? * , the true noise rate, improves performance of all methods. This is in contrast to the theoretically optimal choice ? ? ? * for loss correction approaches <ref type="bibr" target="#b17">[Patrini et al., 2017]</ref>, and indicates it is valuable to treat ? as a tuning parameter.</p><p>Finally, we see that label smoothing is often competitive with loss correction. This is despite it minimising a fundamentally different loss to the unbiased backward correction, as discussed in ?3.4. We note however that loss correction generally produces the best overall accuracy with high ?.</p><p>Denoising Label smoothing (LS) significantly improves over baseline, and choosing ? ? * , the true noise rate, improves even further. Forward correction (FC) outperforms LS and also benefits from choosing large values for ?. Backward correction (BC) is worse than baseline for small ?, and better than baseline for large ?. In <ref type="table">Table 7</ref> in appendix, we report additional results for ResNet-56 and ResNet-32 from different label smearing methods, including where confusion matrix is estimated by pre-training a model as in <ref type="bibr" target="#b17">Patrini et al. [2017]</ref>.    To answer these questions, we separately inspect accuracies on the noisy and clean portions of the training data (i.e., on those samples whose labels are flipped, or not).   accuracy improves on both the noisy and clean parts of the data, with a more significant boost on the noisy part. Consequently, smoothing systematically improves predictions of both clean and noisy samples.</p><formula xml:id="formula_15">? = 0.0 ? = 0.1 ? = 0.2 ? = 0.4 ? = 0.6<label>LS</label></formula><p>Model confidence: Predictive accuracy is only concerned with a model ranking the true label ahead of the others. However, the confidence in model predictions is also of interest, particularly since a danger with label noise is being overly confident in predicting a noisy label. How do smoothing and correction methods affect this confidence under noise?</p><p>To measure this, in <ref type="figure" target="#fig_4">Figure 3</ref> we plot distributions of the differences between the logit activation p(y | x) for a true/noisy label y, and the average logit activation 1 L y ?[L]p (y | x). Compared to the baseline, label smoothing significantly reduces confidence in the noisy label (refer to the left side of <ref type="figure" target="#fig_4">Figure 3(b)</ref>).</p><p>To visualise this effect of smoothing, in <ref type="figure" target="#fig_6">Figure 4</ref> we plot pre-logits (penultimate layer output) of examples from 3 classes projected onto their class vectors as in <ref type="bibr" target="#b14">M?ller et al. [2019]</ref>, for a ResNet-32 trained on CIFAR-100. As we increase ?, the confidences for noisy labels shrink, showing the denoising effects of label smoothing.</p><p>On the other hand, both forward and backward correction systematically increase confidence in predictions. This is especially pronounced for forward correction, demonstrated by the large spike for high differences in <ref type="figure" target="#fig_4">Figure 3(b)</ref>. At the same time, these techniques increase the confidence in predictions of the true label (refer to <ref type="figure" target="#fig_4">Figure 3(a)</ref>): forward correction in particular becomes much more confident in the true label than any other technique.</p><p>In sum, <ref type="figure" target="#fig_4">Figure 3</ref> illustrates both positive and adverse effects on confidence from label smearing techniques: label smoothing becomes less confident in both the noisy and correct labels, while forward and backward correction become more confident in both the correct labels and noisy labels.</p><p>Model calibration: To further tease out the impact of label smearing on model confidences, we ask: how do these techniques affect the calibration of the output probabilities? This measures how meaningful the model probabilities are in a frequentist sense <ref type="bibr" target="#b3">[Dawid, 1982]</ref>.</p><p>In <ref type="table" target="#tab_4">Table 4</ref>, we report the expected calibration error (ECE) <ref type="bibr" target="#b5">[Guo et al., 2017]</ref> on the test set for   each method. While smoothing improves calibration over the baseline with ? = 0.1 -an effect noted also in <ref type="bibr" target="#b14">M?ller et al. [2019]</ref> -for larger ?, it becomes significantly worse. Furthermore, loss correction techniques significantly degrade calibration over smoothing. This is in keeping with the above findings as to these methods sharpening prediction confidences.</p><p>Summary: Overall, our results demonstrate that label smoothing is competitive with loss correction techniques in coping with label noise, and that it is particularly successful in denoising examples while preserving calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Label smoothing as regularisation</head><p>While empirically encouraging, the results in the previous section indicate a gap in our theoretical understanding: from ?3.4, the smoothing loss apparently has the opposite effect to backward correction, which is theoretically unbiased under noise. What, then, explains the success of smoothing?</p><p>To understand the denoising effects of label smoothing, we now study its role as a regulariser. To get some intuition, consider a linear model f (x) = Wx, trained on features X ? R N ?D and one-hot labels Y ? {0, 1} N ?L using the square loss, i.e., min W XW ? Y 2 2 . Label smoothing at level ? transforms the optimal solution W * t?</p><formula xml:id="formula_16">W * = (1 ? ?) ? W * + ? L ? (X T X) ?1 X T J.<label>(7)</label></formula><p>Observe that if our data is centered, the second term will be zero. Consequently, for such data, the effect of label smoothing is simply to shrink the weights. Thus, label smoothing can have a similar effect to shrinkage regularisation.</p><p>Our more general finding is the following. From <ref type="formula">(6)</ref>, label smoothing is equivalent to minimising a</p><formula xml:id="formula_17">regularised risk R sm (f ; D) ? R(f ; D) + ? ? ?(f ),</formula><p>where</p><formula xml:id="formula_18">?(f ) . = E x ? ? y ?[L] (y , f (x)) ? ? , and ? . = ?</formula><p>(1??)?L . The second term above does not depend on the underlying label distribution P(y | x). Consequently, it may be seen as a data-dependent regulariser on our predictor f . Concretely, for the softmax cross-entropy,</p><formula xml:id="formula_19">?(f ) = E x ? ? L ? log ? ? y e f y (x) ? ? ? y f y (x) ? ? .<label>(8)</label></formula><p>To understand the label smoothing regulariser (8) more closely, we study it for the special case of linear classifiers, i.e., f y (x) = W y , x . While we acknowledge that the label smoothing effects displayed in our experiments for deep networks are complex, as a first step, understanding these effects for simpler models will prove instructive.</p><p>Smoothing for linear models. For linear models f y (x) = W y , x , the label smoothing regularization for softmax cross-entropy (8) induces the following shrinkage effect.</p><p>Theorem 1. Let x be distributed as Q with a finite mean. Then W y = 0, ?y ? [L] is the minimiser of (8).</p><p>See Appendix A for the proof. We see that the label smoothing regulariser encourages shrinkage of our weights towards zero; this is akin to the observation for square loss in <ref type="formula" target="#formula_16">(7)</ref>, and similar in effect to 2 regularisation, which is also motivated as increasing the classification margin. This perspective gives one hint as to why smoothing may successfully denoise. For linear models, introducing asymmetric label noise can move the decision boundary closer to a class. Hence, a regulariser that increases margin, such as shrinkage, can help the model to be more robust to noisy labels. We illustrate this effect with the following experiment.</p><p>Effect of shrinkage on label noise. We consider a 2D problem comprising Gaussian classconditionals, centered at ?(1, 1) and with isotropic covariance at scale ? 2 = 0.01. The optimal linear separator is one that passes through the origin, shown in <ref type="figure" target="#fig_7">Figure 5</ref> as a black line. This separator is readily found by fitting logistic regression on this data.</p><p>We inject 5% asymmetric label noise into the negatives, so that some of these have their labels flipped to be positive. The effect of this noise is to move the logistic regression separator closer to the (true) negatives, indicating there is greater uncertainty in its predictions. However, if we apply label smoothing at various levels ?, the separator is seen to gradually converge back to the Bayes-optimal; this is in keeping with the shrinkage property of the regulariser (8).</p><p>Further, as suggested by Theorem 1, an explicit L 2 regulariser has a similar effect to smoothing <ref type="figure" target="#fig_7">(Figure 5(b)</ref>). Formally establishing the relationship between label smoothing and shrinkage is an interesting open question.</p><p>Summary. We have seen in ?3 that from a loss perspective, label smoothing results in a biased risk estimate; this is contrast to the unbiased backward correction procedure. In this section, we  <ref type="table">Table 5</ref>: Knowledge distillation experiments. We use label smoothing parameter ? = 0.1 and temperature parameter T = 2 during distillation, for all these experiments. We notice that doing LS on teacher improves the student accuracy compared to the baseline. LS on the student helps as well but not to the same accuracy. Loss correction using FC on teacher helps as well with the distillation.</p><p>provided an alternate regularisation perspective, which gives insight into why label smoothing can denoise training labels. Combining these two views theoretically, however, remains an interesting topic for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Distillation under label noise</head><p>We now study the effect of label smoothing on distillation, when our data is corrupted with label noise. In distillation, a trained "teacher" model's logits are used to augment (or replace) the one-hot labels used to train a "student" model <ref type="bibr" target="#b9">Hinton et al. [2015]</ref>. While traditionally motivated as a means for a simpler model (student) to mimic the performance of a complex model (teacher), <ref type="bibr" target="#b4">Furlanello et al. [2018]</ref> showed gains even for models of similar complexity. <ref type="bibr" target="#b14">M?ller et al. [2019]</ref> observed that for standard (noise-free) problems, label smoothing on the teacher improves the teacher's performance, but hurts the student's performance. Thus, a better teacher does not result in a better student. <ref type="bibr" target="#b14">M?ller et al. [2019]</ref> attribute this to the erasure of relative information between the teacher logits under smoothing.</p><p>But is a teacher trained with label smoothing on noisy data better for distillation? On the one hand, as we saw in previous section, label smoothing has a denoising effect on models trained on noisy data. On the other hand, label smoothing on clean data may cause some information erasure in logits <ref type="bibr" target="#b14">[M?ller et al., 2019]</ref>. Can the teacher transfer the denoising effects of label smoothing to a student?</p><p>We study this question empirically. On the CIFAR-100 and CIFAR-10 datasets, with the same architectures and noise injection procedure as the previous section, we train three teacher models on the noisy labels: one as-is on the noisy labels, one with label smoothing, and another with forward correction. We distill each teacher to a student model of the same complexity (see Appendix B for a complete description), and measure the student's performance. As a final approach, we distill a vanilla teacher, but apply label smoothing and forward correction on the student. <ref type="table">Table 5</ref> reports the performance of the distilled students using each of the above teachers. Our key finding is that on both datasets, both label smoothing and loss correction on the teacher significantly improves over vanilla distillation; this is in marked contrast to the findings of <ref type="bibr" target="#b14">M?ller et al. [2019]</ref>. On the other hand, smoothing or correcting on the student has mixed results; while there are benefits on CIFAR-10, the larger CIFAR-100 sees essentially no gains.</p><p>Finally, we plot the effect of the teacher's label smoothing parameter ? on student performance in <ref type="figure">Figure 6</ref>. Even for high values of ?, smoothing improves performance over the baseline (? = 0). Per the previous section, large values of ? allow for successful label denoising, and the results indicate the value of this transfer to the student.</p><p>In summary, our experiments show that under label noise, it is strongly beneficial to denoise the teacher -either through label smoothing or loss correction -prior to distillation.  <ref type="figure">Figure 6</ref>: Effect of label smoothing on the teacher on student's accuracy after distillation with temperature T = 1, CIFAR-100. Teacher and student both use ResNet-32. For all values of ?, label smoothing on the teacher improves distillation performance compared to a plain teacher (? = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We studied the effectiveness of label smoothing as a means of coping with label noise. Empirically, we showed that smoothing is competitive with existing loss correction techniques, and that it exhibits strong denoising effects. Theoretically, we related smoothing to one of these correction techniques, and re-interpreted it as a form of regularisation. Further, we showed that when distilling models from noisy data, label smoothing of the teacher is beneficial. Overall, our results shed further light on the potential benefits of label smoothing, and suggest formal exploration of its denoising properties as an interesting topic for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material for "Does label smoothing mitigate label noise?"</head><p>A Proof of Theorem 1</p><p>Note that for linear models, ?(f ) is a convex function of W y . Hence we can find the minimiser of ?(f ) by solving for when the gradient vanishes. We have,</p><formula xml:id="formula_20">??(f ) ?W i = E Q ? ? L ? e Wi,x y e W y ,x ? x ? x ? ? .</formula><p>We can swap differential and expectation in the above equation as ?(f ) is differentiable in both W i and x. Now we show that the gradient evaluates to zero at W i = 0, ?i:</p><formula xml:id="formula_21">??(f ) ?W i Wi=0 = E Q L ? 1 y 1 x ? x = E Q L ? 1 L x ? x = 0. B Experimental setup B.1 Architecture</formula><p>We use ResNet with batch norm <ref type="bibr" target="#b8">[He et al., 2016]</ref> for our experiments with the following configurations. For CIFAR-10 and CIFAR-100 we experiment with ResNet-32 and ResNet-56. We use ResNet-v2-50 for our experiments with ImageNet. We list the architecture configurations in terms of (n layer , n filter , stride) corresponding to each ResNet block in <ref type="table" target="#tab_6">Table 6</ref>.</p><p>Architecture Configuration: [(n layer , n filter , stride)]</p><p>ResNet-32 [(5, 16, 1), (5, 32, 2), (5, 64, 2)] ResNet-56 [(9, 16, 1), (9, 32, 2), (9, 64, 2)] ResNet-v2-50 [(3, 64, 1), (4, 128, 2), (6, 256, 2), (3, 512, 2)] </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Training</head><p>We follow the experimental setup from <ref type="bibr" target="#b14">M?ller et al. [2019]</ref>. For both CIFAR-10 and CIFAR-100 we use a mini-batch size of 128 and train for 64k steps. We use stochastic gradient descent with Nesterov momentum of 0.9. We use an initial learning rate of 0.1 with a schedule of dropping by a factor of 10 at 32k and 48k steps. We set weight decay to 1e-4. On ImageNet we train ResNet-v2-50 using the LARS optimizer <ref type="bibr" target="#b31">You et al. [2017]</ref> for large batch training, with a batch size of 3500, and training for 32768 steps. For data augmentation we used random crops and left-right flips 1 .</p><p>For our distillation experiments we train only with the cross-entropy objective against the teacher's logits. We use a temperature of 2 unless specified otherwise when describing an experiment.</p><p>We ran training on CIFAR-100 and CIFAR-10 using 4 chips of TPU v2 and ImageNet using 128 chips of TPU v3. Training for CIFAR-100 and CIFAR-10 took under 15 minutes, and for ImageNet around 1.5h.  <ref type="table">Table 7</ref>: Label smearing results under added label noise with probability of flipping each label ? = 20%. Label smoothing parameter ? = 0.1. For Patrini estimation of matrices for each label we use logits of an example falling into the 99.9th percentile according to activations for that label. <ref type="figure" target="#fig_10">Figure 7</ref> shows density plots of differences between maximum logit value (or corresponding to true/noisy label) and the average logit value across different portions of the training data. We notice that while label smoothing is reducing the confidence (by lowering the peak around 1.0), backward correction and forward correction methods increase the confidence by boosting the spike. This is the case for both the noisy and true labels, however the effect is much stronger on the correct label logit activation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Logit visualisation plots</head><p>In this section we present additional pre-logit visualization plots -for CIFAR-100 trained with ResNet-56 in <ref type="figure" target="#fig_11">Figure 8 (a-d)</ref>, for CIFAR-10 trained with ResNet-32 in <ref type="figure" target="#fig_11">Figure 8</ref>(e-g). <ref type="figure">Figure 9</ref> visualises the pre-logits for backward and forward correction on CIFAR-100 trained with ResNet-32. As before, we see that both methods are able to denoise the noisy instances. Smoothing is seen to have a denoising effect: the noisy instances' pre-logits become more uniform, and so the model learns to not be overly confident in their label.</p><p>(a) Forward correction ? = 0.1.</p><p>(b) Forward correction ? = 0.7.</p><p>(c) Backward correction ? = 0.1.</p><p>(d) Backward correction ? = 0.7. <ref type="figure">Figure 9</ref>: Visualisation of logits for backward and forward correction of a ResNet-32 for three classes on CIFAR-100, using the procedure of <ref type="bibr" target="#b14">M?ller et al. [2019]</ref>. The red, blue and green colours denote instances from three different classes, and the black coloured points have label noise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>computes a weighted sum of losses for all labels y ? [L], while ? (y, f ) = y, y ?[L] T :y ? f y (x) computes a weighted sum of predictions for all y ? [L].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Effect of label smoothing, backward correction, and forward correction on the logistic loss. The standard logistic loss vanishes for large positive predictions, and is linear for large negative predictions. Smoothing introduces a finite positive minima. Backward correction makes the loss negative for large positive predictions. Forward correction makes the loss saturate for large negative predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>:Figure 2 :</head><label>2</label><figDesc>What explains the effectiveness of label smoothing for training with label noise? Does it correct the predictions on noisy examples, or does it only further improve the predictions on the clean (non-noisy) examples? Effect of ? on smoothing and forward label correction test accuracies on CIFAR-100 and CIFAR-10 from ResNet-32. Standard deviations are denoted by the shaded regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>between noisy (observed) label logit and mean logit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Density of differences between logit corresponding to the true (left plot; corresponding to the "true" label, before injecting label noise) and noisy label (right plot; corresponding to the "noisy" label, after injecting label noise) and the average over all logits on the mis-labeled portion of the train data. Results are with ? = 0.2 on CIFAR-100, and the ResNet-32 model. LS reduces confidence mostly on the noisy label, whereas FC and BC increase confidence mostly on the true label. SeeFigure 7for plots on full and clean data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Label smoothing ? = 0.(b) Label smoothing ? = 0.2.(c) Label smoothing ? = 0.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Effect of label smoothing on pre-logits (penultimate layer output) under label noise. Here, we visualise the pre-logits of a ResNet-32 for three classes on CIFAR-100, using the procedure of<ref type="bibr" target="#b14">M?ller et al. [2019]</ref>. The black markers denote instances which have been labeled incorrectly due to noise. Smoothing is seen to have a denoising effect: the noisy instances' pre-logits become more uniform, and so the model learns to not be overly confident in their label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>(a) Effect of label smoothing on logistic regression separator, on a synthetic problem with asymmetric label noise. The black line is the Bayes-optimal separator, found by logistic regression on the clean data. The other lines are separators learned by applying label smoothing with various ? on the noisy data. Without smoothing, noise draws the separator towards the affected class; smoothing undoes this effect, and brings the separator back to the Bayes-optimal. (b) Shrinkage ( 2 ) regularisation has a similar effect on the separator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>label, non-noisy part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Density plots showing distribution differences between maximum logit (or corresponding to true label) and the average over all logits on different portions of train data and from different label smearing methods. Results using ? = 0.2, dataset CIFAR-100, and the ResNet-32 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Effect of label smoothing on pre-logits (penultimate layer output) under label noise. Here, we visualise the pre-logits of a ResNet-56 for three classes on CIFAR-100 (in the top figures), a ResNet-32 for three classes on CIFAR-10 (in the bottom figures), using the procedure of<ref type="bibr" target="#b14">M?ller et al. [2019]</ref>. The black markers denote instances which have been labeled incorrectly due to noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Test accuracy on ImageNet trained with ? = 20% label noise on ResNet-v2-50, with label smoothing (LS) and forward correction (FC) for varying ?. Both LS and FC successfully denoise, and thus improve over the baseline (? = 0).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">? Full train Clean part of train</cell><cell cols="2">Noisy part of train</cell></row><row><cell></cell><cell>true labels</cell><cell>true labels</cell><cell cols="2">true labels noisy labels</cell></row><row><cell>0.0</cell><cell>77.39</cell><cell>86.75</cell><cell>39.92</cell><cell>17.88</cell></row><row><cell>0.1</cell><cell>80.11</cell><cell>87.99</cell><cell>48.58</cell><cell>12.27</cell></row><row><cell>0.2</cell><cell>81.22</cell><cell>88.27</cell><cell>53.01</cell><cell>8.32</cell></row></table><note>reports this breakdown from the ResNet-32 model on CIFAR-100, for different values of ?. We see that as ? increases,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on different portions of the training set from ResNet-32, trained with different label smoothing values ? on CIFAR-100. As ? increases, accuracy improves on both clean and noisy part of data. Interestingly, the improvement on the noisy part of data is greater than the reduction in fit to the noisy labels (compare the two rightmost columns in the table). Thus, there are noisy examples assigned neither to correct class nor to the observed noisy class without LS, and which LS helps classify correctly.</figDesc><table><row><cell>?</cell><cell>LS</cell><cell>FC</cell><cell>BC</cell></row><row><cell cols="4">0.0 0.111 0.111 0.111</cell></row><row><cell cols="4">0.1 0.108 0.153 0.214</cell></row><row><cell cols="4">0.2 0.156 0.165 0.266</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Expected calibration error (ECE) computed on 100 bins on test set for ResNet-32 on CIFAR-100, trained with different label smearing techniques under varying values of ?. Generally, label smearing is detrimental to calibration.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>DatasetArchitecture Vanilla distillation LS on teacher LS on student FC on teacher FC on student</figDesc><table><row><cell cols="2">CIFAR-100 ResNet-32</cell><cell>63.98?0.26</cell><cell>64.48?0.25</cell><cell>63.83?0.28</cell><cell>66.65?0.18</cell><cell>63.94?0.34</cell></row><row><cell cols="2">CIFAR-100 ResNet-56</cell><cell>64.31?0.26</cell><cell>65.63?0.24</cell><cell>64.50?0.32</cell><cell>66.35?0.20</cell><cell>64.24?0.26</cell></row><row><cell>CIFAR-10</cell><cell>ResNet-32</cell><cell>80.44?0.64</cell><cell>86.95?1.82</cell><cell>85.72?2.61</cell><cell>86.81?1.86</cell><cell>86.92?2.11</cell></row><row><cell>CIFAR-10</cell><cell>ResNet-56</cell><cell>77.98?0.25</cell><cell>87.10?1.66</cell><cell>86.98?1.71</cell><cell>86.88?1.80</cell><cell>86.82?1.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>ResNet Architecture configurations used in our experiments<ref type="bibr" target="#b8">[He et al., 2016]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>C Experiments: additional results C.1 Comparison of smoothing against label noise baselines</figDesc><table><row><cell>Dataset</cell><cell cols="2">Architecture Baseline</cell><cell>LS</cell><cell cols="3">FC smoothing BC smoothing FC Patrini BC Patrini</cell></row><row><cell cols="2">CIFAR100 RESNET-32</cell><cell cols="3">57.06?0.38 60.70?0.28 61.29?0.38</cell><cell>53.91?0.40</cell><cell>57.25?0.24</cell><cell>55.89?0.33</cell></row><row><cell cols="2">CIFAR100 RESNET-56</cell><cell cols="3">54.93?0.37 59.04?0.53 60.00?0.31</cell><cell>52.25?0.51</cell><cell>55.09?0.39</cell><cell>55.00?0.13</cell></row><row><cell>CIFAR10</cell><cell>RESNET-32</cell><cell cols="3">80.44?0.63 83.95?0.18 80.78?0.42</cell><cell>77.23?0.72</cell><cell>80.33?0.29</cell><cell>80.65?0.59</cell></row><row><cell>CIFAR10</cell><cell>RESNET-56</cell><cell cols="3">77.98?0.24 80.98?0.48 79.66?0.26</cell><cell>77.32?0.35</cell><cell>77.97?0.45</cell><cell>77.66?0.44</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/image_utils. py</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Robust bi-tempered logistic loss based on Bregman divergences. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning from noisy examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Angluin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="343" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The well-calibrated Bayesian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">379</biblScope>
			<biblScope unit="page" from="605" to="610" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laurent Itti, and Anima Anandkumar. Born-again neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">Chase</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="page" from="1602" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pumpout: A meta approach for robustly training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno>abs/1809.11008</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor Wai-Hung</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d?lch? Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Positive-unlabeled learning with non-negative risk estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Kiryo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marthinus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1674" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Regularization via structural label smoothing. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Dasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visar</forename><surname>Berisha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random classification noise defeats all convex potential boosters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rocco</forename><forename type="middle">A</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Servedio</surname></persName>
		</author>
		<idno>0885-6125</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="287" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="4696" to="4705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Loss factorization, weakly supervised learning and label noise robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Carioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="708" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: a loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2233" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Regularized Evolution for Image Classifier Architecture Search. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Classification with asymmetric label noise: consistency and maximal denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Handy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="489" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combating label noise in deep learning using abstention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohd-Yusof</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A theory of learning with corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Brendan Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">228</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: the importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Brendan Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6835" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disturblabel: Regularizing CNN on the loss layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="4753" to="4762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Statistical behavior and consistency of classification methods based on convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="85" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Statistical analysis of some multi-category large margin classification methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1225" to="1251" />
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
