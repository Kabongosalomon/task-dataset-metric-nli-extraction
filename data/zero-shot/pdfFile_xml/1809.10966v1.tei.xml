<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Generalization with Domain-Specific Aggregation Modules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>D&amp;apos;innocente</surname></persName>
							<email>dinnocente@diag.unroma1.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Italian Institute of Technology Milan</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>barbara.caputo@iit.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Italian Institute of Technology Milan</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Generalization with Domain-Specific Aggregation Modules</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual recognition systems are meant to work in the real world. For this to happen, they must work robustly in any visual domain, and not only on the data used during training. Within this context, a very realistic scenario deals with domain generalization, i.e. the ability to build visual recognition algorithms able to work robustly in several visual domains, without having access to any information about target data statistic. This paper contributes to this research thread, proposing a deep architecture that maintains separated the information about the available source domains data while at the same time leveraging over generic perceptual information. We achieve this by introducing domainspecific aggregation modules that through an aggregation layer strategy are able to merge generic and specific information in an effective manner. Experiments on two different benchmark databases show the power of our approach, reaching the new state of the art in domain generalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As artificial intelligence, fueled by machine and deep learning, is entering more and more into our everyday lives, there is a growing need for visual recognition algorithms able to leave the controlled lab settings and work robustly in the wild. This problem has long been investigated in the community under the name of Domain Adaptation (DA): considering the underlying statistics generating the data used during training (source domain), and those expected at test time (target domain), DA assumes that the robustness issues are due to a covariate shift among the source and target distributions, and it attempts to align such distributions so to increase the recognition performances on the target domain. Since its definition <ref type="bibr" target="#b18">[19]</ref>, the vast majority of works has focused on the scenario where one single source is available at training time, and one specific target source is taken into consideration at test time, with or without any labeled data (for an overview of previous work we refer to section 2). Although useful, this setup is somewhat limited: given the large abundance of visual data produced daily worldwide and uploaded on the Web, it is very reasonable to assume that several source domains might be available at training time. Moreover, the assumption to have access to data representative of the underlying statistic of the target domain, regardless of annotation, is not always realistic. Rather than equipping a seeing machine with a DA algorithm able to solve the domain gap for a specific single target, one would hope to have methods able to solve the problem for any target domain. This last scenario, much closer to realistic settings, goes under the name of Domain Generalization (DG, <ref type="bibr" target="#b9">[10]</ref>), and is the focus of our work.</p><p>Current approaches to DG tend to follow two alternative routes: the first tries to use all source data together in order to learn a joint, general representation for the categories of interest strong enough to work on any target domain <ref type="bibr" target="#b10">[11]</ref>. The second instead opts for keeping separated the information coming from each source domain, trying to estimate at test time the similarity between the target domain represented by the incoming data and the known sources, and use only the classifier branch trained on that specific source for classification <ref type="bibr" target="#b13">[14]</ref>. Our approach sits across these two philosophies, attempting to get the best of both worlds. Starting from a generic convnet, pre-trained on a general knowledge database like ImageNet <ref type="bibr" target="#b16">[17]</ref>, we build a new multi-branch architecture with as many branches as the source domains available at training time. Each branch leverages over the general knowledge contained into the pre-trained convnet through a deep layer aggregation strategy inspired by <ref type="bibr" target="#b26">[27]</ref>, that we call Domain-Specific Aggregation Modules (D-SAM). The resulting architecture is trained so that all three branches contribute to the classification stage through an aggregation strategy. The resulting convnet can be used in an end-to-end fashion, or its learned representations can be used as features in a linear SVM. We tested both options on two different architectures and two different domain generalization databases, benchmarking against all recent approaches to the problem. Results show that our D-SAM architecture, in all cases, consistently achieve the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Most of work in DA has focused on single source scenarios, with two main research threads. The first deals with features, aiming to learn deep domain representations that are invariant to the domain shift, although discriminative enough to perform well on the target <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Other methods rely on adversarial loss functions <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Also two-step networks have been shown to have practical advantages <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1]</ref>. The second thread focuses on images. The adversarial approach used successfully for feature-based methods, has also been applied directly to the reduction of the visual domain gap. Various GAN-based strategies <ref type="bibr" target="#b5">[6]</ref> have been proposed for generating new images and/or perturb existing ones to mimic the visual style of a domain and reducing the discrepancy at the pixel level <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Recently, some authors addressed the multi-source domain adaptation problem with deep networks. The approach proposed in <ref type="bibr" target="#b25">[26]</ref> builds over <ref type="bibr" target="#b4">[5]</ref> by replicating the adversarial domain discriminator branch for each available source. Moreover these discriminators are also used to get a per-plexity score that indicates how the multiple sources should be combined at test time as in <ref type="bibr" target="#b15">[16]</ref>. A similar multi-way adversarial strategy is used also in <ref type="bibr" target="#b27">[28]</ref>, but this work comes with a theoretical support that frees it from the need of respecting a specific optimal source combination and thus from the need of learning the source weights.</p><p>In the DG setting, access to the target data is not allowed, thus the main objective is to look across multiple sources for shared factors in the hypothesis that they will hold also for any new target domain. Deep DG methods are presented in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> and <ref type="bibr" target="#b9">[10]</ref>. The first works propose a weighting procedure on the source models, while the second aims at separating the source knowledge into domain-specific and domain-agnostic sub-models. A meta-learning approach was recently presented in <ref type="bibr" target="#b10">[11]</ref>: it starts by creating virtual testing domains within each source mini-batch and then it trains a network to minimize the classification loss, while also ensuring that the taken direction leads to an improvement on the virtual testing loss.</p><p>Over the last years, it has emerged a growing interest on studying modules and connectivity patterns, and on how to assemble them systematically. Some studies showed how skipping connections can be beneficial for classification and regression. In particular, <ref type="bibr" target="#b7">[8]</ref> showed how skipping connections concatenating all the layers in stages is effective for semantic fusion, while <ref type="bibr" target="#b11">[12]</ref> exploited conceptually similar ideas for spatial fusion. An unifying framework for these approaches, on which to some extent we build, has been recently proposed in <ref type="bibr" target="#b26">[27]</ref>. There the authors proposed two general structures for deep layer aggregation, one iterative and one hierarchical, that capture the nuances of previous works while being applicable in principle to any convnet.</p><p>We leverage on this work, proposing a variant of iterative deep aggregation leading to a multi branch architecture, able to conjugate the need for general representations while retaining the strength of keeping information from different sources separated in the domain generalization setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Domain Specific Aggregation Modules</head><p>In this section we describe our aggregation strategy for DG. We will assume to have S source domains and T target domains, denoting with N i the cardinality of the i th source domain, for which we have {x i j , y i j } Ni j=1 labeled samples. Source and target domains share the same classification task; however, unlike DA, the target distribution is unknown and the algorithm is expected to generalize to new domains without ever having access to target data, and hence without any possibility to estimate the underlying statistic for the target domain.</p><p>The most basic approach, Deep All, consists of ignoring the domain membership of the images available from all training sources, and training a generic algorithm on the combined source samples. Despite its simplicity, Deep All outperforms many engineered methods in domain generalization, as shown in <ref type="bibr" target="#b9">[10]</ref>. The domain specific aggregation modules we propose can be seen as a way to augment the generalization abilities of given CNN architectures by maintaining a generic core, while at the same time explicitly modeling the single domain specific features separately, in a whole coherent structure.</p><p>Our architecture consists of a main branch ? and a collection of domain specific aggregation modules ? = {? 1 ...? n }, each specialized on a single source domain. The main branch ? is the backbone of our model, and it can be in principle any pre-trained off-the shelf convnet. Aggregation modules, which we design inspired by an iterative aggregation protocol described in <ref type="bibr" target="#b26">[27]</ref>, receive inputs from ? and learn to combine features at different levels to produce classification outputs. At training time, each domain-specific aggregation module learns to specialize on a single source domain. In the validation phase, we use a variation of a leave-one-domain-out strategy: we average predictions of each module but, for each i th source domain, we exclude the corresponding domainspecific module ? i from the evaluation. We test the model in both an end-to-end fashion and by running a linear classifier on the extracted features. In the rest of the section we describe into detail the various components of our approach (section 3.1-3.2) and the training protocol (section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Aggregation Module</head><p>Deep Layer Aggregation <ref type="bibr" target="#b26">[27]</ref> is a feature fusion strategy designed to augment a fully convolutional architecture with a parallel, layered structure whose task is to better process and propagate features from the original network to the classifier. Aggregation nodes, the main building block of the augmenting structure, learn to combine convolutional outputs from multiple layers with a compression technique, which in <ref type="bibr" target="#b26">[27]</ref> is implemented with 1x1 convolutions followed by batch normalization and nonlinearity. The arrangement of connections between aggregation nodes and the augmented network's original layers yields an archi-tecture more capable of extracting the full spectrum of spatial and semantical informations from the original model <ref type="bibr" target="#b26">[27]</ref>.</p><p>Inspired by the aggregations of <ref type="bibr" target="#b26">[27]</ref>, we implement aggregation modules as parallel feature processing branches pluggable in any CNN architecture. Our aggregation consists of a stacked sequence of aggregation nodes N , with each node iteratively combining outputs from ? and from the previous node, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The nodes we use are implemented as 1x1 convolutions followed by nonlinearity. Our aggregation module visually resembles the Iterative Deep Aggregation (IDA) strategy described in <ref type="bibr" target="#b26">[27]</ref>, but the two are different. IDA is an aggregation pattern for merging different scales, and is implemented on top of a hierarchical structure. Our aggregation module is a pluggable augmentation which merges features from various layers sequentially. Compared to <ref type="bibr" target="#b26">[27]</ref>, our structure can be merged with any existing pre-trained model without disrupting the original features' propagation. We also extend its usage to non-fully convolutional models by viewing 2-dimensional outputs of fully connected nodes as 4-dimensional (N x C x H x W) tensors whose H and W dimension are collapsed. As we designed these modules having in mind the DG problem and their usage for domain specific learning, we call them Domain-Specific Aggregation Modules (D-SAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">D-SAM Architecture for Domain Generalization</head><p>The modular nature of our D-SAMs allows the stacking of multiple augmentations on the same backbone network. Given a DG setting in which we have S source domains, we choose a pre-trained model ? and augment it with S aggregation modules, each of which implements its own classifier while learning to specialize on an individual domain. The overall architecture is shown in <ref type="figure">Figure  2</ref>.</p><p>Our intention is to model the domain specific part and the domain generic part within the architecture. While aggregation modules are domain specific, we may see ? as the domain generic part that, via backpropagation, learns to yield general features which aggregation modules specialize upon. Although not explicitly trained to do so, our feature evaluations suggest that thanks to our training procedure, the backbone ? implicitly learns more domain generic representations compared to the corresponding backbone model trained without aggregations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Testing</head><p>We train our model so that the backbone ? processes all the input images, while each aggregation module learns to specialize on a single domain. To accomplish this, at each iteration we feed to the network S equal sized mini-batches grouped by domain. Given an input mini-batch x i from the i th source domain, the corresponding output of our function, as also shown graphically in <ref type="figure">Figure 2</ref>, is:</p><formula xml:id="formula_0">f (x i ) = ? i (?(x i )).</formula><p>(1) We optimize our model by minimizing the cross entropy loss function L C = c y x i j ,c log(p x i j ,c ), which for a training iteration we formalize as:</p><formula xml:id="formula_1">L(?, ?) = S i=1 L C ((? i ? ?)(x i )).<label>(2)</label></formula><p>We validate our model by combining probabilities of the outputs of aggregation modules. One problem of the DG setting is that performance on the validation set is not very informative, since accuracy on source domains doesn't give much indication of the generalization ability. We partially mitigate this problem in our algorithm by calculating probabilities for validation as:</p><formula xml:id="formula_2">p x v j = ?( S i=1,i =v ? i (?(x v j ))),<label>(3)</label></formula><p>where ? is the softmax function. Given an input image belonging to the k source domain, all aggregation modules besides ? k participate in the evaluation. With our validation we keep the model whose aggregation modules are general enough to distinguish between unseen distributions, while still training the main branch on all input data. We test our model both in an end to end fashion and as a feature extractor. For end-to-end classification we calculate probabilities for the label as:</p><formula xml:id="formula_3">p x t j = ?( S i=1 ? i (?(x t j ))),<label>(4)</label></formula><p>When testing our algorithm as a feature extractor, we evaluate ?'s and ?'s features by running an SVM Linear Classifier on the DG task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section we report experiments assessing the effectiveness of our DSAMbased architecture in the DG scenario, using two different backbone architectures (a ResNet-18 <ref type="bibr" target="#b6">[7]</ref> and an AlexNet <ref type="bibr" target="#b8">[9]</ref>), on two different databases. We first describe the datasets used (section 4.1), and then we proceed to report the model setup (section 4.2) and the training protocol adopted (section 4.3). Section 4.4 reports and comments upon the experimental results obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We performed experiments on two different databases. The PACS database <ref type="bibr" target="#b9">[10]</ref> has been recently introduced to support research on DG, and it is quickly becoming the standard reference benchmark for this research thread. It consists of 9.991 images, of resolution 227 ? 227, taken from four different visual domains (Photo, Art paintings, Cartoon and Sketches), depicting seven categories. We followed the experimental protocol of <ref type="bibr" target="#b9">[10]</ref> and trained our models considering three domains as source datasets and the remaining one as target.</p><p>The Office-Home dataset <ref type="bibr" target="#b24">[25]</ref> was introduced to support research on DA for object recognition. It provides images from four different domains: Artistic images, Clip art, Product images and Real-world images. Each domain depicts 65 object categories that can be found typically in office and home settings. We are not aware of previous work using the Office-Home dataset in DG scenarios, hence we decided to follow also here the experimental setup introduced in [10] and described above for PACS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model setup</head><p>Aggregation Nodes. We implemented the aggregation nodes as 1x1 convolutional filters followed by nonlinearity. Compared to <ref type="bibr" target="#b26">[27]</ref>, we did not use batch normalization in the aggregations, since we empirically found it detrimental for our difficult DG targets. Whenever the inputs of a node have different scales, we downsampled with the same strategy used in the backbone model. For ResNet-18 experiments, we further regularized the convolutional inputs of our aggregations with dropout.</p><p>Aggregation of Fully Connected Layers. We observe that a fully connected layer's output can be seen as a 4-dimensional (N, C, H, W) tensor with collapsed height and width dimensions, as each unit's output is a function of the entire input image. A 1x1 convolutional layer whose input is such a tensor coincides with a fully connected layer whose input is a 2-dimensional (N, C) tensor, so for simplicity we implemented those aggregations with fully connected layers instead of convolutions. <ref type="figure">Fig. 3</ref>. Exemplar images for the PACS (left) and Office-Home (right) databases, on selected categories. We see that for both databases, the variations among domains for the same category can vary a lot.</p><p>Model Initialization. We experimented with two different backbone models: AlexNet and ResNet-18, both of which are pre-trained on the ImageNet 1000 object categories <ref type="bibr" target="#b16">[17]</ref>. We initialized our aggregation modules ? with random uniform initialization. We connected the aggregation nodes with the output of the AlexNet's layers when using AlexNet as backbone, or with the exit of each residual block when using ResNet-18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training setup</head><p>We finetuned our models on S = 3 source domains and tested on the remaining target. We splitted our training sets in 90% train and 10% validation, and used the best performing model on the validation set for the final test, following the validation strategy described in Section 3. For preprocessing, we used random zooming with rescaling, horizontal flipping, brightness/contrast/saturation/hue perturbations and normalization using ImageNet's statistics. We used a batch size of 96 (32 images per source domain) and trained using SGD with momentum set at 0.9 and initial learning rate at 0.01 and 0.007 for ResNet's and AlexNet's experiments respectively. We considered an epoch as the minimum number of steps necessary to iterate over the largest source domain and we trained our models for 30 epochs, scaling the learning rate by a factor of 0.2 every 10 epochs. We used the same setup to train our ResNet-18 Deep All baselines. We repeated each experiment 5 times, averaging the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We run a first set of experiments with the D-SAMs using an AlexNet as backbone, to compare our results with those reported in the literature by previous works, as AlexNet has been so far the convnet of choice in DG. Results are reported in table 1. We see that our approach outperforms previous work by a sizable margin, showing the value of our architecture. Particularly, we underline that D-SAMs obtain remarkable performances on the challenging setting where the 'Sketch' domain acts as target.</p><p>We then run a second set of experiments, using both the PACS and Office-Home dataset, using as backbone architecture a ResNet-18. The goal of this set of experiments is on one side to showcase how our approach can be easily used with different ? networks, on the other side to perform an ablation study with respect to the possibility to use D-SAMs not only in an end-to-end classification framework, but also to learn feature representations, suitable for domain generalization. To this end, we report results on both databases using the endto-end approach tested in the AlexNet experiments, plus results obtained using the feature representations learned by ?, ? and the combination of the two. Specifically, we extract and l 2 normalize features from the last pooling layer of each component. We integrate features of ?s modules with concatenation, and train the SVM classifier leaving the hyperparameter C at the default value. Our results in table 2 and 3 show that the SVM classifier trained on the l 2 normalized features always outperforms the corresponding end-to-end models, and that ?s and ?s features have similar performance, with ?s features outperforming the corresponding Deep All features while requiring no computational overhead for inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper presented a Domain Generalization architecture inspired by recent work on deep layer aggregation. We developed a convnet that, starting from a pre-trained model carrying generic perceptual knowledge, aggregates layers iteratively for as many branches as the available source domains data at training time. The model can be used in an end-to-end fashion, or its convolutional layers can be used as features in a linear SVM. Both approaches, tested with two popular pre-trained architectures on two benchmark databases, achieve the new state of the art. Future work will further study deep layer aggregation strategies within the context of domain generalization, as well as scalability with respect to the number of sources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Architecture of an aggregation module (purple) augmenting a CNN model. Aggregation nodes (yellow) iteratively process input from ?'s layers and propagate them to the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 ?Fig. 2 .</head><label>12</label><figDesc>Simplified architecture with 3 aggregation nodes per aggregation module. The main branch ? shares features with S specialized modules. At training time, the i th aggregation module only processes outputs relative to the i th domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .Table 3 .</head><label>123</label><figDesc>PACS end-to-end results using D-SAMs coupled with the AlexNet architecture. PACS results with ResNet-18 using features (top-rows) and end-to-end accuracy (bottom rows). OfficeHome results with ResNet-18 using features (top rows) and end-to-end accuracy (bottom rows).</figDesc><table><row><cell></cell><cell cols="2">Deep All [10] TF [10]</cell><cell>MLDG [11]</cell><cell>SSN [14]</cell><cell>D-SAMs</cell></row><row><cell cols="2">art painting 64.91</cell><cell>62.86</cell><cell>66.23</cell><cell>64.10</cell><cell>63.87</cell></row><row><cell>cartoon</cell><cell>64.28</cell><cell>66.97</cell><cell>66.88</cell><cell>66.80</cell><cell>70.70</cell></row><row><cell>photo</cell><cell>86.67</cell><cell>89.50</cell><cell>88.00</cell><cell>90.20</cell><cell>85.55</cell></row><row><cell>sketch</cell><cell>53.08</cell><cell>57.51</cell><cell>58.96</cell><cell>60.10</cell><cell>64.66</cell></row><row><cell>avg</cell><cell>67.24</cell><cell>69.21</cell><cell>70.01</cell><cell>70.30</cell><cell>71.20</cell></row><row><cell></cell><cell cols="2">art painting cartoon</cell><cell>sketch</cell><cell>photo</cell><cell>Avg</cell></row><row><cell cols="2">Deep All (feat.) 77.06</cell><cell>77.81</cell><cell>74.09</cell><cell>93.28</cell><cell>80.56</cell></row><row><cell>? (feat.)</cell><cell>79.57</cell><cell>76.94</cell><cell>75.47</cell><cell>94.16</cell><cell>81.54</cell></row><row><cell>? (feat.)</cell><cell>79.48</cell><cell>77.13</cell><cell>75.30</cell><cell>94.30</cell><cell>81.55</cell></row><row><cell>? + ? (feat.)</cell><cell>79.44</cell><cell>77.22</cell><cell>75.33</cell><cell>94.19</cell><cell>81.54</cell></row><row><cell>Deep All</cell><cell>77.84</cell><cell>75.89</cell><cell>69.27</cell><cell>95.19</cell><cell>79.55</cell></row><row><cell>D-SAMs</cell><cell>77.33</cell><cell>72.43</cell><cell>77.83</cell><cell>95.30</cell><cell>80.72</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive deep learning through visual domain localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotic Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rota Bul?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Just dial: domain alignment layers for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rota Bul?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5543" to="5551" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Best sources forward: domain generalization through source-specific nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05810</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust place categorization with deep domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From source to target and back: symmetric bi-directional adaptive gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">(IEEE) Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep cocktail network: Multi-source unsupervised domain adaptation with category shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06484</idno>
		<title level="m">Deep layer aggregation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiple source domain adaptation with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of the International Conference on Learning Representations (ICLR-W)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
