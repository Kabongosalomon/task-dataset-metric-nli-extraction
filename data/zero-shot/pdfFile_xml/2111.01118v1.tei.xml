<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minguk</forename><surname>Kang</surname></persName>
							<email>mgkang@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woohyeon</forename><surname>Shim</surname></persName>
							<email>wh.shim@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<email>mscho@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
							<email>jaesik.park@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conditional Generative Adversarial Networks (cGAN) generate realistic images by incorporating class information into GAN. While one of the most popular cGANs is an auxiliary classifier GAN with softmax cross-entropy loss (ACGAN), it is widely known that training ACGAN is challenging as the number of classes in the dataset increases. ACGAN also tends to generate easily classifiable samples with a lack of diversity. In this paper, we introduce two cures for ACGAN. First, we identify that gradient exploding in the classifier can cause an undesirable collapse in early training, and projecting input vectors onto a unit hypersphere can resolve the problem. Second, we propose the Data-to-Data Cross-Entropy loss (D2D-CE) to exploit relational information in the class-labeled dataset. On this foundation, we propose the Rebooted Auxiliary Classifier Generative Adversarial Network (ReACGAN). The experimental results show that ReACGAN achieves state-of-the-art generation results on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets. We also verify that ReACGAN benefits from differentiable augmentations and that D2D-CE harmonizes with StyleGAN2 architecture. Model weights and a software package that provides implementations of representative cGANs and all experiments in our paper are available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While GANs have shown impressive results in the image generation task <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>, training GANs often ends up encountering a mode-collapse problem <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. As one of the prescriptions for stabilizing and reinforcing GANs, training GANs with categorical information, named conditional Generative Adversarial Networks (cGAN), is suggested <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Depending on the presence of explicit classification losses, cGAN can be divided into two groups: Classifier-based GANs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref> and Projection-based GANs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28]</ref>. One of the widely used classifier-based GANs is ACGAN <ref type="bibr" target="#b17">[18]</ref>, and ACGAN utilizes softmax cross-entropy loss to perform classification task with adversarial training. Although ACGAN has shown satisfactory generation results, training ACGAN becomes unstable as the number of classes in the training dataset increases <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>. Besides, ACGAN tends to generate easily classifiable images at the cost of limited diversity <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>. To alleviate those problems, Zhou et al. <ref type="bibr" target="#b37">[38]</ref> have proposed performing adversarial training on the classifier. Gong et al. <ref type="bibr" target="#b19">[20]</ref> have introduced an additional classifier to eliminate a conditional entropy minimization process in the adversarial training. However, ACGAN training still suffers from the early-training collapse issue and the reduced diversity problem when trained on datasets with a large number of class categories, such as Tiny-ImageNet <ref type="bibr" target="#b31">[32]</ref> and ImageNet <ref type="bibr" target="#b30">[31]</ref>.</p><p>In these circumstances, Miyato et al. <ref type="bibr" target="#b18">[19]</ref> have proposed a projection discriminator for cGANs and have shown significant improvement in generating the ImageNet dataset. Motivated by the promising result of the projection discriminator, many projection-based GANs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref> have been proposed and become the standard for conditional image generation. In this paper, we revisit ACGAN and unveil why ACGAN training is so unstable. Coping with the instability, we propose the Rebooted Auxiliary Classifier GANs (ReACGAN) for high-quality and diverse image generation.</p><p>3 Rebooting Auxiliary Classifier GANs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Normalization</head><p>To uncover a nuisance that can cause the instability of ACGAN, we start by analytically deriving the gradients of weight vectors in the softmax classifier. Let the part of the discriminator before the fully connected layer be a Feature extractor F : X ?? F ? R d and let Classifier C : F ?? R c be a single fully connected layer parameterized by W = [w 1 , ..., w c ] ? R d?c , where c denotes the number of classess. We sample training images X = {x 1 , ..., x N } and integer labels y = {y 1 , ..., y N } from the joint distribution p(x, y). Using the notations above, we can express the empirical cross-entropy loss used in ACGAN <ref type="bibr" target="#b17">[18]</ref> as follows:</p><formula xml:id="formula_0">L CE = ? 1 N N i=1 log exp (F (x i ) w yi ) c j=1 exp (F (x i ) w j ) .<label>(3)</label></formula><p>Based on Eq. (3), we can derive the derivative of the cross-entropy loss, w.r.t w k?{1,...,c} as follows:</p><formula xml:id="formula_1">?L CE ?w k = ? 1 N N i=1 F (x i ) 1 yi=k ? p i,k ,<label>(4)</label></formula><p>where 1 yi=k is an indicator function that will output 1 if y i = k is satisfied, and p i,k is a class probability that represents the probability that i-th sample belongs to class k, mathematically exp (F (xi) w k ) c j=1 exp (F (xi) wj ) . The equation above implies that the norm of the gradient of the softmax cross-entropy loss is coupled with the norms and directions of each input feature map F (x i ) and the class probabilities. In the early training stage, the classifier is prone to making incorrect predictions, resulting in low probabilities. This phenomenon occurs more frequently as the number of categories in the dataset increases. As the result, the norm of the gradient | ?LCE ?w k | begins to explode as the vector F (x i ) stretches out to w k direction but being located close to the the other vectors w j?{1,...,c}\{k} . This often breaks the balance between adversarial learning and classifier training, leading to an early-training collapse. Once the early-training collapse occurs, ACGAN training concentrates on classifying categories of images instead of discriminating the authenticity of given samples. We experimentally demonstrate that the average norm of ACGAN's input feature maps increases as the training progresses ( <ref type="figure" target="#fig_0">Fig. 2a)</ref>. Accordingly, the average norm of the gradients increases sharply at the early training stage and decreases with the high class probabilities of the classifier <ref type="figure" target="#fig_0">(Fig. 2b</ref> in the main paper and <ref type="figure" target="#fig_0">Fig. A2b and A3</ref> in Appendix E). While the average norm of gradients decreases All experiments are conducted on Tiny-ImageNet <ref type="bibr" target="#b31">[32]</ref> dataset. (a) Average norms of input feature maps F (?), (b) average norms of gradients of classification losses, and (c) trends of FID scores. Compared with ACGAN <ref type="bibr" target="#b17">[18]</ref>, the proposed ReACGAN does not experience the training collapse problem caused by excessively large norms of feature maps and gradients at the early training stage. In addition, ReACGAN can converge to better equilibrium by considering data-to-data relationships with easy positive and negative sample suppression.</p><p>at some point, the FID value of ACGAN does not decrease, implying the collapse of ACGAN training ( <ref type="figure" target="#fig_0">Fig. 2c</ref>).</p><p>As one of the remedies for the gradient exploding problem, we find that simply normalizing the feature embeddings onto a unit hypersphere F (xi) ||F (xi)|| effectively resolves ACGAN's early-training collapse (see <ref type="figure" target="#fig_0">Fig. 2</ref>). The motivation is that normalizing the features onto the hypersphere makes the norms of feature maps equal to 1.0. Thus, the discriminator does not experience the gradient exploding problem. From the next section, we will deploy a linear projection layer P on the feature extractor F . And, we will normalize both the embeddings from the projection layer and the weight vectors (w 1 , ..., w c ) in the classifier since normalizing both the embeddings and the weight vectors does not degrade image generation performance. We denote the normalized embedding P (F (xi)) ||P (F (xi))|| as f i and the normalized weight vector wy i ||wy i || as v yi .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data-to-Data Cross-Entropy Loss (D2D-CE)</head><p>We expand the feature normalized softmax cross-entropy loss described in Sec. 3.1 to the Data-to-Data Cross-Entropy (D2D-CE) loss. The motivations are summarized into two points: (1) replacing datato-class similarities in the denominator of Eq. (3) with data-to-data similarities and (2) introducing two margin values to the modified softmax cross-entropy loss. We expect that point (1) will encourage the feature extractor to consider data-to-class as well as data-to-data relationships, and that point <ref type="bibr" target="#b1">(2)</ref> will guarantee inter-class separability and intra-class variations in the feature space while preventing ineffective gradient updates induced by easy negative and positive samples. To develop the feature normalized cross-entropy loss into D2D-CE, we replace the similarities between a sample embedding and all proxies except for the positive one in the denominator, j?{1,...,c}\{yi} exp (f i v j ) with similarities between negative samples in the same mini-batch. The modified cross-entropy loss can be expressed as follows:</p><formula xml:id="formula_2">L CE = ? 1 N N i=1 log exp (f i v yi /? ) exp (f i v yi /? ) + j?N (i) exp (f i f j /? ) ,<label>(5)</label></formula><p>where ? is a temperature, and N (i) is the set of indices that point locations of the negative samples whose labels are different from the reference label v yi in the mini-batch. The self-similarity matrix of samples in the mini-batch is used to calculate the similarities between negative samples f i f j?N (i) with a false negative mask (see <ref type="figure" target="#fig_1">Fig. 3</ref>). Thus, Eq. (5) enables the discriminator to contrastively compare visual differences between multiple images and can supply more informative supervision for image conditioning. Finally, we introduce two margin hyperparameters to L CE and name it Data-to-Data Cross-Entropy loss (D2D-CE). The proposed D2D-CE can be expressed as follows: At the same time, ReACGAN tries to minimize data-to-data cross-entropy loss (D2D-CE) on the linear head (P ) to exploit relational information in the labeled dataset. ? means element-wise product. Note that False negatives mask operates on the similarity matrix between two batches of sample embeddings to compute the similarities between negative samples in the denominator of Eq. <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_3">L D2D-CE = ? 1 N N i=1 log exp [f i v yi ? m p ] ? /? exp [f i v yi ? m p ] ? /? + j?N (i) exp [f i f j ? m n ] + /? ,<label>(6)</label></formula><formula xml:id="formula_4">! " # $ ? ? $ # " ! ? (z)~Z ? (x, y)~X ? ? ? False negatives mask Feature Extractor ( ) Linear Head ( ) ? Adv ? D2D?CE !,! !,# !,$ ? !,% #,! #,# #,$ ? #,% $,! $,# $,$ ? $,% %,! %,# %,$ ? !,! ? ? ? ? ? ? ? ? ? ? ? ? ? ? %# %$ ? %% %&amp; ! " # $ ? ? ? ! ? # ? $ ? ? ! ? ? ? ? ? Get mask ? Generator ( )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Get proxy</head><p>where m p is a margin for suppressing a high similarity value between a reference sample and its corresponding proxy (easy positive), m n is a margin for suppressing low similarity values between negatives samples (easy negatives).</p><p>[?] ? and [?] + denote min(?,0) and max(?,0) functions, respectively.  In this subsection, we explain four useful properties of D2D-CE. Let s q be a similarity between the normalized embedding f q and the corresponding normalized proxy v yq , s q,r be a similarity between f q and f r , and a and b be arbitrary indices of negative samples, i.e., a, b ? N (q). Then the properties of D2D-CE can be summarized as follows: Property 1. Hard negative mining. If the value of s q,a is greater than s q,b , the derivative of L D2D-CE w.r.t s q,a is greater than or equal to the derivative w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Useful Properties of Data-to-Data Cross-Entropy Loss</head><formula xml:id="formula_5">r.t s q,b ; that is ?LD2D-CE ?sq,a ? ?LD2D-CE ?s q,b ? 0. Property 2. Easy positive suppression. If s q ? m p ? 0, the derivative of L D2D-CE w.r.t s q is 0. Property 3. Easy negative suppression. If s q,r ? m n ? 0, the derivative of L D2D-CE w.r.t s q,r is 0. Property 4. If s q ? m p ? 0 and s q,r ? m n ? 0 are satisfied, L D2D-CE has the global minima of 1 N N i=1 log (1 + |N (i)|).</formula><p>We put proofs of the above properties in Appendix D. Property 1 indicates that D2D-CE implicitly conducts hard negative mining and benefits from comparing samples with each other. Also, Properties 2 and 3 imply that samples will not affect gradient updates if the samples are trained sufficiently. Consequently, the classifier concentrates on pushing and pulling hard negative and hard positive examples without being dominated by easy negative and positive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Rebooted Auxiliary Classifier Generative Adversarial Networks (ReACGAN)</head><p>With the proposed D2D-CE objective, we propose the Rebooted Auxiliary Classifier Generative Adversarial Networks (ReACGAN). As ACGAN does, ReACGAN jointly optimizes an adversarial loss and the classification objective (D2D-CE). Specifically, the discriminator, which consists of the feature extractor, adversarial head, and linear head, of ReACGAN tries to discriminate whether a given image is sampled from the real distribution or not. At the same time, the discriminator tries to maximize similarities between the reference samples and corresponding proxies while minimizing similarities between negative samples using real images and D2D-CE loss. After updating the discriminator a predetermined number of times, the generator strives to deceive the discriminator by generating well-conditioned images that will output a low D2D-CE value. By alternatively training the discriminator and generator until convergence, ReACGAN generates high-quality images of diverse categories without early-training collapse. We attach the algorithm table in Appendix A.</p><p>Differences between ReACGAN and ContraGAN. The authors of ContraGAN <ref type="bibr" target="#b22">[23]</ref> propose a conditional contrastive loss (2C loss) to cover data-to-data relationships when training cGANs. The main differences between 2C loss and D2D-CE objective are summed up into three points: (1) while 2C loss is derived from NT-Xent loss <ref type="bibr" target="#b38">[39]</ref>, which is popularly used in the field of the self-supervised learning, our D2D-CE is developed to resolve the early-training collapsing problem and the poor generation results of ACGAN, (2) 2C loss holds false-negative samples in the denominator, and they can cause unexpected positive repulsion forces, and (3) 2C loss contains the similarities between all positive samples in the numerator, and they give rise to large gradients on pulling easy positive samples. Consequently, GANs with 2C loss tend to synthesize images of unintended classes as reported in the author's software document <ref type="bibr" target="#b39">[40]</ref>. However, D2D-CE does not contain the false negatives in the denominator and considers only the similarity between a sample and its proxy in the numerator. Therefore, ReACGAN is free from the undesirable repelling forces and does not conduct unnecessary easy positive mining. More detailed explanations are attached in Appendix F.</p><p>Consistency Regularization and D2D-CE Loss. Zhang et al. <ref type="bibr" target="#b15">[16]</ref> propose a consistency regularization to force the discriminator to make consistent predictions if given two images are visually close to each other. They create a visually similar image pair by augmenting a reference image with pre-defined augmentations. After that, they let the discriminator minimize L2 distance between the logit of the reference image and the logit of the augmented counterpart. While ReACGAN locates an image embedding nearby its corresponding proxy but far apart multiple image embeddings of different classes, consistency regularization only pulls a reference and the augmented image towards each other. Since consistency regularization and D2D-CE can be applied together, we will show that ReACGAN benefits from consistency regularization in the experiments section <ref type="table">(Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>To verify the effectiveness of ReACGAN, we conduct conditional image generation experiments using five datasets: CIFAR10 <ref type="bibr" target="#b29">[30]</ref>, Tiny-ImageNet <ref type="bibr" target="#b31">[32]</ref>, CUB200 <ref type="bibr" target="#b32">[33]</ref>, ImageNet <ref type="bibr" target="#b30">[31]</ref>, and AFHQ <ref type="bibr" target="#b40">[41]</ref> datasets and four evaluation metrics: Inception Score (IS) <ref type="bibr" target="#b41">[42]</ref>, Fr?chet Inception Distance (FID) <ref type="bibr" target="#b33">[34]</ref>, and F 0.125 (Precision) and F 8 (Recall) <ref type="bibr" target="#b42">[43]</ref>. The details on the training datasets are in Appendix C.1.</p><p>Inception Score (IS) <ref type="bibr" target="#b41">[42]</ref> and Fr?chet Inception Distance (FID) <ref type="bibr" target="#b33">[34]</ref> are widely used metrics for evaluating generative models. We utilize IS and FID together because some studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25]</ref> have shown that IS has a tendency to measure the fidelity of images better while FID tends to weight capturing the diversity of images.</p><p>Precision (F 0.125 ) and Recall (F 8 ) <ref type="bibr" target="#b42">[43]</ref> are metrics for estimating precision and recall of the approximated distribution p(G(z)) against the true data distribution p(x). Instead of evaluating generative models using one-dimensional scores, such as IS and FID, Sajjadi et al. <ref type="bibr" target="#b42">[43]</ref> have suggested using a two-dimensional score F 0.125 and F 8 that can quantify how precisely the generated images are and how well the generated images cover the reference distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Details</head><p>The implementation of ReACGAN basically follows details of PyTorch-StudioGAN library <ref type="bibr" target="#b39">[40]</ref> 1 that supports various experimental setups from ACGAN <ref type="bibr" target="#b17">[18]</ref> to StyleGAN2 [7] + ADA <ref type="bibr" target="#b7">[8]</ref> with <ref type="table">Table 1</ref>: Comparison with classifier-based GANs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> and projection-based GANs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29]</ref> on CIFAR10 <ref type="bibr" target="#b29">[30]</ref>, Tiny-ImageNet <ref type="bibr" target="#b31">[32]</ref>, and CUB200 <ref type="bibr" target="#b32">[33]</ref> datasets using IS <ref type="bibr" target="#b41">[42]</ref>, FID <ref type="bibr" target="#b33">[34]</ref>, F 0.125 , and F 8 <ref type="bibr" target="#b42">[43]</ref> metrics. For baselines, both the numbers from the cited paper (denoted as * in method) and from our experiments using StudioGAN library <ref type="bibr" target="#b39">[40]</ref> are reported. The numbers in bold-faced denote the best performance and in underline indicate the values are in one standard deviation from the best. Method CIFAR10 <ref type="bibr" target="#b29">[30]</ref> Tiny-ImageNet <ref type="bibr" target="#b31">[32]</ref> CUB200 <ref type="bibr" target="#b32">[33]</ref> IS different scales of datasets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref> and architectures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref>. For a fair comparison, we use the same backbones over all baselines, except otherwise noted, for both the discriminator and generator. For stable training, we apply spectral normalization (SN) <ref type="bibr" target="#b2">[3]</ref> to the generator and discriminator except for experiments using SNGAN (in this case, we apply SN to the discriminator only) and StyleGAN2 <ref type="bibr" target="#b6">[7]</ref>. We also use the same conditioning method for generators with conditional batch normalization (cBN) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b18">19]</ref>. This allows us to investigate solely the conditioning methods of the discriminator, which are the main interest of our paper. If not specified, we use hinge loss <ref type="bibr" target="#b45">[46]</ref> as a default for the adversarial loss.</p><formula xml:id="formula_6">? FID ? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ? SNGAN * [3] 8.22 21.7 - - - - - - - - - - BigGAN * [4] 9.22 14.73 - - - - - - - - - - ContraGAN * [23] - 10.60 - - - 29.49 - - - - - - ACGAN [</formula><p>Before conducting main experiments, we perform hyperparameter search with candidates of a temperature ? ? {0.125, 0.25, 0.5, 0.75, 1.0} and a positive margin m p ? {0.5, 0.75, 0.9, 0.95, 0.98, 1.0}. We set a negative margin m n as 1 ? m p to reduce search time. Through extensive experiments with 3 runs per each setting, we select ? with {0.5, 0.75, 0.25, 0.5, 0.25} and m p with {0.98, 1.0, 0.95, 0.98, 0.90} for CIFAR10, Tiny-ImageNet, CUB200, ImageNet 256 B.S., and ImageNet 2048 B.S. experiments, respectively. A low temperature seems to work well on fine-grained image generation tasks, but generally, ReACGAN is robust to the choice of hyperparameters. The results of the hyperparameter search and other hyperparameter setups are provided in Appendix C.2.</p><p>We evaluate all methods through the same protocol of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, which uses the same amounts of generated images from the reference split specialized for each dataset. <ref type="bibr" target="#b1">2</ref> Besides, we run all the experiments three times with random seeds and report the averaged best performances for reliable evaluation with the lone exception of ImageNet and StyleGAN2 related experiments. Please refer to Appendix C.2 for other experimental details. The numbers in bold-faced denote the best performance and in underline indicate that the values are in one standard deviation from the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Results</head><p>Comparison with Other cGANs. We compare ReACGAN with previous state-of-the-art cGANs in <ref type="table" target="#tab_1">Tables 1 and 2</ref>. We employ the implementations of GANs in PyTorch-StudioGAN library as it provides improved results on standard benchmark datasets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. For a fair comparison, we provide results from each original paper (denoted as * in method) as well as those from StudioGAN  <ref type="bibr" target="#b39">[40]</ref>. We also conduct experiments with popular augmentation-based methods: consistency regularization (CR) <ref type="bibr" target="#b15">[16]</ref> and differentiable augmentation (DiffAug) <ref type="bibr" target="#b8">[9]</ref>.</p><p>Compared with other cGANs, ReACGAN performs the best on most benchmarks, surpassing the previous methods by 2.5%, 5.1%, 15.8%, and 14.5% in FID on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet (256 B.S.), respectively. ReACGAN also harmonizes with augmentation-based regularizations, bringing incremental improvements on all the metrics. For the ImageNet experiments using a batch size of 256, ReACGAN reaches higher IS and lower FID with fewer iterations than other models in comparison. Finally, we demonstrate that ReACGAN can learn with a larger batch size on ImageNet. While some recent methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b26">27]</ref> have been built on ACGAN to improve the generation performance of ACGAN, large-scale image generation experiments with the batch size of 2048 have never been reported. <ref type="table" target="#tab_1">Table 2</ref> shows that our ReACGAN reaches FID score of 8.23 on ImageNet, being comparable with the value of 7.89 from BigGAN implementation in PyTorch-StudioGAN library. ReACGAN, however, provides better synthesis results than other implementations of BigGAN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>. Note that our result on ImageNet is obtained in only two runs while the training setup and architecture of BigGAN have been extensively searched and finely tuned.</p><p>Comparison with Other Conditioning Losses. We investigate how the generation qualities vary with different conditioning losses while keeping the other configurations fixed. We compare D2D-CE loss with cross-entropy loss of ACGAN (AC) <ref type="bibr" target="#b17">[18]</ref>, loss used in the projection discriminator (PD) <ref type="bibr" target="#b18">[19]</ref>, multi-hinge loss (MH) <ref type="bibr" target="#b25">[26]</ref>, and conditional contrastive loss (2C) <ref type="bibr" target="#b22">[23]</ref>. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. AC-and MH-based models present decent results on CIFAR10, but undergo early-training collapse on Tiny-ImageNet and CUB200 datasets. Replacing them with PD, 2C, and D2D-CE losses produce satisfactory performances across all datasets, where PD loss makes the best F 8 (recall) on CUB200 dataset while giving third F 0.125 (precision) value. The noticeable point is that the proposed D2D-CE shows consistent results across all datasets, showing the lowest FID and the highest F 8 and F 0.125 values in most cases. This means ReACGAN can generate high-fidelity images and is relatively free from the precision and recall trade-off <ref type="bibr" target="#b42">[43]</ref> than the others.</p><p>Consistent Performance of ReACGAN on Adversarial Loss Selection. We validate the consistent performance of ReACGAN on four adversarial losses: non-saturation loss <ref type="bibr" target="#b0">[1]</ref>, least square loss <ref type="bibr" target="#b9">[10]</ref>, Wasserstein loss with gradient penalty regularization (W-GP) <ref type="bibr" target="#b12">[13]</ref>, and hinge loss <ref type="bibr" target="#b45">[46]</ref> on CIFAR10 and Tiny-ImageNet datasets in <ref type="table" target="#tab_4">Table 4</ref>. The experimental results show that BigGAN + D2D-CE (ReACGAN) consistently outperforms the projection discriminator (PD) and conditional contrastive loss (2C) counterparts over three adversarial losses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46]</ref>. However, for the experiments using the least square loss <ref type="bibr" target="#b9">[10]</ref>, ReACGAN exhibits inferior generation performances to the projection discriminator. We speculate that minimizing the least square distance between an adversarial logit and the target scalar (1 or 0) might affect the norms of feature maps and spoil the classifier training performed by D2D-CE loss.    We study the effect of D2D-CE with different GAN architectures. In <ref type="table" target="#tab_5">Table 5</ref>, we validate that D2D-CE is effective for StyleGAN2 <ref type="bibr" target="#b6">[7]</ref> backbone and also fits well with the adaptive discriminator augmentation (ADA) <ref type="bibr" target="#b7">[8]</ref>. StyleGAN2 with D2D-CE loss produces 13.9% better generation result than the conditional version of StyleGAN2 (cStyleGAN2) on CIFAR10. Moreover, StyleGAN2 with D2D-CE can be reinforced with ADA or DiffAug when train StyleGAN2 + D2D-CE under the limited data situation. Among GANs, StyleGAN2 + DiffAug + D2D-CE + Tuning achieves the best performance on CIFAR10, even outperforming some diffusion-based methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. Additional results with other architectures, i.e., a deep convolutional network <ref type="bibr" target="#b1">[2]</ref> and a resnet style backbone <ref type="bibr" target="#b12">[13]</ref>, are provided in <ref type="table" target="#tab_2">Table A3</ref> in Appendix E.</p><formula xml:id="formula_7">IS ? FID ? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ? BigGAN w/o</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We study how each component of ReACGAN affects ACGAN training. By adding or ablating each part of ReACGAN, as shown in <ref type="table" target="#tab_7">Table 6</ref>, we identify four major observations. (1) Feature and weight normalization greatly stabilize ACGAN training and improve generation performances on Tiny-ImageNet and CUB200 datasets. (2) D2D-CE enhances the generation performance by considering data-to-data relationships and by performing easy sample suppression (3th and 4th rows). Tiny-ImageNet -64 2 CIFAR10 -32 2 <ref type="figure">Figure 5</ref>: Curated images generated by the proposed ReACGAN. More qualitative results on ReAC-GAN, BigGAN <ref type="bibr" target="#b3">[4]</ref>, ContraGAN <ref type="bibr" target="#b22">[23]</ref>, and ACGAN <ref type="bibr" target="#b17">[18]</ref> are attached in Appendix J. We attribute the improvement to the discriminator that successfully leverages informative data-to-data and data-to-class relationships with easy sample suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have analyzed why training ACGAN becomes unstable as the number of classes in the dataset increases. By deriving the analytic form of gradient in the classifier and numerically checking the gradient values, we have discovered that the unstable training comes from a gradient exploding problem caused by the unboundedness of input feature vectors and poor classification ability of the classifier in the early training stage. To alleviate the instability and reinforce ACGAN, we have proposed the Data-to-Data Cross-Entropy loss (D2D-CE) and the Rebooted Auxiliary Classifier Generative Adversarial Network (ReACGAN </p><formula xml:id="formula_9">Sample X, y real = {x i } N i=1 , {y i } N i=1 ? p real (x, y) 5: Sample Z = {z i } N i=1 ? p(z) and y fake = {y fake i } N i=1 ? P (y) 6: L D_Adv ?? L Adv (X, y real , G(Z, y fake ), y fake ) 7: L D_Cond ?? L D2D-CE (X, y real ; ?, m p , m n ) Eq. (6) with real images. 8: ? ?? Adam(L D_Adv + ?L D_Cond , ? 1 , ? 1 , ? 2 ) 9:</formula><p>end for 10:</p><formula xml:id="formula_10">Sample Z = {z i } N i=1 ? p(z) and y fake = {y fake i } N i=1 ? P (y) 11: L G_Adv ?? L Adv (G(Z, y fake ), y fake ) 12: L G_Cond ?? L D2D-CE (G(Z, y fake ), y fake ; ?, m p , m n )</formula><p>Eq. (6) with fake images. <ref type="bibr" target="#b12">13</ref>: GANs: DCGAN <ref type="bibr" target="#b1">[2]</ref>, LSGAN <ref type="bibr" target="#b9">[10]</ref>, GGAN <ref type="bibr" target="#b45">[46]</ref>, WGAN-WC <ref type="bibr" target="#b10">[11]</ref>, WGAN-GP <ref type="bibr" target="#b12">[13]</ref>, WGAN-DRA <ref type="bibr" target="#b13">[14]</ref>, ACGAN <ref type="bibr" target="#b17">[18]</ref>, Projection discriminator <ref type="bibr" target="#b18">[19]</ref>, SNGAN <ref type="bibr" target="#b2">[3]</ref>, SAGAN <ref type="bibr" target="#b28">[29]</ref>, TACGAN <ref type="bibr" target="#b19">[20]</ref>, LGAN <ref type="bibr" target="#b14">[15]</ref>, BigGAN <ref type="bibr" target="#b3">[4]</ref>, BigGAN-deep <ref type="bibr" target="#b3">[4]</ref>, StyleGAN2 <ref type="bibr" target="#b6">[7]</ref>, CRGAN <ref type="bibr" target="#b15">[16]</ref>, ICRGAN <ref type="bibr" target="#b16">[17]</ref>, LO-GAN <ref type="bibr" target="#b5">[6]</ref>, MHGAN <ref type="bibr" target="#b25">[26]</ref>, ContraGAN <ref type="bibr" target="#b22">[23]</ref>, ADCGAN <ref type="bibr" target="#b26">[27]</ref>, ReACGAN (ours).</p><formula xml:id="formula_11">? ?? Adam(L G_Adv + ?L G_Cond , ? 2 , ? 1 ,<label>?</label></formula><p>Adversarial losses: Logistic loss <ref type="bibr" target="#b6">[7]</ref>, Non-saturation loss <ref type="bibr" target="#b0">[1]</ref>, Least square loss <ref type="bibr" target="#b9">[10]</ref>, Wasserstein loss <ref type="bibr" target="#b10">[11]</ref>, Hinge loss <ref type="bibr" target="#b45">[46]</ref>, Multiple discriminator loss <ref type="bibr" target="#b21">[22]</ref>, Multi-hinge loss <ref type="bibr" target="#b25">[26]</ref>.</p><p>Regularizations: Feature matching regularization <ref type="bibr" target="#b41">[42]</ref>, R1 regularization <ref type="bibr" target="#b21">[22]</ref>, Weight clipping regularization <ref type="bibr" target="#b10">[11]</ref>, Spectral normalization <ref type="bibr" target="#b2">[3]</ref>, Path length regularization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, Top-k training <ref type="bibr" target="#b50">[51]</ref>.</p><p>Metrics: IS <ref type="bibr" target="#b41">[42]</ref>, FID <ref type="bibr" target="#b33">[34]</ref>, Intra-class FID, CAS <ref type="bibr" target="#b51">[52]</ref>, Precision and recall <ref type="bibr" target="#b42">[43]</ref>, Improved precision and recall <ref type="bibr" target="#b52">[53]</ref>, Density and coverage <ref type="bibr" target="#b53">[54]</ref>, SwAV backbone FID <ref type="bibr" target="#b54">[55]</ref>.</p><p>Differentiable augmentations: SimCLR augmentation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b55">56]</ref>, BYOL augmentation <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b55">56]</ref>, DiffAugment <ref type="bibr" target="#b8">[9]</ref>, Adaptive discriminator augmentation (ADA) <ref type="bibr" target="#b7">[8]</ref>.</p><p>Miscellaneous: Mixed precision training <ref type="bibr" target="#b57">[58]</ref>, Distributed data parallel (DDP), Data parallel (DP), Synchronized batch normalization <ref type="bibr" target="#b58">[59]</ref>, Standing statistics <ref type="bibr" target="#b3">[4]</ref>, Truncation trick <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>, Freeze discriminator (FreezeD) <ref type="bibr" target="#b59">[60]</ref>, Discriminator driven latent sampling (DDLS) <ref type="bibr" target="#b60">[61]</ref>, Closed-form factorization (SeFa) <ref type="bibr" target="#b61">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training Details</head><p>C.1 Datasets CIFAR10 <ref type="bibr" target="#b29">[30]</ref> is a widely used benchmark dataset for evaluating cGANs. The dataset contains 60k 32 ? 32 RGB images which belong to 10 different classes. The dataset is split into 50k images for training and 10k images for testing.</p><p>Tiny-ImageNet <ref type="bibr" target="#b31">[32]</ref> contains 120k 64?64 RGB images and is split into 100k training, 10k validation, and 10k test images. Tiny-ImageNet consists of 200 categories, and training GANs on Tiny-ImageNet is more challenging than CIFAR10 since there is less data (500 images) per class.</p><p>CUB200 <ref type="bibr" target="#b32">[33]</ref> provides around 12k fine-grained RGB images for 200 bird classes. We apply the center crop to each image using a square box whose lengths are the same as the short side of the image, and we resize the images to 128?128 pixels. We train cGANs on CUB200 dataset to identify the generation ability of cGANs on images with fine-grained characteristics in a limited data situation.</p><p>ImageNet <ref type="bibr" target="#b30">[31]</ref> provides around 1,281k and 50k RGB images for training and validation. We preprocess each image in the same way as applied to CUB200.</p><p>AFHQ <ref type="bibr" target="#b40">[41]</ref> consists of 14,630 and 1,500 numbers of 512 ? 512 RGB images for training and validation. The dataset is divided into 3 different animal classes (cat, dog, and wild animals).</p><p>For training and testing, we apply horizontal flip augmentation for all datasets and normalize image pixel values to a range between -1 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Hyperparameter Setup</head><p>Selecting proper hyperparameter values greatly affects GAN training. So it might be helpful to specify details of hyperparameter setups used in our work for future study. In this section, we aim to provide training specifications as much as possible, and if there exists a missing experimental setup, it follows configurations and details of StudioGAN implementation <ref type="bibr" target="#b39">[40]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proofs of Properties of D2D-CE</head><p>In the main paper, we introduce a new objective, the D2D-CE to train ACGAN stably. In this section, we provide proofs of the properties of D2D-CE. Using the notations defined in the main paper, our proposed D2D-CE loss can be expressed as follows:</p><formula xml:id="formula_12">L D2D-CE = ? 1 N N i=1 log exp [f i v yi ? m p ] ? /? exp [f i v yi ? m p ] ? /? + j?N (i) exp [f i f j ? m n ] + /? ,<label>(A1)</label></formula><p>where i is a sample index and N (i) is the set of indices that indicate the locations of negative samples in the mini-batch. To understand properties of D2D-CE loss, we can rewrite Eq. (A1) as follows:</p><formula xml:id="formula_13">L D2D-CE = ? 1 N N i=1 log exp [s i ? m p ] ? /? exp [s i ? m p ] ? /? + j?N (i) exp [s i,j ? m n ] + /? .<label>(A2)</label></formula><p>Let s q be a similarity between a normalized reference sample embedding f q and the corresponding normalized proxy v yq , s q,r be a similarity between f q and one of the its negative samples f r , and a, b ? N (q) be arbitrary indices of negative samples. Then, we can summarize the four properties of D2D-CE loss as follows: Property 1. Hard negative mining. If the value of s q,a is greater than s q,b , the derivative of L D2D-CE w.r.t s q,a is greater than or equal to the derivative w.r.t s q,b ; that is ?LD2D-CE ?sq,a ? ?LD2D-CE ?s q,b ? 0. Property 2. Positive suppression. If s q ?m p ? 0, the derivative of L D2D-CE w.r.t s q is 0.  Proof of Property 1. By expanding Eq. (A2), we have the following equation:</p><formula xml:id="formula_14">L D2D-CE = ? 1 ? N N i=1 [s i ? m p ] ? Positive attraction + 1 N N i=1 log exp [s i ? m p ] ? /? + j?N (i) exp [s i,j ? m n ] + /? Negative repulsion . (A3)</formula><p>Based on this, we calculate the derivative of L D2D-CE w.r.t s q,r as follows:</p><formula xml:id="formula_15">?L D2D-CE ?s q,r = 1 si,j ?mn&gt;0 (i = q, j = r) exp (s q,r ? m n )/? ? N exp [s q ? m p ] ? /? + j?N (q) exp [s q,j ? m n ] + /? . (A4)</formula><p>Since we assume s q,a &gt; s q,b is satisfied, the derivative of ?LD2D-CE ?sq,a is greater than ?LD2D-CE ?s q,b except when the indicator functions 1 si,j ?mn&gt;0 (i = q, j = a) and 1 si,j ?mn&gt;0 (i = q, j = b) are 0. Note that the value of the derivative ?LD2D-CE ?sq,r exponentially increases as the similarity s q,r linearly increases, and this means that L D2D-CE conducts hard negative mining.</p><p>Proof of Property 2. Based on Eq. (A3), we can derive the derivative of L D2D-CE w.r.t s q as follows: Proof of Property 4. We can get the global minima of L D2D-CE by plugging in 0 values inside of the exponential components in Eq. (A3) as follows:</p><formula xml:id="formula_16">?L D2D-CE ?s q = ? 1 ? N 1 si?mp&lt;0 (i = q) + 1 si?mp&lt;0 (i = q)exp (s q ? m p )/? ? N exp [s q ? m p ] ? /? + j?N (q) exp [s q,</formula><formula xml:id="formula_17">L D2D-CE = 1 N N i=1 log exp 0 + j?N (i) exp 0 = 1 N N i=1 log (1 + |N (i)|). (A6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Experimental Results</head><p>Gradients Exploding Problem in ACGAN. We conduct additional experiments regarding the gradient exploding problem of ACGAN using CUB200 dataset, and the results can be seen in <ref type="figure" target="#fig_0">Fig. A2</ref>. Same as the experimental results using Tiny-ImageNet <ref type="figure" target="#fig_0">(Fig. 2</ref> in the main paper), normalizing feature maps resolves the early-training collapse problem. Also, considering data-to-data relationships brings extra performance gain with easy positive and negative sample suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACGAN Focuses on Classifier Training instead of Adversarial Learning.</head><p>To identify ACGAN is prone to being biased toward classifying categories of images instead of discriminating the authenticity of given samples, we track the trend of classifier's target probabilities as the training progresses using Tiny-ImageNet and CUB200 datasets. As can be seen in <ref type="figure" target="#fig_0">Fig. A2, A3, and Fig. 2</ref> in the main paper, classifier's target probabilities continuously increase, but the FID scores do not decrease as of certain  <ref type="figure" target="#fig_1">Figure A3</ref>: Trends of the probability values from the classifiers trained on Tiny-ImageNet <ref type="bibr" target="#b31">[32]</ref> and CUB200 <ref type="bibr" target="#b32">[33]</ref> datasets.</p><p>points in time. Therefore the experimental results imply that ACGAN training is likely to become biased toward label classification instead of adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Can ReACGAN Approximate a Mixture of Gaussian Distributions Whose Supports Overlap?</head><p>We conduct distribution approximation experiments using a 1-D mixture of Gaussian distributions (MoG). The experiments are proposed by Gong et al. <ref type="bibr" target="#b19">[20]</ref> and are devised to identify if a given GAN can estimate any true data distribution, even the mixture of Gaussians with overlapped supports. Although ReACGAN has shown successful outputs on real images, ReACGAN fails to estimate the 1-D MoG, resulting in poor approximation ability similar to ACGAN (see <ref type="figure" target="#fig_3">Fig. A4</ref>). However, this phenomenon is not weird because ReACGAN follows the same optimization process as ACGAN does, which inherently reduces a conditional entropy H(y|x). As the result, ACGAN can only accurately approximate marginal distributions generated by conditional distributions with non-overlapped supports.</p><p>To deal with this problem, Gong et al. <ref type="bibr" target="#b19">[20]</ref> have suggested using a twin auxiliary classifier (TAC) on the top of the discriminator and have demonstrated that TAC enables ACGAN to exactly estimate the 1-D MoG. Therefore, ReACGAN can be reinforced by introducing TAC, and the experimental result verifies that ReACGAN can approximate the 1-D MoG exactly (see <ref type="figure" target="#fig_3">Fig. A4</ref>). <ref type="figure" target="#fig_3">Figure A4</ref>: Comparison with ACGAN <ref type="bibr" target="#b17">[18]</ref>, Projection discriminator <ref type="bibr" target="#b18">[19]</ref>, TACGAN <ref type="bibr" target="#b19">[20]</ref>, ReAC-GAN, and ReACGAN + TAC on a synthetic 1-D MoG dataset <ref type="bibr" target="#b19">[20]</ref>. We conduct all experiments using the same setup specified in the paper <ref type="bibr" target="#b19">[20]</ref>.  <ref type="table" target="#tab_1">Table A2</ref> shows that ReACGAN + TAC provides comparable or marginally better results on CIFAR10 and Tiny-ImageNet datasets over the ReACGAN. We speculate that this is because benchmark datasets are highly refined and might follow a mixture of non-overlapped conditional distributions.</p><p>Effect of D2D-CE for Different GAN Architectures. We perform additional experiments to identify the effect of D2D-CE loss for different GAN architectures. We utilize a deep convolutional neural network (Deep CNN) <ref type="bibr" target="#b1">[2]</ref> and a ResNet-style network (ResNet) <ref type="bibr" target="#b12">[13]</ref> to train GANs on CIFAR10 and Tiny-IamgeNet datasets. The experimental results show that ReACGAN provides consistent generation results on different architectures (see <ref type="table" target="#tab_2">Table A3</ref>). Effect of Number of Negative Samples on ReACGAN Training. We investigate how the number of negative samples affects the generation performance of ReACGAN using CIFAR10 and Tiny-ImageNet datasets. First, we compute pairwise similarities between all negative samples in the mini-batch. Then, we drop similarities between negative samples using a randomly generated mask whose element has a value of 0 according to the pre-defined probability p and otherwise has a value of 1. For example, p = 0.1 will lead approximately 10% of the total similarities between negative samples not to account for calculating the denominator part of D2D-CE loss. As can be seen in <ref type="table" target="#tab_4">Table A4</ref>, D2D-CE loss benefits from more negative samples. This implies that the more the data-to-data relations are provided, the richer supervision signals for conditioning become, resulting in better image generation results. In addition, from the optimization point of view, the generator and discriminator can receive gradient signals at an exponential rate as the number of negative samples increases linearly.</p><p>Are There Any Possible Prescriptions for Preventing the Gradient Exploding Problem? In the main paper, we verify that simply normalizing feature embeddings onto a unit hypersphere resolves ACGAN's early-training collapse problem. In this section, we explore if there exist other cures for resolving the early-training collapse problem: (1) lowering classification strength, (2) gradient clipping, and (3) feature clipping. The experimental results <ref type="table" target="#tab_5">(Table A5</ref>) indicate that lowering classification strength and normalizing feature maps can prevent ACGAN training from collapsing at the early training phase. However, we cannot succeed in training ACGAN by clipping gradients of the classifier. We speculate that this is because gradient clipping restricts not only the norms of feature maps but also the class probability values; thus, ACGAN can be updated by inaccurate gradients and ends up collapsing. Among those methods, the normalization and D2D-CE present better performances than the others, demonstrating the effectiveness of our proposals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Analysis of the differences between ReACGAN and ContraGAN</head><p>This section explains the differences between ReACGAN and ContraGAN <ref type="bibr" target="#b22">[23]</ref> from a mathematical point of view. Kang and Park <ref type="bibr" target="#b22">[23]</ref> have proposed the conditional contrastive loss (2C loss), which is formulated from NT-Xent loss <ref type="bibr" target="#b38">[39]</ref>, and developed contrastive generative adversarial networks (Con-traGAN) for conditional image generation. Using the notations used in our main paper, we can write down 2C loss as follows:</p><formula xml:id="formula_18">L 2C = ? 1 N N i=1 log exp f i v yi /? ) + np?P(i) exp(f i f np /? ) exp f i v yi /? + j?{1,...,N }\{i} exp f i f j /? ,<label>(A7)</label></formula><p>where P(i) is the set of indices that indicate locations of positive samples in the mini-batch. To clearly identify how each sample embedding updates, we start by considering 2C loss on a single sample. We can rewrite a single sample version of Eq. (A7) as follows:</p><formula xml:id="formula_19">L 2C = ? 1 N log exp f q v yq /? ) + np?P(q) exp(f q f np /? ) Positive attraction + 1 N log exp f q v yq /? + j?{1,...,N }\{q} exp f q f j /? Negative repulsion .<label>(A8)</label></formula><p>Using Eq. (A8), we can calculate the derivative of L 2C w.r.t f q as follows:</p><formula xml:id="formula_20">?L 2C ?f q = ? exp(f q v yq /? )v yq + np?P(q) exp (f q f np /? )f np ? N exp(f q v yq /? ) + np?P(q) exp (f q f np /? ) + exp(f q v yq /? )v yq + j?{1,...,N }\{q} exp (f q f j /? )f j ? N exp(f q v yq /? ) + j?{1,...,N }\{q} exp (f q f j /? ) .<label>(A9)</label></formula><p>To understand Eq. (A9) more intuitively, we replace the denominator terms of Eq. (A9) with A and B and re-organize the equation as follows:</p><formula xml:id="formula_21">?L 2C ?f q = ? exp(f q v yq /? )v yq + np?P(q) exp (f q f np /? )f np ? N A + exp(f q v yq /? )v yq + j?{1,...,N }\{q} exp (f q f j /? )f j ? N B = ? 1 ? N exp(f q v yq /? )v yq + (P) Positive samples np?P(q) exp (f q f np /? )f np A ? exp(f q v yq /? )v yq B Positive attraction + 1 ? N nq?N (q) exp (f q f nq /? )f nq + (F) False negative samples j?{1,...,N }\({q}?N (q)) exp (f q f j /? )f j B Negative repulsion . (A10)</formula><p>The above equation implies that the positive samples (P) in Eq (A10) can cause easy positive mining, i.e., if a similarity f q f np has a large value, the gradient ?L2C ?fq can be biased towards f np direction with large magnitude. In addition, the false-negative samples (F) can attenuate the negative repulsion force, which is already being addressed in the contrastive learning community <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref>. Unlike 2C loss, our D2D-CE loss does not experience the easy positive mining and the attenuation caused by false-negative samples. To demonstrate this, we write down D2D-CE loss as follows:</p><formula xml:id="formula_22">L D2D-CE = ? 1 N N i=1 log exp ([f i v yi ? m p ] ? /? ) exp [f i v yi ? m p ] ? /? + j?N (i) exp [f i f j ? m n ] + /? .<label>(A11)</label></formula><p>In the same way as before, we can expand a single sample version of Eq. (A11) as follows:</p><formula xml:id="formula_23">L D2D-CE = ? 1 ? N [f q v yq ? m p ] ? Positive attraction + 1 N log exp [f q v yq ? m p ] ? /? + j?N (q) exp [f q f j ? m n ] + /? Negative repulsion .<label>(A12)</label></formula><p>Based on Eq. (A12), we can calculate the derivative of L D2D-CE w.r.t f q as follows:  To compare the conditioning performance of ReACGAN with other cGANs, we calculate Top-1 and Top-5 classification accuracies on ImageNet <ref type="bibr" target="#b30">[31]</ref> using the pre-trained Inception-V3 network <ref type="bibr" target="#b66">[67]</ref>.</p><formula xml:id="formula_24">?L D2D-CE ?f q = ? 1 ? N 1 f i vy i ?mp&lt;0 (i = q)v yq + 1 ? N 1 f i vy i ?mp&lt;0 (i = q) exp (f q v yq ? m p )/? v yq exp [f q v yq ? m p ] ? /? + j?N (q) exp [f q f j ? m n ] + /? + 1 ? N j?N (q) 1 f i fj ?mn&gt;0 (i = q) exp (f q f j ? m n )/? f j exp [f q v yq ? m p ] ? /? + j?N (q) exp [f q f j ? m n ] + /? = ? 1 f i vy i ?mp&lt;0 (i = q) ? N v yq ? exp (f q v yq ? m p )/? v yq C Positive attraction + 1 ? N j?N (q) 1 f i fj ?mn&gt;0 (i = q) exp (f q f j ? m n )/? f j C Negative repulsion ,<label>(A13)</label></formula><p>The results are summarized in <ref type="table" target="#tab_7">Table A6</ref>. Although ReACGAN has a lower FID value and higher IS score compared with BigGAN <ref type="bibr" target="#b3">[4]</ref> and ContraGAN <ref type="bibr" target="#b22">[23]</ref>, the top-1 and top-5 accuracies of ReACGAN are slightly below that of BigGAN. This implies that ReACGAN tends to approximate overall distribution with a slight loss of the exact conditioning. On the other hand, ContraGAN fails to perform conditional image generation, and it provides 2.866 % Top-1 accuracy on ImageNet dataset. This indicates that ContraGAN is likely to generate undesirably conditioned but visually satisfactory images (see <ref type="figure" target="#fig_5">Fig. A10</ref>, A14, A18, and A21 for quantitative results). One more interesting point is that although generated images from ACGAN give the best classification accuracy, they show a poor FID value compared with the others. This implies that ACGAN generates well-classifiable images without considering the diversity and fidelity of generated samples (see <ref type="figure" target="#fig_0">Fig. A12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Potential Negative Societal Impacts</head><p>The success in generating photo-realistic images in GANs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref> has attracted a myriad of applications to be developed, such as photo editing (filtering <ref type="bibr" target="#b67">[68]</ref>, stylization <ref type="bibr" target="#b68">[69]</ref> and object removal <ref type="bibr" target="#b69">[70]</ref>), image translation (sketch ? clip art <ref type="bibr" target="#b70">[71]</ref>, photo ? cartoon <ref type="bibr" target="#b71">[72]</ref>), image in-painting <ref type="bibr" target="#b72">[73]</ref>, and image extrapolation to arbitrary resolutions <ref type="bibr" target="#b24">[25]</ref>. While, in most cases, GANs are helpful for content creation or fast prototyping, there exist potential threats that one can maliciously use the synthesized results to deceive others. A well-known example is deepfake <ref type="bibr" target="#b73">[74]</ref>, where a person in the video appears with the voice and appearance of a celebrity and conveys a message to deceive or confuse others, e.g., fake news. Other examples include sexual harnesses <ref type="bibr" target="#b74">[75]</ref> and hacking machine vision applications.</p><p>As an effort to circumvent the negative issues, a number of techniques have been proposed. Masi et al. <ref type="bibr" target="#b75">[76]</ref> have utilized color and frequency information to detect deepfake. Naseer et al. <ref type="bibr" target="#b76">[77]</ref> have developed a general defense method from self-attacking via feature perturbation. We anticipate that further development of synthetic image detection techniques, well-established policies on the technique, and ethical awareness of researchers/developers will enable us to enjoy the broad applicability and benefits of GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Computation resources</head><p>In this section, we provide a summary of the total number of performed experiments, computing resources, and approximated training time spent on our research in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Standard Deviations of Experiments</head><p>We run all the experiments three times with random seeds and report the averaged best performances for reliable evaluation with the lone exception of ImageNet experiments. This section provides standard deviations for reference. <ref type="table">Table A8</ref>: Comparisons with classifier-based GANs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> and projection-based GANs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29]</ref> on CIFAR10 <ref type="bibr" target="#b29">[30]</ref>, Tiny-ImageNet <ref type="bibr" target="#b31">[32]</ref>, and CUB200 <ref type="bibr" target="#b32">[33]</ref> datasets using IS <ref type="bibr" target="#b41">[42]</ref>, FID <ref type="bibr" target="#b33">[34]</ref>, F 0.125 and F 8 <ref type="bibr" target="#b42">[43]</ref> metrics. We report the standard deviations of three different runs in this  <ref type="bibr" target="#b17">[18]</ref> 0.06 0.02 0.001 0.000 0.12 8.04 0.021 0.048 0.73 9.20 0.036 0.102 SNGAN <ref type="bibr" target="#b2">[3]</ref> 0.02 0.20 0.002 0.004 0.16 0.84 0.009 0.022 0.35 2.08 0.020 0.010 SAGAN <ref type="bibr" target="#b28">[29]</ref> 0.06 0.45 0.003 0.004 0.52 6.97 0.056 0.034 0.20 11.62 0.070 0.039 BigGAN <ref type="bibr" target="#b3">[4]</ref> 0.08 0.04 0.001 0.004 2.35 4.48 0.014 0.068 0.11 2.83 0.020 0.008 ContraGAN <ref type="bibr" target="#b22">[23]</ref> 0.07 0.09 0.000 0.001 0.34 0.57 0.001 0.011 0.13 1.36 0.008 0.021 ReACGAN 0.10 0.07 0.001 0.002 0.07 0.56 0.003 0.009 0.09 0.80 0.007 0.009 <ref type="bibr" target="#b15">[16]</ref> 0.15 0.09 0.001 0.001 0.47 0.15 0.003 0.002 0.00 0.85 0.004 0.003 ContraGAN [23] + CR <ref type="bibr" target="#b15">[16]</ref> 0.28 0.94 0.003 0.015 0.41 0.46 0.001 0.001 0.07 0.15 0.004 0.003 ReACGAN + CR <ref type="bibr" target="#b15">[16]</ref> 0.16 0.02 0.001 0.001 0.16 0.82 0.001 0.005 0.07 0.45 0.003 0.003 <ref type="bibr" target="#b8">[9]</ref> 0.16 0.06 0.001 0.000 0.87 0.81 0.001 0.002 0.18 1.10 0.004 0.001 ContraGAN [23] + DiffAug <ref type="bibr" target="#b8">[9]</ref> 0.07 0.08 0.001 0.000 0.78 0.31 0.002 0.001 0.07 0.15 0.004 0.003 ReACGAN + DiffAug <ref type="bibr" target="#b8">[9]</ref> 0.04 0.12 0.000 0.002 0.12 0.14 0.002 0.001 0.05 0.66 0.002 0.001  <ref type="bibr" target="#b22">[23]</ref> 0.07 0.09 0.000 0.001 0.34 0.57 0.001 0.011 0.13 1.36 0.008 0.021 Big + D2D-CE (ReACGAN) 0.10 0.07 0.001 0.002 0.07 0.56 0.003 0.009 0.09 0.80 0.007 0.009  <ref type="table" target="#tab_9">Table A11</ref>: Ablation study on normalization, data-to-data consideration, and easy negative and positive sample suppression.</p><formula xml:id="formula_25">IS ? FID ? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ? SNGAN * [3] - - - - - - - - - - - - BigGAN * [4] - - - - - - - - - - - - ContraGAN * [23] - - - - - - - - - - - - ACGAN</formula><formula xml:id="formula_26">BigGAN + CR * [16] - - - - - - - - - - - - BigGAN [4] + CR</formula><formula xml:id="formula_27">BigGAN + DiffAug * [9] - - - - - - - - - - - - BigGAN [4] + DiffAug</formula><p>Ablation Tiny-ImageNet <ref type="bibr" target="#b31">[32]</ref> CUB200 <ref type="bibr" target="#b32">[33]</ref> IS ? FID ? F 0.125   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Qualitative Results</head><p>We provide images that are generated by our ReACGAN and baseline approaches (ContraGAN <ref type="bibr" target="#b22">[23]</ref>, BigGAN <ref type="bibr" target="#b3">[4]</ref>, and ACGAN <ref type="bibr" target="#b17">[18]</ref>).     <ref type="figure" target="#fig_5">Figure A11</ref>: Generated images on ImageNet <ref type="bibr" target="#b30">[31]</ref> dataset using BigGAN <ref type="bibr" target="#b3">[4]</ref> and the batch size of 256 (FID=16.36).  <ref type="figure" target="#fig_0">Figure A21</ref>: Generated images on CIFAR10 <ref type="bibr" target="#b29">[30]</ref> dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Merits of integrating feature normalization and data-to-data relationship consideration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of ReACGAN. ReACGAN performs adversarial training using the loss L Adv .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Graph showing regions where hard negative mining, easy positive and negative suppression, and no gradient update occur.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 ) 14 :</head><label>214</label><figDesc>end for B Software: PyTorch-StudioGAN Generative Adversarial Network (GAN) is one of the popular generative models for realistic image generation. Although GAN has been actively studied in the machine learning community, only a few open-source libraries provide reliable implementations for GAN training. In addition, the existing libraries do not support various training and test configurations for loss functions, backbone architectures, regularizations, differentiable augmentations, and evaluation metrics. In this paper, we expand StudioGAN [40] library, and the StudioGAN provides about 40 implementations of GAN-related papers as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure A1 :</head><label>A1</label><figDesc>FID<ref type="bibr" target="#b33">[34]</ref> values of ReACGANs with different temperatures and positive margins. The dotted lines indicate the best performances on each dataset. with ADA. For ReACGAN experiments, we utilize the settings (B, D, F, H, J, L, O, Q, S) for experiments on the datasets stated above. To select an appropriate temperature ? and positive margin m p , we conduct two-stage linear search with the candidates of a temperature ? ? {0.125, 0.25, 0.5, 0.75, 1.0} and a positive margin m p ? {0.5, 0.75, 0.9, 0.95, 0.98, 1.0} while fixing the dimensionalities of feature embeddings to 512, 768, 1024, and 2048 for CIFAR10, Tiny-ImageNet, CUB200, and Ima-geNet experiments. We set the balance coefficient ? equal to the temperature ? except for ImageNet generation experiments. Specifically, we explore the best temperature value on each dataset and fix the temperature for the linear search on the positive margin. Linear search results are summarized inFig. A1, and the results show that ReACGAN provides stable performances across various temperature and positive margin values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Property 3 .</head><label>3</label><figDesc>Negative suppression. If s q,r ?m n ? 0, the derivative of L D2D-CE w.r.t s q,r is 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Property 4 .</head><label>4</label><figDesc>If s q ? m p ? 0 and s q,r ? m n ? 0 are satisfied, L D2D-CE has the global minima of 1 N N i=1 log (1 + |N (i)|).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>j ? m n ] + /? . (A5) Since the indicator function 1 si?mp&lt;0 (i = q) gives 0 value when s q ? m p ? 0, the derivative of L D2D-CE w.r.t s q is 0 when s q ? m p ? 0 is satisfied. Proof of Property 3. Based on Eq. (A4), the derivative of L D2D-CE w.r.t s q,r is 0 when s q,r ? m n ? 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure A2 :</head><label>A2</label><figDesc>Merits of integrating feature normalization and data-to-data relationship consideration. The experiments are conducted using CUB200<ref type="bibr" target="#b32">[33]</ref> dataset. (a) Average norms of input feature maps, (b) average norms of gradients of classification losses, and (c) trends of FID scores. Compared to AC-GAN<ref type="bibr" target="#b17">[18]</ref>, the proposed ReACGAN does not experience the early-training collapse problem caused by excessively large norms of feature maps and gradients. In addition, ReACGAN can converge to a better equilibrium by considering data-to-data relationships with easy sample suppression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>where C = exp [f q v yq ? m p ] ? /? + j?N (q) exp [f q f j ? m n ] + /? . Unlike2C loss, D2D-CE loss does not contain multiple positive samples in the positive attraction bracket and only consists of negative samples in the negative repulsion part, which means that D2D-CE loss does not perform easy-positive mining and does not attenuate the negative repulsion force.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>F 8 ?ACGAN</head><label>8</label><figDesc>IS ? FID ? F 0.125 ? F 8 ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure A5 :</head><label>A5</label><figDesc>Generated images on AFHQ [41] dataset using StyleGAN2 [7] + ADA [8] + D2D-CE (ReACGAN) (FID=4.95).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure A6 :Figure A7 :</head><label>A6A7</label><figDesc>Generated images on AFHQ<ref type="bibr" target="#b40">[41]</ref> dataset using StyleGAN2 [7] + ADA<ref type="bibr" target="#b7">[8]</ref> (FID=4.99). Generated images on ImageNet<ref type="bibr" target="#b30">[31]</ref> dataset using ReACGAN and the batch size of 2048 (FID=8.23).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure A8 :Figure A9 :</head><label>A8A9</label><figDesc>Generated images on ImageNet<ref type="bibr" target="#b30">[31]</ref> dataset using BigGAN<ref type="bibr" target="#b3">[4]</ref> and the batch size of 2048 (FID=7.89). Generated images on ImageNet<ref type="bibr" target="#b30">[31]</ref> dataset using ReACGAN and the batch size of 256 (FID=13.98).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure A10 :</head><label>A10</label><figDesc>Generated images on ImageNet<ref type="bibr" target="#b30">[31]</ref> dataset using ContraGAN<ref type="bibr" target="#b22">[23]</ref> and the batch size of 256 (FID=25.16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure A12 :Figure A13 :Figure A14 :Figure A15 :Figure A16 :Figure A17 :Figure A18 :Figure A19 :Figure A20 :</head><label>A12A13A14A15A16A17A18A19A20</label><figDesc>Generated images on ImageNet<ref type="bibr" target="#b30">[31]</ref> dataset using ACGAN<ref type="bibr" target="#b17">[18]</ref> and the batch size of 256 (FID=26.<ref type="bibr" target="#b34">35)</ref>. Generated images on CUB200<ref type="bibr" target="#b32">[33]</ref> dataset using ReACGAN (FID=14.67). Generated images on CUB200<ref type="bibr" target="#b32">[33]</ref> dataset using ContraGAN<ref type="bibr" target="#b22">[23]</ref> (FID=20.89). Generated images on CUB200<ref type="bibr" target="#b32">[33]</ref> dataset using BigGAN<ref type="bibr" target="#b3">[4]</ref> (FID=17.80). Generated images on CUB200<ref type="bibr" target="#b32">[33]</ref> dataset using ACGAN<ref type="bibr" target="#b17">[18]</ref> (FID=61.29). Generated images on Tiny-ImageNet<ref type="bibr" target="#b31">[32]</ref> dataset using ReACGAN (FID=26.82). Generated images on Tiny-ImageNet<ref type="bibr" target="#b31">[32]</ref> dataset using ContraGAN [23] (FID=28.41). Generated images on Tiny-ImageNet [32] dataset using BigGAN [4] (FID=31.92). Generated images on Tiny-ImageNet [32] dataset using ACGAN [18] (FID=61.50). ReACGAN (FID=7.81) (c) BigGAN [4] (FID=8.05) (d) ACGAN [18] (FID=8.47) (b) ContraGAN [23] (FID=8.18)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>DiffAug [9] 9.95 7.27 0.995 0.992 18.20 15.40 0.986 0.963 5.39 11.02 0.978 0.970 ReACGAN + DiffAug [9] 10.22 6.79 0.996 0.993 20.60 14.25 0.988 0.972 5.22 9.27 0.985 0.983</figDesc><table><row><cell>18]</cell><cell cols="12">9.84 8.45 0.993 0.992 6.00 96.04 0.656 0.368 6.09 60.73 0.726 0.891</cell></row><row><cell>SNGAN [3]</cell><cell cols="12">8.67 13.33 0.985 0.976 8.71 51.15 0.900 0.702 5.41 47.75 0.754 0.912</cell></row><row><cell>SAGAN [29]</cell><cell cols="12">8.66 14.31 0.983 0.973 8.74 49.90 0.872 0.712 5.48 54.29 0.728 0.882</cell></row><row><cell>BigGAN [4]</cell><cell cols="12">9.81 8.08 0.993 0.992 12.78 32.03 0.948 0.868 4.98 18.30 0.924 0.967</cell></row><row><cell>ContraGAN [23]</cell><cell cols="12">9.70 8.22 0.993 0.991 13.46 28.55 0.974 0.881 5.34 21.16 0.935 0.942</cell></row><row><cell>ReACGAN</cell><cell cols="12">9.89 7.88 0.994 0.992 14.06 27.10 0.970 0.894 4.91 15.40 0.970 0.954</cell></row><row><cell>BigGAN + CR  *  [16]</cell><cell>-</cell><cell>11.48</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigGAN [4] + CR [16]</cell><cell cols="12">9.97 7.18 0.995 0.993 15.94 19.96 0.972 0.950 5.14 11.97 0.978 0.981</cell></row><row><cell>ContraGAN [23] + CR [16]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[9]</cell><cell cols="2">9.17 8.49</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BigGAN [4] + DiffAug [9]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Real Data</cell><cell>11.54</cell><cell></cell><cell></cell><cell></cell><cell>34.11</cell><cell></cell><cell></cell><cell></cell><cell>5.49</cell><cell></cell><cell></cell><cell></cell></row></table><note>9.59 8.55 0.992 0.972 15.81 19.21 0.983 0.941 4.90 11.08 0.984 0.967 ReACGAN + CR [16] 10.11 7.20 0.996 0.994 16.56 19.69 0.984 0.940 4.87 10.72 0.985 0.971 BigGAN + DiffAug*9.94 7.17 0.995 0.992 18.08 15.70 0.980 0.972 5.53 12.15 0.967 0.981 ContraGAN [23] +</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experiments using ImageNet [31] dataset. B.S. means batch size for training. (Left): Comparisons with previous cGAN approaches. (Right): Learning curves of BigGAN<ref type="bibr" target="#b3">[4]</ref>, ContraGAN<ref type="bibr" target="#b22">[23]</ref>, and ReACGAN (ours) which are trained using the batch size of 256.</figDesc><table><row><cell></cell><cell>Method</cell><cell>IS ?</cell><cell cols="3">ImageNet [31] FID ? F0.125 ? F8 ?</cell><cell></cell><cell></cell><cell></cell><cell cols="2">IS and FID scores during training</cell></row><row><cell>B.S.= 256</cell><cell cols="2">ACGAN  *  [20] SNGAN  *  [3] SAGAN  *  [29] BigGAN  *  [20] TAC-GAN  *  [20] ContraGAN  *  [23] ACGAN [18] SNGAN [3] SAGAN [29] BigGAN [4] ContraGAN [23] ReACGAN BigGAN [4] + DiffAug [9] 36.97 7.26 36.80 52.52 38.05 28.86 31.10 62.99 32.25 29.85 43.97 25.25 68.27 ReACGAN + DiffAug [9] 69.74</cell><cell>184.41 27.62 18.28 22.77 23.75 19.69 26.35 26.79 34.73 16.36 25.16 13.98 18.57 11.95</cell><cell>-----0.951 0.935 0.938 0.849 0.964 0.947 0.976 0.956 0.977</cell><cell>-----0.927 0.963 0.913 0.914 0.955 0.855 0.977 0.941 0.975</cell><cell>FID (lower is better)</cell><cell>130 50 70 90 110</cell><cell></cell><cell>FID Score BigGAN ContraGAN ReACGAN</cell><cell>IS Score BigGAN ContraGAN ReACGAN</cell><cell>15 45 25 35</cell><cell>IS (higher is better)</cell></row><row><cell>B.S.= 2048</cell><cell>BigGAN  *  [4] BigGAN  *  [25] Real Data BigGAN [4] ReACGAN</cell><cell>99.31 104.57 173.33 99.71 92.74</cell><cell>8.51 9.18 7.89 8.23</cell><cell>--0.985 0.991</cell><cell>--0.989 0.990</cell><cell></cell><cell>30 10</cell><cell>0k</cell><cell cols="3">Iteration 20k 40k 60k 80k 100k 120k 140k 160k 180k 200k 5</cell></row><row><cell cols="2">library</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Experiments on the effectiveness of D2D-CE loss compared with other conditioning losses.</figDesc><table><row><cell>Conditioning Method</cell><cell>CIFAR10 [30]</cell><cell>Tiny-ImageNet [32]</cell><cell>CUB200 [33]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Condition [4] 9.46 12.21 0.987 0.982 7.38 76.15 0.804 0.576 5.16 35.17 0.852 0.936 .991 13.46 28.55 0.974 0.881 5.34 21.16 0.935 0.942 Big + D2D-CE (ReACGAN) 9.89 7.88 0.994 0.992 14.06 27.10 0.970 0.894 4.91 15.40 0.970 0.954</figDesc><table><row><cell>(Abbreviated to Big)</cell><cell></cell><cell></cell></row><row><cell>Big + AC [18]</cell><cell>9.84 8.45</cell><cell>0.993 0.992 6.00 96.04 0.656 0.368 6.09 60.73 0.726 0.891</cell></row><row><cell>Big + PD [19]</cell><cell>9.81 8.08</cell><cell>0.993 0.992 12.78 32.03 0.948 0.868 4.98 18.30 0.924 0.967</cell></row><row><cell>Big + MH [26]</cell><cell>10.05 7.94</cell><cell>0.994 0.990 4.37 140.74 0.282 0.156 5.18 245.69 0.625 0.832</cell></row><row><cell>Big + 2C [23]</cell><cell>9.70 8.22</cell><cell>0.993 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Experiments to identify the consistent performance of D2D-CE on adversarial loss selection.</figDesc><table><row><cell>Adversarial Loss</cell><cell>Conditioning Method</cell><cell cols="4">CIFAR10 [30] IS ? FID ? F 0.125 ? F 8 ? Better?</cell><cell>IS ?</cell><cell cols="3">Tiny-ImageNet [32] FID ? F 0.125 ? F 8 ? Better?</cell></row><row><cell></cell><cell>PD [19]</cell><cell>9.75</cell><cell>8.29</cell><cell>0.993</cell><cell>0.991</cell><cell>8.27</cell><cell>58.85</cell><cell>0.816</cell><cell>0.713</cell></row><row><cell>Non-saturation [1]</cell><cell>2C [23]</cell><cell cols="2">9.30 10.47</cell><cell>0.990</cell><cell>0.959</cell><cell>6.57</cell><cell>84.27</cell><cell>0.745</cell><cell>0.556</cell></row><row><cell></cell><cell>D2D-CE</cell><cell>9.79</cell><cell>8.27</cell><cell>0.993</cell><cell>0.991</cell><cell cols="2">11.76 39.32</cell><cell>0.942</cell><cell>0.852</cell></row><row><cell></cell><cell>PD [19]</cell><cell>9.94</cell><cell>8.26</cell><cell>0.993</cell><cell>0.992</cell><cell cols="2">12.74 37.14</cell><cell>0.920</cell><cell>0.900</cell></row><row><cell>Least square [10]</cell><cell>2C [23]</cell><cell cols="2">8.66 12.18</cell><cell>0.986</cell><cell>0.941</cell><cell>9.58</cell><cell>53.10</cell><cell>0.916</cell><cell>0.706</cell></row><row><cell></cell><cell>D2D-CE</cell><cell>9.70</cell><cell>9.56</cell><cell>0.991</cell><cell>0.987</cell><cell>9.50</cell><cell>57.67</cell><cell>0.848</cell><cell>0.692</cell></row><row><cell></cell><cell>PD [19]</cell><cell cols="2">5.71 64.75</cell><cell>0.792</cell><cell>0.652</cell><cell>6.67</cell><cell>84.16</cell><cell>0.696</cell><cell>0.498</cell></row><row><cell>W-GP [13]</cell><cell>2C [23]</cell><cell cols="2">5.93 55.99</cell><cell>0.842</cell><cell>0.709</cell><cell>6.89</cell><cell>74.45</cell><cell>0.812</cell><cell>0.536</cell></row><row><cell></cell><cell>D2D-CE</cell><cell cols="2">7.30 35.94</cell><cell>0.942</cell><cell>0.847</cell><cell>8.92</cell><cell>52.74</cell><cell>0.856</cell><cell>0.689</cell></row><row><cell></cell><cell>PD [19]</cell><cell>9.81</cell><cell>8.08</cell><cell>0.993</cell><cell>0.992</cell><cell cols="2">12.78 32.03</cell><cell>0.948</cell><cell>0.868</cell></row><row><cell>Hinge [46]</cell><cell>2C [23]</cell><cell>9.70</cell><cell>8.22</cell><cell>0.993</cell><cell>0.991</cell><cell cols="2">13.46 28.55</cell><cell>0.974</cell><cell>0.881</cell></row><row><cell></cell><cell>D2D-CE</cell><cell>9.89</cell><cell>7.88</cell><cell>0.994</cell><cell>0.992</cell><cell cols="2">14.06 27.10</cell><cell>0.970</cell><cell>0.894</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>FID<ref type="bibr" target="#b33">[34]</ref> values of conditional StyleGAN2 (cStyleGAN2)<ref type="bibr" target="#b6">[7]</ref> and StyleGAN2 [7] + D2D-CE on CIFAR10<ref type="bibr" target="#b29">[30]</ref> and AFHQ<ref type="bibr" target="#b40">[41]</ref> datasets. ADA means the adaptive discriminator augmentation<ref type="bibr" target="#b7">[8]</ref>. FID is exceptionally computed using the training split for calculating the reference moments since FID value of StyleGAN2 is often calculated using the moments of the training dataset.</figDesc><table><row><cell>Conditioning method</cell><cell cols="2">CIFAR10 [30] AFHQ [41]</cell></row><row><cell>cStyleGAN2 [7]</cell><cell>3.88</cell><cell>-</cell></row><row><cell>StyleGAN2 [7] + D2D-CE</cell><cell>3.34</cell><cell>-</cell></row><row><cell>cStyleGAN2 [7] + ADA [8]</cell><cell>2.43</cell><cell>4.99</cell></row><row><cell>StyleGAN2 [7] + ADA [8] + D2D-CE</cell><cell>2.38</cell><cell>4.95</cell></row><row><cell>StyleGAN2 [7] + DiffAug [9] + D2D-CE + Tuning</cell><cell>2.26</cell><cell></cell></row></table><note>- Effect of D2D-CE for Different GAN Architectures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on feature map normalization, data-to-data consideration, and easy positive and negative sample suppression. Eq.(6)) 14.06 27.10 0.970 0.894 4.91 15.40 0.970 0.954 63.16 14.59 0.974 0.974</figDesc><table><row><cell>Ablation</cell><cell>Tiny-ImageNet [32]</cell><cell>CUB200 [33]</cell><cell></cell><cell cols="2">ImageNet [49]</cell></row><row><cell>ACGAN [18]</cell><cell cols="6">6.00 96.04 0.656 0.368 6.09 60.73 0.726 0.891 62.99 26.35 0.935 0.963</cell></row><row><cell>+ Normalization</cell><cell cols="6">13.46 30.33 0.955 0.889 4.78 25.54 0.883 0.952 18.16 36.40 0.879 0.787</cell></row><row><cell cols="3">+ Data-to-data (Eq. (5)) 12.96 28.71 0.967 0.863 5.08 25.12 0.894 0.946</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>+ Suppression (-Data-to-data</cell><cell cols="2">12.96 30.79 0.960 0.857 5.39 30.36 0.863 0.947</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note>IS ? FID ? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ?poor FID, F 0.125 , and F 8 values compared with the model trained with normalization, data-to-data consideration, and the suppression technique. This result is consistent with the qualitative results on ImageNet, where the images from ACGAN are easily classifiable, but the images from ReACGAN are high quality and diverse.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Batch size: N . Temperature: ? . Margin values: m p , m n . Balance coefficient: ?. Parameters of the generator: ?. Parameters of the discriminator (feature extractor + adversarial head + linear head): ?. # of discriminator updates per single generator update: n dis . Adversarial loss: L Adv [1, 10, 13, 46]. Learning rate: ? 1 , ? 2 . Adam hyperparameters [50]: ? 1 , ? 2 . Output: Optimized (?, ?). Initialize (?, ?) 2: for {1, ..., # of training iterations} do 3:for {1, ..., n dis } do</figDesc><table><row><cell>Appendices</cell></row><row><cell>A Algorithm</cell></row><row><cell>Algorithm 1 : Training ReACGAN</cell></row><row><cell>Input:</cell></row></table><note>). The experimental results verify the superiority of ReACGAN compared with the existing classifier-and projection-based GANs on five benchmark datasets. Moreover, exhaustive analyses on ReACGAN prove that ReACGAN is robust to hyperpa- rameter selection and harmonizes with various architectures and differentiable augmentations.1:4:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A1 :</head><label>A1</label><figDesc>Hyperparameter setups for cGAN training.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">The settings (A, C, E, I, K, M, P, R) are</cell></row><row><cell cols="5">commonly used practices in previous studies [29, 4, 23, 8]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Setting Batch size</cell><cell>Adam(? 1 , ? 2 , ? 1 , ? 2 ) [50]</cell><cell>n dis</cell><cell>(?, ? )</cell><cell cols="4">m p G_Ema [63] Ema start Total iterations</cell></row><row><cell>A</cell><cell>64</cell><cell>(2e-4, 2e-4, 0.5, 0.999)</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>True</cell><cell>1k</cell><cell>100k</cell></row><row><cell>B</cell><cell>128</cell><cell>(2.82e-4, 2.82e-4, 0.5, 0.999)</cell><cell>5</cell><cell>(0.5, 0.5)</cell><cell>0.98</cell><cell>True</cell><cell>1k</cell><cell>100k</cell></row><row><cell>C</cell><cell>64</cell><cell>(2e-4, 2e-4, 0.5, 0.999)</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>True</cell><cell>1k</cell><cell>200k</cell></row><row><cell>D</cell><cell>128</cell><cell>(2.82e-4, 2.82e-4, 0.5, 0.999)</cell><cell>5</cell><cell>(0.5, 0.5)</cell><cell>0.98</cell><cell>True</cell><cell>1k</cell><cell>200k</cell></row><row><cell>E</cell><cell>64</cell><cell>(2.5e-3, 2.5e-3, 0.0, 0.99)</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>True</cell><cell>0</cell><cell>200k</cell></row><row><cell>F</cell><cell>64</cell><cell>(2.5e-3, 2.5e-3, 0.0, 0.99)</cell><cell>2</cell><cell cols="2">(0.25, 0.25) 0.98</cell><cell>True</cell><cell>0</cell><cell>200k</cell></row><row><cell>G</cell><cell>64</cell><cell>(2.5e-3, 2.5e-3, 0.0, 0.99)</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>True</cell><cell>0</cell><cell>800k</cell></row><row><cell>H</cell><cell>64</cell><cell>(2.5e-3, 2.5e-3, 0.0, 0.99)</cell><cell>2</cell><cell cols="2">(0.25, 0.25) 0.98</cell><cell>True</cell><cell>0</cell><cell>800k</cell></row><row><cell>I</cell><cell>1024</cell><cell>(4e-4, 1e-4, 0.0, 0.999)</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>True</cell><cell>20k</cell><cell>100k</cell></row><row><cell>J</cell><cell>1024</cell><cell>(4e-4, 1e-4, 0.0, 0.999)</cell><cell>1</cell><cell cols="2">(0.75, 0.75) 1.0</cell><cell>True</cell><cell>20k</cell><cell>100k</cell></row><row><cell>K</cell><cell>256</cell><cell>(2e-4, 5e-5, 0.0, 0.999)</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>True</cell><cell>4k</cell><cell>40k</cell></row><row><cell>L</cell><cell>256</cell><cell>(2e-4, 5e-5, 0.0, 0.999)</cell><cell>2</cell><cell cols="2">(0.25, 0.25) 0.95</cell><cell>True</cell><cell>4k</cell><cell>40k</cell></row><row><cell>M</cell><cell>256</cell><cell>(2e-4, 5e-5, 0.0, 0.999)</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>True</cell><cell>20k</cell><cell>200k</cell></row><row><cell>N</cell><cell>256</cell><cell>(2e-4, 5e-5, 0.0, 0.999)</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>True</cell><cell>20k</cell><cell>600k</cell></row><row><cell>O</cell><cell>256</cell><cell>(2e-4, 5e-5, 0.0, 0.999)</cell><cell>2</cell><cell>(1.0, 0.5)</cell><cell>0.98</cell><cell>True</cell><cell>20k</cell><cell>600k</cell></row><row><cell>P</cell><cell>2048</cell><cell>(2e-4, 5e-5, 0.0, 0.999)</cell><cell>2</cell><cell>-</cell><cell>-</cell><cell>True</cell><cell>20k</cell><cell>200k</cell></row><row><cell>Q</cell><cell>2048</cell><cell>(2e-4, 5e-5, 0.0, 0.999)</cell><cell>2</cell><cell cols="2">(0.5, 0.25) 0.90</cell><cell>True</cell><cell>20k</cell><cell>200k</cell></row><row><cell>R</cell><cell>64</cell><cell>(2.5e-3, 2.5e-3, 0.0, 0.99)</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>True</cell><cell>0</cell><cell>200k</cell></row><row><cell>S</cell><cell>64</cell><cell>(2.5e-3, 2.5e-3, 0.0, 0.99)</cell><cell>2</cell><cell>(0.5, 0.5)</cell><cell>0.95</cell><cell>True</cell><cell>0</cell><cell>200k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A1</head><label>A1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="5">Linear search results for temperature value selection</cell><cell></cell><cell cols="5">Linear search results for negative threshold value selection</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR10 CIFAR10</cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CIFAR10</cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tiny-Imagenet Tiny-ImageNet CUB200 CUB200</cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tiny-Imagenet CUB200</cell></row><row><cell>FID values (lower is better)</cell><cell>10 15 20 25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FID values (lower is better)</cell><cell>10 15 20 25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.125</cell><cell>0.25</cell><cell>0.5 Temperature</cell><cell>0.75</cell><cell>1.0</cell><cell>0</cell><cell>0.75</cell><cell>0.9</cell><cell>0.95 Negative threshold Positive margin</cell><cell>0.98</cell><cell>1.0</cell></row></table><note>shows hyperparameter setups used in our experiments. The settings (A, C, E, G) are used for baseline experiments on CIFAR10: BigGAN [4], BigGAN with DiffAug [9], StyleGAN2 [7], and StyleGAN2 with ADA [8], the setting (I) on Tiny-ImageNet: BigGAN and BigGAN with DiffAug, the setting (K) on CUB200: BigGAN and BigGAN with DiffAug, the settings (M, N, P) on ImageNet: ACGAN/SNGAN/ContraGAN, BigGAN/ReACGAN/DiffAug-BigGAN/DiffAug-ReACGAN with a batch size of 256, and BigGAN with a batch size of 2048, and the setting (R) on AFHQ: StyleGAN2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A2 :</head><label>A2</label><figDesc>Experiments to identify the effectiveness of ReACGAN with TAC [20] on CIFAR10 [30] and Tiny-ImageNet [32] datasets. ImageNet [32] 6.00 96.04 7.62 65.99 14.06 27.10 13.71 26.14 Finally, to validate the effectiveness of TAC for ReACGAN on real datasets, we perform CIFAR10 and Tiny-ImageNet generation experiments. Contrary to our expectations,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FID ?</cell></row><row><cell>CIFAR10 [30]</cell><cell>9.84 8.45 9.78</cell><cell>8.01</cell><cell>9.89 7.88 9.70</cell><cell>7.94</cell></row><row><cell>Tiny-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Dataset ACGAN [18] TACGAN [20] ReACGAN ReACGAN + TAC [20] IS ? FID ? IS ? FID ? IS ? FID ? IS ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A3 :</head><label>A3</label><figDesc>Experiments for investigating the effect of D2D-CE for different architectures using CIFAR10<ref type="bibr" target="#b29">[30]</ref> and Tiny-ImageNet<ref type="bibr" target="#b31">[32]</ref> datasets. We report only FID<ref type="bibr" target="#b33">[34]</ref> for a compact expression.</figDesc><table><row><cell cols="4">Conditioning method Deep CNN [2] on CIFAR10 [30] ResNet [13] on CIFAR10 [30] ResNet [13] on Tiny-ImageNet [32]</cell></row><row><cell>AC [18]</cell><cell>20.35</cell><cell>13.04</cell><cell>87.84</cell></row><row><cell>PD [19]</cell><cell>19.49</cell><cell>13.47</cell><cell>47.88</cell></row><row><cell>2C [23]</cell><cell>21.47</cell><cell>14.38</cell><cell>40.56</cell></row><row><cell>D2D-CE (ReACGAN)</cell><cell>18.94</cell><cell>12.47</cell><cell>40.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table A4 :</head><label>A4</label><figDesc>Ablation study on the number of negative samples. FID score<ref type="bibr" target="#b33">[34]</ref> is used for evaluation.</figDesc><table><row><cell>Dataset</cell><cell cols="6">Masking probability p for negative samples in Eq. (6) p = 1.0 0.8 0.6 0.4 0.2 0.0</cell></row><row><cell>CIFAR10 [30]</cell><cell>11.15</cell><cell>8.07</cell><cell>8.11</cell><cell>7.83</cell><cell>8.01</cell><cell>7.88</cell></row><row><cell>Tiny-ImageNet [32]</cell><cell>60.74</cell><cell cols="4">30.04 28.64 28.59 28.68</cell><cell>27.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A5 :</head><label>A5</label><figDesc>Experiments for studying available cures for preventing the gradient exploding problem in ACGAN. FID<ref type="bibr" target="#b33">[34]</ref> scores are reported for evaluation. ? is a balance coefficient between adversarial learning and classifier training.</figDesc><table><row><cell>Dataset</cell><cell cols="8">? = 0.25 ? = 0.5 ? = 0.75 ? = 1 Normalization Feature clipping Gradient clipping D2D-CE</cell></row><row><cell>CIFAR100 [30]</cell><cell>12.30</cell><cell>13.61</cell><cell>15.60</cell><cell>16.92</cell><cell>13.17</cell><cell>17.47</cell><cell>40.23</cell><cell>12.25</cell></row><row><cell>Tiny-ImageNet [32]</cell><cell>62.86</cell><cell>92.05</cell><cell>104.34</cell><cell>98.75</cell><cell>28.04</cell><cell>57.65</cell><cell>108.30</cell><cell>27.10</cell></row><row><cell cols="9">Training Time per 100 Generator Updates. We investigate training times of BigGAN, ContraGAN,</cell></row><row><cell cols="9">and ReACGAN on ImageNet using 8 Nvidia V100 GPUs. The batch size is set to 2048. We identify</cell></row><row><cell cols="9">that ReACGAN brings in a slight computational overhead and takes about 1.05?1.1 longer time than</cell></row><row><cell cols="9">the other GANs. Specifically, BigGAN takes 17m 37s, ContraGAN 18m 24s, and ReACGAN 18m</cell></row><row><cell cols="3">52s per 100 generator updates.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table A6</head><label>A6</label><figDesc></figDesc><table><row><cell cols="6">: Top-1 and Top-5 ImageNet classification accuracies on generated images from AC-</cell></row><row><cell cols="6">GAN [18], BigGAN [4], ContraGAN [23], and ReACGAN (ours). We use ImageNet pre-trained</cell></row><row><cell cols="6">Inception-V3 model [67] as a classifier. To generate images from GANs, we use the best checkpoints</cell></row><row><cell cols="2">saved during 200k generator updates.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Real data (validation) ACGAN [18] BigGAN [4] ContraGAN [23] ReACGAN</cell></row><row><cell>Top-1 Accuracy (%)</cell><cell>70.822</cell><cell>62.412</cell><cell>29.994</cell><cell>2.866</cell><cell>23.210</cell></row><row><cell>Top-5 Accuracy (%)</cell><cell>89.574</cell><cell>84.899</cell><cell>53.842</cell><cell>11.482</cell><cell>51.602</cell></row><row><cell>IS [42] ?</cell><cell>173.33</cell><cell>62.99</cell><cell>28.63</cell><cell>25.25</cell><cell>50.30</cell></row><row><cell>FID [34] ?</cell><cell>-</cell><cell>26.35</cell><cell>24.68</cell><cell>25.16</cell><cell>16.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table A7 .</head><label>A7</label><figDesc>Since we have conducted a lot of experiments with various configurations using different resources, we divide our experiments into 16 divisions and calculate approximate time spent on each division of experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table A7 :</head><label>A7</label><figDesc>Approximate total training time (days) provided for reference.</figDesc><table><row><cell>Division of experiments</cell><cell>GPU Type</cell><cell cols="3">Days # of experiments Approximate Time (days)</cell></row><row><cell>CIFAR10 [30]</cell><cell>RTX 2080 Ti</cell><cell>0.75</cell><cell>100</cell><cell>75</cell></row><row><cell>CIFAR10 [30] + CR [16]</cell><cell>RTX 2080 Ti</cell><cell>1.17</cell><cell>9</cell><cell>10.53</cell></row><row><cell>CIFAR10 [30] + DiffAug [9]</cell><cell>RTX 2080 Ti</cell><cell>2.04</cell><cell>9</cell><cell>18.36</cell></row><row><cell>CIFAR10 [30] + StyleGAN2 [7]</cell><cell>TITAN Xp?2</cell><cell>2.58</cell><cell>2</cell><cell>5.16</cell></row><row><cell>CIFAR10 [30] + StyleGAN2 [7] + ADA [8]</cell><cell>TITAN Xp?2</cell><cell>9.52</cell><cell>2</cell><cell>19.04</cell></row><row><cell>Tiny-ImageNet [32]</cell><cell cols="2">TITAN RTX?4 1.54</cell><cell>84</cell><cell>132.44</cell></row><row><cell>Tiny-ImageNet [32] + CR [16]</cell><cell cols="2">TITAN RTX?4 1.42</cell><cell>9</cell><cell>12.78</cell></row><row><cell>Tiny-ImageNet [32] + DiffAug [9]</cell><cell cols="2">TITAN RTX?4 2.83</cell><cell>9</cell><cell>25.47</cell></row><row><cell>CUB200 [33]</cell><cell cols="2">TITAN RTX?4 1.63</cell><cell>24</cell><cell>39.12</cell></row><row><cell>CUB200 [33] + CR [16]</cell><cell cols="2">TITAN RTX?4 0.92</cell><cell>9</cell><cell>8.28</cell></row><row><cell>CUB200 [33] + DiffAug [9]</cell><cell cols="2">TITAN RTX?4 0.67</cell><cell>9</cell><cell>6.03</cell></row><row><cell>ImageNet [31] (200k iter., B.S.=256)</cell><cell>Tesla V100?4</cell><cell>4.17</cell><cell>6</cell><cell>25.02</cell></row><row><cell>ImageNet [31] (600k iter., B.S.=256)</cell><cell cols="2">Tesla V100?4 12.51</cell><cell>2</cell><cell>25.02</cell></row><row><cell>ImageNet [31] + DiffAug [9] (600k iter., B.S.=256)</cell><cell>A100?4</cell><cell>12.43</cell><cell>2</cell><cell>24.86</cell></row><row><cell>ImageNet [31] (B.S.=2048)</cell><cell cols="2">Tesla V100?8 26.90</cell><cell>2</cell><cell>53.80</cell></row><row><cell>AFHQ [41] + StyleGAN2 [7] + ADA [8]</cell><cell>A100?4</cell><cell>1.96</cell><cell>2</cell><cell>3.92</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell>280</cell><cell>484.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>table .</head><label>.</label><figDesc></figDesc><table><row><cell>Method</cell><cell>CIFAR10 [30]</cell><cell>Tiny-ImageNet [32]</cell><cell>CUB200 [33]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table A9 :</head><label>A9</label><figDesc>Experiments on the effectiveness of D2D-CE loss compared with other conditioning losses. FID? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ?</figDesc><table><row><cell cols="5">CIFAR10 [30] 0.12 0.20 IS ? BigGAN w/o Condition [4] Conditioning Method 0.001 0.001 0.79 11.69 0.055 0.097 0.23 7.00 Tiny-ImageNet [32] CUB200 [33] 0.031 0.015 (Abbreviated to Big)</cell></row><row><cell>Big + AC [18]</cell><cell>0.06 0.02</cell><cell>0.001 0.000 0.12 8.04</cell><cell>0.021 0.048 0.73 9.20</cell><cell>0.036 0.102</cell></row><row><cell>Big + PD [19]</cell><cell>0.08 0.04</cell><cell>0.001 0.004 2.35 4.48</cell><cell>0.014 0.068 0.11 2.83</cell><cell>0.020 0.008</cell></row><row><cell>Big + MH [26]</cell><cell>0.12 0.04</cell><cell cols="3">0.000 0.002 0.72 15.42 0.102 0.056 0.42 40.23 0.124 0.087</cell></row><row><cell>Big + 2C</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table A10 :</head><label>A10</label><figDesc>Experiments to identify the consistent performance of D2D-CE on adversarial loss selection. FID ? F 0.125 ? F 8 ? IS ? FID ? F 0.125 ? F 8 ?</figDesc><table><row><cell cols="3">Conditioning Method PD [19] IS ? Non-Saturation [1] Adversarial Loss 0.10 2C [23] 0.07</cell><cell cols="2">CIFAR10 [30] 0.12 0.001 0.08 0.001</cell><cell>0.002 0.41 0.006 0.98</cell><cell cols="2">Tiny-ImageNet [32] 1.56 0.029 6.72 0.020</cell><cell>0.032 0.034</cell></row><row><cell></cell><cell>D2D-CE</cell><cell>0.07</cell><cell>0.06</cell><cell>0.001</cell><cell>0.001 0.67</cell><cell>3.09</cell><cell>0.011</cell><cell>0.028</cell></row><row><cell></cell><cell>PD [19]</cell><cell>0.08</cell><cell>0.26</cell><cell>0.001</cell><cell>0.002 0.49</cell><cell>0.72</cell><cell>0.008</cell><cell>0.006</cell></row><row><cell>Least Square [10]</cell><cell>2C [23]</cell><cell>0.29</cell><cell>1.28</cell><cell>0.002</cell><cell cols="2">0.012 3.01 14.32</cell><cell>0.082</cell><cell>0.123</cell></row><row><cell></cell><cell>D2D-CE</cell><cell>0.05</cell><cell>0.23</cell><cell>0.002</cell><cell cols="2">0.002 2.56 16.55</cell><cell>0.105</cell><cell>0.172</cell></row><row><cell></cell><cell>PD [19]</cell><cell>0.30</cell><cell>2.82</cell><cell>0.010</cell><cell cols="2">0.024 2.05 37.22</cell><cell>0.261</cell><cell>0.234</cell></row><row><cell>W-GP [13]</cell><cell>2C [23]</cell><cell>0.27</cell><cell>7.35</cell><cell>0.056</cell><cell cols="2">0.031 2.20 33.35</cell><cell>0.187</cell><cell>0.190</cell></row><row><cell></cell><cell>D2D-CE</cell><cell>0.41</cell><cell>6.75</cell><cell>0.015</cell><cell cols="2">0.043 2.43 20.38</cell><cell>0.086</cell><cell>0.155</cell></row><row><cell></cell><cell>PD [19]</cell><cell>0.08</cell><cell>0.04</cell><cell>0.001</cell><cell>0.004 2.35</cell><cell>4.48</cell><cell>0.014</cell><cell>0.068</cell></row><row><cell>Hinge [46]</cell><cell>2C [23]</cell><cell>0.07</cell><cell>0.09</cell><cell>0.000</cell><cell>0.001 0.34</cell><cell>0.57</cell><cell>0.001</cell><cell>0.011</cell></row><row><cell></cell><cell>D2D-CE</cell><cell>0.10</cell><cell>0.07</cell><cell>0.001</cell><cell>0.002 0.07</cell><cell>0.56</cell><cell>0.003</cell><cell>0.009</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table A12 :</head><label>A12</label><figDesc>Experiments for investigating the effect of D2D-CE for different architectures using CIFAR10<ref type="bibr" target="#b29">[30]</ref> and Tiny-ImageNet<ref type="bibr" target="#b31">[32]</ref> datasets.</figDesc><table><row><cell cols="4">Conditioning method Deep CNN [2] on CIFAR10 [30] ResNet [13] on CIFAR10 [30] ResNet [13] on Tiny-ImageNet [32]</cell></row><row><cell>AC [18]</cell><cell>0.02</cell><cell>0.11</cell><cell>0.34</cell></row><row><cell>PD [19]</cell><cell>0.67</cell><cell>0.25</cell><cell>1.29</cell></row><row><cell>2C [23]</cell><cell>1.28</cell><cell>1.33</cell><cell>0.18</cell></row><row><cell>D2D-CE (ReACGAN)</cell><cell>0.03</cell><cell>0.23</cell><cell>0.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table A13 :</head><label>A13</label><figDesc>Ablation study on the number of negative samples.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Masking probability p for negative samples in Eq. (6) p = 1.0 0.8 0.6 0.4 0.2 0.0</cell></row><row><cell>CIFAR10 [30]</cell><cell>0.33</cell><cell>0.04 0.11 0.13 0.05</cell><cell>0.07</cell></row><row><cell>Tiny-ImageNet [32]</cell><cell>8.62</cell><cell>1.82 0.76 0.92 0.87</cell><cell>0.56</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PyTorch-StudioGAN is an open-source library under the MIT license (MIT) with the exception of Style-GAN2 and StyleGAN2 + ADA related implementations, which are under the NVIDIA source code license.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use the validation split as the default reference set, but we use the test split of CIFAR10 and the training split of CUB200 and AFHQ due to the absence or lack of the validation dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>arXiv 1511.06434</idno>
		<title level="m">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral Normalization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logan</surname></persName>
		</author>
		<idno>arXiv 1912.00953</idno>
		<title level="m">Latent Optimisation for Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training Generative Adversarial Networks with Limited Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for dataefficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Least Squares Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno>arXiv 1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards Principled Methods for Training Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><forename type="middle">D</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<title level="m">On Convergence and Stability of GANs. arXiv preprint arXiv 1705.07215</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lipschitz generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiadong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7584" to="7593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Consistency Regularization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv 2002.04724</idno>
		<title level="m">Improved Consistency Regularization for GANs</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional Image Synthesis with Auxiliary Classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">cGANs with Projection Discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Twin Auxilary Classifiers GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Whitening and Coloring Batch Transform for GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Few-Shot Unsupervised Image-to-Image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ContraGAN: Contrastive Learning for Conditional Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minguk</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CircleGAN: Generative Adversarial Learning across Spherical Circles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woohyeon</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Omni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13074</idno>
		<title level="m">On the Secrets of cGANs and Beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A multi-class hinge loss for conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10060</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">cGANs with Auxiliary Discriminative Classifier. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dual Projection Generative Adversarial Networks for Conditional Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasis</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-Attention Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Tiny ImageNet Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
		<ptr target="https://tiny-imagenet.herokuapp.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training Generative Neural Samplers using Variational Divergence Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka. F-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazar</forename><surname>Valkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles A</forename><surname>Michael U Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno>arXiv 1411.1784</idno>
		<title level="m">Conditional Generative Adversarial Nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Activation Maximization Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Kan Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>arXiv 2002.05709</idno>
		<title level="m">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minguk</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch-Studiogan</surname></persName>
		</author>
		<ptr target="https://github.com/POSTECH-CVLab/PyTorch-StudioGAN" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">StarGAN v2: Diverse Image Synthesis for Multiple Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A Learned Representation For Artistic Style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geometric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno>arXiv 1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05527</idno>
		<title level="m">Score Matching Model for Unbounded Data Score</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>arXiv 1412.6980</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Classification Accuracy Score for Conditional Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Suman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improved Precision and Recall Metric for Assessing Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Reliable Fidelity and Diversity Metrics for Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Muhammad Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On self-supervised image representations for gan evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Training GANs with Stronger Augmentations via Contrastive Discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bootstrap Your Own Latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mixed Precision Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Freeze the discriminator: a simple baseline for fine-tuning gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwoo</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10964</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Closed-form factorization of latent semantics in gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The Unusual Effectiveness of Averaging in GAN Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yaz?c?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Sheng</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Piliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chandrasekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Debiased contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Boosting Contrastive Self-Supervised Learning with False Negative Cancellation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khademi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11765</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Contrastive learning with hard negative samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Controllable artistic text style transfer via shape-matching gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adversarial scene editing: Automatic object removal from weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Sketchygan: Towards diverse and realistic sketch to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Cartoongan: Generative adversarial networks for photo cartoonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teck Yian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5485" to="5493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Deep learning for deepfakes creation and detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><forename type="middle">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dung</forename><forename type="middle">Tien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc</forename><forename type="middle">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nahavandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11573</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fairgan</surname></persName>
		</author>
		<title level="m">Fairness-aware Generative Adversarial Networks. International Conference on Big Data (Big Data)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="570" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Royston Marian Mascarenhas, Shenoy Pratik Gurudatt, and Wael AbdAlmageed. Two-branch recurrent network for isolating deepfakes in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Killekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A selfsupervised approach for adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="262" to="271" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
