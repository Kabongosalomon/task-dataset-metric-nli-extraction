<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Effectiveness of Compact Biomedical Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-07">7 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Rohanian</surname></persName>
							<email>omid.rohanian@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadmahdi</forename><surname>Nouriborji</surname></persName>
							<email>m.nouriborji@nlpie.com</email>
							<affiliation key="aff3">
								<orgName type="institution">NLPie Research</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Kouchaki</surname></persName>
							<email>samaneh.kouchaki@surrey.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. Electrical and Electronic Engineering</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
							<email>david.clifton@eng.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Oxford-Suzhou Centre for Advanced Research</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Effectiveness of Compact Biomedical Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-07">7 Sep 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language models pre-trained on biomedical corpora, such as BioBERT, have recently shown promising results on downstream biomedical tasks. Many existing pre-trained models, on the other hand, are resource-intensive and computationally heavy owing to factors such as embedding size, hidden dimension, and number of layers. The natural language processing (NLP) community has developed numerous strategies to compress these models utilising techniques such as pruning, quantisation, and knowledge distillation, resulting in models that are considerably faster, smaller, and subsequently easier to use in practice. By the same token, in this paper we introduce six lightweight models, namely, BioDistilBERT, BioTiny-BERT, BioMobileBERT, DistilBioBERT, TinyBioBERT, and CompactBioBERT which are obtained either by knowledge distillation from a biomedical teacher or continual learning on the Pubmed dataset via the Masked Language Modelling (MLM) objective. We evaluate all of our models on three biomedical tasks and compare them with BioBERT-v1.1 to create efficient lightweight models that perform on par with their larger counterparts. All the models will be publicly available on our Huggingface profile at https://huggingface.co/nlpie and the codes used to run the experiments will be available on our Github page.</p><p>Recent advances in Natural Language Processing (NLP) and deep learning have made it possible to computationally process biomedical texts as varied as the above using powerful generic methods ? The two authors contributed equally to this work.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been an ever-increasing abundance of medical texts in recent years, both in private and public domains, which provide researchers with the opportunity to automatically process and extract useful information to help develop better diagnostic and analytic tools <ref type="bibr" target="#b29">(Locke et al., 2021)</ref>. Medical corpora can come in various forms, each with its own specific context. These include Electronic Health Records (EHR), medical texts on social media, online knowledge bases, and scientific literature <ref type="bibr" target="#b22">(Kalyan and Sangeetha, 2020)</ref>.  <ref type="figure">Figure 1</ref>: The two general strategies proposed for training compact biomedical models. The first approach is to directly distil a compact model from a biomedical teacher which in our work is BioBERT-v1.1. The distillation depicted in this figure is the same technique used for obtaining Dis-tilBioBERT. TinyBioBERT and CompactBioBERT, on the other hand, employ different approaches, which are not shown here. The second method involves additionally pre-training a compact model on biomedical corpora. For this approach, we use compact models which have been distilled from powerful teachers, namely, DistilBERT <ref type="bibr" target="#b36">(Sanh et al., 2019)</ref>, TinyBERT <ref type="bibr" target="#b21">(Jiao et al., 2020)</ref>, and Mo-bileBERT <ref type="bibr" target="#b43">(Sun et al., 2020)</ref>.</p><p>that learn text representations based on contextual information around each word. These methods alleviate the need for cumbersome feature engineering or extensive preprocessing, and when combined with the appropriate GPU technology, can handle large volumes of data with a high level of efficiency <ref type="bibr" target="#b50">(Wu et al., 2020)</ref>. Contextualised embeddings like ELMo <ref type="bibr" target="#b34">(Peters et al., 2018)</ref> and BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, while having been derived primarily using a generic language modelling objective, are able to capture task-agnostic and generalisable syntactic and semantic properties of words in their context, making them useful for various downstream applications <ref type="bibr" target="#b11">(Ethayarajh, 2019;</ref><ref type="bibr" target="#b44">Tenney et al., 2019)</ref>.</p><p>In recent years, different 'probing' methods have been developed to study different aspects of word embeddings and understand their internal mechanics <ref type="bibr" target="#b8">(Conneau et al., 2018;</ref><ref type="bibr" target="#b20">Jawahar et al., 2019;</ref><ref type="bibr" target="#b7">Clark et al., 2019)</ref>. These studies have shown that BERT encapsulates a surprising amount of knowledge about the world and can be used to solve tasks that traditionally would require encoded information from knowledge-bases <ref type="bibr" target="#b35">(Rogers et al., 2020)</ref>. These models are not without their own drawbacks and come with certain limitations. For example, it has been shown that BERT does not understand negation by default <ref type="bibr" target="#b12">(Ettinger, 2020)</ref> or struggles with representations of numbers <ref type="bibr" target="#b48">(Wallace et al., 2019)</ref>. Regardless of these shortcomings, BERT and its different variants are still the state-of-theart in different areas of NLP.</p><p>With the advent of the transformers architecture <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>, the NLP community has moved towards utilising pre-trained models that could be used as a strong baseline for different tasks and also serve as a backbone to other sophisticated models. The standard procedure is to use a general model pre-trained on a very large amount of unstructured text and then fine-tune the model and adapt it to the specific characteristics of each task. Most state-of-the-art NLP models are based on this procedure.</p><p>A related alternative to the standard pretrain and fine-tune approach is domain-adaptive pretraining, which has been shown to be effective on different textual domains. In this paradigm, instead of finetuning the pretrained model on the task-specific labelled data, pretraining continues on the unlabeled training set. This allows a smaller pretraining corpus, but one that is assumed to be more relevant to the final task <ref type="bibr" target="#b15">(Gururangan et al., 2020)</ref>. This method is also known as continual learning, which refers to the idea of incrementally training models on new streams of data while retaining prior knowledge <ref type="bibr" target="#b33">(Parisi et al., 2019)</ref>.</p><p>NLP researchers working with biomedical data have naturally started to incorporate these techniques into their models. Apart from vanilla fine-tuning on medical texts, specialised BERT-based models have also been developed that are specifically trained on medical and clinical corpora. ClinicalBERT <ref type="bibr" target="#b19">(Huang et al., 2019)</ref>, SciBERT <ref type="bibr" target="#b0">(Beltagy et al., 2019a)</ref>, and BioBERT <ref type="bibr" target="#b27">(Lee et al., 2020)</ref> are successful attempts at developing pretrained models that would be relevant to biomedical NLP tasks. They are regularly used in the literature to develop the latest best performing models on a wide range of tasks.</p><p>Regardless of the successes of these architectures, their applicability is limited because of the large number of parameters they have and the amount of resources required to employ them in a real setting. For this reason, there is a separate line of research in the literature to create compressed versions of larger pretrained models with minimal performance loss. DistilBERT <ref type="bibr" target="#b36">(Sanh et al., 2019)</ref>, MobileBERT <ref type="bibr" target="#b43">(Sun et al., 2020)</ref>, and TinyBERT <ref type="bibr" target="#b21">(Jiao et al., 2020)</ref> are prominent examples of such attempts, which aim to produce a lightweight version of BERT that closely mimics its performance while having significantly less trainable parameters. The process used in creating such models is called distillation <ref type="bibr" target="#b17">(Hinton et al., 2015)</ref>.</p><p>In this work we first train three distilled versions of the BioBERT-v1.1 using different distillation techniques, namely, DistilBioBERT, CompactBioBERT, and TinyBioBERT. Following that, we pretrain three well-known compact models (DistilBERT, TinyBERT, and MobileBERT) on the PubMed dataset using continual learning. The resultant models are called BioDistilBERT, BioTinyBERT, and BioMobileBERT. Finally, we compare our models to BioBERT-v1.1 through a series of extensive experiments on a diverse set of biomedical datasets and tasks. The analyses show that our models are efficient compressed models that can be trained significantly faster and with far fewer parameters compared to their larger counterparts, with minimal performance drops on different biomedical tasks.</p><p>To the best of our knowledge, this is the first attempt to specifically focus on training compact models on biomedical corpora and by making the models publicly available we provide the community with a resource to implement powerful specialised models in an accessible fashion.</p><p>The contributions of this paper can be summarised as follows:</p><p>? We are the first to specifically focus on training compact biomedical models using distillation and continual learning.</p><p>? Utilising continual learning via the Masked Language Modelling (MLM) objective, we train three well-known pre-trained compact models, namely DistilBERT, MobileBERT, and TinyBERT for 200k steps on the PubMed dataset.</p><p>? We distil three students from a biomedical teacher (BioBERT-v1.1) using three different distillation procedures, which generated the following models: DistilBioBERT, TinyBioBERT, and CompactBioBERT.</p><p>? We evaluate our models on a wide range of biomedical NLP tasks that include Named Entity Recognition (NER), Question Answering (QA), and Relation Extraction (RE).</p><p>? We make all of our 6 compact models freely available on Huggingface and Github. These models cover a wide range of parameter sizes, from 15M parameters for the smallest model to 65M for the largest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Pre-training followed by fine-tuning has become a standard procedure in many areas of NLP and forms the backbone for most state-of-the-art models such as BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>. The goal of language model pre-training is to acquire effective in-context representations of words based on a large corpus of text, such as Wikipedia. This process is often self-supervised, which means that the representations are learned without using human-provided labels. There are two main strategies for self-supervised pre-training, namely, MLM and Causal Language Modeling (CLM). In this work, we focus on models pre-trained with the MLM objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Masked Language Modeling</head><p>MLM is the process of randomly omitting portions of a given text and having the model predict the omitted portions. The masking percentage is normally 15%, with an 80% probability that the selected word will be substituted with a specific mask token (e.g. &lt;MASK&gt;) and a 20% chance that it will be replaced with another random word <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>. Contextualised representations generated using these pre-trained language models are referred to as bidirectional, which means that information from previous and following contexts is used to construct representations for each given word.</p><p>MLM utilises distributional hypothesis, an idea introduced originally by <ref type="bibr" target="#b16">Harris (1954)</ref> and later popularised by <ref type="bibr" target="#b13">Firth (1957)</ref>. The premise is that words that occur in the same contexts tend to have a similar meaning, or as Firth phrased it, "a word is characterised by the company it keeps". As a result, BERT shares conceptual similarities with other representation learning schemes in NLP. There is strong evidence to suggest that MLM relies on distributional semantic information significantly more than grammatical structure of sentences <ref type="bibr" target="#b39">(Sinha et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BERT: Bidirectional Encoder Representation from Transformers</head><p>The most prominent transformer pre-trained with MLM is BERT. BERT is an encoder-only transformer that relies on the Multi-Head Attention mechanism for learning in-context representations.</p><p>BERT has different variations such as BERT base and BERT large which vary in the number of layers and the size of the hidden dimension. Original BERT is trained on English Wikipedia and BooksCorpus datasets for about 1 million training steps, making it a strong model for various downstream NLP tasks.</p><p>Fine-tuning pre-trained BERT on a downstream task involves training the model for a few more epochs using a labelled dataset and with a lower learning rate <ref type="bibr" target="#b42">(Sun et al., 2019)</ref>. It has been shown that, since this procedure only affects the weights in the top layers of BERT, it will not lead to catastrophic forgetting of linguistic information <ref type="bibr" target="#b30">(Merchant et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">BioBERT and other Biomedical Models</head><p>While generic pre-trained language models can perform reasonably well on a variety of downstream tasks even in domains other than those on which they have been trained, in recent years researchers have shown that continual learning and pre-training of language models on domain-specific corpora leads to noticeable performance boosts compared to simple fine-tuning. BioBERT is an example of such a domain-specific BERT-based model and the first that is trained on biomedical corpora.</p><p>BioBERT takes its initial weights from BERT base (pre-trained on Wikipedia + Books) and is further pre-trained using the MLM objective on the PubMed and optionally PMC datasets. BioBERT has shown promising performance in many biomedical tasks including NER, RE, and QA. Aside from BioBERT, numerous additional models have been trained entirely or partially on biomedical data, including ClinicalBERT <ref type="bibr" target="#b19">(Huang et al., 2019)</ref>, SciBERT <ref type="bibr" target="#b1">(Beltagy et al., 2019b)</ref>, BioMedRoBERTa <ref type="bibr" target="#b15">(Gururangan et al., 2020)</ref>, and BioELECTRA <ref type="bibr" target="#b23">(Kanakarajan et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Overparametrisation of Language Models</head><p>The BERT base model has 110M parameters, which is a modest number compared to T5 (111B), GPT-3 (175B), or MT-NLG (530B). Training models of this magnitude comes with considerable financial and environmental costs. This trend is unlikely to be reversed anytime soon given the increasing computational power and the resources that large technology companies devote to creating such models <ref type="bibr" target="#b2">(Bender et al., 2021)</ref>. <ref type="bibr" target="#b41">Strubell et al. (2019)</ref> studied several major transformer-based models and estimated the carbon footprint and cloud compute costs incurred during their training. Warning against environmentally unfriendly practices in AI and NLP research has created interest in the community to develop lighter but computationally efficient models that come with minimal reduction in performance. This trend has been described as 'Green AI' <ref type="bibr" target="#b37">(Schwartz et al., 2020)</ref>. Model compression can be considered a step in this direction. It is predicated on the idea of creating a quick and compact model to imitate a slower, bigger, but more performant model <ref type="bibr" target="#b5">(Bucilua et al., 2006)</ref>. Several different model compression methods exist, with the aim to encode large models and create smaller more compact versions of them. The present work focuses on knowledge distillation but we will also briefly mention quantisation and pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Quantisation and Pruning</head><p>Quantisation is a technique that attempts to reduce the memory footprint of a pre-trained language model by reducing the precision of its weights and uses low bit hardware operations to speed up computation <ref type="bibr" target="#b38">(Shen et al., 2020)</ref>. It is an effective method for model compression and acceleration that can be applied to both pre-trained models or models trained from scratch <ref type="bibr" target="#b6">(Cheng et al., 2017)</ref>. This method requires hardware compatibility to function <ref type="bibr" target="#b35">(Rogers et al., 2020)</ref>.</p><p>Pruning is another model compression method that disables certain parts of a larger model to create a compressed faster version of it. It has been shown that zeroing out different parts of the multi-head attention mechanism in BERT does not result in a significant drop during inference time <ref type="bibr" target="#b31">(Michel et al., 2019)</ref>. Pruning can be performed in a structured way, where certain components of the model are removed, or in an unstructured fashion, where weights are dropped regardless of location in the network <ref type="bibr" target="#b35">(Rogers et al., 2020)</ref>. Since quantisation and pruning are independently developed and complementary to each other, they can be used in tandem to develop a single compressed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Knowledge Distillation</head><p>Knowledge distillation <ref type="bibr" target="#b17">(Hinton et al., 2015)</ref> is the process of transferring knowledge from a larger model called "teacher" to a smaller one called "student" using the larger model's outputs as soft labels. Distillation can be done in a task-specific way where the pre-trained model is first fine-tuned on a task and then the student attempts to imitate the teacher network. This is an effective method, however, fine-tuning of a pre-trained model can be computationally expensive. Task-agnostic distillation, on the other hand, allows the student to mimic the teacher by looking at its masked language predictions or intermediate representations. The student can subsequently be directly fine-tuned on the final task <ref type="bibr" target="#b49">(Wang et al., 2020;</ref><ref type="bibr" target="#b51">Yao et al., 2021)</ref>.</p><p>DistilBERT is a well-known example of a compressed model that uses knowledge distillation to transfer the knowledge within the BERT base model to a much smaller student network which is about 40% smaller and 60% faster. It uses a triple loss which is a linear combination of language modeling, distillation and cosine-distance losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this work, we focus on training compact transformers on biomedical corpora. Among the available compact models in the literature, we use DistilBERT, MobileBERT, and TinyBERT models which have shown promising results in NLP. We train compact models using two different techniques as shown in <ref type="figure">Figure 1</ref>. The first is continual learning of pre-trained compact models on biomedical corpora. In this strategy, each model is further pre-trained on the PubMed dataset for 200k steps via the MLM objective. The obtained models are named BioDistilBERT, BioMobile-BERT, and BioTinyBERT.</p><p>For the second strategy, we employ three distinct techniques: the DistilBERT and TinyBERT distillation processes, as well as a mixture of the two. The obtained models are named DistilBioBERT, TinyBioBERT, and CompactBioBERT. We test our models on three well-known biomedical tasks and compare them with BioBERT-v1.1 as shown in Tables 1 to 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>In this section, we describe the internal architecture of each compact model that is explored in the paper, the method used to initialise its weights, and the distillation procedure employed to train it. In this model, the size of the hidden dimension and the embedding layer are both set to 768. The vocabulary size is 28996 for the cased version which is the one employed in our experiments. The number of transformer layers is 6 and the expansion rate of the feed-forward layer is 4. Overall this model has around 65 million parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Initialisation of the Student</head><p>Effective initialisation of the student model is critical due to the size of the model and the computational cost of distillation. As a result, there are numerous techniques available for initialising the student. One method introduced by <ref type="bibr" target="#b46">Turc et al. (2019)</ref> is to initialise the student via MLM pretraining and then perform distillation. Another approach, which we have followed in this work, is to take a subset of the larger model by using the same embedding weights and initialising the student from the teacher by taking weights from every other layer <ref type="bibr" target="#b36">(Sanh et al., 2019)</ref>. With this approach, the hidden dimension of the student is restricted to that of the teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Distillation Procedure</head><p>For distillation, we mainly follow the work of <ref type="bibr" target="#b36">Sanh et al. (2019)</ref> in which the loss is a combination of three different terms. In this section, we explain each of these in detail. The first term is normal cross entropy loss used for the MLM objective which can be expressed with the below equation:</p><formula xml:id="formula_0">L mlm (X, Y ) = ? N n=1 W n ( |V | i=1 Y n i ln(f s (X) n i ))<label>(1)</label></formula><p>where X is the input of the model, Y denotes MLM labels which is a collection of N one-hot vectors each with the size of |V | where |V | is the size of the vocabulary of the model and N is the number of input tokens 1 and W n is 1 for masked tokens and 0 for others. This ensures that only masked tokens will contribute to the computation of loss. f s represents the student model whose output is a probability distribution vector with the size of the vocabulary (|V |) for each token.</p><p>The second loss term used for distillation is a KL Divergence loss over the outputs (aka soft labels) of the teacher model which can be expressed in the below equation where f t represents the teacher model:</p><formula xml:id="formula_1">L sof tMLM (X) = ? N n=1 W n D KL (f t (X) n i || f s (X) n i )<label>(2)</label></formula><p>Finally, there is an optional loss that is intended to align the last hidden state of the teacher and student models via a cosine embedding loss:</p><formula xml:id="formula_2">L align (X) = 1 N N n=1 1 ? ?(h t (X) n , h s (X) n )<label>(3)</label></formula><p>where h t and h s represent functions that output the last hidden state of the teacher and student models respectively (each of which is a collection of N , D-dimensional vectors where D is the size of the hidden dimension) and ? is a cosine similarity function 2 .</p><p>Finally, the combined distillation loss can be expressed as follows:</p><formula xml:id="formula_3">L(X, Y ) = ? 1 L mlm (X, Y ) (4) + ? 2 L sof tMLM (X) + ? 3 L align (X)</formula><p>where ? 1 , ? 2 and ? 3 are weighting terms for combining different losses. In our settings ? 1 = 2.0, ? 2 = 5.0, and ? 3 = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TinyBioBERT</head><p>This model uses a unique distillation method called 'transformer-layer distillation' which is applied on each layer of the student to align the attention maps and the hidden states of the student with the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Architecture</head><p>This model is available in two sizes: The first one is a 4-layer transformer with a hidden dimension and embedding size of 312 and about 15M parameters. The second is a 6-layer transformer with the same design as DistilBERT, as described in Section 4.1. This model contains around 30.5K words in its vocabulary and employs an uncased tokeniser, which means it does not include upper-cased letters in its vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Initialisation of the Student</head><p>The initial weight initialisation of this model is random since the hidden and the embedding size of this model differ from its teacher. However, the weight initialisation of the DistilBERT can be used when the hidden and embedding size of the student are the same as the ones in the teacher which to the best of our knowledge was not tried in the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Transformer-layer distillation</head><p>This distillation is applied on attention maps and outputs of each transformer layer of the student along with the final output layer and embedding layer of the student. Since the student is smaller than the teacher, the numbers of layers are not equal. As a result, each layer of the student will be mapped to a specific layer of the teacher with which the distillation will be performed. The mapping from the student layer index to the corresponding teacher layer index is determined by the equation below:</p><formula xml:id="formula_4">T i = g(i)<label>(5)</label></formula><p>where i is the index of the student layer, g(.) is the mapping function, and T i is the index of the respective transformer layer of the teacher. In both models, g(0) = 0 which is the index of the embedding layer and g(M + 1) = N + 1 which is the index of the output layer.</p><p>The mean squared error loss between each student layer and its corresponding layer in the teacher is calculated as follows:</p><formula xml:id="formula_5">L Layer (X, l) = M SE(h l s (X)W h , h g(l) t (X))<label>(6)</label></formula><formula xml:id="formula_6">+ 1 H H i=1 M SE(a l s (X) i , a g(l) t (X) i )</formula><p>where h l s (X) and h g(l) t (X) will output the hidden states of the l th layer of the student and the g(l) th of the teacher respectively. a l s (X) and a g(l) t (X) will output the attention maps of the l th layer of the student and the g(l) th of the teacher, respectively. Because these models use multi-head attention, we have H attention maps per layer, and the mean squared error is applied to each head independently, as shown in the Equation 6. Finally, W h is a projection weight used when the hidden dimensions of the student and the teacher are not the same.</p><p>In addition to the transformer-layer loss described above, TinyBERT use two additional losses, one for the embedding layer and one for the student's output probabilities. The embedding loss is designed to align the embedding of the student (E s ) with that of the teacher (E t ). This loss is only required if the student and teacher do not share the same embedding layer. The embedding loss is expressed in the below equation:</p><formula xml:id="formula_7">L Embed = M SE(E s W e , E t )<label>(7)</label></formula><p>where W e is a projection weight as discussed in Equation 6. TinyBERT employs one additional loss to align the final probability distributions of teacher and student, which is a cross entropy loss over the teacher's soft labels:</p><formula xml:id="formula_8">L output (X) = ? 1 N N n=1 |V | i=1 f t (X) n i ln(f s (X) n i )<label>(8)</label></formula><p>The complete loss function used for TinyBERT distillation is as follows:</p><formula xml:id="formula_9">L(X) = ? 0 L Embed (9) + M l=1 ? l L Layer (X, l) + ? (M+1) L output (X)</formula><p>where ? 0 to ? (M+1) are hyperparameters, controlling the importance of each layer. In this work all lambdas are set to 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CompactBioBERT</head><p>This model has the same overall architecture as DistilBioBERT (Section 4.1), with the difference that here we combine the distillation approaches of DistilBERT and TinyBERT. We utilise the same initialisation technique as in DistilBioBERT, and apply a layer-to-layer distillation with three major components, namely, MLM, layer, and output distillation.</p><p>Layer distillation is performed between each student layer and its corresponding teacher layer based on Equation 6, with the MSE losses substituted with cosine embedding loss for hidden states alignment and KL Divergence for attention maps alignment. Below is the final layer distillation loss proposed for CompactBioBERT:</p><formula xml:id="formula_10">L compact (X, l) = 1 N N n=1 1 ? ?(h l s (X) n , h g(l) t (X) n ) (10) + 1 HN H i=1 N n=1 D KL (a l s (X) i n || a g(l) t (X) i n )</formula><p>The MLM and output distillations are the same losses used in DistilBioBERT. MLM distillation corresponds to L mlm (X, Y ) in Equation 1 and L sof tMLM (X) denotes output distillation from Equation 2. Finally, the complete distillation loss used in CompactBioBERT is as follows:</p><formula xml:id="formula_11">L(X, Y ) = ? 1 L mlm (X, Y ) (11) + ? 2 L sof tMLM (X) + ? 3 M l=1 L compact (X, l)</formula><p>where ? 1 , ? 2 , and ? 3 are weighting terms for combining different losses. In our settings, ? 1 = 1.0, ? 2 = 5.0, and ? 3 = 3.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">BioMobileBERT</head><p>MobileBERT <ref type="bibr" target="#b43">(Sun et al., 2020</ref>) is a compact model that uses a unique design comprised of different components to reduce the model's width (hidden size) while maintaining the same depth as BERT large (24 Transformer Layers). MobileBERT has proved to be competitive in many NLP tasks while also being efficient in terms of both computational and parameter complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Architecture and Initialisation</head><p>MobileBERT uses a 128-dimensional embedding layer followed by 1D convolutions to up-project its output to the desired hidden dimension expected by the transformer blocks. For each of these blocks, MobileBERT uses linear down-projection at the beginning of the transformer block and upprojection at its end, followed by a residual connection originating from the input of the block before down-projection. Because of these linear projections, MobileBERT can reduce the hidden size and hence the computational cost of multi-head attention and feed-forward blocks. This model additionally incorporates up to four feed-forward blocks in order to enhance its representation learning capabilities. Thanks to the strategically placed linear projections, a 24-layer MobileBERT (which is used in this work) has around 25M parameters. To the best of our knowledge MobileBERT is initialised from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Distillation Procedure</head><p>MobileBERT uses layer-wise distillation similar to TinyBERT <ref type="bibr" target="#b21">(Jiao et al., 2020)</ref> and . Unlike TinyBERT, where the student's hidden dimension and number of layers may differ from those of the teacher, MobileBERT utilises a unique teacher named IB-BERT which has the same hidden dimension and number of layers as the student 3 . As a result, mapping each transformer layer in the student to its matching teacher layer is unnecessary.</p><p>The loss employed by MobileBERT for layer-wise distillation is shown below:</p><formula xml:id="formula_12">L mobile (X, l) = M SE(h l s (X), h l t (X)) (12) + 1 H H i=1 N n=1 D KL (a l s (X) i n || a l t (X) i n )</formula><p>The loss 4 used for distillation of the MobileBERT is as follows:</p><formula xml:id="formula_13">L(X, Y ) = ?L mlm (X, Y )<label>(13)</label></formula><formula xml:id="formula_14">+ (1 ? ?)( 1 M M l=1 L mobile (X, l))</formula><p>where M is the number of transformer layers and ? is a hyperparameter between (0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>We evaluate our models on three biomedical tasks, namely, NER, QE, and RE. For a fair comparison, we fine-tune all of our models using a constant seed. Note that the results obtained in this work are for comparison with BioBERT-v1.1 in a similar setting and we are not focusing on reproducing or outperforming state-of-the-art on any of the datasets since that is not the objective of this work.</p><p>We distil our students solely from BioBERT and also compare our continually learnt models with it. While there are other recent biomedical transformers available in the literature (Sec. 1), BioBERT is the most general (trained on large biomedical corpora for 1M steps) and is widely used as a backbone for building new architectures. Direct comparison with one major model helps us to keep the work focused on compression techniques and assessing their efficiency in preserving information from a well-performing and reliable teacher. These experiments can in the future be expanded to cover other biomedical models.</p><p>For biomedical NER we use 8 well-known datasets, namely, NCBI-disease <ref type="bibr" target="#b10">(Dogan et al., 2014)</ref>, BC5CDR (disease and chem) <ref type="bibr" target="#b28">(Li et al., 2016)</ref>, BC4CHEMD <ref type="bibr" target="#b25">(Krallinger et al., 2015)</ref>, BC2GM <ref type="bibr" target="#b40">(Smith et al., 2008)</ref>, JNLPBA <ref type="bibr" target="#b24">(Kim et al., 2004)</ref>, LINNAEUS <ref type="bibr" target="#b14">(Gerner et al., 2010)</ref>, and Species-800 <ref type="bibr" target="#b32">(Pafilis et al., 2013)</ref> which will test the biomedical knowledge of our models in different categories such as Disease, Drug/chem, Gene/protein, and Species. All of our models were trained for 5 epochs with a batch size of 16 and a learning rate of 5e ? 5. In a few cases, a learning rate of 3e ? 5 and a batch size of 32 were also used. Because our models contain word-piece tokenisers which may split a single word into several sub-word units, we assigned each word's label to all of its sub-words and then fine-tuned our models based on the new labels. As shown in <ref type="table" target="#tab_1">Table 1</ref>, DistilBioBERT and CompactBioBERT outperformed other distilled models on all the datasets. Among the continually learned models, BioDistilBERT and BioMobileBERT fared best <ref type="table" target="#tab_2">(Table 2)</ref>, while TinyBioBERT and BioTinyBERT were the fastest and most efficient models.</p><p>For RE we used the GAD <ref type="bibr" target="#b3">(Bravo et al., 2015)</ref> and CHEMPROT <ref type="bibr" target="#b26">(Krallinger et al., 2017)</ref> datasets and followed the same pre-processing used in <ref type="bibr" target="#b27">Lee et al. (2020)</ref>. On the GAD dataset, we randomly selected 10% of the data for the test set using a constant seed and used the rest for training. For both datasets, we trained all of our models for 3 epochs with learning rates of 5e ? 5 or 3e ? 5 and a batch size of 16. We used the latest version of CHEMPROT which has 13 different types of relations.</p><p>CompactBioBERT achieved the best results in both tasks among the distilled models <ref type="table" target="#tab_3">(Table 3)</ref>, and similarly, BioDistilBERT outperformed all of our continually trained models in both tasks <ref type="table" target="#tab_4">(Table 4)</ref>.</p><p>For QA, we used the BioASQ 7b dataset <ref type="bibr" target="#b45">(Tsatsaronis et al., 2015)</ref> and followed the same preprocessing steps as <ref type="bibr" target="#b27">Lee et al. (2020)</ref>. All the models were trained with a batch size of 16. For TinyBERT, TinyBioBERT, and BioTinyBERT a learning rate of 5e ? 5 was used while for the remaining models this value was set to 3e ? 5. As seen in <ref type="table">Table 5</ref>, among our distilled models</p><p>CompactBioBERT and TinyBioBERT performed best, and among our continually learned models BioMobileBERT and BioDistilBERT outperformed other distilled models <ref type="table">(Table 6</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this study, we investigated two approaches for compressing biological language models. The first strategy was to distil a model from a biomedical teacher, and the second was to use MLM pretraining to adapt an already distilled model to a biomedical domain. Due to computational and time constraints, we trained our distilled models for 100k steps and our continually learned models for 200k steps; as a result, directly comparing these two types of models may be unfair. We observed that distilling a compact model from a biomedical teacher increases its capacity to perform better on complex biomedical tasks while decreasing its general language understanding and reasoning. This means that while our distilled models perform exceptionally well on biomedical NER and RE <ref type="table" target="#tab_1">(Tables 1 and 3)</ref>, they perform comparatively poorly on tasks that require more general knowledge and language understanding such as biomedical QA <ref type="table">(Table 5)</ref>.</p><p>Weaker results on QA (compared to continually learned models) suggest that by distilling a model from scratch using a biomedical teacher, the model may lose some of its ability to capture complex grammatical and semantic features while becoming more powerful in identifying and understanding biomedical correlations in a given context (as seen in <ref type="table" target="#tab_3">Table 3</ref>). On the other hand, adapting already compact models to the biomedical domain via continual learning seems to preserve general knowledge regarding natural language structure and semantics in the model <ref type="table">(Table 6</ref>). It should be noted that the distilled models are only trained for 100k steps and this analysis is based on the current results obtained by these models.</p><p>Furthermore, despite having nearly half as many parameters, BioMobileBERT outscored BioDistil-BERT on QA. As previously stated, MobileBERT employs a unique structure that allows it to get as deep as 24 layers while maintaining less than 30M parameters. On the other hand, BioDistilBERT is only 6 layers deep. Because of this architectural difference, we hypothesise that the increased number of layers in BioMobileBERT allows it to capture more complex grammatical and semantic features, resulting in superior performance in biomedical QA, which requires not only biomedical knowledge but also some general understanding about natural language.</p><p>We trained models of varied sizes and topologies, ranging from small models with only 25M parameters to larger models with up to 65M. In our experiments, we discovered that when fine-tuned with a high learning rate (e.g. 5e ? 5), our tiny models, TinyBioBERT and BioTinyBERT, perform well on downstream tasks while our bigger models tend to perform better with a lower learning rate (e.g. 3e ? 5).</p><p>In addition, we found that compact models that have been trained on the PubMed dataset for fewer training steps (e.g. 50k) tend to achieve better results on more general biomedical datasets such as NCBI-disease which are annotated for disease mentions and concepts and perform worse on more specialised datasets like BC5CDR-disease and BC5CDR-chem which include extra domain-specific information (e.g. chemicals and chemical-disease interactions), and the reverse is true for the models that are trained longer on the PubMed dataset.</p><p>TinyBioBERT and BioTinyBERT are the most efficient models in terms of both memory and time complexity (as evidenced by <ref type="figure">Figure 4)</ref>. DistilBioBERT, CompactBioBERT, and BioDistilBERT are the second most efficient set of models in terms of time complexity. BioMobileBERT, on the other hand, is the second most efficient model with regards to memory complexity. In conclusion, if efficiency is the most important factor, the tiny models are the most suitable resources to use. In other use cases, we recommend either the distilled models or BioMobileBERT depending on the relative importance of memory, time, and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we employed a number of compression strategies to develop compact biomedical transformer-based models that proved competitive on a range of biomedical datasets. We introduced six different models ranging from 15M to 65M parameters and evaluated them on three different tasks. We found that competitive performance may be achieved by either pre-training existing compact models on biomedical data or distilling students from a biomedical teacher. The choice of distillation or pre-training is dependent on the task, since our pre-trained students outperformed their distilled counterparts in some tasks and vice versa. We discovered, however, that distillation from a biomedical teacher is generally more efficient than pre-training when using the same number of training steps. Due to computational and time constraints, we trained all of our distilled models for 100k steps, and for continual learning, we trained models for 200k steps. For future work, we plan to pre-train models for 500k to 1M steps and publicly release the new models. In addition, since CompactBioBERT and DistilBioBERT performed similarly on most of the tasks, we plan to investigate the effect of hyperparameters on training these models in order to determine which distillation technique is more efficient. Some of the compact biomedical models proposed in this study may be used for inference on mobile devices, which we hope will open new avenues for researchers with limited computational resources.   <ref type="table">Table 5</ref>: Test results of the models that were directly distilled from the BioBERT-v1.1 on the BioASQ QA dataset. The metrics used for reporting the results are taken from the BioASQ competition and the models were assessed using the same evaluation script. The metrics are as follows: Strict Accuracy (S), Lenient Accuracy (L) and Mean Reciprocal Rank (M).  <ref type="table">Table 6</ref>: BioASQ QA test results for the models that were pre-trained on the PubMed dataset via MLM objective and continual learning. The metrics used for reporting the results are taken from the BioASQ competition and the models were assessed using the same evaluation script. The metrics are as follows: Strict Accuracy (S), Lenient Accuracy (L) and Mean Reciprocal Rank (M) scores. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The inference time/memory comparison of our proposed models. 'small' refers to Tiny-BioBERT, 'mobile' to MobileBioBERT, 'distilled' to DistilBioBERT and CompactBioBERT (since they share the same architecture), and 'base' to BioBERT-v1.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test results for the models that were directly distilled from the BioBERT-v1.1 on NER datasets. The * symbol indicates that any direct comparison should take into account the fact that other models include over 60M parameters, whereas TinyBioBERT has only 15M.</figDesc><table><row><cell>Type</cell><cell>Task</cell><cell cols="2">Metrics DistilBERT</cell><cell cols="3">DistilBioBERT CompactBioBERT TinyBioBERT  *</cell><cell>BioBERT-v1.1</cell></row><row><cell>Disease Drug/chem. Gene/protein Species</cell><cell>NCBI disease BC5CDR BC5CDR BC4CHEMD BC2GM JNLPBA LINNAEUS Species-800</cell><cell>P R F P R F P R F P R F P R F P R F P R F P R F</cell><cell>85.02 87.78 86.38 81.57 82.47 82.01 92.11 92.90 92.50 90.91 88.19 89.53 83.93 85.29 84.61 73.37 85.90 79.14 83.42 78.21 80.73 73.61 70.51 72.03</cell><cell>86.74 89.14 87.93 84.34 86.54 85.42 94.04 95.04 94.53 92.48 91.06 91.77 86.11 87.10 86.60 74.36 86.49 79.97 86.32 80.45 83.29 75.76 73.70 74.72</cell><cell>86.91 90.50 88.67 84.76 86.01 85.38 94.03 94.60 94.31 91.97 90.83 91.40 85.55 87.90 86.71 73.84 86.98 79.88 85.22 80.70 82.90 76.21 75.19 75.70</cell><cell>82.11 88.57 85.22 79.91 82.71 81.28 91.31 93.09 92.20 88.77 89.29 89.03 80.49 84.65 82.52 72.58 86.07 78.75 78.08 78.51 78.29 67.89 71.36 69.59</cell><cell>87.23 90.07 88.62 85.81 87.54 86.67 94.47 95.00 94.73 92.77 91.51 92.14 87.07 88.17 87.62 74.81 86.72 80.33 87.61 80.60 83.96 76.74 79.03 77.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>NER test results for models that were pre-trained on the PubMed dataset via the MLM objective and continual learning. Note that the models beginning with the prefix 'Bio' are pretrained, while the rest are baselines.</figDesc><table><row><cell>Task</cell><cell>Metrics</cell><cell>DistilBERT</cell><cell>TinyBERT</cell><cell cols="4">MobileBERT BioDistilBERT BioTinyBERT BioMobileBERT</cell></row><row><cell>NCBI disease BC5CDR(disease) BC5CDR(chem) BC4CHEMD BC2GM JNLPBA LINNAEUS Species-800</cell><cell>P R F P R F P R F P R F P R F P R F P R F P R F</cell><cell>85.02 87.78 86.38 81.57 82.47 82.01 92.11 92.90 92.50 90.91 88.19 89.53 83.93 85.29 84.61 73.37 85.90 79.14 83.42 78.21 80.73 73.61 70.51 72.03</cell><cell>79.59 81.36 80.46 76.12 78.83 77.45 90.19 86.87 88.50 85.84 81.79 83.76 76.43 77.43 76.93 71.04 83.55 76.79 77.16 67.38 71.94 66.62 66.04 66.33</cell><cell>84.29 88.07 86.14 80.52 83.51 81.99 92.45 91.95 92.20 90.65 88.58 89.60 82.62 83.09 82.86 73.18 85.54 78.88 74.72 82.75 78.53 71.76 77.59 74.56</cell><cell>86.93 88.31 87.61 84.59 86.66 85.61 94.70 94.25 94.48 92.18 91.00 91.59 86.28 87.68 86.97 73.56 85.54 79.10 85.69 79.66 82.56 74.39 74.98 74.68</cell><cell>80.41 85.66 82.95 78.69 83.79 81.16 89.90 91.83 90.85 88.17 86.59 87.37 78.86 82.36 80.57 71.74 85.14 77.87 78.88 74.10 76.42 67.80 73.82 70.68</cell><cell>86.36 88.07 87.21 84.03 85.23 84.62 93.88 94.58 94.23 92.29 90.36 91.31 84.44 86.10 85.26 74.81 86.28 80.13 81.63 82.03 81.83 74.33 76.14 75.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test results of the models that were directly distilled from the BioBERT-v1.1 on RE datasets. The * symbol indicates that any direct comparison between TinyBioBERT and other models should account for the significance difference in model size (15M vs 60M ). Scores for GAD are in the binary mode and the metrics reported for CHEMPROT are macro-averaged.</figDesc><table><row><cell>Relation</cell><cell>Task</cell><cell cols="2">Metrics DistilBERT</cell><cell cols="3">DistilBioBERT CompactBioBERT TinyBioBERT  *</cell><cell>BioBERT-v1.1</cell></row><row><cell cols="3">Gene-disease Protein-chemical CHEMPROT P GAD P R F R F</cell><cell>77.60 88.15 82.54 47.41 47.89 47.52</cell><cell>78.76 93.03 85.30 49.90 50.30 49.79</cell><cell>80.18 91.63 85.52 52.74 52.93 52.46</cell><cell>77.20 88.50 82.46 31.02 33.61 30.33</cell><cell>79.82 95.12 86.80 52.00 53.03 52.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test results on RE datasets for the models that were pre-trained on PubMed via MLM objective and continual learning. Model names beginning with the prefix 'Bio' are pre-trained and the others are baselines. Scores for GAD are in the binary mode and the metrics reported for CHEMPROT are macro-averaged.</figDesc><table><row><cell>Task</cell><cell>Metrics</cell><cell>DistilBERT</cell><cell>TinyBERT</cell><cell cols="3">MobileBERT BioDistilBERT BioTinyBERT</cell><cell>BioMobileBERT</cell></row><row><cell>GAD CHEMPROT</cell><cell>P R F P R F</cell><cell>77.60 88.15 82.54 47.41 47.89 47.52</cell><cell>71.42 80.13 75.53 28.50 27.53 23.18</cell><cell>76.31 90.94 82.98 47.61 48.67 47.92</cell><cell>81.36 91.28 86.04 51.56 51.84 51.48</cell><cell>74.22 83.27 78.48 31.33 29.56 25.54</cell><cell>78.50 91.63 84.56 50.77 51.60 51.03</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that one-hot vectors for non-masked tokens are zero vectors. 2 Cosine similarity is expressed with the formula: ?( u, v) = u. v || u|| 2 || v|| 2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Since MobileBERT's teacher is a custom variant of the BERT large called IB-BERT, we were not able to distil a compact model with the same procedure as MobileBERT. Therefore, we solely pre-trained MobileBERT on the PubMed dataset via MLM objective and continual learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that the original formula contains a Next Sentence Prediction (NSP) loss term as well which is omitted here for brevity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SciBERT: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A survey of model compression and acceleration for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09282</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2126" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ncbi disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Dogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ethayarajh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Applications of general linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Philological Society</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linnaeus: a species name identification system for biomedical literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">Distilling the knowledge in a neural network. cite arxiv:1503.02531Comment: NIPS 2014 Deep Learning Workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Clinicalbert: Modeling clinical notes and predicting hospital readmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05342</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What does bert learn about the structure of language?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TinyBERT: Distilling BERT for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Secnlp: A survey of embeddings in clinical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Kalyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sangeetha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">103323</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BioELECTRA:pretrained biomedical text encoder using discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Kanakarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Workshop on Biomedical Language Processing</title>
		<meeting>the 20th Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to the bio-entity recognition task at jnlpba</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international joint workshop on natural language processing in biomedicine and its applications</title>
		<meeting>the international joint workshop on natural language processing in biomedicine and its applications</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The chemdner corpus of chemicals and drugs and its annotation principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of the biocreative vi chemical-protein interaction track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth BioCreative challenge evaluation workshop</title>
		<meeting>the sixth BioCreative challenge evaluation workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="141" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Natural language processing in medicine: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Locke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Anaesthesia and Critical Care</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="4" to="9" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What happens to bert embeddings during fine-tuning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Are sixteen heads really better than one? Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The species and organisms resources for fast and accurate identification of taxonomic names in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pafilis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">65390</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Continual lifelong learning with neural networks: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Parisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="54" to="71" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A primer in bertology: What we know about how bert works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Green ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Q-bert: Hessian based ultra low precision quantization of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8815" to="8821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Masked language modeling and the distributional hypothesis: Order word matters pre-training for little</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2888" to="2913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Overview of biocreative ii gene mention recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How to fine-tune bert for text classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China national conference on Chinese computational linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="194" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MobileBERT: a compact task-agnostic BERT for resource-limited devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2158" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06316</idno>
		<title level="m">What do you learn from context? probing for sentence structure in contextualized word representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An overview of the bioasq large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Well-read students learn better: On the importance of pre-training compact models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Turc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Do nlp models know numbers? probing numeracy in embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5307" to="5315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5776" to="5788" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep learning in clinical natural language processing: a methodical review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="457" to="470" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adapt-and-distill: Developing small, fast and effective pretrained language models for domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="460" to="470" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
