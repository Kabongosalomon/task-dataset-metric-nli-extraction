<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ENHANCING PROTOTYPICAL FEW-SHOT LEARNING BY LEVERAGING THE LOCAL-LEVEL STRATEGY</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-Sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ENHANCING PROTOTYPICAL FEW-SHOT LEARNING BY LEVERAGING THE LOCAL-LEVEL STRATEGY</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Few-shot Learning</term>
					<term>Image Classification</term>
					<term>Local-level Feature</term>
					<term>Knowledge Transfer</term>
					<term>Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aiming at recognizing the samples from novel categories with few reference samples, few-shot learning (FSL) is a challenging problem. We found that the existing works often build their few-shot model based on the image-level feature by mixing all local-level features, which leads to the discriminative location bias and information loss in local details. To tackle the problem, this paper returns the perspective to the locallevel feature and proposes a series of local-level strategies. Specifically, we present (a) a local-agnostic training strategy to avoid the discriminative location bias between the base and novel categories, (b) a novel local-level similarity measure to capture the accurate comparison between local-level features, and (c) a local-level knowledge transfer that can synthesize different knowledge transfers from the base category according to different location features. Extensive experiments justify that our proposed local-level strategies can significantly boost the performance and achieve 2.8%-7.2% improvements over the baseline across different benchmark datasets, which also achieves the state-of-the-art accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Few-shot learning aims to recognize samples from novel categories with only a few labeled samples. Existing methods can be roughly divided into two main streams: (a) Metricbased approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> focus on learning an appropriate feature space for both base and novel categories, and then classify the query data according to their distance between support data on the feature space. ProtoNet <ref type="bibr" target="#b2">[3]</ref> proposes to build a prototype for each category and recognize by the nearest neighbor rule. The recent FEAT <ref type="bibr" target="#b7">[8]</ref> also proposes to refine the prototype by a Transformer module, and Deep-EMD proposes a high-complexity distance. (b) Optimizationbased approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> focus on improving the transferability of the fully trained model and transferring the knowledge of base categories to novel categories. For example, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> propose an attention module to refine query features/prototypes by the knowledge of the base category. Although these methods have achieved encouraging success, we observe that they often follow the common routine in the traditional image classification task, i.g., extracting the image-level feature vector by global pooling <ref type="bibr" target="#b13">[14]</ref> and then building their models on the image-level feature space. Under the fully supervised training by a large amount of labeled data, global pooling can improve the performance by driving the model to focus attention on the local features considered discriminative and suppress the features on the insignificant location. For example, in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, the high-weight local features are only distributed at concentrated locations. However, these discriminative features are distributed in different locations for different categories. We call this difference discriminative location bias. Few-shot learning requires the model to recognize the object belonging to the novel category. Hence, the discriminative location bias will cause the model to overpay attention to the location beneficial to the base category while ignoring the discriminative features of the novel category. As shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, the model trained with global pooling only pays attention to the insignificant location on the novel category images, resulting in the wrong classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>*Corresponding author</head><p>To avoid the discriminative location bias, we first propose a local-level classification loss and a local-level regularization loss to realize local-agnostic training (LAT), which forces the model to pay attention to all locations instead of only the discriminative location. As shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>, the high-weight local features from the model trained with the LAT strategy can cover the complete object of novel categories.</p><p>Furthermore, during the inference phase, the image-level features of the novel category are either misled by the discriminative location bias (from the model trained with global pooling) or confused by too much information (from the model trained with LAT strategy). Meanwhile, the image-level feature vector also destroys the local details in the image. Therefore, we further propose two local-level strategies during inference for a comprehensive understanding of novel category images. First, we propose a hybrid local-level similarity measure (LSM) to measure the similarity between local-level features accurately, composed of local distance and matching distance. Then, we propose a local-level knowledge transfer (LKT), which can synthesize the different knowledge transfers from base categories for different location features.</p><p>Extensive experiments justify that our proposed locallevel strategies can significantly boost the few-shot performance. On the ProtoNet <ref type="bibr" target="#b2">[3]</ref> baseline, it achieve 2.8%-7.2% improvements. Compared with the competitive works, it doesn't require the complex algorithm <ref type="bibr" target="#b4">[5]</ref> or additional model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> but still achieves the state-of-the-art accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND KNOWLEDGE</head><p>Given disjoint category sets {C base , C val , C novel } and the corresponding datasets {D base , D val , D novel }, the objective of few-shot learning (FSL) is to train a robust model on D base which can generalize well on D novel with few reference samples per category. The reference data is called the support set and the recognized data is called the query set. Generally, the model is evaluated by the N -way K-shot episode classification <ref type="bibr" target="#b3">[4]</ref> in FSL. In each episode, a support set S and a query set Q are sampled from D novel with the same N categories, where S contains K examples for each category with available labels. Then, the evaluation metric is defined as the average accuracy on Q under multiple episodes. In this work, we use the setting in <ref type="bibr" target="#b3">[4]</ref>, i.e., N = 5, K ? {1, 5} and Q randomly samples 15 examples for each category.</p><p>We build our model on top of ProtoNet <ref type="bibr" target="#b2">[3]</ref>, which is a popular metric-based method due to its simplicity and effectiveness. In each episode, the ProtoNet first computes a prototype (center) for each class in S,</p><formula xml:id="formula_0">P c = 1 |S c | I?Sc f ? (I)<label>(1)</label></formula><p>where f ? means the embedding extractor, S c means the subset of S with label c and | ? | means the set size. After that, the probability distribution of the query image I q is predicted by the distance function D and softmax function ?,  <ref type="figure">Fig. 2</ref>. Overview of framework: The red and blue arrows mean the process in the training and inference phase respectively. We also mark the proposed enhancement in yellow.</p><formula xml:id="formula_1">q(I q ) = ?([?D(I q , P c )] N c=1 )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head><p>In this section, we describe the model architecture with localagnostic training objective (Sec. 3.1), the local-level similarity measure (Sec. 3.2), and the local-level knowledge transfer (Sec. 3.3) in order. The overview of our framework is depicted in <ref type="figure">Fig. 2</ref>. In particular, we mark the position of three proposed strategies in the framework by yellow, and the local-agnostic training and local-level knowledge transfer are visualized in <ref type="figure">Fig. 3</ref> and <ref type="figure">Fig. 4</ref> in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>Model Architecture: Our model consists of an embedding extractor ? and a classifier f W . Specifically, we use ResNet-12 <ref type="bibr" target="#b14">[15]</ref> with four blocks as the extractor and extract the feature map from the last block ? (4) (I) for classification. The classifier f W : R d?w?h ? R c?w?h consists of a 1x1 convolution function W ? R d?c without bias and a softmax function ?, which classifies each local feature on the last feature map for the local-level classification:</p><formula xml:id="formula_2">f W (? (4) (I)) = ?([W T l ? (4) i,j (I)] c l=1 ) w,h i=1,j=1<label>(3)</label></formula><p>Training Objective: We adopt the episode-based training <ref type="bibr" target="#b2">[3]</ref> to avoid the over-fitting on D base . In each training, an N -way K -shot episode E that is considered as both support set and query set was sampled. As shown in <ref type="figure">Fig. 3</ref>, the training objective consists of the image-level similarity loss L S , as well as the local-level classification L C and regularization loss L R for local-agnostic training. The image-level similarity loss aims to minimize the intra-class distance and maximize the inter-class distance in the feature space, which is defined as</p><formula xml:id="formula_3">L S (E, ?) = (I,y)?E l(?([?D(? (4) (I), P c )] N c=1 ), y) (4)</formula><p>where l is cross-entropy function, ? is softmax function, the P c is the prototype of category c computed by Eq. 1, and the distance function D is described in Sec. 3.2. Then we consider local-agnostic training: In the imagelevel classification loss, the gradient of low-discrimination locations is diluted by the global pooling function, which leads ... ... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature maps</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-level similarity loss</head><p>Local-level classificatoin ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learnable Vector</head><p>Mean Local-level regularization <ref type="figure">Fig. 3</ref>. The complete training objective, in which the learnable vectors are the weights of the 1x1 convolution classifier.</p><p>to the discriminative location bias. To avoid the problem, we first replace the image-level classification loss with the locallevel classification loss (Eq. 5). It optimizes the classification loss for each location on the feature map, which encourages the model to focus on all location features, whether high or low discrimination. Specifically, we use a 1x1 convolution function described above as the classifier, and the local-level classification loss is computed by the cross-entropy l:</p><formula xml:id="formula_4">L C (E, ?, W ) = (I,y)?E w,h i=1 j=1 l(f (i,j) W (? (4) (I)), y) (5)</formula><p>Then, we propose a local-level regularization loss L R , which further avoids the discriminative location bias by punishing the model for paying too much attention to some location features. Specifically, we adopt the L 2 -norm of each local feature to evaluate the attention of the model to this location. Thus the local-level regularization loss is defined as</p><formula xml:id="formula_5">L R (E, ?) = (I,y)?E V ar {||? (4) (i,j) (I)|| 2 } w,h i=1,j=1<label>(6)</label></formula><p>where V ar means the variance across all local features. The final optimization objective is the sum of three losses:</p><formula xml:id="formula_6">J = L C (E, ?, W ) + ? S L S (E, ?) + ? R L R (E, ?)<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local-level Similarity Measure</head><p>Different from the image-level feature, the local-level feature is structural, so the similarity between local-level features should also consider their spatial structures. DeepEMD <ref type="bibr" target="#b4">[5]</ref> proposed to compute the minimal spatial matching between local-level features, but this destroys the original spatial structure. MetaOptNet <ref type="bibr" target="#b10">[11]</ref> and MCT <ref type="bibr" target="#b1">[2]</ref> directly compare the spatial similarity, but it may not be accurate when the object posture and position vary. Different from them, we consider both the original spatial structures and the minimal spatial matching, which are visualized in <ref type="figure">Fig. 2</ref>. For each feature map x ? R d?w?h , after normalizing them by Eq. (8), we first directly compare the spatial similarity of local-level features between query sample and prototype p by local distance d L :</p><formula xml:id="formula_7">N (x) = x ||x|| F (8) d L (x, p) = (N (x) ? N (p)) 2 F (9)</formula><p>Then we propose a simple minimum matching distance d M between local-level features to capture the correct comparison when object position and posture vary, which is defined as</p><formula xml:id="formula_8">d M (x, p) = w,h i=1 j=1 min a,b (N i,j (x) ? N a,b (p)) 2 2 ,<label>(10)</label></formula><p>The final distance between the feature map and prototype is defined as the weighted sum of the two distances:</p><formula xml:id="formula_9">D(x, p) = d L (x, p) + ? d M (x, p)<label>(11)</label></formula><p>Local-level Classificatoin Transfer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? (I)</head><p>Weighted Add ? s (I) ) ( I ? <ref type="figure">Fig. 4</ref>. The process of LKT. "Transfer" means to replace each local feature as the weighted sum of the base knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Local-level Knowledge Transfer</head><p>In FSL, the semantic information of novel datasets is unavailable during training, which leads the distribution of the novel category samples difficult to converge in the feature space. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> propose an attention module to refine the distribution of novel samples by the similarity with the base category. Different from them, we argue that the image-level feature cannot correctly describe the semantic information of the novel category sample (Sec. 1). Thus we introduce the local-level knowledge transfer during inference that refines all the location features separately. Specifically, we adopt the convergent classifier weight W (described in Sec. 3.1) as the knowledge (prototype) of base categories. As shown in <ref type="figure">Fig. 4</ref>, for each sample from D novel , we first compute a D base similar-version ? s (I) for the feature map ? (4) (I), on which each location feature is computed by the probability-weighted sum of W ,</p><formula xml:id="formula_10">? s i,j (I) = |W | l=1 [f (i,j) W (? (4) (I))] l ? W l<label>(12)</label></formula><p>Then, the local-level feature is refined as the weighted sum of the similar-version and original-version after normalization:</p><formula xml:id="formula_11">? (4) i,j (I) = ?N i,j (? (4) (I)) + (1 ? ?)N i,j (? s (I))<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we evaluate the proposed local-level strategies (LLS), composed of the comparison with related works (Sec. 4.2) and the ablation studies (Sec. 4.3). For TieredImageNet, we also adopt a batch-based pre-training step with only locallevel classification loss followed by the episode-based training, in which the learning rates are initialized to 0.1 and 0.01 with the decay of factor 0.1 at every 20,000 episodes. For other datasets, the learning rate is initialized to 0.1 and declines to 0.006 and 0.0012 at 30,000 episodes and 45,000 episodes, respectively. The balanced factor in Eq. <ref type="formula" target="#formula_6">(7)</ref> is set to ? S = 0.2, ? R = 0.0001. Other hyper-parameters are selected by the dataset validation, resulting in ? = 0.6, ? = 0.8 for MiniImage, ? = 0.4, ? = 0.9 for TieredImageNet, and ? = 0.2, ? = 0.5 for CIFAR-FS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State-of-the-art Methods</head><p>Tab. 1 shows the comparison with the latest methods. For a fair comparison, we report the model backbone and the average accuracy with 95% confidence interval over 1,000 episodes. As shown in Tab. 1, LLS can significantly boost the performance of the baseline ProtoNet with up 2.8%-7.2% on all benchmark datasets. Compared with competitive FEAT <ref type="bibr" target="#b7">[8]</ref>, Multi-Proto <ref type="bibr" target="#b6">[7]</ref>, and DeepEMD <ref type="bibr" target="#b4">[5]</ref> adopting extra Transformer module, BERT module, and high-complexity distance metric respectively, we only adopt the simple ProtoNet framework and light-weight strategies, but still achieve the state-ofthe-art accuracy on all benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Tab. 2 shows the ablation studies of three local-level strategies: local-agnostic training (LAT), local-level similarity measure (LSM), and local-level knowledge transfer (LKT). The performance when none of them is adopted can be regarded as our baseline, which is re-implemented from Pro-toNet <ref type="bibr" target="#b2">[3]</ref>. As can be seen, the proposed LLS can achieve performance improvements of 3.7%-7.2% on the two datasets. The specific improvements are 2.3%-4.4% by LAT, 0.8%-2.1% by LSM, and 0.4%-1.3% by LKT, respectively.  <ref type="table">Table 2</ref>. Ablation studies of three local-level strategies with 1,000 episodes on MiniImageNet and CIFAR-FS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we focus on the local-level feature and propose a series of local-level strategies to enhance the few-shot image classification by avoiding the discriminative location bias and information loss in local details. Extensive experiments show that the proposed local-level strategies can achieve significant improvements with 2.8%-7.2% over the baseline, which also achieves the state-of-the-art accuracy. We hope our proposed new perspective on the local-level feature can inspire future works in few-shot learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Feature heat map of novel category samples extracted from models with global pooling or local-agnostic training (LAT): The color overlaid on the original image represents the L 2 -norm of the location feature (Red means high and blue means low). The higher L 2 -norm can represent more model attention to the corresponding location because the corresponding feature has greater weight for the classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We use Pytorch to implement all our experiments on one NVIDIA TITAN GPU. All input images are resized to 84 ? 84 and the channel size of each model block output is 64-128-256-512, respectively. In addition, we also adopt horizontal flip and random crop as data augmentation in the training phase. During training, the model is trained by an SGD optimizer for 50,000 episodes, in which N and K are set to 15 and 9.</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>MiniImageNet 1-shot 5-shot</cell><cell cols="2">TieredImageNet 1-shot 5-shot</cell><cell>1-shot</cell><cell cols="2">Cifar-FS</cell><cell>5-shot</cell></row><row><cell cols="3">MetaOptNet-SVM [11] ResNet-12 62.64 ? 0.61 78.63 ? 0.46</cell><cell>65.99 ? 0.72</cell><cell>81.56 ? 0.53</cell><cell cols="2">72.00 ? 0.70</cell><cell>84.20 ? 0.50</cell></row><row><cell>Simple-Shot [6]</cell><cell cols="2">WRN-28-10 63.50 ? 0.20 80.33 ? 0.14</cell><cell>69.75 ? 0.20</cell><cell>85.50 ? 0.14</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>CAN [1]</cell><cell cols="2">ResNet-12 63.85 ? 0.48 79.44 ? 0.34</cell><cell>69.89 ? 0.51</cell><cell>84.23 ? 0.37</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>R2-D2+Aug [16]</cell><cell cols="2">ResNet-12 64.79 ? 0.45 81.08 ? 0.32</cell><cell>-</cell><cell>-</cell><cell cols="2">76.51 ? 0.47</cell><cell>87.63 ? 0.34</cell></row><row><cell>S2M2 R [17]</cell><cell cols="2">WRN-28-10 64.93 ? 0.18 83.18 ? 0.11</cell><cell>-</cell><cell>-</cell><cell cols="2">74.81 ? 0.19</cell><cell>87.47 ? 0.13</cell></row><row><cell>MCT (Inst) [2]</cell><cell cols="2">ResNet-12 65.34 ? 0.63 82.15 ? 0.45</cell><cell>69.66 ? 0.81</cell><cell>85.29 ? 0.49</cell><cell cols="2">77.84 ? 0.64</cell><cell>89.11 ? 0.45</cell></row><row><cell>DeepEMD [5]</cell><cell cols="2">ResNet-12 65.91 ? 0.82 82.41 ? 0.56</cell><cell>71.16 ? 0.87</cell><cell>86.03 ? 0.58</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>FEAT [8]</cell><cell cols="2">ResNet-12 66.78 ? 0.20 82.05 ? 0.14</cell><cell>70.80 ? 0.23</cell><cell>84.79 ? 0.16</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="3">FEAT+Multi-Proto [7] ResNet-12 67.24 ? 0.58 82.51 ? 0.66</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="3">ProtoNet* (baseline) ResNet-12 60.85 ? 0.69 79.50 ? 0.46</cell><cell>67.67 ? 0.73</cell><cell>83.67 ? 0.55</cell><cell cols="2">71.65 ? 0.68</cell><cell>86.15 ? 0.44</cell></row><row><cell>ProtoNet+LLS</cell><cell cols="2">ResNet-12 68.01 ? 0.63 83.26 ? 0.43</cell><cell>72.27 ? 0.71</cell><cell>86.50 ? 0.46</cell><cell cols="2">78.76 ? 0.67</cell><cell>89.60 ? 0.43</cell></row><row><cell cols="8">Table 1. Average classification performance over 1,000 episodes with 95% confidence interval. "-" means that the experiment</cell></row><row><cell cols="3">is unavailable. * means the results is re-implemented.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1. Experimental Setup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Datasets: We validate our framework on MiniImageNet [4],</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">TieredImageNet [18], and CIFAR-FS [19] datasets. MiniIma-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">geNet and TieredImageNet are subsets of ILSVRC-2012 [20],</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">while CIFAR-FS is a variant of CIFAR-100 dataset. They</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">contain 64/16/20, 351/97/160, and 64/16/20 categories for</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">training/validation/test, respectively.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Implementation Details:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross attention network for fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bingpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4003" to="4014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Transductive few-shot learning with meta-learned confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae</forename><forename type="middle">Beom</forename><surname>Seong Min Kye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoirin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12017</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12203" to="12213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearest-neighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-shot image classification with multi-facet prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zied</forename><surname>Bouraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><surname>Jameel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="1740" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-shot learning via embedding adaptation with setto-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Empirical bayes transductive metalearning with synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">G</forename><surname>Shell Xu Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damianou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12696</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="4367" to="4375" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Task augmentation by rotating for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Min</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00804</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2218" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 6th International Conference on Learning Representations ICLR</title>
		<meeting>6th International Conference on Learning Representations ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closedform solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
