<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ATTENTION-FREE KEYWORD SPOTTING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mashrur</forename><forename type="middle">M</forename><surname>Morshed</surname></persName>
							<email>mashrurmahmud@iut-dhaka.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Islamic University of Technology Dhaka</orgName>
								<address>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><forename type="middle">Omar</forename><surname>Ahsan</surname></persName>
							<email>ahmadomar@iut-dhaka.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Islamic University of Technology Dhaka</orgName>
								<address>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ATTENTION-FREE KEYWORD SPOTTING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Till now, attention-based models have been used with great success in the keyword spotting problem domain. However, in light of recent advances in deep learning, the question arises whether self-attention is truly irreplaceable for recognizing speech keywords. We thus explore the usage of gated MLPs-previously shown to be alternatives to transformers in vision tasks-for the keyword spotting task. We provide a family of highly efficient MLP-based models for keyword spotting, with less than 0.5 million parameters. We show that our approach achieves competitive performance on Google Speech Commands V2-12 and V2-35 benchmarks with much fewer parameters than self-attention-based methods. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transformers <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref> have shown remarkable success in Computer Vision tasks with the advent of the Vision Transformer (ViT) <ref type="bibr" target="#b3">(Dosovitskiy et al., 2020)</ref>. They have lately been studied in the field of keyword spotting (KWS). Several works <ref type="bibr" target="#b0">(Berg et al., 2021;</ref><ref type="bibr" target="#b5">Gong et al., 2021)</ref> have obtained exceptional results with ViT-like approaches on KWS.</p><p>Recent research <ref type="bibr" target="#b16">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b8">Liu et al., 2021;</ref><ref type="bibr" target="#b10">Melas-Kyriazi, 2021;</ref><ref type="bibr" target="#b17">Touvron et al., 2021a)</ref> shows that a core component of Transformers, self-attention, may not be necessary for achieving good performance in vision and language tasks. This finding necessitates a study on whether MLPs can be an alternative to self-attention, which has been a main focus of several state-of-the-art methods for the KWS problem. Our contributions can be summarized as follows:</p><p>1. We introduce the Keyword-MLP (KW-MLP), a memory-efficient, attention-free alternative to the Keyword Transformer (KWT) <ref type="bibr" target="#b0">(Berg et al., 2021)</ref>. It achieves 97.63% and 97.56% accuracy on the Google Speech Commands V2-12 and V2-35 benchmarks <ref type="bibr" target="#b21">(Warden, 2018)</ref> respectively-showing comparable performance to the KWT, while having much fewer parameters. 2. We distill smaller and shallower versions of KW-MLP, with the smallest having only 0.213 million parameters, and accuracies of 97.12% and 97.17% on Google Speech Commands V2-12 and V2-35 benchmarks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">KEYWORD SPOTTING</head><p>Keyword spotting (KWS) deals with identifying some pre-specified speech keywords from an audio stream. As it is commonly used in always-on edge applications, KWS research often focuses on both accuracy and efficiency. While research in keyword spotting goes back to the 1960s <ref type="bibr" target="#b15">(Teacher et al., 1967)</ref>, most of the recent and relevant works have been focused on the Google Speech Commands dataset <ref type="bibr" target="#b21">(Warden, 2018)</ref>, which has inspired numerous works and has rapidly grown to be the standard benchmark in this field. The dataset contains 1 second long audio clips, each containing an utterance of a word. Notably, there are two versions, V1 and V2, consisting of 30 and 35 keywords respectively. There is also a 12 keyword task for either version, where it is required to identify 10 keywords and two additional classes, 'silence' and 'unknown' (containing instances of the unused keywords).</p><p>Initial approaches to keyword spotting on the speech commands consisted of convolutional models <ref type="bibr" target="#b21">(Warden, 2018)</ref>. <ref type="bibr" target="#b9">Majumdar &amp; Ginsburg (2020)</ref>, <ref type="bibr" target="#b11">Mordido et al. (2021)</ref> and <ref type="bibr" target="#b22">Zhang &amp; Koishida (2017)</ref> proposed lightweight CNN models with depth-wise separable convolutions. de <ref type="bibr" target="#b1">Andrade et al. (2018)</ref> proposed using a convolutional recurrent model with attention, introducing the usage of attention in the KWS task. <ref type="bibr" target="#b13">Rybakov et al. (2020)</ref> proposed a multi-headed, self-attention-based RNN (MHAtt-RNN). <ref type="bibr" target="#b20">Vygon &amp; Mikhaylovskiy (2021)</ref> proposed an efficient representation learning method with triplet loss for KWS. While the state of the art in KWS at that time was the method of <ref type="bibr" target="#b13">Rybakov et al. (2020)</ref>, it was empirically seen that triplet loss performed poorly with RNN-based models. The authors later obtained excellent results with ResNet <ref type="bibr" target="#b14">(Tang &amp; Lin, 2018)</ref> variants.</p><p>Recently, <ref type="bibr" target="#b0">Berg et al. (2021)</ref> and <ref type="bibr" target="#b5">Gong et al. (2021)</ref> proposed the Keyword Transformer (KWT) and Audio Spectrogram Transformer (AST) respectively. Both approaches are inspired by the success of the Vision Transformer (ViT) <ref type="bibr" target="#b3">(Dosovitskiy et al., 2020)</ref>, and show that patch-based transformer models with self-attention can obtain state of the art or comparable results on the keyword spotting task. A key difference between these two transformer approaches is that AST uses ImageNet <ref type="bibr" target="#b2">(Deng et al., 2009</ref>) and Audioset (Gemmeke et al., 2017) pre-training. Furthermore, the best performing KWT models are trained with attention-based distillation method <ref type="bibr" target="#b18">(Touvron et al., 2021b</ref>) using a teacher MHAtt-RNN <ref type="bibr" target="#b13">(Rybakov et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MLP-BASED VISION</head><p>The Vision Transformer <ref type="bibr" target="#b3">(Dosovitskiy et al., 2020)</ref> has thus far shown the remarkable capability of Transformers on image and vision tasks. However, several recent works have questioned the necessity of self-attention in ViT. Melas-Kyriazi (2021) directly raises the question on the necessity of attention, and shows that the effectiveness of the Vision Transformer may be more related to the idea of the patch embedding rather than self-attention. <ref type="bibr" target="#b16">Tolstikhin et al. (2021)</ref> proposed the MLP-Mixer, which performs token mixing and channel mixing on image patches/tokens, and shows competitive performance on the ImageNet benchmark. <ref type="bibr" target="#b17">Touvron et al. (2021a)</ref> showed similarly good ImageNet results with ResMLP, a residual network with patch-wise and channel-wise linear layers. <ref type="bibr" target="#b8">Liu et al. (2021)</ref> proposed the gMLP, consisting of very simple channel projections and spatial projections with multiplicative gating-showing remarkable performance without any apparent use of self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">KEYWORD-MLP</head><p>Inputs to KW-MLP consist of mel-frequency cepstrum coefficients (MFCC). Let an arbitrary input MFCC be denoted as X ? R F ?T , where F and T are the frequency bins and time-steps respectively. We divide X into patches of shape F ? 1, getting a total of T patches. Each patch is effectively a vector of mel-frequencies for a particular time-step.</p><p>The T patches are flattened, giving us X 0 ? R T ?F . We then map X 0 to a higher dimension d, with a linear projection matrix P 0 ? R F ?d , getting the frequency domain patch embeddings X E .</p><formula xml:id="formula_0">X E = X 0 P 0<label>(1)</label></formula><p>The obtained X E is passed through L consecutive, identical gated-MLP (gMLP) blocks <ref type="bibr" target="#b8">(Liu et al., 2021)</ref>. On a high level, we can summarize the gMLP blocks used in KW-MLP as a pair of projections across the embedding dimension separated by a projection across the temporal dimension. The block can also be formulated with the following set of equations (omitting bias and normalization for the sake of conciseness): <ref type="figure">Figure 1</ref>: The Keyword-MLP architecture, consisting of L blocks (equation 2). Note that we move the LayerNorm to the end, before the skip-connection, different from <ref type="bibr" target="#b8">Liu et al. (2021)</ref> where norm is applied at the beginning. (? represents a residual skip-connection, while represents element-wise product.)</p><formula xml:id="formula_1">Z = ?(X in U ) Z = g(Z) = g([Z r Z g ]) = Z r (Z T g G) T X out =ZV ? X in<label>(2)</label></formula><p>First, we use the matrix U ? R d?D to linearly project X in from the embedding dimension d to the projection dimension D (essentially a matmul operation). ? represents the GELU activation function. g(Z) represents the the Temporal Gating Unit (TGU) shown in <ref type="figure">Figure 1</ref>. The input to TGU, Z, is first split into Z r and Z g ? R T ?D/2 , the residual and the gate respectively. We use the matrix G ? R T ?T to performs the linear projection across the temporal axis. This is followed by the linear gating-an element-wise multiplication with the residual Z r . While the temporal projection operation can be implemented as passing Z g transposed through a Dense(T, T) layer, in practice, it can be implemented more efficiently by instead passing Z g through a Conv1D(T, T, 1) layer.Z ? R T ?D/2 is projected back to the embedding dimension d with the matrix V and then added with the skip-connected input X in .</p><p>The original gMLP paper <ref type="bibr" target="#b8">(Liu et al., 2021)</ref> applies LayerNorm before the initial channel projection (analogous to embedding projections for images). We however find that applying norm after the second embedding projection results in a notably faster and more optimal convergence. <ref type="bibr" target="#b0">Berg et al. (2021)</ref> also observe a similar phenomenon in their work.</p><p>The overall system is shown in <ref type="figure">Figure 1</ref>. In KW-MLP, we primarily use L = 12 (12 consecutive gMLP blocks), embedding dim d = 64, and projection dim D = 256. We also explore a group of smaller KW-MLP models with shallower depth, i.e. L = 10, 8, 6. The input MFCCs to the model are of shape 40 ? 98, where 40 is the number of frequency bins, and 98 is the number of timesteps. All settings are also shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>It is to be noted that the largest KW-MLP model has only 0.424M parameters, which is smaller than the smallest KWT variant (KWT-1 with 0.607M params) and much smaller than AST models (87M params). However, from <ref type="table" target="#tab_0">Table 1</ref> we can see that KW-MLP shows competitive accuracy with these models, particularly on the Speech Commands V2-35 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">KNOWLEDGE DISTILLATION</head><p>In order to boost the accuracies of shallower KW-MLP variants, we use knowledge distillation (KD) <ref type="bibr" target="#b6">(Hinton et al., 2015)</ref>, using the KW-MLP model with L = 12 as the teacher model. We use an annealed KD approach <ref type="bibr" target="#b7">(Jafari et al., 2021)</ref> where the temperature parameter decreases every step following a cosine-annealing rule till it reaches 1. Other KD parameters, such as alpha, are shown in <ref type="table" target="#tab_1">Table 2</ref>. We also do not use label smoothing when training with KD, as soft targets are obtained from teacher predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL DETAILS</head><p>We follow similar hyperparameters to <ref type="bibr" target="#b13">Rybakov et al. (2020)</ref>; <ref type="bibr" target="#b0">Berg et al. (2021)</ref>, with minor changes; all our hyperparameters and settings are shown in <ref type="table" target="#tab_1">Table 2</ref>. For training, we use a smaller batch-size of 256, and train for 140 epochs. No other augmentation apart from Spectral Augmentation <ref type="bibr" target="#b12">(Park et al., 2019)</ref> is applied (to enable fast training). As an additional regularization method, each gMLP block has a survival probability of 0.9 (alternatively, a 0.1 probability to drop each block). We run experiments on Google Speech Commands V2-12 and V2-35 benchmarks, following the standard protocol described in <ref type="bibr" target="#b21">Warden (2018)</ref>.</p><p>As seen from <ref type="table" target="#tab_0">Table 1</ref>, the largest KW-MLP model has only 424K parameters, which is much fewer than the KWT models, while having comparable accuracy. Furthermore, since we do not apply expensive run-time augmentations like resampling, time-shifting, adding background noise, mixup, etc. (used by <ref type="bibr" target="#b13">(Rybakov et al., 2020;</ref><ref type="bibr" target="#b0">Berg et al., 2021;</ref><ref type="bibr" target="#b5">Gong et al., 2021)</ref>), it is possible to train KW-MLP models in a very short time on free cloud compute such as the NVIDIA Tesla K80 or Tesla P100 provided by Google Colab and Kaggle.</p><p>As a trade-off for fast training, a limitation of the KW-MLP experiments is that the effect of various augmentation methods have not been explored. This is more apparent in the V2-12 task, which contains much fewer training examples (? 37000) than the V2-35 task (? 84000). The KW-MLP model does not generalize as well here, as compared to V2-35. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">TEMPORAL PROJECTION MATRICES</head><p>We additionally visualize the weights of the temporal gating unit (the temporal projection matrix G ? R T ?T in equation 2). Interestingly, we can observe that our model learns weights which seem similar to diagonal, identity, or toeplitz matrices. This suggests that KW-MLP may partially learn a form of shift-invariance, which is necessary for the keyword spotting task. For instance, in a 1 second audio clip, a keyword can occur at different temporal positions; so the model needs to be invariant to temporal shift. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>The Keyword-MLP has shown itself to be an efficient solution to the keyword spotting task, and an alternative to self-attention-based methods. We hope that we provide an additional avenue of future research in audio and speech domains, particularly when resource-efficiency is concerned.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of the temporal projection matrices, G, for each of the L = 12 gMLP blocks of KW-MLP. The matrices are arranged in row major order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of Model Parameters and Accuracy on Google Speech Commands V2</figDesc><table><row><cell>-12 and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overview of Hyper-Parameters and Settings</figDesc><table><row><cell>Training</cell><cell></cell><cell>Augmentation</cell><cell></cell><cell>Model</cell><cell></cell></row><row><cell>Epochs</cell><cell>140</cell><cell># Time Masks</cell><cell>2</cell><cell># Blocks, L</cell><cell>12</cell></row><row><cell>Batch Size</cell><cell>256</cell><cell cols="2">Time Mask Width [0, 25]</cell><cell cols="2">Input Shape 40 ? 98</cell></row><row><cell>Optimizer</cell><cell>AdamW</cell><cell># Freq Masks</cell><cell>2</cell><cell>Patch Size</cell><cell>40 ? 1</cell></row><row><cell>Learning Rate</cell><cell>0.001</cell><cell>Freq Mask Width</cell><cell>[0, 7]</cell><cell>Dim, d</cell><cell>64</cell></row><row><cell>Warmup Epochs</cell><cell>10</cell><cell></cell><cell></cell><cell>Dim Proj.</cell><cell>256</cell></row><row><cell>Scheduling</cell><cell>Cosine</cell><cell></cell><cell></cell><cell># Classes</cell><cell>35</cell></row><row><cell>Regularization</cell><cell></cell><cell cols="2">Audio Processing</cell><cell>KD</cell><cell></cell></row><row><cell>Label Smoothing</cell><cell>0.1</cell><cell>Sampling Rate</cell><cell>16000</cell><cell>?</cell><cell>0.9</cell></row><row><cell>Weight Decay</cell><cell>0.1</cell><cell>Window Length</cell><cell>30 ms</cell><cell>Init Temp</cell><cell>5.0</cell></row><row><cell>Block Survival Prob.</cell><cell>0.9</cell><cell>Hop Length</cell><cell>10 ms</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>n mfcc</cell><cell>40</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Keyword Transformer: A Self-Attention Model for Keyword Spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel Tairum</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cruz</surname></persName>
		</author>
		<idno>doi: 10.21437</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2021</title>
		<meeting>Interspeech 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2021" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A neural attention model for speech command recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Coimbra De Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabato</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin Loesener Da Silva</forename><surname>Viana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bernkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP 2017</title>
		<meeting>IEEE ICASSP 2017<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AST: Audio Spectrogram Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2021-698</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2021</title>
		<meeting>Interspeech 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="571" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aref</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07163</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Annealing knowledge distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Matchboxnet: 1d time-channel separable convolutional neural network architecture for speech commands recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somshubra</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08531</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02723</idno>
		<title level="m">Do you even need attention? a stack of feed-forward layers does surprisingly well on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Compressing 1d time-channel separable convolutions using sparse random ternary matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gon?alo</forename><surname>Mordido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Van Keirsbilck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Keller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17142</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Streaming keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Rybakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natasha</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Laurenzo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06720</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for small-footprint keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5484" to="5488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Experimental, limited vocabulary, speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kellett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Focht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio and Electroacoustics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="130" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning efficient representations for keyword spotting with triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vygon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Mikhaylovskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04792</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end text-independent speaker verification with triplet loss on short utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhito</forename><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1487" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
