<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Channel-Wise Attention-Based Network for Self-Supervised Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory for Manufacturing System Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhao</surname></persName>
							<email>zhaohong@mail.</email>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory for Manufacturing System Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghui</forename><surname>Bu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory for Manufacturing System Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Jin</surname></persName>
							<email>yushengj@stu.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory for Manufacturing System Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Channel-Wise Attention-Based Network for Self-Supervised Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning has shown very promising results for monocular depth estimation. Scene structure and local details both are significant clues for high-quality depth estimation. Recent works suffer from the lack of explicit modeling of scene structure and proper handling of details information, which leads to a performance bottleneck and blurry artefacts in predicted results. In this paper, we propose the Channel-wise Attention-based Depth Estimation Network (CADepth-Net) with two effective contributions: 1) The structure perception module employs the selfattention mechanism to capture long-range dependencies and aggregates discriminative features in channel dimensions, explicitly enhances the perception of scene structure, obtains the better scene understanding and rich feature representation. 2) The detail emphasis module re-calibrates channel-wise feature maps and selectively emphasizes the informative features, aiming to highlight crucial local details information and fuse different level features more efficiently, resulting in more precise and sharper depth prediction. Furthermore, the extensive experiments validate the effectiveness of our method and show that our model achieves the state-of-the-art results on the KITTI benchmark and Make3D datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate depth estimation from a single image is a fundamental task in computer vision. High quality depth information can provide useful cues for various fields, including robotics navigation <ref type="bibr" target="#b3">[4]</ref>, autonomous driving <ref type="bibr" target="#b32">[33]</ref> and augmented reality <ref type="bibr" target="#b33">[34]</ref>. Recently, the fully-supervised methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref> for monocular depth estimation have produced outstanding results, while they need large numbers of accurate ground truth which could only be sparsely collected from expensive LiDAR sensors <ref type="bibr" target="#b8">[9]</ref>. As an attractive alternative, self-supervised methods can alleviate this lim-Monodepth2 <ref type="bibr" target="#b10">[11]</ref> PackNet <ref type="bibr" target="#b12">[13]</ref> Ours</p><p>Monodepth2 <ref type="bibr" target="#b10">[11]</ref> PackNet <ref type="bibr" target="#b12">[13]</ref> Ours itation, as they use geometrical constraints on monocular video <ref type="bibr" target="#b55">[56]</ref> or synchronized stereo image pairs <ref type="bibr" target="#b9">[10]</ref> as the sole source of supervision. In depth estimation, the most important information is the scene structure aiming at accurately obtaining the overall structure and relative depth information in 3D space. Most previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b55">56]</ref> just simply use convolutional neural networks to extract semantic features of input images and implicitly learn the structural information of the scene. However, the lack of explicit exploration of the robust representation of 3D scene geometry leads to an incomplete perception of overall layout for the complex scenes.</p><p>Local detail is another critical feature that focuses on object boundaries and attempts to generate sharp depth maps. Most depth estimation networks are based on the U-Net <ref type="bibr" target="#b36">[37]</ref> framework, and the decoder simply leverages concatenation and a basic convolution to fuse high-level and low-level features. We found that these operations can not preserve sufficient details or precisely recover spatial information, leading to inefficient integration of different levels features and blurry artefacts at the depth discontinuous regions.</p><p>To address the above problems and efficiently handle both the overall structure and local details, we propose a novel Channel-wise Attention-based Depth Estimation Network with structure perception module and detail emphasis module. To better understand the 3D structure of the whole scene, we perform an in-depth analysis of semantic information in the monocular depth estimation task and conclude that each high-level feature map can be regarded as a regionspecific response. Based on this observation, we provide the structure perception module to capture more contextual information of scene geometry and enhance the feature representations. Specifically, we first employ the self-attention mechanism to capture the long-range dependencies between any two channel maps, then each feature map is updated via aggregating features from all channel maps by weighted summation and fuse different local depth responses from non-contiguous regions. To generate sharper object boundaries, instead of using the above naive operations, we propose the detail emphasis module employing channel attention mechanism to re-calibrate channel-wise features and emphasize the specific semantic information. Specifically, we sequentially adopt the detail emphasis module at different scales in the decoding stage to highlight features containing crucial local details (e.g. object boundaries information). To summarize our contributions in this paper:</p><p>? We introduce a novel Channel-wise Attention-based Depth Network (CADepth-Net) for self-supervised monocular depth estimation employing two channelwise attention modules to perform the information aggregation and feature re-calibration respectively.</p><p>? We propose structure perception module utilizing selfattention mechanism to obtain rich context of scene structure and better feature representation.</p><p>? We carefully design the detail emphasis module with channel attention mechanism to efficiently fuse different scale features and emphasize important details for sharper depth estimation.</p><p>? We conduct extensive experiments on KITTI and Make3D datasets, demonstrating our model significantly outperforms existing methods and achieve the state-of-the-art results on KITTI benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Supervised Depth Estimation</head><p>Estimating depth from a single image is an inherently ill-posed problem as pixels in the image can have multiple plausible depths. Recently, fully supervised methods had shown the capacity of fitting predictive models to estimate depth from color input images correctly. Eigen et al. <ref type="bibr" target="#b5">[6]</ref> produced dense pixel depth estimates by utilizing the multiscale neural networks, one that estimated a coarse global depth prediction and another locally refined prediction produced by the first network. Rapidly, various fully supervised methods based on deep learning had been continuously explored <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. However, all the above methods required high-quality ground truth depth, which can be costly to obtain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-supervised Monocular Depth Estimation</head><p>To overcome the limitation of supervised approaches, self-supervised methods unified depth estimation and egomotion estimation into one framework with view synthesis as supervision signal. SfMLearner introduced by Zhou et al. <ref type="bibr" target="#b55">[56]</ref> simultaneously learned depth and ego-motion from monocular video by training a depth estimation network along with a separate pose network. Furthermore, Yin et al. <ref type="bibr" target="#b49">[50]</ref> decomposed scene motion into rigid and non-rigid parts to account for object motion. Wang et al. <ref type="bibr" target="#b43">[44]</ref> incorporated Direct Visual Odometry to estimate the relative camera pose. <ref type="bibr" target="#b30">[31]</ref> proposed 3D constraints loss to enforce consistency of the estimated depth and ego-motion across consecutive frames. Guizilini et al. <ref type="bibr" target="#b12">[13]</ref> learned to compress and decompress detail-preserving representations by symmetrical packing and unpacking blocks. Other published methods were based upon edge and normal <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>, Competitive Collaboration <ref type="bibr" target="#b35">[36]</ref>, semantic segmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref> and feature representations learning <ref type="bibr" target="#b40">[41]</ref>. A state-of-theart framework was Monodepth2 proposed by Godard et al. <ref type="bibr" target="#b10">[11]</ref>, which introduced a minimum re-projection loss to deal with occlusions and auto-masking scheme removing invalid pixels robustly. Our model is based on Monodepth2 extended with our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Self-attention Mechanism</head><p>Self-attention mechanism had been widely used to capture long-range dependencies in various tasks. The Transformer <ref type="bibr" target="#b42">[43]</ref> was the first work that proposed the selfattention mechanism to handle long-range dependencies between words in machine translation. Wang et al. <ref type="bibr" target="#b44">[45]</ref> modeled the spatial-temporal dependencies in video sequences and images via aggregating query-specific global context to each query position. Zhang et al. <ref type="bibr" target="#b51">[52]</ref> incorporated the selfattention mechanism into the GAN framework and learned a better image generator. Fu et al. <ref type="bibr" target="#b7">[8]</ref> enhanced the ability of feature representations for scene segmentation by designing two types of attention modules. For the monocular depth estimation task, Johnston et al. <ref type="bibr" target="#b17">[18]</ref> captured the context of similar disparity values at non-contiguous regions by exploring the feature similarity at spatial dimensions. Unlike previous works, we demonstrate that capturing global dependencies along the channel dimensions and aggregating discriminative features will achieve better performance for depth estimation, as each channel map gains more relative depth information from the distant regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we firstly review the training methods for self-supervised monocular depth estimation, then introduce the architecture of our Channel-wise Attention-based Network, finally describe our main contributions, the structure perception module and detail emphasis module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Supervised Training</head><p>The goal of self-supervised monocular depth estimation is to predict the depth map from a single RGB image without ground truth. Specifically, given a single input image I t , the depth network predicts its corresponding depth map D t , then the pose network takes temporally adjacent images as input and predicts relative pose T t?t between the target image I t and source images I t , t ? {t?1, t+1}, finally we use the predicted D t and T t?t to perform view synthesis as the supervisory signal.</p><p>At training time, both the depth network and pose network are optimized jointly by minimizing the per-pixel minimum photometric re-projection error L p <ref type="bibr" target="#b10">[11]</ref> L p = min t pe (I t , I t ?t ) ,</p><p>where pe() denotes the photometric error which consists of L1 and the Structural Similarity (SSIM) <ref type="bibr" target="#b45">[46]</ref>, I t ?t is the warped result from I t to I t as in</p><formula xml:id="formula_1">pe = ? 2 (1 ? SSIM (I t , I t ?t )) + (1 ? ?) I t ? I t ?t 1 ,<label>(2)</label></formula><formula xml:id="formula_2">I t ?t = I t proj (D t , T t?t , K) ,<label>(3)</label></formula><p>where proj() represents the resulting 2D coordinates of the projected depths D t in I t and is the sampling operator. We use the differentiable bilinear sampling mechanism proposed in the STN <ref type="bibr" target="#b16">[17]</ref> to sample the source images. In a real-world scenario, situations like stationary camera and moving objects will break down the assumptions of a moving camera and a static scene. To handle this issue, we apply auto-masking method <ref type="bibr" target="#b10">[11]</ref> to filter out stationary pixels that remain with the same appearance between two frames in a sequence. Since the binary mask ? is computed in this form on the forward pass</p><formula xml:id="formula_3">? = min t pe (I t , I t ?t ) &lt; min t pe (I t , I t ) ,<label>(4)</label></formula><p>where [] is the Iverson bracket. In addition, in order to regularize the disparities in texture-less regions, an edges-aware smoothness regularization term L s is used</p><formula xml:id="formula_4">L s = |? x d * t | e ?|?xIt| + |? y d * t | e ?|?yIt| ,<label>(5)</label></formula><p>where d * t = d t /d t is the mean-normalized inverse depth from <ref type="bibr" target="#b43">[44]</ref> to discourage shrinking of the estimated depth.</p><p>The final loss L is computed as the combination of photometric loss L p and smoothness loss L s at multiple scales</p><formula xml:id="formula_5">L = 1 S S i ?L i p + ?L i s ,<label>(6)</label></formula><p>where S is the number of scales, and ? is the weighting for the smoothness regularization term. </p><formula xml:id="formula_6">? reshape ? ? reshape &amp; transpose softmax max ? ? reshape ? reshape reshape ? ? ? : element-wise subtraction ?: matrix multiplication ?: element-wise sum</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Channel-wise Attention-based Network</head><p>As shown in <ref type="figure">Fig. 2</ref>, our CADepth-Net is a fully convolutional U-Net architecture. We adopt a pretrained residual network as the backbone to extract semantic features. Then these features would be fed into the structure perception module and generate new features to explicitly enhance the perception of scene structure. Moreover, we gradually recover the spatial resolution at decoder stage, with skipconnection to facilitate the flow of gradients and information throughout the model, and sequentially employ our detail emphasis module to generate sharp edges and finer details. Finally, we successively upsample the predicted inverse depth maps until original input resolutions using nearest neighbors interpolation at multiple scales, and compute the training loss at this higher input resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Structure Perception Module</head><p>In depth estimation, each high-level feature map can be regarded as a region-specific response as shown in <ref type="figure" target="#fig_4">Fig. 6</ref> (b), and different region responses are associated with each other. If each channel map captures more different region responses from all the other channel maps, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref> (c), it will obtain more relative depth information from distant regions and significantly enhance the perception of scene structure. Therefore, we propose a selfattention module to model interdependencies between channels and aggregate different region responses.</p><p>The first step is to generate an attention matrix which models the relationship between any two channel maps. As illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>, given the feature map F ? R C?H?W produced by ResNet encoder, we firstly reshape F to R C?N , where N = H ? W is the number of pixels, then perform a matrix multiplication between F and the transpose of F to compute the feature similarity S ? R C?C</p><formula xml:id="formula_7">S ij = F i ? F T j .<label>(7)</label></formula><p>The similarity between channel maps indicates the spatial relationship of region responses i.e. any two feature maps have higher similarity means that they also have strong responses to the same region. As we need to fuse  more responses from different regions, we convert the similarity S to discrimination D ? R C?C by performing the element-wise subtraction</p><formula xml:id="formula_8">D ij = max i (S) ? S i,j ,<label>(8)</label></formula><p>where D ij measures the j th channel's impact on the i th channel. For each channel map, other channels with discriminative features (i.e. different region response) will get higher scores D ij during feature aggregation. Then we apply a softmax layer to obtain the attention map A ? R C?C</p><formula xml:id="formula_9">A ij = exp (D ij ) C j=1 exp (D ij ) .<label>(9)</label></formula><p>In addition, we perform a matrix multiplication between the transpose of A and F and reshape the result to R C?H?W . Finally we perform an element-wise sum operation between F and the result to obtain the final output E ? R C?H?W as follows</p><formula xml:id="formula_10">E i = C j=1 (A ij F j ) + F i .<label>(10)</label></formula><p>The Eq. 10 shows that the final feature of each channel is the weighted sum of the features from all channels and the original feature. By capturing the long-range dependencies between feature maps, we obtain the aggregated features encoding rich context information of scene structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Detail Emphasis Module</head><p>The decoder recovers the resolution by fusing the following features at different scales: the low-level information from skip-connection encoding rich spatial details, and the high-level information encoding more context information. The simple fusion operations like sum or concatenation lack the further processing of local details and neglect the semantic gap between different level features, leading to blurry artefacts in predicted depth maps. The core of predicting sharper edges is to properly handle local details, and it is easy for network to recover accurate depth predictions if it knows the category and location of features describing object boundaries clearly. Therefore, by using the channel attention mechanism that can make network pay attention to specific channel features, we propose a detail emphasis module to emphasize important details and efficiently fuse features at different scales.</p><p>Specifically, we first concatenate the low-level features L and the high-level features H, then utilize a convolution layer to obtain U with the batch normalization <ref type="bibr" target="#b15">[16]</ref> to balance the scales of the features</p><formula xml:id="formula_11">U = ? (BN (W 1 ? f (L, H))) ,<label>(11)</label></formula><p>where f () denotes concatenation and ? denotes the 3 ? 3 or 1 ? 1 convolution, BN refers to the batch normalization and we use the ReLU as the activation function ?().</p><p>Next, we squeeze U to a vector by global average pooling to obtain global context and use two 1 ? 1 convolution layers followed by a sigmoid function to compute a weights vector V ? R 1?1?C , to recalibrate channel-wise features and measure the importance of them in the meantime <ref type="bibr" target="#b11">(12)</ref> where H and W refer to the height and width of U, and ?() denotes the sigmoid function. Then we perform the element-wise multiplication between V and U to generate re-weight features. As the weights scores in V indicate the importance of corresponding channels, i.e. the channel maps containing critical information will get higher scores, this recalibration operation can adaptively emphasize the crucial details at multiple scales for sharp edges <ref type="figure">(Fig. 7</ref>). Finally, we sum up U and re-weight features for stability</p><formula xml:id="formula_12">V = ?(W 2 ? ?(W 3 ? ( 1 H ? W H i=1 W j=1 U i,j ))),</formula><formula xml:id="formula_13">O = V U + U,<label>(13)</label></formula><p>where denotes element-wise dot product and O refers to the final outputs. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the design of the detail emphasis module, this design recalibrates channel-wise feature responses and produce more precise depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we show extensive experiments for evaluating the performance of our methods and demonstrate the effectiveness of the proposed approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Our model are implemented based on PyTorch <ref type="bibr" target="#b34">[35]</ref>, trained for 20 epochs on a single Nvidia 3090 with a batch size of 12 and an input/output resolution of 640 ? 192. We jointly train both depth network and pose network with the Adam Optimizer <ref type="bibr" target="#b19">[20]</ref> with ? 1 = 0.9, ? 2 = 0.999. The initial learning rate is set to 1e ?4 and decay to 1e ?5 after 15 epochs. We set the SSIM weight to ? = 0.85 and smoothness term weight to ? = 1e ? 3. DepthNet. We implement our depth estimation network as an encoder-decoder architecture. Moreover, we start the ResNet50 encoder with weights pretrained on ImageNet <ref type="bibr" target="#b37">[38]</ref> as it has been shown to improve accuracy compared to training from scratch. We set sigmoid follow the output of network and convert result ? to depth with D = 1/(a? +b), where use a and b to constrain D between 0.1 and 100 units. PoseNet. Our PoseNet is built on ResNet50, modified to accept six channels tensor as input, which allows the adjacent frames to feed into the network. The outputs of PoseNet is the 6-DoF relative pose consist of translation vectors and Euler angles, scaled by a factor of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">KITTI Results</head><p>We train and evaluate our methods using the KITTI 2015 stereo data set <ref type="bibr" target="#b8">[9]</ref>. We adopt the data split of Eigen et al. <ref type="bibr" target="#b4">[5]</ref> for distance to 80m and use pre-processing to remove static frames before training. Ultimately, this results in 39, 810 training monocular triplets, and 4, 424 for validation and 697 for evaluation. We report results using the per-image median ground truth scaling during evaluation.</p><p>As shown in <ref type="table">Table 1</ref>, our proposed CADepth-Net significantly outperforms the existing SoTA self-supervised approaches in all metrics. In addition, we score dramatically higher in the hardest accuracy metric ? &lt; 1.25, which indicates that our model predicts more accurate and realistically detailed depth estimation than all other competing models. For a fair comparison, we also give the results on various input resolution and training settings, and our model still improves the performance at higher image resolutions. <ref type="figure">Fig. 5</ref> shows that our model produces sharper depth estimation on thinner structures e.g. road signs and poles. Moreover, our model successfully estimates correct depth at the highly reflective car roof (6 th row), which are the challenging problems for previous advanced methods. These improvements can be explained by the better perception of scenes and objects afforded by the structure perception module, and the further regularisation provided by detail emphasis module. Additional results can be seen in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Make3D result</head><p>To evaluate the generalization ability of our model on the unseen dataset, we report the quantitative results for the Make3D dataset <ref type="bibr" target="#b38">[39]</ref> using our model trained on KITTI 2015 <ref type="bibr" target="#b8">[9]</ref>. Following the same evaluation protocol as <ref type="bibr" target="#b9">[10]</ref>, we test on a center crop of 2 ? 1 ratio and apply median scaling. As shown in <ref type="table">Table 3</ref>, our approach produces superior results compared with the other SOTA self-supervised methods. Qualitative results can be seen in <ref type="figure">Fig. 8</ref>, which show that our model generates more accurate and sharper depth estimation on the previously unseen Make3D dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Structure Perception Module</head><p>To demonstrate the effectiveness of the structure perception module, <ref type="figure" target="#fig_4">Fig. 6</ref> presents a comparison of features before and after treatment. For clear visualization, we map the channel maps to the RGB color cube and project them to the original RGB image. <ref type="figure" target="#fig_4">Fig. 6 (b)</ref> refers to the high-level features produced by encoder, which mainly indicates regionspecific responses and various kinds of structural information of 3D scene, e.g. vanish point, region with same depth range and the area with same color or texture (e.g. sky) .</p><p>As mentioned earlier, the structure perception module produces the aggregated features ( <ref type="figure" target="#fig_4">Fig. 6 (c)</ref>) by performing the weighted summation of all channel maps. As the attention map describes the feature discrimination and the spatial relationship of region responses between channels, by aggregating discriminative features at channel dimension, we can make each single channel map get rich scene structure representation and more complete region responses from non-contiguous regions. By doing so, the structure perception module obtains rich contextual information of overall scene geometry perception, which results in better scene understanding and feature representation for depth estimation. <ref type="figure" target="#fig_4">Fig. 6</ref> adequately demonstrates that each channel map obtains more extra depth perception from distant regions e.g. foreground (1 st row) and midground (2 nd row). In addition, it also particularly emphasizes the vanishing point regions, which are naturally a strong cue to understand the geometry of a scene. In the last row, the network originally focuses on foreground objects such as cars and obtains more Input Monodepth2 <ref type="bibr" target="#b10">[11]</ref> PackNet <ref type="bibr" target="#b12">[13]</ref> Our CADepth-Net <ref type="figure">Figure 5</ref>. Qualitative results on the KITTI Eigen split. Our model consistently predicts sharper boundaries and fine-gained details on thinner objects, e.g. trees, pedestrians and signs.</p><p>(a) (b) (c) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Detail Emphasis Module</head><p>The detail emphasis module generates a weights vector to re-calibrate channel maps and emphasizes the informative features for subsequent transformations. The weights scores produced by the sigmoid function are between 0 and 1, indicating the importance of corresponding features, i.e. the more important of the channel, the higher score is earned. As shown in <ref type="figure">Fig. 7</ref>, we report the top n (n = 10) feature maps with the highest scores to show which features the network focuses on. We observe that our model mainly highlights the crucial low-level features that describe the object boundaries precisely. This observation is consistent with the fact that the decoder mainly uses detail information to recover spatial resolution gradually. In general, the detail emphasis module adaptively selects the informative local details and helps network handle and locate the object edges for sharper depth prediction. More visualization results are provided in the supplementary material.  <ref type="table">Table 2</ref>. Ablation Studies. We evaluate the performance of our structure perception module (SPM) and detail emphasis module (DEM) contributions with Monodepth2 (MD2) <ref type="bibr" target="#b10">[11]</ref> as the baseline. R: ResNet, Para: parameters. All models in this table are trained with monocular self-supervised (M) and standard resolution <ref type="figure" target="#fig_0">(640 ? 192)</ref>. The inference time is tested on a single RTX3090 GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>For further demonstrating the performance improvements of our provided methods, <ref type="table">Table 2</ref> and supplemental material show the ablation study of our various components with Monodepth2 <ref type="bibr" target="#b10">[11]</ref> as the baseline. It shows that all of our contributions achieve a steady improvement in almost all evaluation measures and obtain a consistent performance gain on different backbones, showing the robustness of our CADepth-Net to the backbone architecture capacity. Note that the structure perception module shows superior performance on metric ? &lt; 1.25, with little time cost and no additional parameters, demonstrating the improvements benefit from the better scene understanding rather than an increase in network complexity. Finally, the combination of all our modules with ResNet50 achieves the best results, with 59M parameters and an inference time of 28ms on an RTX3090 GPU, meets the requirements of real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce a novel architecture named channel-wise attention-based depth estimation network, with two effective components, the structure perception module and the detail emphasis module. The structure perception module aggregates the discriminative features by capturing the long-range dependencies to obtain the context of scene structure and rich feature representation. Additionally, the detail emphasis module employs the channel attention mechanism to highlight objects' boundaries information and efficiently fuse different level features. Furthermore, the experiments demonstrate that our CADepth-Net produces sharper depth estimation and achieves the stateof-the-art results on KITTI datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This document provides additional details and results concerned with paper "Channel-Wise Attention-Based Network for Self-Supervised Monocular Depth Estimation". The supplementary material is organized as follows: Section 1 provides the details of depth estimation network, Section 2 and Section 3 reports the quantitative results on the KITTI improved ground truth and online evaluation server, Section 4 collects additional ablation experiments, Section 5 provides the additional qualitative comparisons, and Section 6 shows visualization results of our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>Except where note, for all experiments, we use a ResNet50 encoder with pretrained ImageNet weights for both depth and pose networks. For depth model, the structure perception module takes the features from encoder as input. In addition, we successively adopt the detail emphasis module at different scales in decoder stage. Note that although there is no skip-connection at the highest resolution, we still use the detail emphasis module for further regularization. For high resolution input, e.g. 1024 ? 320 and 1280 ? 384, we employ a lightweight setup, ResNet18 and 640 ? 192, for pose encoder at training for memory savings. The depth network architecture is shown in <ref type="table" target="#tab_5">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. KITTI Improved Ground Truth</head><p>The evaluation method proposed by Eigen et al. <ref type="bibr" target="#b4">[5]</ref> for KITTI uses the reprojected LIDAR points to generate the ground truth depth, but does not handle moving objects and occlusions. <ref type="bibr" target="#b41">[42]</ref> created a set of high-quality depth maps for the KITTI dataset using five consecutive frames and stereo pairs to handle moving objects better. This improved ground truth depth is provided for 652 (or 93%) of the 697 original test frames contained in the Eigen test split et al. <ref type="bibr" target="#b4">[5]</ref>. We utilize the same error metrics and evaluation strategy as the main paper, and evaluate our model on these 652 improved ground truth frames without having to retrain each method. As shown in <ref type="table">Table 5</ref>, we observe that our CADepth-Net still outperforms all existing advanced methods on all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. KITTI Evaluation Server Benchmark</head><p>In <ref type="table">Table 6</ref> we report the results of our models on the KITTI single image depth prediction benchmark <ref type="bibr" target="#b20">[21]</ref> which were computed on the KITTI online evaluation server. We train a new model on the new split consisting of 72,084 training examples, 6,060 validation, and 500 test with the same training protocols mentioned in the main paper. As we cannot use median scaling for evaluation, we calculate <ref type="table" target="#tab_5">Depth network  layer  k s chns res input  activation  conv1  7 2 64  2  image  RELU  maxpool 3 2 64  4  conv1  -econv1  3 1 256  4  maxpool  RELU  econv2  3 2 512  8  econv1  RELU  econv3  3 2 1024 16 econv2  RELU  econv4  3 2 2048 32</ref>    the scale factor on the 2,000 KITTI training samples which have ground truth depths available. <ref type="table">Table 6</ref> shows that our CADepth-Net outperforms the existing self-supervised approaches and significantly reduces the performance gap to   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Ablation Experiments</head><p>To further demonstrate the effectiveness of our structure perception module, we evaluate the performance of distant objects with the absolute relative error, <ref type="figure" target="#fig_0">Fig. 11</ref> shows that we can effectively reduce the error produced by distant objects. Besides, <ref type="figure" target="#fig_6">Fig. 9</ref> reports that our model improves the accuracy at all depth intervals, and the performance gap consistently increases when larger distances are considered, thanks to the better 3D scene geometric perception intro-Input MD2 (M) <ref type="bibr" target="#b10">[11]</ref> Ours (M) Ground truth <ref type="figure" target="#fig_0">Figure 10</ref>. Qualitative Make3D results. Our CADepth-Net generates more fine-gained details compared to other method.</p><p>duced by the structure perception module. <ref type="figure" target="#fig_0">Fig. 12</ref> shows that our method generates more fine-gained details and accurate object boundaries e.g. pedestrians and thin road signs by using detail emphasis module individually. For a fair comparison, <ref type="table" target="#tab_8">Table 7</ref> reports that our model outperforms other methods with the same backbone, which means that the gain in performance shown in our experiments is mainly due to the effectiveness of our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Qualitative Comparisons</head><p>We provide additional qualitative results from the KITTI datasets in <ref type="figure" target="#fig_0">Fig. 13</ref>. We can observe that compared to existing baselines e.g. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>, our CADepth-Net produces higher quality outputs and possesses the clearest border overall. We also show additional results from Make3D datasets <ref type="bibr" target="#b38">[39]</ref> in <ref type="figure" target="#fig_0">Fig. 10</ref>, our methods preserve sharp discon- tinuities in depth prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Visualization Results</head><p>To better understand our main contributions, <ref type="figure" target="#fig_0">Fig. 14</ref>  <ref type="figure" target="#fig_0">Fig. 15</ref> introduce additional visualization results of intermediate features. As shown in <ref type="figure" target="#fig_0">Fig. 14, each</ref> feature map obtains more region responses from the distant regions and aggregates relative depth relationships over 3D scene. By doing so, our model produces better scene understanding and rich feature representation. <ref type="figure" target="#fig_0">Fig. 15</ref> lists the top n (n = 8) feature maps with the highest scores in the detail emphasis module, and we can see that our model highlights critical local details at multiple scales, by assigning higher scores to them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Monodepth2 <ref type="bibr" target="#b10">[11]</ref> PackNet <ref type="bibr" target="#b12">[13]</ref> Our CADepth-Net <ref type="figure" target="#fig_0">Figure 13</ref>. Qualitative results on the KITTI Eigen split. Our model produces higher quality outputs and possesses the most clear border. Here n is 8 and the weights value range is between 0 and 1. Specifically, (e)(g) are produced by dem2 (see <ref type="table" target="#tab_5">Table 4</ref>) and (f)(h) are generated from dem3. The detail emphasis module mainly highlights the crucial local details.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Depth prediction from a single image. Our proposed CADepth-Net produces more precise and sharper depth estimation, especially for thin structures e.g. road signs and pedestrians.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Details of the structure perception module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Design of the detail emphasis module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>The visualizations of the structure perception module. (a) Input image. (b) Input feature maps. (c) Corresponding output of the structure perception module. All feature maps are projected onto RGB images for clear visualization. The structure perception module explicitly enhances the perception of scene structure and feature representation. relative depth relationships after adding the vanishing point information. More visualization results are described in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>The visualizations of detail emphasis module. (a) Input image. (b) Predicted depth map. (c) Top n (n = 10) feature maps with highest weights score that range in [0, 1]. Detail emphasis module mainly highlights the crucial local details. Qualitative Make3D results. All methods were trained on KITTI using monocular supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Depth Evaluation on KITTI binned at different intervals, calculated independently by only considering ground-truth depth pixels in that range (0-20m, 20-40m, ...).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Qualitative ablation study of Structure Perception Module (SPM). (a) Input Image. (b) Error maps without SPM. (c) Error maps with SPM. The error induced by distance objects (yellow circles) is improved by the structure perception module. Qualitative ablation study of Detail Emphasis Module (DEM). (a) Input image. (b) Predicted depth maps without DEM. (c) Predicted depth maps with DEM. Thanks to the detail emphasis module, we could obtain the more precise and sharper depth estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 .Figure 15 .</head><label>1415</label><figDesc>The visualizations of structure perception module. (a) Input Image. (b) Input feature maps. (c) Output feature maps. All feature representation are projected onto original image for clear visualizations. Our structure perception module explicitly enhances the scene understanding and feature representation. The visualizations of detail emphasis module. (a)(b) Input Image. (c)(d) Predicted depth maps. (e) ? (h) Top n feature maps with highest weights score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Details Emphasis Module</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>reshape</cell><cell>? reshape ?</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell></row><row><cell>Input Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Attention Map</cell><cell></cell><cell>Weights Vector</cell><cell></cell></row><row><cell></cell><cell>ResNet Encoder</cell><cell cols="3">Structure Perception Module</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multi-scale Outputs</cell><cell></cell><cell cols="2">Multi-scale Outputs</cell></row><row><cell></cell><cell></cell><cell></cell><cell>? ?</cell><cell>Conv3?3 + Upsample</cell><cell>Channel matrix operation</cell><cell>?</cell><cell>Matrix multiplication</cell></row><row><cell>[ , ? ]</cell><cell>Pose Network</cell><cell>6DOF Pose</cell><cell></cell><cell>Structure Perception Module Details Emphasis Module 2 * Conv3?3 + Sigmoid</cell><cell>Upsample Skip Connection Convolution operation</cell><cell>? ?</cell><cell>Element-wise multiplication Element-wise sum</cell></row><row><cell cols="8">Figure 2. Overview of Framework. Our proposed CADepth-Net is a fully convolutional U-Net architecture. We first use a ResNet encoder</cell></row><row><cell cols="8">to extract semantic features and input them to the Structure Perception Module. We perform the matrix multiplication between input</cell></row><row><cell cols="8">features and attention maps to generate aggregated features. The low-resolution feature maps are passed through successive blocks of</cell></row><row><cell cols="8">UpConv(upsample + convolution), as well as the Detail Emphasis Module which computes a weight vector to re-calibrate channel-wise</cell></row><row><cell cols="8">features. Finally, we upsample the predicted disparities at multiple scales to original input resolutions. Besides, the pose network takes</cell></row><row><cell cols="2">temporally adjacent images It, I</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>t as input and outputs relative pose T t?t .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Depth network architecture. For symbols in this table, k: kernel size, s: stride, chns: the number of output channels, res: the downscaling factor relative to the input image, input: corresponds to the input of each layer, activation: activation function. ? refers to a 2? nearest-neighbor upsampling. Encoder blocks are denoted by econv * naming convention. The spm and dem represent structure perception module and detail emphasis module.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Quantitative results on the KITTI improved ground truth. Comparison of the existing methods to our CADepth-Net on KITTI 2015<ref type="bibr" target="#b8">[9]</ref> using annotated depth maps from<ref type="bibr" target="#b41">[42]</ref>. Best results are in bold, with second best underlined. For Abs Rel, Sq Rel, RMSE and RMSE log lower is better, and for ? &lt; 1.25, ? &lt; 1.25 2 and ? &lt; 1.25 3 higher is better. In the Train column, S: Self-supervised stereo supervision, M: Self-supervised mono supervision, D*: Auxiliary depth supervision. ? refers to the newer results from github. In Dataset column, CS: Cityscapes dataset<ref type="bibr" target="#b2">[3]</ref>, K: KITTI datasets<ref type="bibr" target="#b8">[9]</ref>. Results on KITTI depth prediction benchmark. D refers to ground truth depth supervision, while M and S are monocular and stereo self-supervision respectively.</figDesc><table><row><cell>Method</cell><cell>Train</cell><cell cols="5">SILog sqErrorRel absErrorRel iRMSE</cell></row><row><cell>DHGRL [53]</cell><cell>D</cell><cell>15.47</cell><cell>4.04</cell><cell></cell><cell>12.52</cell><cell>15.72</cell></row><row><cell>CSWS [25]</cell><cell>D</cell><cell>14.85</cell><cell>3.48</cell><cell></cell><cell>11.84</cell><cell>16.38</cell></row><row><cell>APMoE [23]</cell><cell>D</cell><cell>14.74</cell><cell>3.88</cell><cell></cell><cell>11.74</cell><cell>15.63</cell></row><row><cell>DABC [26]</cell><cell>D</cell><cell>14.49</cell><cell>4.08</cell><cell></cell><cell>12.72</cell><cell>15.53</cell></row><row><cell>DORN [7]</cell><cell>D</cell><cell>11.77</cell><cell>2.23</cell><cell></cell><cell>8.78</cell><cell>12.98</cell></row><row><cell>Monodepth [10]</cell><cell>S</cell><cell>22.02</cell><cell>20.58</cell><cell></cell><cell>17.79</cell><cell>21.84</cell></row><row><cell>LSIM [12]</cell><cell>S</cell><cell>17.92</cell><cell>6.88</cell><cell></cell><cell>14.04</cell><cell>17.62</cell></row><row><cell>Monodepth2 [11]</cell><cell>M</cell><cell>15.57</cell><cell>4.52</cell><cell></cell><cell>12.98</cell><cell>16.70</cell></row><row><cell>SGDepth [22]</cell><cell>M</cell><cell>15.30</cell><cell>5.00</cell><cell></cell><cell>13.29</cell><cell>15.80</cell></row><row><cell>Monodepth2 [11]</cell><cell>MS</cell><cell>15.07</cell><cell>4.16</cell><cell></cell><cell>11.64</cell><cell>15.27</cell></row><row><cell>Ours</cell><cell>MS</cell><cell>13.34</cell><cell>3.33</cell><cell></cell><cell>10.67</cell><cell>13.61</cell></row><row><cell></cell><cell cols="2">Backbone</cell><cell>Abs Rel</cell><cell>Sq Rel</cell><cell>RMSE</cell><cell>? &lt;1.25</cell></row><row><cell>SGDepth [22]</cell><cell>R18</cell><cell></cell><cell>0.117</cell><cell>0.907</cell><cell>4.844</cell><cell>0.875</cell></row><row><cell>monodepth2 [11]</cell><cell>R18</cell><cell></cell><cell>0.115</cell><cell>0.903</cell><cell>4.863</cell><cell>0.877</cell></row><row><cell>Ours</cell><cell>R18</cell><cell></cell><cell>0.110</cell><cell>0.812</cell><cell>4.686</cell><cell>0.882</cell></row><row><cell>monodepth2 [11]</cell><cell>R50</cell><cell></cell><cell>0.110</cell><cell>0.831</cell><cell>4.642</cell><cell>0.883</cell></row><row><cell>Johnston et al. [18]</cell><cell cols="2">R101</cell><cell>0.106</cell><cell>0.861</cell><cell>4.699</cell><cell>0.889</cell></row><row><cell>Ours</cell><cell>R50</cell><cell></cell><cell>0.105</cell><cell>0.769</cell><cell>4.535</cell><cell>0.892</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Comparisons of methods with the same backnone on the KITTI Eigen Split. R: ResNet. All models are trained with the same settings.</figDesc><table /><note>the supervised methods.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8001" to="8008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vision for mobile robot navigation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guilherme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Avinash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="237" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2283</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learn stereo, infer mono: Siamese networks for self-supervised, monocular, depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantically-guided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12319</idno>
	</analytic>
	<monogr>
		<title level="m">Rares Ambrus, and Adrien Gaidon</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">KITTI Single Depth Evaluation Server</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pixel-wise attentional gating for parsimonious pixel labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01556</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
	<note>Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with hierarchical fusion of dilated cnns and softweighted-sum inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep attention-based classification network for robust depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Hang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="663" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Hr-depth: High resolution self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07356</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Signet: Semantic instance aided unsupervised 3d geometry perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Sunarjo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Bharadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9810" to="9820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dtam: Dense tracking and mapping in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2320" to="2327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feature-metric loss for self-supervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="572" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Defeat-net: General monocular depth via simultaneous unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hadfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14402" to="14413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Miguel</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth hints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turmukhambetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2162" to="2171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lego: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03665</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamara</forename><surname>Saroj Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep hierarchical guidance and regularization learning for end-to-end depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="430" to="442" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards better generalization: Joint depth-pose learning without posenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhi</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9151" to="9161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised high-resolution depth learning from videos with dual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihuai</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6872" to="6881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="36" to="53" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
