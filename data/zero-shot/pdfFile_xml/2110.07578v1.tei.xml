<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Temporal 3D Human Pose Estimation with Pseudo-Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arij</forename><surname>Bouazizi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Universit?t Ulm</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Kressel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mercedes-Benz AG</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universit?t Ulm</orgName>
								<address>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Temporal 3D Human Pose Estimation with Pseudo-Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple, yet effective, approach for selfsupervised 3D human pose estimation. Unlike the prior work, we explore the temporal information next to the multi-view self-supervision. During training, we rely on triangulating 2D body pose estimates of a multiple-view camera system. A temporal convolutional neural network is trained with the generated 3D ground-truth and the geometric multi-view consistency loss, imposing geometrical constraints on the predicted 3D body skeleton. During inference, our model receives a sequence of 2D body pose estimates from a singleview to predict the 3D body pose for each of them. An extensive evaluation shows that our method achieves state-of-theart performance in the Human3.6M and MPI-INF-3DHP benchmarks. Our code and models are publicly available at https://github.com/vru2020/TM_HPE/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the 3D human body posture from an image is a long-standing problem in computer vision with many applications such as trajectory forecasting <ref type="bibr" target="#b10">[11]</ref> and gesture recognition <ref type="bibr" target="#b29">[30]</ref>. The main research trend is the end-to-end 3D body pose estimation with deep neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>. In this course, several approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref> adopt an off-the-shelf 2D body pose estimator to predict the 2D joint positions in the image space, followed by a 2D-3D lifting. Despite the fact that these approaches achieve promising results in standard benchmarks, most of them have the disadvantage of requiring ground-truth data. Acquiring 3D keypoints is not only expensive, but also hard to obtain due to the lack of the third dimension when annotating images. This bottleneck significantly hinders the application in unconstrained scenarios, since it requires annotating new data.</p><p>Weakly and self-supervised learning methods relaxed the need of 3D ground-truth body poses by exploiting unpaired 2D and 3D body poses <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref> or multi-view images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. Nevertheless, not a single method explores the E-mail: firstname.lastname@{daimler.com, uni-ulm.de}. <ref type="figure" target="#fig_0">Figure 1</ref>: Qualitative results on Human3.6M <ref type="bibr" target="#b11">[12]</ref>. We present a self-supervised learning approach for single human 3D body pose estimation from sequences of 2D body pose estimates. Our model achieves competitive results with stateof-the art fully-supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours Sequence</head><p>temporal information next to the multi-view self-supervision. This work presents a simple and effective approach for temporal 3D human pose estimation using 3D pseudo-labels and a temporal model.</p><p>We phrase the 3D human pose estimation problem as a 2D pose estimation followed by a 2D-3D lifting. During training, we rely on a multiple-view camera system and 2D body pose estimates from each camera view to create 3D pseudo-labels via triangulation. A temporal convolutional neural network <ref type="bibr" target="#b23">[24]</ref> is then trained with the generated 3D ground-truth. To further constrain the 3D search space, we present the multiview consistency objective as a geometrical constraint on the predicted 3D body skeleton. During inference, our approach receives a sequence of 2D body pose estimates as input to predict the 3D body pose for each of them. It is important to note that a multiple-view configuration is only necessary during training.</p><p>We empirically show the benefit of modeling the temporal information over single-frame approaches. We conduct an extensive evaluation on two publicly available benchmarks, Human3.6M <ref type="bibr" target="#b11">[12]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b21">[22]</ref>. Our approach achieves state-of-the-art performance on Human3.6M, improving upon the previous self-supervised approaches by 25.0%. Our results are also competitive to the fully supervised approaches, which rely on 3D ground-truth body poses and temporal information. On MPI-INF-3DHP, our method yields the lowest average error, which outperforms most state-of-the-art fully-supervised approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In the remainder of the paper, we first discuss the related 3D human pose estimation approaches. We then introduce our self-supervised video based approach and finally demonstrate that our model achieves competitive results compared with the state-of-the art fully-supervised methods on standard 3D human pose estimation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Similar to the literature <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5]</ref>, the related work can be partitioned into supervised, unsupervised, self-and weakly-supervised learning. Below, we discuss the related approaches to our method.</p><p>Supervised Learning There is a vast literature on 3D human pose estimation based on ground-truth labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b3">4]</ref>. While the current state-of-the-art is completely based on deep neural networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, the prior work also includes graphical models with hand-crafted features, such as pictorial structures models <ref type="bibr" target="#b2">[3]</ref>. Most of the currently top-performing methods build on top of an off-the-shelf 2D image-based pose estimator for 2D keypoint detection and then perform lifting to the 3D coordinate space <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>. Also, there have been efforts in exploring temporal information with deep neural networks. For instance, Pavallo et. al. <ref type="bibr" target="#b23">[24]</ref> show that video-based 3D body pose can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. Despite the remarkable results, these approaches require ground-truth information during training. Our work reaches similar performance to supervised learning by only harnessing 3D pseudo-labels.</p><p>Unsupervised Learning Unsupervised learning approaches omit the need of labeling the data. Adversarial learning, for instance compensates for the lack of groundtruth information <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17]</ref> by imposing a geometric prior on the 3D structure. Chen et. al. <ref type="bibr" target="#b5">[6]</ref> introduce the project-lift-project training strategy. The approach is motivated by the fact that predicted 3D skeletons can be randomly rotated and projected without any change in the distribution of the resulting 2D skeletons. While the adversarial training arguably removes the dependence on data annotation, it is inherently still ambiguous as multiple 3D poses can map to the same 2D keypoints. Wandt et. al. <ref type="bibr" target="#b28">[29]</ref> propose to alleviate the modeling ambiguity of the 3D body pose by projecting the 2D detections from one view to another view via a canonical pose space. Noteworthy, these works, although effective, still need adversarial learning and are far inferior to fully supervised approaches. In this work, we present a simpler yet more effective approach, which learns a temporal model from body pose estimates and multiple-view geometry. As our experiments show, we achieve better performance without the need for adversarial learning.</p><p>Self-and Weakly-Supervised Learning Self and weaklysupervised approaches tackle the generalization problem by learning a meaningful representation from unlabeled samples in other domains. The supervision stems from unpaired 2D and 3D body pose annotations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b7">8]</ref> or from multi-view images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>. Kocabas et. al. <ref type="bibr" target="#b14">[15]</ref> triangulate 2D pose estimates in a multi-view environment to generate pseudo-labels for 3D body pose training. Wandt et. al. <ref type="bibr" target="#b27">[28]</ref> propose the re-projection network to learn the mapping from 2D to the 3D body pose distribution using adversarial learning. In particular, the critic network improves the generated 3D body pose estimate based on the Wasserstein loss <ref type="bibr" target="#b0">[1]</ref> and unpaired 2D and 3D body poses. Similarly, Drover et. al. <ref type="bibr" target="#b7">[8]</ref> rely on a discriminator network for supervision of 2D body pose projections. However, the method additionally utilizes 3D ground-truth data to generate synthetic 2D body joints during the training. Kundu et. al. <ref type="bibr" target="#b16">[17]</ref> present a self-supervised 3D pose estimation approach with an interpretable latent space. To better generalize across scenes and datasets, the approach still relies on unpaired 3D poses. Tripathi et. al. <ref type="bibr" target="#b25">[26]</ref> propose a method to regress the 3D keypoints by incorporating the temporal information next to the adversarial objective. A network distillation is employed for additional supervision. Instead of adversarial learning with unpaired 3D poses, we rely on a multi-view system to reach the same goal. Different from all these methods, we are the first to leverage multi-view video sequences via self-supervised learning. In Tab. 1 we give a characteristic comparison of our approach against prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We present our approach to infer the 3D body pose of a single person in video sequences. Instead of performing 3D body pose estimation directly on each image frame, we extract 2D body pose estimates y 0 , . . . , y T over time T , which composes the input to our approach. Our goal is to regress the 3D body pose Y 0 , . . . , Y T for each 2D pose estimate, where the 3D body pose Y t ? R 3?N at the time step t ? T consists of N joints. We describe the 2D-to-3D body pose mapping as:</p><formula xml:id="formula_0">Y 0 , . . . , Y T = f (y 0 , . . . , y T ; ?)<label>(1)</label></formula><p>where f : R 2?N ?T ? R 3?N ?T corresponds to the mapping function. We approximate the mapping function with a convolutional neural network that is parametrized by ?.</p><p>Learning the model parameters normally is performed with ground-truth information. In this work, we propose to learn the model parameters with supervision, which we derive from a multiple-view camera system and 2D body pose estimates.</p><p>We assume having access to a multiple-view timesynchronized system with C cameras and an off-the-shelf 2D body pose detector. During training, a 2D pose sequence (y c,0 , . . . , y c,T ) from a camera c is subsequently fed to the convolutional neural network f ? to predict the 3D body poses. To constrain the three-dimensional search space of possible poses, we make use of multiple-view geometry to obtain pseudo 3D body poses for L tri . The triangulation loss L tri minimizes the difference between the 3D body pose predictions Y and triangulated poses? tri (set as ground-truth). To ensure a view consistency, we rely on the geometric consistency loss L con , which enforces the network to learn poses that are view invariant. The estimated keypoints from two different views can be transformed to each other via a known rigid transformation (Translation and Rotation). The proposed learning approach is then self-supervised with the geometric loss functions, namely the input triangulation L tri and the geometric consistency L con . <ref type="figure">Fig. 2</ref> shows the overall framework of our proposed method. Note that our learning algorithm makes use of all available camera views during training, while the inference is single-view. Next, we present the motivation and elements of the proposed loss functions in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Triangulation Loss</head><p>We make use of the Direct Linear Triangulation (DLT) <ref type="bibr" target="#b9">[10]</ref> method for obtaining the 3D keypoint position from the 2D body pose estimates. We apply the same approach to all body pose landmarks. Similar to <ref type="bibr" target="#b14">[15]</ref>, we consider the obtained 3D human pose estimate? tri as ground-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional</head><p>Neural Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Pose Sequence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Pose Estimation</head><p>Convolutional Neural Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lifting Network</head><p>Shared Weights Video Sequence Camera 1 Camera 2 <ref type="figure">Figure 2</ref>: Illustration of our learning algorithm. During training, we optimize the lifting network with dilated temporal convolution f ? (?) to map a sequence of 2D pose estimate to 3D body poses with geometric loss functions. As input to our model we assume multiple view 2D pose detections as shown on the left. First we compute the triangulated 3D pose from the 2D detections as pseudo-labels for the input triangulation loss L tri . The Multi-view consistency loss L con is then used to enforce that the estimated keypoints from both views can be transformed to each other via the known rigid transformation. Both losses are then combined to train the 3D pose network f ? . During inference, our approach is only single-view. Note that our approach targets single person 3D body pose estimation.</p><p>truth. The input triangulation loss is defined by:</p><formula xml:id="formula_1">L tri = C c=1 T t=1 ? w?c (? tri,t ) ? f ? (y c,t ) 2 , (2)</formula><p>where ? w?c (?) corresponds to the transformation from the world coordinate system w to the camera coordinate system c. It is given by:</p><formula xml:id="formula_2">? w?c = R w?c (? tri ? T w?c ),<label>(3)</label></formula><p>where R w?c ? R 3?3 denote the rotation matrix and T w?c ? R 3?1 for the corresponding translation vector.</p><p>Since the triangulation loss highly depends on the quality of the detected landmarks for each camera view, relying only on the input triangulation would make the 3D body poses fixed during the training. The errors of 3D reconstruction will then directly propagate to the network f ? . To this end, we propose to transform each predicted pose to another camera view using the geometric consistency loss, as presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Geometric Consistency Loss</head><p>The goal of the geometric consistency loss is to ensure that the predicted 3D body pose is consistent across different views. Specifically, the 3D body pose Y, when accordingly rotated and translated should be consistent with the corresponding pose in the second view, regardless of the 2D body pose input. Based on this observation, we use the consistency loss that is given by:</p><formula xml:id="formula_3">L con = C c=1 C c =1 c =c T t=1 f ? (y c,t ) ? ? c ?c (f ? (y c ,t )) 2 (4)</formula><p>where ? c ?c (?) corresponds to the transformation from the camera c to the camera c coordinate system. It is defined as:</p><formula xml:id="formula_4">? c ?c = R c ?c f ? (y c,t ) ? T c ?c .<label>(5)</label></formula><p>Moreover, the camera transformation is given by:</p><formula xml:id="formula_5">R c ?c = R c R c and T c ?c = R c (T c ? T c ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Complete Objective</head><p>With the geometric consistency loss, the model learns to predict body poses that are robust to camera view changes. Due to the fact that the ground-truth 3D body pose is unknown during training, the multi-view consistency is used as a form of self-supervision. Enforcing only multi-view consistency is not sufficient to infer accurate 3D body poses across different camera views, since it will lead to a degenerate solution where all keypoints collapse to the same pose. To this end, both the triangulation loss L tri and L con are used to train the network f ? . To learn the parameters ?, we train our model based on the proposed loss functions and the training samples from all camera views. We obtain the model parameters by minimizing the following objective:</p><formula xml:id="formula_6">? = arg min ? L tri + L con ,<label>(7)</label></formula><p>A summary of the training of our method is illustrated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We quantitatively evaluate our method on two 3D human pose estimation benchmarks, namely the Human3.6M <ref type="bibr" target="#b11">[12]</ref> and MPII-INF-3DHP <ref type="bibr" target="#b21">[22]</ref>. For Human3.6M, we train our model on five subjects (S1, S5, S6, S7 and S8) and evaluate on two subjects (S9 and S11). We use three evaluation protocols: Protocol 1 refers to the Mean Per Joint Position Error (MPJPE). Protocol 2 results in the Mean Per Joint Position after procrustes alignment to the ground-truth 3D poses by a rigid transformation (PMPJPE) and Protocol 3 aligns the predicted poses with the ground-truth only in scale (N-MPJPE). For MPI-INF-3DHP <ref type="bibr" target="#b21">[22]</ref>, which is a recently published dataset with 8 actors performing 8 actions, </p><formula xml:id="formula_7">i = 1 to Sc do Compute ? ? Ltri(?c,i, Y c,i; ?i) Compute ? ? Lcon(?c?c ,i, Y c ,i; ?i) Compute complete objective ? ? Ls(?i, Y i; ? i ) using Equation 7</formula><p>Update parameters:</p><formula xml:id="formula_8">? i ? ? i ? ?? ? Ls(?i, Y i; ? i ) . end end</formula><p>we also follow the standard protocol <ref type="bibr" target="#b21">[22]</ref>: The five chestheight cameras, which provide 17 joints (compatible with Human3.6M <ref type="bibr" target="#b11">[12]</ref>) are used for training. For evaluation, we use the official test set which includes challenging outdoor scenes. We report the results by means of 3D Percentage of Correct Keypoints (PCK) with a threshold of 150mm and the corresponding Area Under Curve AUC so that we are consistent with <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our network is a fully convolutional architecture with residual connections and dilated convolutions <ref type="bibr" target="#b23">[24]</ref>. Unlike recurrent structures that do not support parallelization over time and tend to drift over long sequences <ref type="bibr" target="#b23">[24]</ref>, dilated temporal convolutions are computationally efficient and maintain the long-term coherence. We choose four different frame sequence lengths when conducting our experiments, i.e.f = 1, f = 27, f = 81, f = 243. The influence of the number of frames is discussed in section 4.5. Pose flipping is applied as data augmentation during training. We train our model using the Adam optimizer for 60 epochs with weight decay of 0.1. An exponential learning rate decay schedule with the initial learning rate of 2e ?4 and decay factor of 0.98 after each epoch is adopted. The batch size is set to 1024. As a 2D pose detector, we used, following <ref type="bibr" target="#b23">[24]</ref> the cascaded pyramid network (CPN) <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Human3.6M Evaluation</head><p>We first report the result of all single 15 action on the Human3.6M dataset and compare with state-of-the-art approaches. The results in Tab. 2 and 3, show that our self-supervised method outperforms all weakly and selfsupervised methods by a large margin. Our approach also compares favorably to the state-of-the-art fully-supervised approaches. It achieves an MPJPE of 50.6mm, which is only  3mm higher than the supervised approach from <ref type="bibr" target="#b23">[24]</ref> using 3D ground-truth keypoints and 26mm better than <ref type="bibr" target="#b14">[15]</ref> that is relying only on multi-view geometry. For protocol 2, we also obtain the best overall result of 40.0mm as shown in Tab. 3. This clearly demonstrates the advantage of incorporating the temporal information over single-frame approaches. Compared to the self-supervised approach of <ref type="bibr" target="#b25">[26]</ref>, which makes use of unpaired 3D pose keypoints next to the temporal information, our approach still yields an error reduction of 33.2%.</p><p>In addition, our method reaches more accurate pose predictions than the fully-supervised approach of Pavallo et. al.</p><p>[24] on difficult actions like SittingDown, WalkDog, Photo. <ref type="figure" target="#fig_1">Fig. 3</ref> provides a visual comparison between the predicted poses and the ground-truth 3D body poses. We evaluate our self-supervised approach on the three challenging actions Directions, WalkDog, Greeting and show that our network is able to infer the 3D body poses in an accurate way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">MPII-INF-3DHP Evaluation</head><p>We report the results on the MPII-INF-3DHP dataset in Tab. 4 and compare to other state-of-the-art approaches. For a fair comparison, our model utilizes the same 2D pose key-points as in <ref type="bibr" target="#b14">[15]</ref>. Our method significantly outperforms all weakly and self-supervised approaches across different evaluation metrics and obtains 30% better PCK than <ref type="bibr" target="#b14">[15]</ref> that also uses multiple-view geometry. This clearly demonstrates the advantage of incorporating the temporal information in a multi-view setting. Our method yields the lowest average PMPJPE of 51.1mm as shown in Tab. 4, which is comparable to most state-of-the-art method. We also quantitatively evaluate the performance on MPII-INF-3DHP given a model trained on Human3.6M. Without any training or fine-tuning, our model reaches a PCK of 74%, which is better than previous weakly-and self-supervised approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref> trained on this dataset. These results suggest that the generalization is significantly improved by incorporating the temporal information and the self-consistency reasoning. To further demonstrate the effectiveness of our training strategy, we qualitatively evaluate our approach on constrained indoor scenes and complex outdoor scenes, covering a notable diversity of poses. As shown in the <ref type="figure" target="#fig_2">Fig. 4</ref>, our approach is able to generalize across unseen poses, appearances and subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>To verify the contribution of the individual components of our method on the performance, we conduct extensive ablation experiments on the Human3.6M dataset.</p><p>Impact of the 2D Pose Estimation Since the performance of the lifting network highly depends on the 2D pose estimator, we evaluate different 2D detectors that do not rely on ground-truth bounding boxes and report the results in Tab. 6. The performance with Cascaded Pyramid Network CPN <ref type="bibr" target="#b6">[7]</ref> is about 5mm better than the model trained with Detectron 1 landmarks. To further investigate the lower bound of our method, we directly use the ground-truth 2D keypoints as input to alleviate error caused by noisy detections. Following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>, we then add different levels of gaussian noise N (0, ?) to the ground-truth 2D keypoints and report the results under protocol 2 in Tab. 8. The results indicate that the noise has a major impact on the model performance.</p><p>Impact of the multi-view information We show the results for the training with different number of cameras in Tab. 5. Since multiple views are observed and the temporal information is not discarded leading to more accurate body posture and constant bone lengths, our method yields the lowest average PMPJPE of 50.60mm with four cameras. While the performance slightly drops due to the lower number of training samples and views, our approach still produces promising results which enables the use of our model in the-wild. We further illustrate the ablation study on the consistency constraint. We see a 2.1% error drop (51.4mm ? 50.60mm), when removing the consistency constraint. This shows that the geometric consistency reasoning indeed improve the 3D body pose predictions.</p><p>Impact of the receptive field To verify the impact of the receptive field, we choose four different frame sequence lengths when conducting our experiments. Since the temporal dimension of video sequences encodes valuable information, the lowest average error is achieved with the largest receptive field. Nevertheless we still achieve very promising results with f = 27 and f = 81. This clearly demonstrates the advantage of incorporating the temporal information, which encourages smooth motions and alleviates drift over long sequences .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented an approach for 3D human pose estimation that explores the body pose temporal information combined with multi-view self-supervision. Our method requires a multi-view configuration only during training to obtain 3D body pose estimates by triangulation. The obtained pseudolabels are then used to train a temporal convolutional neural network by additionally employing a geometric multi-view    consistency constraint. During inference, our approach predicts the 3D body pose of a single individual from a sequence of 2D body pose estimates. In our experiments, we can achieve a performance that is competitive to fullysupervised learning. Without fine-tuning or retraining, our model is able to generalize to different scenes in the wild. Finally, we further examined the contribution of each loss function and the impact of different 2D body pose detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>Part of this work was supported by the research project "KI Delta Learning" (project number: 19A19013A) funded by the Federal Ministry for Economic Affairs and Energy <ref type="table">Table 8</ref>: Evaluation results for protocol-II with the 243-frames temporal model and different levels of gaussian noise N (0, ?) (? is the standard deviation) added to the ground-truth 2D positions (GT). All values are given in mm.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>3D Human Pose Estimation Training Algorithm using Temporal Information and Multiview Geometry. Input :2D pose estimate (y0, . . . , yT ), learning rate ?, number of samples Sc, cameras c and c . Output :3D pose estimations (Y0, . . . , YT ). for epoch &lt; epochmax do for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative results on Human3.6M dataset Top: Video Sequence Middle: 3D Ground-truth Bottom: 3D Pose Reconstruction (BMWi) on the basis of a decision by the German Bundestag. Test sequence1 frame 1, 200, 400, 600, 800, 1000, 1200, 1400. Test sequence2 frame 1, 200, 400, 600, 800, 1000, 1200, 1400. Test sequence3 frame 1, 200, 400, 600, 800, 1000, 1200, 1400.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on MPII-INF-3DHP dataset Top: Video Sequence Middle: 3D Ground-truth Bottom: 3D Pose Reconstruction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Characteristic comparison of our approach against prior weakly, self-and supervised approaches, in terms of different levels of supervision</figDesc><table><row><cell>Methods</cell><cell>Paired supervision (MV: muti-view) MV 2D pair pose</cell><cell>Unpaired 3D pose Supervision</cell><cell>3D pose Ground-Truth Supervision</cell></row><row><cell>Pavllo et al. [24]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Martinez et al. [21]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kocabas et al. [15]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wandt et al. [28]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Chen et al. [6]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tripathi et al. [26]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on the Human3.6M dataset. Comparison of our self-supervised approach with state-of-the-art weakly-and fully-supervised methods following evaluation Protocol-I (without rigid alignment) individually for all 15 actions. All values are given in mm.</figDesc><table><row><cell cols="2">Supervision Method</cell><cell>Dir.</cell><cell>Dis.</cell><cell cols="5">Eat Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="6">SitD Smoke Wait WalkD Walk WalkT</cell><cell>Avg</cell></row><row><cell></cell><cell>Zhou et al. [33]</cell><cell cols="3">91.8 102.4 96.7 98.8</cell><cell cols="3">113.4 125.2 90.0</cell><cell>93.8</cell><cell cols="3">132.2 159.0 107.0</cell><cell>94.4</cell><cell>126.0</cell><cell>79.0</cell><cell>99.0</cell><cell>107.3</cell></row><row><cell></cell><cell>Lu et al. [20]</cell><cell cols="3">68.4 77.3 70.2 71.4</cell><cell>75.1</cell><cell>86.5</cell><cell>69.0</cell><cell>76.7</cell><cell cols="2">88.2 103.4</cell><cell>73.8</cell><cell>72.1</cell><cell>83.9</cell><cell>58.1</cell><cell>65.4</cell><cell>76.0</cell></row><row><cell>Full</cell><cell>Pavlakos et al. [23]</cell><cell cols="3">67.4 71.9 66.7 69.1</cell><cell>72.0</cell><cell>77.0</cell><cell>65.0</cell><cell>68.3</cell><cell>83.7</cell><cell>96.5</cell><cell>71.7</cell><cell>65.8</cell><cell>74.9</cell><cell>59.1</cell><cell>63.2</cell><cell>71.9</cell></row><row><cell></cell><cell>Martinez et al. [21]</cell><cell cols="3">51.8 56.2 58.1 59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell></cell><cell>Pavllo et al. [24]</cell><cell cols="3">45.1 47.4 42.0 46.0</cell><cell>49.1</cell><cell>56.7</cell><cell>44.5</cell><cell>44.4</cell><cell>57.2</cell><cell>66.1</cell><cell>47.5</cell><cell>44.8</cell><cell>49.2</cell><cell>32.6</cell><cell>34.0</cell><cell>47.1</cell></row><row><cell></cell><cell>Li et al. [18]</cell><cell cols="3">62.0 69.7 64.3 73.6</cell><cell>75.1</cell><cell>84.8</cell><cell>68.7</cell><cell>75.0</cell><cell cols="2">81.2 104.3</cell><cell>70.2</cell><cell>72.0</cell><cell>75.0</cell><cell>67.0</cell><cell>69.0</cell><cell>73.9</cell></row><row><cell>Weak</cell><cell>Wandt et al. [28]</cell><cell cols="3">77.5 85.2 82.7 93.8</cell><cell>93.9</cell><cell cols="5">101.0 82.9 102.6 100.5 125.8</cell><cell>88.0</cell><cell>84.8</cell><cell>72.6</cell><cell>78.8</cell><cell>79.0</cell><cell>89.9</cell></row><row><cell></cell><cell cols="4">Ours (self-supervised) 48.2 49.3 46.5 48.4</cell><cell>52.4</cell><cell>46.5</cell><cell>46.4</cell><cell>61.4</cell><cell>72.3</cell><cell>51.0</cell><cell>59.8</cell><cell>46.7</cell><cell>37.5</cell><cell>52.1</cell><cell>39.1</cell><cell>50.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on the Human3.6M dataset. Comparison of our self-supervised approach with state-of-the-art supervised and weakly-and self-supervised methods following evaluation Protocol-II (with rigid alignment) individually for all 15 actions. All values are given in mm.</figDesc><table><row><cell cols="2">Supervision Method</cell><cell cols="8">Dir. Dis. Eat Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="6">SitD Smoke Wait WalkD Walk WalkT</cell><cell>Avg</cell></row><row><cell></cell><cell>Zhou et al. [34]</cell><cell cols="7">99.7 95.8 87.9 116.8 108.3 107.3 93.5</cell><cell>95.3</cell><cell cols="8">109.1 137.5 106.0 102.2 110.4 106.5 115.2 106.7</cell></row><row><cell></cell><cell>Bogo et al. [4]</cell><cell cols="4">62.0 60.2 67.8 76.5</cell><cell>92.1</cell><cell>77.0</cell><cell>73.0</cell><cell>75.3</cell><cell cols="2">100.3 137.3</cell><cell>83.4</cell><cell>77.3</cell><cell>79.7</cell><cell>48.0</cell><cell>87.7</cell><cell>82.3</cell></row><row><cell>Full</cell><cell>Martinez et al. [21]</cell><cell cols="4">39.5 43.2 46.4 47.0</cell><cell>51.0</cell><cell>56.0</cell><cell>41.4</cell><cell>40.6</cell><cell>56.5</cell><cell>69.4</cell><cell>49.2</cell><cell>45.0</cell><cell>38.0</cell><cell>49.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell></cell><cell>Lu et al. [20]</cell><cell cols="4">40.8 44.6 42.1 45.1</cell><cell>48.3</cell><cell>54.6</cell><cell>41.2</cell><cell>42.9</cell><cell>55.5</cell><cell>69.9</cell><cell>46.7</cell><cell>42.5</cell><cell>36.0</cell><cell>48.0</cell><cell>41.4</cell><cell>46.6</cell></row><row><cell></cell><cell>Pavllo et al. [24]</cell><cell cols="4">34.2 36.8 33.9 37.5</cell><cell>37.1</cell><cell>43.2</cell><cell>34.4</cell><cell>33.5</cell><cell>45.3</cell><cell>52.7</cell><cell>37.7</cell><cell>34.1</cell><cell>38.0</cell><cell>25.8</cell><cell>27.7</cell><cell>36.8</cell></row><row><cell></cell><cell>Wu et al. [31]</cell><cell cols="4">78.6 90.8 92.5 89.4</cell><cell cols="7">108.9 112.4 77.1 106.7 127.4 139.0 103.4</cell><cell>91.4</cell><cell>79.1</cell><cell>-</cell><cell>-</cell><cell>98.4</cell></row><row><cell></cell><cell>Tung et al. [27]</cell><cell cols="4">77.6 91.4 89.9 88.0</cell><cell cols="7">107.3 110.1 75.9 107.5 124.2 137.8 102.2</cell><cell>90.3</cell><cell>78.6</cell><cell>-</cell><cell>-</cell><cell>97.2</cell></row><row><cell>Weak</cell><cell>Wandt et al. [28]</cell><cell cols="4">53.0 58.3 59.6 66.5</cell><cell>72.8</cell><cell>71.0</cell><cell>56.7</cell><cell>69.6</cell><cell>78.3</cell><cell>95.2</cell><cell>66.6</cell><cell>58.5</cell><cell>63.2</cell><cell>57.5</cell><cell>49.9</cell><cell>65.1</cell></row><row><cell></cell><cell>Zhou et al. [32]</cell><cell cols="4">54.8 60.7 58.2 71.4</cell><cell>62.0</cell><cell>65.5</cell><cell>53.8</cell><cell>55.6</cell><cell cols="2">75.2 111.6</cell><cell>64.1</cell><cell>66.0</cell><cell>50.4</cell><cell>63.2</cell><cell>55.3</cell><cell>64.9</cell></row><row><cell></cell><cell>Drover et al. [8]</cell><cell cols="4">60.2 60.7 59.2 65.1</cell><cell>65.5</cell><cell>63.8</cell><cell>59.4</cell><cell>59.4</cell><cell>69.1</cell><cell>88.0</cell><cell>64.8</cell><cell>60.8</cell><cell>64.9</cell><cell>63.9</cell><cell>65.2</cell><cell>64.6</cell></row><row><cell></cell><cell>Chen et al. [6]</cell><cell cols="4">55.0 58.3 67.5 61.8</cell><cell>76.3</cell><cell>64.6</cell><cell>54.8</cell><cell>58.3</cell><cell>89.4</cell><cell>90.5</cell><cell>71.7</cell><cell>63.8</cell><cell>65.2</cell><cell>63.1</cell><cell>65.6</cell><cell>68.0</cell></row><row><cell>Self</cell><cell>Kocabas et al. [15]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>67.5</cell></row><row><cell></cell><cell>Bouazizi et al. [5]</cell><cell cols="4">49.4 51.7 61.7 56.5</cell><cell>64.9</cell><cell>67.1</cell><cell>51.6</cell><cell>52.1</cell><cell cols="2">83.9 111.3</cell><cell>60.5</cell><cell>54.7</cell><cell>56.9</cell><cell>45.9</cell><cell>53.6</cell><cell>62.0</cell></row><row><cell></cell><cell>Tripathi et al. [26]</cell><cell cols="4">49.1 52.4 57.5 56.4</cell><cell>63.5</cell><cell>59.5</cell><cell>51.3</cell><cell>48.4</cell><cell>77.1</cell><cell>81.5</cell><cell>60.4</cell><cell>59.6</cell><cell>53.5</cell><cell>59.1</cell><cell>51.4</cell><cell>59.4</cell></row><row><cell></cell><cell cols="5">Ours (self-supervised) 37.1 38.4 38.2 39.7</cell><cell>40.9</cell><cell>36.3</cell><cell>35.2</cell><cell>49.4</cell><cell>59.2</cell><cell>40.9</cell><cell>46.3</cell><cell>36.5</cell><cell>29.6</cell><cell>40.6</cell><cell>31.3</cell><cell>40.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Evaluation results for the MPII-INF-3DHP dataset. Best results are marked in bold. MPJPE and PMPJPE are given in mm, PCK and AUC are given in %. Best in bold, second best underlined.</figDesc><table><row><cell cols="2">Supervision Method</cell><cell cols="4">MPJPE? PMPJPE? PCK? AUC?</cell></row><row><cell>Full</cell><cell>Habibie [9]</cell><cell>127.0</cell><cell>92.0</cell><cell>69.6</cell><cell>35.5</cell></row><row><cell></cell><cell>HMR [14]</cell><cell>124.2</cell><cell>86.3</cell><cell>72.9</cell><cell>36.5</cell></row><row><cell></cell><cell cols="2">Kolotouros [16] 105.2</cell><cell>67.5</cell><cell>76.4</cell><cell>37.1</cell></row><row><cell>weak</cell><cell>Rhodin [25]</cell><cell>121.8</cell><cell>-</cell><cell>72.7</cell><cell>-</cell></row><row><cell></cell><cell>HMR [14]</cell><cell>169.5</cell><cell>113.2</cell><cell>59.6</cell><cell>27.9</cell></row><row><cell></cell><cell>Habibie [9]</cell><cell>127.0</cell><cell>92.0</cell><cell>66.8</cell><cell>35.5</cell></row><row><cell></cell><cell cols="2">Kolotouros [16] 124.8</cell><cell>80.4</cell><cell>66.8</cell><cell>30.2</cell></row><row><cell></cell><cell>Iqbal [13]</cell><cell>110.1</cell><cell>-</cell><cell>76.5</cell><cell>-</cell></row><row><cell>self</cell><cell>Kocabas [15]</cell><cell>125.7</cell><cell>-</cell><cell>64.7</cell><cell>-</cell></row><row><cell></cell><cell>Bouazizi [5]</cell><cell>-</cell><cell>-</cell><cell>65.9</cell><cell>32.5</cell></row><row><cell></cell><cell>Chen [6]</cell><cell>-</cell><cell>-</cell><cell>71.1</cell><cell>36.3</cell></row><row><cell></cell><cell>Li [19]</cell><cell>-</cell><cell>-</cell><cell>74.1</cell><cell>41.4</cell></row><row><cell></cell><cell>Kundu [17]</cell><cell>103.8</cell><cell>-</cell><cell>82.1</cell><cell>56.3</cell></row><row><cell></cell><cell>Ours (H36M)</cell><cell>114.7</cell><cell>75.4</cell><cell>74.1</cell><cell>38.8</cell></row><row><cell></cell><cell>Ours (3DHP)</cell><cell>93.0</cell><cell>51.1</cell><cell>81.0</cell><cell>50.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Effect of the multi-view information.</figDesc><table><row><cell></cell><cell>MPJPE?</cell><cell cols="2">PMPJPE? NMPJPE?</cell></row><row><cell>2 cameras</cell><cell>60.00</cell><cell>45.20</cell><cell>57.40</cell></row><row><cell>3 cameras</cell><cell>52.70</cell><cell>41.10</cell><cell>51.10</cell></row><row><cell>4 cameras</cell><cell>50.60</cell><cell>40.00</cell><cell>50.40</cell></row><row><cell>w/o consistency</cell><cell>51.40</cell><cell>41.00</cell><cell>51.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Effect of the 2D Pose Estimation on Human3.6M.</figDesc><table><row><cell></cell><cell>MPJPE?</cell><cell cols="2">PMPJPE? NMPJPE?</cell></row><row><cell>Ground-truth 2D</cell><cell>43.0</cell><cell>33.1</cell><cell>41.1</cell></row><row><cell>Detectron</cell><cell>55.1</cell><cell>43.1</cell><cell>52.2</cell></row><row><cell>CPN [7]</cell><cell>50.6</cell><cell>40.0</cell><cell>48.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Effect of the Receptive Field on Human3.6M.</figDesc><table><row><cell></cell><cell>MPJPE?</cell><cell cols="2">PMPJPE? NMPJPE?</cell></row><row><cell>1 frame</cell><cell>54.7</cell><cell>43.4</cell><cell>52.6</cell></row><row><cell>27 frames</cell><cell>53.1</cell><cell>42.1</cell><cell>51.2</cell></row><row><cell>81 frames</cell><cell>51.2</cell><cell>41.0</cell><cell>49.4</cell></row><row><cell>243 frames</cell><cell>50.6</cell><cell>40.0</cell><cell>48.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Directions S9 frame 1, 100, 200, 300, 400, 500, 600, 700. WalkDog S9 frame 200, 300, 400, 500, 600, 700, 800, 900.</figDesc><table><row><cell>Protocol-II</cell><cell>Dir.</cell><cell>Dis.</cell><cell>Eat</cell><cell cols="5">Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="3">SitD Smoke Wait</cell><cell cols="3">WalkD Walk WalkT</cell><cell>Avg.</cell></row><row><cell>GT</cell><cell cols="3">30.3 33.9 31.0</cell><cell>31.3</cell><cell>33.5</cell><cell>32.4</cell><cell>30.6</cell><cell>40.2</cell><cell>43.6</cell><cell>33.8</cell><cell>37.1</cell><cell>32.2</cell><cell>26.0</cell><cell>33.4</cell><cell>26.5</cell><cell>33.1</cell></row><row><cell>GT + N (0, 5)</cell><cell cols="3">30.4 34.1 30.7</cell><cell>33.0</cell><cell>34.8</cell><cell>32.5</cell><cell>31.2</cell><cell>40.7</cell><cell>47.7</cell><cell>34.8</cell><cell>39.0</cell><cell>31.8</cell><cell>26.8</cell><cell>35.6</cell><cell>27.5</cell><cell>34.1</cell></row><row><cell>GT + N (0, 10)</cell><cell cols="3">32.9 35.7 33.6</cell><cell>36.1</cell><cell>38.0</cell><cell>35.0</cell><cell>32.9</cell><cell>43.1</cell><cell>49.7</cell><cell>36.8</cell><cell>41.6</cell><cell>33.7</cell><cell>28.2</cell><cell>38.3</cell><cell>29.0</cell><cell>36.4</cell></row><row><cell>GT + N (0, 15)</cell><cell cols="3">35.4 37.9 36.5</cell><cell>39.1</cell><cell>39.7</cell><cell>37.5</cell><cell>35.6</cell><cell>45.6</cell><cell>53.6</cell><cell>39.2</cell><cell>44.9</cell><cell>36.8</cell><cell>29.8</cell><cell>40.8</cell><cell>31.4</cell><cell>39.0</cell></row><row><cell>GT + N (0, 20)</cell><cell cols="3">37.2 39.6 38.8</cell><cell>40.9</cell><cell>42.0</cell><cell>39.4</cell><cell>38.0</cell><cell>48.7</cell><cell>55.5</cell><cell>41.9</cell><cell>47.7</cell><cell>38.5</cell><cell>30.1</cell><cell>42.5</cell><cell>31.7</cell><cell>41.0</cell></row><row><cell>Sequence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a) Sequence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b) Sequence</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">(c) Greeting S11 frame 1, 200, 400, 600, 800, 1000, 1200, 1400.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/Detectron/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Holistic human pose estimation with regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Articulated Motion and Deformable Objects</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Self-supervised 3d human pose estimation with multiple-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouazizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiederer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kressel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07777</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised 3d pose estimation with geometric self-supervision. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Rohith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can 3d pose be learned from 2d projections alone?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Phuoc</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="10905" to="10914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Forecasting people trajectories and head poses by jointly reasoning on tracklets and vislets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Setti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsesmelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1267" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hu-man3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly-supervised 3d human pose learning via multi-view images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Endto-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised learning of 3d human pose using multi-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via modelfitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-supervised 3d human pose estimation via part guided novel image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6152" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05770</idno>
		<title level="m">Weakly supervised generative network for multiple 3d human pose hypotheses</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geometry-driven self-supervised method for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1811.04989</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation using transfer learning and improved CNN supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>abs/1611.09813</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno>abs/1811.11742</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Posenet3d: Unsupervised 3d human shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ranade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4364" to="4372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Canonpose: Self-supervised monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13294" to="13304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Traffic control gesture recognition for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiederer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouazizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kressel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10676" to="10683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="365" to="382" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
