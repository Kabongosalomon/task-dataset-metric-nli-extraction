<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Jang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utkarsh</forename><surname>Mall</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: From these unannotated images, we would like a recognition system to discover the concepts of house, grass, trees and sky, and segment each image accordingly without any supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a new framework for semantic segmentation without annotations via clustering. Off-the-shelf clustering methods are limited to curated, single-label, and objectcentric images yet real-world data are dominantly uncurated, multi-label, and scene-centric. We extend clustering from images to pixels and assign separate cluster membership to different instances within each image. However, solely relying on pixel-wise feature similarity fails to learn high-level semantic concepts and overfits to lowlevel visual cues. We propose a method to incorporate geometric consistency as an inductive bias to learn invariance and equivariance for photometric and geometric variations. With our novel learning objective, our framework can learn high-level semantic concepts. Our method, PiCIE (Pixel-level feature Clustering using Invariance and Equivariance), is the first method capable of segmenting both things and stuff categories without any hyperparameter tuning or task-specific pre-processing. Our method largely outperforms existing baselines on COCO [31] and Cityscapes [8] with +17.5 Acc. and +4.5 mIoU. We show that PiCIE gives a better initialization for standard supervised training. The code is available at https:// github.com/janghyuncho/PiCIE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised learning from a set of unlabelled images has gained large popularity, but still is mostly limited to single-class, object-centric images. Consider the images shown in <ref type="figure">Figure 1</ref>  <ref type="bibr">(top)</ref>. Given a collection of these and other unlabeled images, can a machine discover the concepts of "grass", "sky", "house" and "trees" from each image? Going further, can it identify where in each image each concept appears, and segment it out?</p><p>A system that is capable of such unsupervised semantic segmentation can then automatically discover classes of objects with their precise boundaries, thus removing the substantial cost of collecting and labeling datasets such as COCO. It might even discover objects, materials and textures that an annotator may not know of a priori. This can be particularly useful for analyzing novel domains: for example, discovering new kinds of visual structures in satellite imagery. The ability of the system to discover and segment out unknown objects may also prove useful for robots trying to manipulate these objects in the wild.</p><p>However, while unsupervised semantic segmentation might be useful, it is also challenging. This is because it combines the problem of class discovery with the challenge of exhaustive pixel labeling. Recent progress in self- supervised and unsupervised learning suggests that recognition systems can certainly discover image-level classes. However, image-level labeling is easier since the network can simply rely on just a few distinctive, stable features and discard the rest of the image. For example, a recognition system might be able to group all four images of <ref type="figure">Figure 1</ref> together simply by detecting the presence of roof tiles in each image, and ignoring everything else in the images. In contrast, when segmenting the image, no pixel can be ignored; whether it is a distinct object (thing) or a background entity (stuff ), each and every pixel must be recognized and accurately characterized in spite of potentially large intraclass variation. As such, very little prior work has tried to tackle this problem of discovering semantic segmentations, with results limited to extremely coarse stuff segmentation.</p><p>In this paper, we take a step towards a practically useful unsupervised semantic segmentation system: we present an approach that is able to segment out all pixels, be they things or stuff, at a much finer granularity than prior art. Our approach is based on a straightforward objective that codifies only two common-sense constraints. First, pixels that have a similar appearance (i.e., they cluster together in a learned feature space) should be labeled similarly and vice versa. Second, pixel labels should be invariant to color space transformations and equivariant to geometric transformations. Our results show that using these two objectives alone, we can train a ConvNet based semantic segmentation system end-to-end without any labels.</p><p>We find that in spite of its simplicity, our approach far outperforms prior work on this task, more than doubling the accuracy of prior art <ref type="figure">(Figure 1, bottom)</ref>. Our clusteringbased loss function (the first objective above) leads to a much simpler and easier learning problem compared to prior work, which instead tries to learn parametric pixel classifiers. But the invariance and equivariance objectives are key. They allow the convolutional network to connect together pixels across scale, pose and color variation, something that prior systems are unable to do. This increased robustness to invariance also allows our approach to effectively segment objects. We vindicate these intuitions through an ablation study, where we find that each of these contributes significant improvements in performance.</p><p>In sum, our results show that convolutional networks can learn to not only discover image-level concepts, but also semantically parse images without any supervision. This opens the door to true large-scale discovery, where such a trained network can automatically surface new classes of objects, materials or textures from only an unlabeled, uncurated dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning for clustering. Using deep neural networks to learn cluster-friendly embedding space has been widely studied <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b53">54]</ref>. DEC <ref type="bibr" target="#b50">[51]</ref> and IDEC <ref type="bibr" target="#b13">[14]</ref> train embedding function by training autoencoder (AE) <ref type="bibr" target="#b48">[49]</ref> with reconstruction loss. DeepCluster and its variants <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b56">57]</ref> explicitly cluster the feature vectors of the entire dataset using k-means <ref type="bibr" target="#b37">[38]</ref> in order to assign pseudo-labels to each data point, and then train an encoder network. All these methods share a common philosophy that iterative optimization of clustering loss improves the feature space to account for high-level visual similarity.</p><p>Apart from a representation learning perspective, there have been a number of recent works that tackle classification without labels by clustering data points <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55]</ref>. IIC <ref type="bibr" target="#b22">[23]</ref>, SeLa <ref type="bibr" target="#b54">[55]</ref> and other works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b15">16]</ref> maximize mutual information between two versions of soft cluster assignments from a single image. Maximizing mutual information prevents the network from falling into a degenerate solution, but effectively enforces uniform distribution over clusters. Hence, unsupervised clustering is expected to work only with well-balanced datasets such as MNIST <ref type="bibr" target="#b27">[28]</ref> and CIFAR <ref type="bibr" target="#b25">[26]</ref>. Recent works <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b54">55]</ref> tested on larger-scale datasets such as ImageNet <ref type="bibr">[9]</ref>, still assume a balanced set of single-class, object-centric images. Since these methods do not explicitly perform clustering on data, they are called implicit clustering methods, contrary to explicit clustering <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation without labels.</head><p>In clustering, each data point is assumed to be semantically homogeneous. This condition is invalid when images contain more than one semantic class, such as scene-centric datasets <ref type="bibr">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b14">15]</ref>. In fact, the majority of common images are not objectcentric, and therefore one cannot simply use off-the-shelf clustering methods to obtain semantic understanding of an arbitrary dataset. The problem reduces to semantic segmentation by clustering pixel-level features.</p><p>There has been a number of recent attempts to semantic segmentation without labels. IIC <ref type="bibr" target="#b22">[23]</ref> simply extends mutual information-based clustering to pixel-level representation by outputting a probability map over image pixels. AC <ref type="bibr" target="#b36">[37]</ref> uses an autoregressive model <ref type="bibr" target="#b46">[47]</ref> to obtain probabilities of pixels over categories, which then maximizes mutual information across two different "orderings" of autoregression. Both works are limited to stuff categories due to the following two reasons. First, a mixture of stuff and things categories introduces severe data imbalance since there are far more stuff pixels than things pixels in real-world images. Such imbalance leads the mutual information maximization to forcibly balance the size of clusters and hence leads to noisy representation as major classes (stuff categories) subsume minor classes (things categories). Second, each method exploits the local spatial consistency condition; a pixel needs to be semantically (and visually) consistent with its neighboring pixels. This condition is only valid with stuff categories (e.g., sky) and not often true with things categories. Other methods <ref type="bibr">[6,</ref><ref type="bibr">2]</ref> based on GANs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref> learn to generate foreground masks of a given image, but are limited to a single-category setting. Our method is free from such assumptions and the results show that our method is capable of segmenting both stuff and things categories together well with uncurated images.</p><p>Equivariance learning. Equivariance learning has been studied in object and keypoints tracking <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b26">27]</ref>, facial landmark detection <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b21">22]</ref>, and keypoint detection <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b31">32]</ref>. The central idea in these works is to train a model that predicts consistent key points between two images, with the underlying assumption that two images share a common instance. This enables unsupervised learning of semantically consistent and geometrically structured representation learning. The general objective is to directly minimize the L2 distance between two feature vectors that correspond to the semantically equivalent locations on images. However, using MSE loss with clustering is often sensitive to the choice of hyper-parameters, which is often infeasible or prone to overfit in unsupervised setting. Furthermore, individual feature vector may contain noisy low-level visual cues which can overwhelm the gradient flow during back-propagation. Our method instead learns equivariance by enforcing consistent clustering assignments between two views and hence only cluster-centered visual cues affect the loss (detail in Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PiCIE</head><p>We are given an uncurated, unlabeled dataset of images taken from some domain D. On this dataset, we want to discover a set of visual classes C and learn a semantic segmentation function f ? . When provided an unseen image from D, f ? should be able to assign every pixel a label from the set of classes C.</p><p>We formulate this task of unsupervised image segmentation as pixel-level clustering, where every pixel is assigned to a cluster. Clustering typically requires a good feature space, but no such feature representation exists a priori. We therefore propose an approach that learns the feature representation jointly with the clustering. The overall pipeline of PiCIE, which stands for Pixel-level feature Clustering using Invariance and Equivariance, is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. We describe our approach below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A baseline clustering approach</head><p>We begin with prior work that learns a neural network end-to-end for clustering unlabeled images into image-level classes <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b52">53]</ref>. The key issue tackled in these papers is that clustering images into classes requires strong feature representations, but for training strong feature representations one needs class labels. To solve this chickenand-egg problem, the simplest solution is the one identified by DeepCluster <ref type="bibr">[4]</ref>: alternate between clustering using the current feature representation, and use the cluster labels as pseudo-labels to train the feature representation. One can follow a similar strategy for the unsupervised semantic segmentation task. The only difference is that we need to use an embedding function f ? that produces a feature map, producing a feature vector for every pixel. The classifier must also operate on individual pixels. One can then alternate between clustering the pixel feature vectors to get pixel pseudo-labels, and using these pseudo-labels to train the pixel feature representation.</p><p>Concretely, suppose we have a set of unlabeled images x i , i = 1, . . . , n. Suppose our embedding, denoted by f ? produces a feature tensor f ? (x). This yields a feature representation for every pixel p in the image x. Denote by f ? (x)[p] this pixel-level feature representation. Denote by g w (?) a classifier operating on these pixel feature vectors. Then our baseline approach alternates between two steps:</p><p>1. Use the current embedding and k-means to cluster the pixels in the dataset.</p><formula xml:id="formula_0">min y,? i,p f ? (x i )[p] ? ? yip 2<label>(1)</label></formula><p>where y ip denotes the cluster labels of the p-th pixel in the i-th image, and ? k is the k-th cluster centroid. (We use mini-batch k-means <ref type="bibr" target="#b38">[39]</ref>).</p><p>2. Use the cluster labels to train a pixel classifier using standard cross entropy loss.</p><formula xml:id="formula_1">min ?,w i,p L CE (g w (f ? (x i )[p]), y ip ) (2) L CE (g w (f ? (x i )[p]), y ip ) = ? log e sy ip k e s k<label>(3)</label></formula><p>where s k is the k-th class score output by the classifier</p><formula xml:id="formula_2">g w (f ? (x i , p)).</formula><p>Given this baseline, we now propose the following modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Non-parametric prototype-based classifiers</head><p>The DeepCluster inspired framework above uses a separate, learned classifier. However, in the unsupervised setting with constantly changing pseudo-labels, training a classifier jointly with the feature representation can be challenging. An insufficiently trained classifier can feed noisy gradients into the feature extractor, resulting in noisy clusters for the next training round.</p><p>We therefore propose to jettison the parametric pixel classifier g w entirely. Instead, we label pixels based on their distance to the centroids ("prototypes" <ref type="bibr" target="#b40">[41]</ref>) estimated by kmeans. This results in the following changed objective.</p><formula xml:id="formula_3">min ? i,p L clust (f ? (x i )[p], y ip , ?) (4) L clust (f ? (x i )[p], y ip , ?) = ? log e ?d(f ? (xi)[p],?y ip ) l e ?d(f ? (xi)[p],? l )<label>(5)</label></formula><p>where d(?, ?) is cosine distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Invariance and Equivariance</head><p>Jointly learning the feature representation along with the clustering as above will certainly produce clusters that are compact in feature space, but there is no reason why these clusters must be semantic. To get a semantic grouping of pixels, we need to introduce an additional inductive bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What must this inductive bias be if we have no labels?</head><p>The inductive bias we introduce is invariance to photometric transformations and equivariance to geometric transformations: the labeling should not change if the pixel colors are jittered slightly, and when the image is warped geometrically, the labeling should be warped similarly. Concretely, if Y is the output semantic labeling for an image Above: PiCIE pseudo-code. Notations consistent with Sec. 3.3.</p><p>x, and if P and G are photometric and geometric transformations respectively, then the output semantic labeling of a transformed image G(P (x)) should be G(Y ).</p><p>Implementing this constraint in a joint clustering and learning framework is tricky, since there isn't a ground truth label for each image. The pseudo-ground truth labeling is itself derived from clustering, which is itself produced from the feature maps, and as such itself sensitive to input transformations. Invariance/equivariance in this case therefore means two things: one, we should produce the same clusters irrespective of the transformations, and two, the predicted pixel labels should exhibit the desired in/equivariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Invariance to photometric transformations</head><p>We first address the question of invariance. For each image x i in the dataset, we randomly sample two photometric transformations, P (1) i and P <ref type="bibr">(2)</ref> i . This yields two feature vectors for each pixel p in each image x i :</p><formula xml:id="formula_4">z (1) ip = f ? (P (1) i (x i ))[p] (6) z (2) ip = f ? (P (2) i (x i ))[p]<label>(7)</label></formula><p>We then perform clustering separately in the two "views" to get two sets of pseudo-labels and centroids:</p><formula xml:id="formula_5">y (1) , ? (1) = arg min y,? i,p z (1) ip ? ? yip 2 (8) y (2) , ? (2) = arg min y,? i,p z (2) ip ? ? yip 2<label>(9)</label></formula><p>Given these two sets of centroids and these two sets of pseudo-labels, we use two sets of loss functions:</p><p>1. As before, we want the feature vectors to adhere to the clustering labels. Now that we have two views, we want this to be true in each view:</p><formula xml:id="formula_6">L within = i,p L clust (z (1) ip , y (1) ip , ? (1) ) + L clust (z (2) ip , y (2) ip , ? (2) )<label>(10)</label></formula><p>2. Because we posit that the clustering should be invariant to photometric transformations, we also want feature vectors from one view to match the cluster labels and centroids of the other:</p><formula xml:id="formula_7">L cross = i,p L clust (z (1) ip , y (2) ip , ? (2) ) + L clust (z (2) ip , y (1) ip , ? (1) )<label>(11)</label></formula><p>This multi-view framework and the cross-view loss achieve two things. First, by forcing feature vectors from one transformation to adhere to labels produced by another, it encourages the network to learn feature representations that will be labeled identically irrespective of any photometric transformations. Second, by forcing the same feature representation to be consistent with two different clustering solutions, it encourages the two solutions themselves to match, thus ensuring that the set of concepts discovered by clustering is invariant to photometric transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Equivariance to geometric transformations</head><p>A house and a zoomed-in version of the house should be labeled similarly, but may produce vastly different features. More precisely, the segmentation of the zoomed-in house should be a zoomed-in version of the original segmentation. This is the notion of equivariance to geometric transformations (such as random crops), which we add in next.</p><p>To learn equivariance with respect to geometric transformations, we sample a geometric transformation (concretely, random crop and horizontal flip) G i for each image. Then, in the above framework, one view uses feature vectors of the transformed image, while the other uses the transformed feature vectors of the original:</p><formula xml:id="formula_8">z (1) ip = f ? (G i (P (1) i (x i )))[p]<label>(12)</label></formula><formula xml:id="formula_9">z (2) ip = G i (f ? (P (2) i (x i )))[p]<label>(13)</label></formula><p>The other steps are exactly the same. The two views are clustered separately, and the final training objective is the combination of the within-view and cross-view objectives:</p><formula xml:id="formula_10">L total = L within + L cross<label>(14)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training details</head><p>For all our experiments, we use the Feature Pyramid Network <ref type="bibr" target="#b29">[30]</ref> with ResNet-18 <ref type="bibr" target="#b19">[20]</ref> backbone pre-trained on Im-ageNet <ref type="bibr">[9]</ref>. The fusion dimension of the feature pyramid is 128 instead of 256. We apply L2 normalization on the feature map of our network. The cluster centroids are computed with mini-batch approximation with GPUs using the FAISS library <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b23">24]</ref>. For the baselines, we do not use image gradients as an additional input when we use ImageNetpretrained weight. Except in <ref type="table">Table 4</ref>, all images are resized and center-cropped to 320 ? 320 during training. We used the published codes <ref type="bibr">[4,</ref><ref type="bibr" target="#b22">23]</ref> with minimal modification for the baselines. Other details are in supplementary.</p><p>Pre-trained vs random initialization. Prior works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref> train the network from random initialization, but for semantic segmentation it is unnecessary; unlike representation learning literature <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b56">57]</ref>, our goal is to segment a given dataset as accurately as possible, and in a practical scenario one will always choose to initialize from a pre-trained network such as on the ImageNet dataset <ref type="bibr">[9]</ref>. Therefore, we train all models with ImageNet-pretrained weights, except that in <ref type="table">Table 4</ref> we show PiCIE outperforms all the baselines when trained from scratch as well.</p><p>Loss Balancing and Overclustering. As shown in <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b22">23]</ref>, jointly optimizing for a separate set of clusters with higher number improves the stability of clustering as well as the accuracy of the prediction. However, in unsupervised settings hyper-parameter tuning is often infeasible. Thus, we use the generic approach to balance the loss:</p><formula xml:id="formula_11">L = ? K1 L K1 + ? K2 L K2 (15) ? K1 = log K2</formula><p>log K1+log K2 and ? K2 = log K1</p><p>log K1+log K2 where K 1 and K 2 are the number of clusters. The intuition is that the magnitude of the cross-entropy loss depends logarithmically on the number of clusters, hence we prevent the overclustering to overwhelm the gradient flow. We fix K 2 = 100 and add "+H." in results when applied. Similarly, due to the imbalance of datasets, the computed clusters will have largely different sizes; we apply a balance term for each cluster during the cross-entropy computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>We describe the baseline methods that we compare PiCIE to: IIC <ref type="bibr" target="#b22">[23]</ref> and modified DeepCluster <ref type="bibr">[4]</ref> for segmentation purposes. They are state-of-the-art implicit and explicit clustering-based learning methods.</p><p>IIC. IIC <ref type="bibr" target="#b22">[23]</ref> is an implicit clustering method where the network directly predicts the (soft) clustering assignment of each pixel-level feature vector. The main objective is maximizing the mutual information between the predictions of a pixel and neighboring pixel(s). For controlled experiments, we use FPN with ResNet-18 same as PiCIE as well as the first two residual blocks of ResNet-18 (IIC -res12) similar to the original shallow VGG-like <ref type="bibr" target="#b39">[40]</ref> model (details in <ref type="figure">Figure 3</ref>: Overall qualitative results on COCO-All <ref type="bibr" target="#b30">[31]</ref> (left) and Cityscapes <ref type="bibr">[8]</ref>(right). Note that we show IIC-res12 for COCO and IIC for Cityscapes to show the best result of the method on each dataset. Each ground truth label is assigned a color and for each cluster, the majority label's color is used. We show some of the color and name matches for better understanding. More in supplementary materials. supplementary). Following the original paper <ref type="bibr" target="#b22">[23]</ref>, we used auxiliary over-clustering loss with K = 45.</p><p>Modified DeepCluster. DeepCluster is an explicit clustering method where the network clusters the feature vectors of given images and uses the cluster assignment as labels to train the network. To adjust to our problem setup, we modify the original DeepCluster to instead cluster pixel-level feature vectors before the final pooling layer. This allows the network to assign a label to each pixel. However, since the size of image explodes the number of feature vectors to cluster, we apply mini-batch k-means <ref type="bibr" target="#b38">[39]</ref> to first compute the cluster centroids, assign labels, and train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Datasets</head><p>COCO. Following <ref type="bibr" target="#b22">[23]</ref>, we evaluate our model on the COCO-Stuff dataset <ref type="bibr">[3]</ref>. The COCO-Stuff dataset is a large-scale scene-centric dataset of images with 80 things categories and 91 stuff categories. We follow the same preprocess as <ref type="bibr" target="#b22">[23]</ref> where classes are merged to form 27 (15 stuff and 12 things) categories. Unless otherwise stated, we evaluate both things and stuff categories, unlike prior works which evaluate only stuff.</p><p>Cityscapes. We further evaluate our model on the Cityscapes dataset <ref type="bibr">[8]</ref>. Cityscapes is a set of images of street scenes from 50 different cities. There are 30 classes of instances that can be further categorized into 8 groups.</p><p>After filtering out void group, we have 27 categories. We train our method as well as IIC and modified DeepCluster with K = 27 where K is the number of clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we compare PiCIE with the following baselines: No Train, modified DeepCluster <ref type="bibr">[4]</ref>, and IIC <ref type="bibr" target="#b22">[23]</ref>. Unlike the prior works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref> where only stuff categories are considered, we evaluate the models on both stuff and things categories to test on more realistic setting. Since the majority of scene-centric image dataset consists of stuff categories, our evaluation now faces a severe imbalance problem. Also, the learning mechanism of IIC assumes local spatial consistency, which is not often true for things cat-     <ref type="table">Table 4</ref>: COCO-Stuff results without ImageNet pretrained weight following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref>. First section is from prior works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref> and the last two sections are from our implementation.</p><p>egories due to more dynamic shape variations. We found that IIC tends to overfit to low-level visual cues since (implicit) clustering is done within a batch and insufficient supervisory signal is present when an instance has dynamic and complex visual cues. Indeed, in <ref type="figure">Figure 3</ref> no things categories are correctly segmented from IIC results. On the other hand, PiCIE's novel in/equivariance loss enforces geometric consistency as an inductive bias to learn high-level visual concepts, and as shown in <ref type="figure">Figure 3</ref> PiCIE ("Ours") is capable of segmenting both stuff and things categories with high accuracy. As a result, <ref type="table" target="#tab_0">Table 1</ref> shows that PiCIE largely outperforms other baselines (+ 17.5 Acc. and 4.5 mIoU). In <ref type="table" target="#tab_3">Table 3</ref>, we test the baselines and our method on Cityscapes and show similar level of advantages (+ 18 Acc. and 5.3 mIoU). Finally, <ref type="table">Table 4</ref> shows PiCIE outperforms the other models on the benchmark from <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref> where the image size is 128?128, models are trained from scratch, and only stuff labels are considered for evaluation.</p><p>Things vs stuff. In <ref type="table" target="#tab_2">Table 2</ref>, we show that PiCIE improves mainly on things categories (+10 mIoU) while maintaining better or compatible performance on stuff categories compared to other methods. This indicates that enforcing geometric transformation equivariance was highly effective on things categories where the instances objects with distinct shape and boundaries. Furthermore, we show in <ref type="table" target="#tab_2">Table 2</ref> and 4 that PiCIE still outperforms on stuff categories with or without ImageNet-pretrained weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>In <ref type="table" target="#tab_5">Table 5</ref>, we decompose our method to examine which component affects the performance the most. We gain 5 points by using a non-parametric classifier with cluster centroids. We further gain 3 points with cross-view learning with invariance transformations. Equivariance learning adds another 5.5 points, and with auxiliary over-clustering, we arrive at 49.99 pixel accuracy and 14.36 mIoU.</p><p>In   vectors of the two views. This leads PiCIE to a suboptimal solution: 1) the direct distance between two feature vectors can be overwhelmed by low-level or irrelevant signals whereas cross-view loss directs the gradient to the nearest centroid, hence only considers relevant signals and 2) MSE loss requires hyperparameter tuning to be jointly used with cross-entropy loss, which is infeasible in purely unsupervised setting. Also, one could doubt if two sets of clustering are necessary; a single clustering with geometric transformation on the predicted labels can be used as an alternative to compute the cross-view loss. However, the two versions of an image contain different information (e.g., zoomed-in vs full house) that can be mutually beneficial. We test them all (and more) in <ref type="table" target="#tab_4">Table 6</ref> and the results justify our choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Analysis</head><p>Nearest neighbor analysis. In <ref type="figure" target="#fig_1">Figure 4</ref>, we show the nearest neighbors of correctly (left) and incorrectly (right) predicted instances. The nearest neighbors of correctly predicted segments share close high-level semantics (e.g., person playing tennis, zebra, giraffe, and a building with a clock). This indicates that intra-class semantics are well preserved. The incorrectly predicted segments also have semantically and visually close nearest neighbors. For example, the first row shows that snow pixels are confused with sky as the two concepts are visually alike. Such visual ambiguity is an inherent limitation of unsupervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation quality.</head><p>In <ref type="table" target="#tab_8">Table 7</ref>, we compare the learned representations by training a linear classifier for each trained method from our main experiments on COCO-All. We train with ? = 0.001 for 10 epochs with crossentropy loss. This allows us to analyze whether the difficulty is from the representation or from clustering. Compared to the unsupervised results from   have a huge performance gap whereas PiCIE has a minimal gap. This indicates that clustering is where the major difficulty is and PiCIE gives close-to-optimal clustering given learned representation. In <ref type="table" target="#tab_9">Table 8</ref>, we show that PiCIE can give better network initialization for supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced a new framework for unsupervised semantic segmentation with clustering. Our main contribution is to incorporate geometric consistency as an inductive bias to learn invariance and equivariance for photometric and geometric variations. Our novel cross-view loss is simple yet highly effective in learning high-level visual concepts necessary to segment things categories. Our method is the first unsupervised semantic segmentation that works for both stuff and things categories without rigorous hyper-parameter tuning or task-specific pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering -Supplementary Materials</head><p>Jang Hyun Cho 1 Utkarsh Mall 2 Kavita Bala 2 Bharath Hariharan 2 1 University of Texas at Austin 2 Cornell University S1. More experiment details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network architectures</head><p>For every method, we used the Feature Pyramid Network <ref type="bibr">[11]</ref> to effectively encode representations from multiple scales. However, we only use pixel-wise randomly initialized linear (1 ? 1 convolutional) layer for each level of the intermediate feature maps from ResNet-18 <ref type="bibr">[5]</ref>. As noted in the main paper, we projected each of the feature maps to 128 dimensions instead of 256 from the FPN. After the linear projection, we directly bilinear-upsampled to 1/4 scale of image resolution and element-wise summed to get the final 128 ? H ? W representation without the last 3 ? 3 convolutional smoothing layers (H = W = 80 during training with 320 ? 320 images). Note that this is a simplified version of the semantic segmentation branch of Panoptic FPN <ref type="bibr">[8]</ref>, a simple application of FPN to segmentation task. At the end, the only added parameters from ResNet-18 are 4 1 ? 1 convolutional layers.</p><p>IIC For controlled experiments, we changed the network architecture of default IIC from the original shallow VGG-like model to FPN with ResNet-18 as described above. Following the original paper <ref type="bibr">[6]</ref>, we used auxiliary over-clustering loss: We kept the original k = 45 since the difference was minimal between k ? {45, 100, 250}. Also, the original IIC objective has a hyper-parameter ? which controls the "strictness" of the uniform distribution of clustering constraint. This could potentially alleviate the problem that IIC faces. In <ref type="table" target="#tab_0">Table 1</ref> we tested with ? ? {1, 1.25, 1.5, 1.75, 2, 3} on COCO-All and ? = 1 performed the best, hence we kept ? = 1 in all our experiments. Similarly, we tested different learning rates ? ? {0.1, 0.01, 0.001, 0.0001} and ? = 0.0001 was optimal. Both of these ? and ? coincide with those in the original paper.</p><p>IIC-res12. We discovered that the shallow version of IIC performs better qualitatively on the (processed) COCO dataset <ref type="bibr">[6]</ref>. This is because a shallow network tends to overfit to low-level visual signals such as color and texture due to its narrow receptive field. Since the dataset is preprocessed to reduce images that have too many pixels in the things categories, which are often visually more complex, perhaps the shallow IIC can be more effective for solving simple background segmentation compared to deep IIC. Therefore, we tested both versions. Note that the shallow VGG-like network used in the original IIC paper is unable to load ImageNet-pretrained weight, hence we instead used the first two residual layers res1 and res2 of ResNet-18 <ref type="bibr">[5]</ref> as an alternative. They have nearly the same number of parameters and in the main paper, <ref type="table" target="#tab_3">Table 3</ref>, we show that IIC with res12 achieves similar accuracy on the original COCO-Stuff benchmark <ref type="bibr">(27.7 and 27.92)</ref>  <ref type="bibr">[6]</ref>. Similar to IIC, we apply auxiliary over-clustering with k = 45.</p><p>Modified DC. Since DeepCluster <ref type="bibr">[1]</ref> was originally designed for the task of image clustering, we modified the framework to fit the task of segmentation (pixel-wise classification). The network alternates between computing pseudo-labels and training. As mentioned in the main paper, the representation is pixel-level by removing the final pooling layer. This makes storing the feature vectors of the entire dataset infeasible, so we perform mini-batch k-means to first estimate cluster centroids, assign pseudolabels, and train the network with the pseudo-labels. The same set of transformations as PiCIE is used on each image during training. Note that similar to IIC, image gradient is not concatenated in the input when initialized from ImageNet-weight. We do not apply over-clustering since the model without over-clustering performed the best compared to k ? {100, 250, 1000, 2500}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>For training modified DC and PiCIE, we used simple pre-processing: resizing and center-crop to 320 ? 320. For IIC, we used the original paper's pre-processing with their published code.</p><p>Transformations. For photometric transformations, we randomly applied color jitter, gray scale, and Gaussian 7.0 6.4 6.0 6.5 6.5 blur. Random jitter consists of jittering brightness, contrast, saturation, and hue. All jittering transformations are applied with probability p = 0.8 and control factors 0.3, 0.3, 0.3, 0.1, respectively. Random gray scale is applied with probability of p = 0.2. Random Gaussian blur is applied with probability of p = 0.5 and radius randomly chosen: ? ? [0. <ref type="bibr">1,</ref><ref type="bibr">2]</ref>. For geometric transformations, we applied random crop and random horizontal flip with crop factor r ? [0.5, 1] and flipping probability p = 0.5. In order to ensure that the same transformations are applied during clustering and training, we first sample transformations during clustering and store them in a list to re-use during training.These hyper-parameters are a standard choice adopted in many other works <ref type="bibr">[4,</ref><ref type="bibr">2,</ref><ref type="bibr">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>Clustering. The cluster centroids are computed with mini-batch k-means with GPUs using the FAISS library <ref type="bibr">[7]</ref>. The initial cluster centroids are computed with 50 batches with batch size of 128, then the centroids are updated every 20 iterations. For every other hyperparameters related to clustering, we followed Caron et al. <ref type="bibr">[1]</ref>. Since this process is highly optimized, it takes about 20 minutes to prepare the pseudo-labels for training every epoch on the COCO dataset, which makes less than half for training the network in total compared to IIC using the published code.</p><p>Training details. We trained every method with 10 epochs when trained with ImageNet weight initialization, and 20 epochs when trained from scratch. For modified DC and PiCIE, we used ADAM optimizer with learning rate ? = 1 ? 10 ?3 , ? = (0.9, 0.999) and weight decay 0. For IIC, their original hyperparameter setting was better, so we kept their setting (? = 1 ? 10 ?4 ). For the transfer learning and supervised training experiments, we used ? = 1 ? 10 ?3 , ? = (0.9, 0.999) and weight decay 0, consistent with the setting from the main experiments. For the final objective, we applied weighted cross-entropy loss with per-cluster weight is balanced with the size of each cluster. We simply average the cross and within losses.</p><p>Evaluation metric. For evaluating our model, we followed the evaluation metric from <ref type="bibr">[6]</ref> with pixel accuracy after Hungarian-matching <ref type="bibr">[10]</ref> the cluster assignments to the ground truth labels. We also report mean IoU to account for false positives and negatives. In <ref type="table" target="#tab_2">Table 2</ref> of the main paper, we compute the accuracy and mIoU from the same model trained on COCO-All (K = 27), but evaluated by only accounting for the labels in each partition. This can be done efficiently by computing the confusion matrix of the all classes K = 27 first and partitioning the matrix accordingly. In <ref type="table" target="#tab_3">Table 3</ref> of the main paper, we closely follow the experiment setting of <ref type="bibr">[6]</ref>: the image resolution is 128 ? 128, the images are pre-scaled and constant-padded, and K = 15 which means only stuff categories are considered for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizations</head><p>For producing consistent visualizations, we used majority vote for each obtained cluster. That is, we first assigned color values to each ground truth label and for each obtained set of clusters, we assign the color of the majority class. In the main paper, notice that we showed IIC-res12 for COCO and IIC for Cityscapes. We included the version that had better qualitative results. We hypothesize that since COCO was preprocessed to include more stuff categories, it is easier for the shallow network which overfits to low-level cues (e.g., color and texture) to segment images well since the majority of stuff instances are visually simple. For the nearest neighbor result, we first chose successful and failure results from the large set of randomly selected images (results below), picked a pixel coordinate of interest, and computed the nearest neighbor on the entire validation set of COCO-All. Then, we extracted the images that the neighbors belong to and visualized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. More results</head><p>In this section, we show more qualitative results randomly chosen for both IIC and IIC-res12, as well as modified DC and PiCIE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness on Color and Geometric transformations</head><p>We show that PiCIE successfully learns photometric invariance and geometric equivariance by evaluating our model with test-time augmentation. We apply the same set of photometric transformations (color jitter, Gaussian blur, and greyscale) and geometric transformations (horizontal flip and random crop) and report the results in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3. Analysis</head><p>We discuss a few possible directions for future study. Note that MDC stands for modified DeepCluster.</p><p>Visual ambiguity. As shown in visualization, visual ambiguity leads to mis-classification of certain classes. Snowy ground is often confused with either sky or water, and grass on a flat ground is confused with ground. The core problem is twofold: First, the classification of the segment masks  are done with cluster centroids, which follow the "majority trend." For example, the majority of "ground" instances is not covered by snow, making the confidence low. Second, the visual similarity does not always correlate to the semantic similarity, and such discrepancy leads to confusion. "Snow ground" is often texture-less and mono-colore, similar to "sky" or "water." This is an inherent limitation of unsupervised learning methods.</p><p>Co-occurrence. Some foreground classes such as "boat" or "airplane", only occur surrounded by "water" or "sky." Since stuff categories have far more pixels, they are often subsumed in the co-occurring background classes. We hypothesize that this effect will be mitigated if the dataset had more images of stand-alone "boat" or "airplane." or with an effective way to contrast between the two entities such as using either a generic or a learned boundary detector, which can be a future work.</p><p>Boundary precision. Since we do not have any supervision to train for precise boundaries, many foreground instances are segmented with over-confidence. Pixels around boundaries are hard samples to correctly predict. Using a generic edge detector or post-processing through iterative refinement such as CRF <ref type="bibr">[9]</ref> may improve the result, which is outside the scope of our project.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>PiCIE overview (left) and illustration of multi-view feature computation (right). More details in Sec. 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Nearest neighbor results for correctly predicted (left) instances and incorrectly predicted (right) instances. The red box indicates the position of the particular feature vector (size exaggerated). More details in supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Classifier</cell><cell>Acc. mIoU</cell></row><row><cell cols="3">No Train No Train Modified DC IIC -res12 [23] Linear Linear Prototype 26.26 8.41 17.45 3.70 Linear 32.21 9.79 22.45 4.11 IIC [23] Linear 21.79 6.71</cell></row><row><cell>PiCIE PiCIE + H.</cell><cell cols="2">Prototype 48.09 13.84 Prototype 49.99 14.36</cell></row></table><note>COCO-All [23] results. Our method is compared to clustering methods adapted to semantic segmentation. "+H." de- notes PiCIE trained with auxiliary clustering.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on different partitions of the COCO dataset.</figDesc><table><row><cell>Method</cell><cell cols="3"># Classes Accuracy mIoU</cell></row><row><cell>IIC IIC -res12 Modified DC PiCIE</cell><cell>27</cell><cell>47.88 29.78 40.67 65.50</cell><cell>6.35 4.96 7.06 12.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Cityscapes results.</figDesc><table><row><cell>Method</cell><cell>COCO-Stuff</cell></row><row><cell>Random CNN K-means [38] SIFT [33] Doersch 2015 [10] Isola 2016 [21] DeepCluster [4] IIC [23] AC [37]</cell><cell>19.4 14.1 20.2 23.1 24.3 19.9 27.7 30.8</cell></row><row><cell>Modified DC IIC IIC -res12</cell><cell>25.26 27.97 27.92</cell></row><row><cell>PiCIE</cell><cell>31.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>, we test alternatives of different components of PiCIE. First, one could wonder if our cross-view loss can be replaced by MSE loss, directly minimizing the feature</figDesc><table><row><cell cols="2">Nonpara-Photo-Geo-metric metric metric cluster Over-Accuracy mIoU</cell></row><row><cell>34.35 39.25 42.55 46.97 48.09 49.99</cell><cell>9.88 9.82 9.84 12.04 13.84 14.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study 1. Our method is decomposed to examine which components affect the performance the most.</figDesc><table><row><cell cols="2">Single MSE eqv. No inv. No balance Accuracy mIoU</cell></row><row><cell>48.09 40.56 44.31 44.15 41.70</cell><cell>13.84 11.46 11.71 10.98 9.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Ablation study 2. One or more components in our method is replaced with alternative options.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>, baselines</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Transfer learning results. A new linear classifier has been trained on top of the learned embedding network.</figDesc><table><row><cell cols="4">Initialization Normalization Acc. mIoU C-Acc. C-mIoU</cell></row><row><cell>ImageNet ImageNet</cell><cell>75.48 44.69 74.74 43.44</cell><cell>55.82 57.24</cell><cell>17.36 31.51</cell></row><row><cell>Modified DC Modified DC IIC IIC</cell><cell>75.25 44.37 75.27 43.82 75.16 44.26 74.81 44.11</cell><cell>55.16 57.41 56.07 57.30</cell><cell>18.43 30.27 20.32 29.47</cell></row><row><cell>PiCIE PiCIE PiCIE + H. PiCIE + H.</cell><cell>75.61 44.40 76.02 44.97 75.90 45.60 76.01 45.04</cell><cell>54.84 59.77 58.95 58.94</cell><cell>17.39 32.81 18.38 32.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Re-training results. Trained networks are used as an initialization for standard supervised training. "C-Acc." and "C-mIoU" are clustering results after supervised training. All models are trained from ImageNet-pretrained initialization.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 1 :</head><label>1</label><figDesc>IIC with different ?.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Brightness Contrast Saturation Hue Grayscale Gaussian blur Horizontal Flip Random Crop Accuracy mIoU</figDesc><table><row><cell>48.09 47.98 48.08 48.09 48.09 47.98 48.03 47.42 47.61 48.08 47.60 46.28</cell><cell>13.84 13.59 13.63 13.64 13.65 13.63 13.59 13.39 13.71 13.63 13.76 13.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2 :</head><label>2</label><figDesc>We evaluate PiCIE with test-time augmentation where each transformation follows the same hyper-parameters as training, when applied. The result shows that PiCIE is robust to photometric and geometric transformations during inference.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by DARPA Learning with Less Labels program (HR001118S0044), CHS-1617861, CHS-1513967, CHS-1900783, and CHS-1930755.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09549</idno>
		<title level="m">Fully-convolutional siamese networks for object tracking</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emergence of object segmentation in perturbed generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised object segmentation by redrawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micka?l</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Arti?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4323</biblScope>
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4323</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved deep embedded clustering with local structure preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised clustering using pseudosemi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divam</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramachandran</forename><surname>Ramjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nipun</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muthian</forename><surname>Sivathanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatically discovering and learning new visual categories with ranking statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Ehrhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to discover novel visual categories via deep transfer clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06811</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks through conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Billionscale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object keypoints for perception and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised deep metric learning with transformed attention consistency and contrastive clustering loss. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihai</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised part-based disentangling of object shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Bereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Milbich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4327</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to Cluster under Domain Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willi</forename><surname>Menapace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Edinburgh, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object structure and dynamics from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-supervised learning of geometrically stable features through probabilistic introspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Autoregressive unsupervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="volume">4325</biblScope>
			<biblScope unit="page">4327</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python. the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">4327</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Web-scale k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4324</biblScope>
			<biblScope unit="page">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4324</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Predict &amp; cluster: Unsupervised skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discovery of latent 3d keypoints via end-to-end geometric reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object frames by dense equivariant image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks by factorized spatial embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modelling and unsupervised learning of symmetric deformable object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno>2020. 4322</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">4322</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-supervised learning of a facial attribute embedding from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Clusterfit: Improving generalization of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep clustering by gaussian mixture variational autoencoders with graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Yew-Soon Ong, and Chen Change Loy. Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4322</biblScope>
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Selfsupervised convolutional subspace clustering network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of object landmarks as structural representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep image clustering with category-style representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12359</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page">4325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709,2020.4322</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029,2020.4322</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4321</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4321</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Billionscale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4321</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">4323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4322</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4321</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
