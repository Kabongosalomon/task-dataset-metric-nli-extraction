<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perceive Where to Focus: Learning Visibility-aware Part-level Features for Partial Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
							<email>liyali13@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<email>zhangchi@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Perceive Where to Focus: Learning Visibility-aware Part-level Features for Partial Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper considers a realistic problem in person reidentification (re-ID) task, i.e., partial re-ID. Under partial re-ID scenario, the images may contain a partial observation of a pedestrian. If we directly compare a partial pedestrian image with a holistic one, the extreme spatial misalignment significantly compromises the discriminative ability of the learned representation. We propose a Visibility-aware Part Model (VPM), which learns to perceive the visibility of regions through self-supervision. The visibility awareness allows VPM to extract region-level features and compare two images with focus on their shared regions (which are visible on both images). VPM gains two-fold benefit toward higher accuracy for partial re-ID. On the one hand, compared with learning a global feature, VPM learns region-level features and benefits from fine-grained information. On the other hand, with visibility awareness, VPM is capable to estimate the shared regions between two images and thus suppresses the spatial misalignment. Experimental results confirm that our method significantly improves the learned representation and the achieved accuracy is on par with the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification (re-ID) aims to spot the appearances of same person in different observations by measuring the similarity between the query image and the gallery images (i.e., the database). In spite that the re-ID research community has achieved significant progress during the past few years, re-ID systems are still faced with a series of realistic difficulties. A prominent challenge is the partial re-ID problem <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref>, which requires accurate retrieval with partial observation of the pedestrian. More concretely, in realistic re-ID systems, a pedestrian may happen to be partially occluded or be walking out of the field of camera * Corresponding author.  view, and the camera fails to capture the holistic pedestrian.</p><p>Intuitively, partial re-ID increases the difficulty to make correct retrieval. Analytically, we find that partial re-ID raises two more unique challenges, compared with the holistic person re-ID, as illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>? First, partial re-ID aggravates the spatial misalignment between probe and gallery images. Under holistic re-ID setting, the spatial misalignment mainly originates from the articulate movement of pedestrian and the viewpoint variation. Under partial re-ID setting, even when two pedestrian with same pose are captured from same viewpoints, there still exists severe spatial misalignment between the two images ( <ref type="figure" target="#fig_1">Fig. 1 (a)</ref>).</p><p>? Second, when we directly compare a partial pedestrian against a holistic one, the unshared body regions in the holistic pedestrian become distracting noises, rather than discriminative clues. We note that the same situation also happens when any two compared images contain different proportion of the holistic pedestrian ( <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>).</p><p>We propose the Visibility-aware Part Model (VPM) for partial re-ID. VPM avoids or alleviates the two unique difficulties related to partial re-ID by focusing on their shared regions, as shown in <ref type="figure" target="#fig_1">Fig. 1 (c)</ref>. More specifically, we first define a set of regions on the holistic person image. During training, given partial pedestrian images, VPM learns to locate all the pre-defined regions on convolutional feature maps. After locating each region, VPM perceives which regions are visible and learns region-level features. During testing, given two images to be compared, VPM first calculates the local distances between their shared regions and then concludes the overall distance.</p><p>VPM gains two-fold benefit toward higher accuracy for partial re-ID. On the one hand, compared with learning a global feature, VPM learns region-level features and thus benefits from fine-grained information, which is similar to the situation in holistic person re-ID <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12]</ref>. On the other hand, with visibility-awareness, VPM is capable to estimate the shared regions between two images and thus suppresses the spatial misalignment as well as the noises originated from unshared regions. Experimental results confirm that VPM achieves significant improvement on partial re-ID accuracy, compared with a global feature learning baseline <ref type="bibr" target="#b33">[34]</ref>, as well as a strong part-based convolutional baseline <ref type="bibr" target="#b22">[23]</ref>. The achieved performance are on par with the state of the art.</p><p>Moreover, VPM is featured for employing selfsupervision for learning the region visibility awareness. We randomly crop partial pedestrian images from the holistic ones and automatically generate region labels, yielding the so-called self-supervision. Self-supervision enables VPM to learn locating pre-defined regions. It also helps VPM to focus on visible regions during feature learning, which is critical to the discriminative ability of the learned features, as to be accessed in Section 4.4.</p><p>The main contributions of this paper are summarized as follows:</p><p>? We propose a visibility-aware part model (VPM) for partial re-ID task. VPM learns to locate the visible regions on pedestrian images through self-supervision. Given two images to be compared, VPM conducts a region-to-region comparison within their shared regions, and thus significantly suppresses the spatial misalignment as well as the distracting noises originated from unshared regions.</p><p>? We conduct extensive partial re-ID experiments on both synthetic datasets and realistic datasets and validate the effectiveness of VPM. On two realistic dataset, Partial-iLIDs and Partial-ReID, VPM achieves performance on par with the state of the art. So far as we know, few previous works on partial re-ID reported the performance on synthetic large-scale datasets e.g., Market-1501 or DukeMTMC-ReID. We experimentally validate that VPM can be easily scaled up to large-scale (synthetic) partial re-ID datasets, due to its fast matching capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deeply-learned part features for re-ID</head><p>Deep learning methods currently dominate the re-ID research community with significant superiority on retrieval accuracy <ref type="bibr" target="#b33">[34]</ref>. Recent works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref> further advance the state of the art on holistic person re-ID, through learning part-level deep features. For example, Wei et al. <ref type="bibr" target="#b25">[26]</ref>, Kalayeh et al. <ref type="bibr" target="#b11">[12]</ref> and Sun et al. <ref type="bibr" target="#b22">[23]</ref> extract several region parts, with pose estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1]</ref>, human parsing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref> and uniform partitioning, respectively. Then they learn a respective feature for each part and assemble the part-level features to form the final descriptor. These progresses motivate us to extend learning part-level features to the specified problem of partial re-ID.</p><p>However, learning part-level features does not naturally improve partial re-ID. We find that PCB <ref type="bibr" target="#b22">[23]</ref>, which maintains the latest state of the art on holistic person re-ID, encounters a substantial performance decrease when applied in partial re-ID scenario. The achieved retrieval accuracy even drops below the global feature learning baseline (to be accessed in Sec. 4.2). Arguably, it is because part models rely on precisely locating each part and are inherently more sensitive to the severe spatial misalignment problem in partial re-ID.</p><p>Our method is similar to PCB in that both methods perform uniform division instead of semantic body parts for part extraction. Moreover, similar to SPReID <ref type="bibr" target="#b11">[12]</ref>, our method also uses probability maps to extract each part during inference. However, while SPReID requires an extra human parser and human parsing dataset (strong supervision) for learning part extraction, our method relies on selfsupervision. During matching stage, both PCB and SPReID adopt the common strategy of concatenating part features. In contrast, VPM first measures the region-to-region distance and then conclude the overall distance by dynamically crediting the local distances with high visibility confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-supervised learning</head><p>Self-supervision learning is a specified unsupervised learning approach. It explores the visual information to automatically generate surrogate supervision signal for feature learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref>. Larsson et al. <ref type="bibr" target="#b12">[13]</ref> train the deep model to predict per-pixel color histograms and consequen-  <ref type="figure">Figure 2</ref>. The structure of VPM. We first define p = m?n (3?1 in the figure for instance) densely aligned rectangle regions on the holistic pedestrian. VPM resizes a partial pedestrian image to fixed size, inputs it into a stack of convolutional layers ("conv") and transforms it into a 3D tensor T . Upon T , VPM appends a region locator to discover each regions through pixel-wise classification. By predicting a probability of belonging to each region for every pixel g, the region locator generates p probability maps to infer the location of each region. It also generates p visibility scores through " " operation over each probability map. Given the predicted probability maps, the feature extractor extracts a respective feature for each pre-defined region through weighted pooling ("WP"). VPM, as a whole, outputs p region-level features and p visibility scores for inference.</p><p>tially facilitate automatic colorization. Doersch et al. <ref type="bibr" target="#b2">[3]</ref> and Noroozi et al. <ref type="bibr" target="#b18">[19]</ref> propose to predict the relative position of image patches. Gidaris et al. train the deep model to recognize the rotation applied to original images. Self-supervision is an elemental tool in our work. We employ self-supervision to learn visibility awareness. VPM is especially close to <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b18">[19]</ref> in that all the three methods employ the position information of patches for selfsupervision. However, VPM significantly differs from them in the following aspects.</p><p>Self-supervision signal. <ref type="bibr" target="#b2">[3]</ref> randomly samples a patch and one of its eight possible neighbors, and then trains the deep model to recognize the spatial configuration. Similarly, <ref type="bibr" target="#b18">[19]</ref> encodes the neighborhood relationship into a jigsaw puzzle. Different from <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b18">[19]</ref>, VPM does not explore the spatial relationship between multiple images or patches. VPM pre-defines a division on the holistic pedestrian image and then assigns an independent label to each region. Then VPM learns to directly predict which regions are visible on a partial pedestrian image, without comparing it against the holistic one.</p><p>Usage of the self-supervision. Both <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b18">[19]</ref> transfer the model trained through self-supervision to the object detection or classification task. In comparison, VPM utilizes self-supervision in a more explicit manner: with the visibility awareness gained from self-supervision, VPM decides which regions to focus when comparing two images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Structure of VPM</head><p>VPM is designed as a fully convolutional network, as illustrated in <ref type="figure">Fig. 2</ref>. It takes a pedestrian image as the input and outputs a constant number of region-level features, as well as a set of visibility scores indicating which regions are visible on the input image.</p><p>We first define p = m ? n densely aligned rectangle regions on the holistic pedestrian image through uniform division. Given a partial pedestrian image, we resize it to a fixed size, i.e., H ? W and input it into VPM. Through a stack of convolutional layers ("conv" in <ref type="figure">Fig. 2</ref>, which uses all the convolutional layers in ResNet-50 <ref type="bibr" target="#b5">[6]</ref>), VPM transfers the input image into a 3D tensor T . The size of T is c ? h ? w (which are the number of channels, height and width, respectively), and we view the c ? dim vector g as a pixel on T . On tensor T , VPM appends a region locator and a region feature extractor. The region locator discovers regions on tensor T . Then the region feature extractor generates a respective feature for each region.</p><p>A region locator perceives which regions are visible and predicts their locations on tensor T . To this end, the region locator employs a 1 ? 1 convolutional layer and a following Softmax function to classify each pixel g on T into the predefined regions, which in formulated by,</p><formula xml:id="formula_0">P (R i |g) = sof tmax(W T g) = exp W T i g p j=1 exp W T j g ,<label>(1)</label></formula><p>where P (R i |g) is the predicted probability of g belonging to R i , W is the learnable weight matrix of the 1 ? 1 convolutional layer, p is the total number of pre-defined regions. By sliding over every pixel g on T , the region locator predicts g as belonging to all the pre-defined regions with corresponding probabilities, and thus gets p probability maps (one h ? w map for each region), as shown in <ref type="figure">Fig.   2</ref>. Each probability map indicates the location of a corresponding region on T , which allows region feature extraction.</p><p>The region locator also predicts the visibility score C for each region, by accumulating P (R i |g) over all the g on T , which is formulated by,</p><formula xml:id="formula_1">C i = f ?T P (R i |g),<label>(2)</label></formula><p>Eq. 2 is natural in that if considerable pixels on T belongs to R i (with large probability), it indicates that R i is visible on the input image and is assigned with a relatively large C i . In contrast, if a region is actually invisible, the region locator will still return a probability map (with all the values approximating 0). In this case, C i will be very small, indicating possibly-invisible region. The visibility score is important for calculating the distance between two images, as to be detailed in Section 3.2.</p><p>A region feature extractor generates a respective feature f for a region by weighted pooling, which is formulated by,</p><formula xml:id="formula_2">f i = g?T P (R i |g)g C i , ?i ? {1, 2, ? ? ? , p},<label>(3)</label></formula><p>where the division of C i is to maintain the norm invariance against the size of the region. The region locator returns a probability map for each region, even if the region is actually invisible on the input image. Correspondingly, we can see from Eq. 3 that the region feature extractor always generates a constant number (i.e., p) of region features for any input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Employing VPM</head><p>Given two images to be compared, i.e., I k and I l , VPM extracts their region features and predicts the region visibility scores through Eq. 3 and Eq. 2, respectively. With region features and region visibility scores</p><formula xml:id="formula_3">{f k i , C k i } , {f l i , C l i }, VPM first calculates region-to-region euclidean distances D kl i = f k i ? f l i (i = 1, 2, ? ? ? , p)</formula><p>. Then VPM concludes the overall distance from the local distances by,</p><formula xml:id="formula_4">D kl = p i=1 C k i C l i D kl i p i=1 C k i C l i .<label>(4)</label></formula><p>In Eq. 4, the visible regions are with relative large visibility scores. The local distances between shared regions are highly credited by VPM and thus dominate the overall distance D kl . In contrast, if a region is invisible in any one of the compared images, its region feature is considered as unreliable and the corresponding local distance contributes little to D kl .</p><p>Employing VPM adds very light computational cost, compared with popular part-based deep learning methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12]</ref>. While some prior partial re-ID methods require pairwise comparison before feature extraction and may have efficiency problems, VPM presents high scalability, which allows experiments on large re-ID datasets such as Market-1501 <ref type="bibr" target="#b32">[33]</ref> and DukeMTMC-reID <ref type="bibr" target="#b36">[37]</ref>, as to be accessed in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training VPM</head><p>Training VPM consists of training the region classifier and training the region feature extractor. The region classifier and the region feature extractor share the convolutional layers before tensor T , and are trained end to end in a multitask training manner. Training VPM is also featured for employing auxiliary self-supervision.</p><p>Self-supervision is critical to VPM. It supervises VPM to learn region visibility awareness, as well as to focus on visible regions during feature learning. Specifically, given a holistic pedestrian image, we randomly crop a patch and resize it to H ? W . The random crop operation excludes several pre-defined regions and the remaining regions are reshaped during the resizing. Then, we project the regions on the input image to tensor T through ROI projection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>. To be concrete, let us assume a region with its up-left corner located at (x 1 , y 1 ) and its bottom-right corner located at (x 2 , y 2 ) on the input image. Then the ROI projection defines a corresponding region on tensor T with its up-left corner located at ([x 1 /S] , [y 1 /S]) and its right-bottom corner located at ([x 2 /S] , [y 2 /S]), in which the [?] denotes the rounding and S is the down-sampling rate from input image to T . Finally, we assign every pixel g on T with a region label L(L ? 1, 2, ? ? ? , p) to indicate which region g belongs to. We also record all the visible regions in a set V . As we will see, self-supervision contributes to training VPM in the following three aspects:</p><p>? First, self-supervision generates the ground truth of region labels for training the region locator.</p><p>? Second, self-supervision enables VPM to focus on visible regions when learning feature through classification loss (cross-entropy loss).</p><p>? Finally, self-supervision enables VPM to focus on the shared regions when learning features through triplet loss.</p><p>Without the auxiliary self-supervision, VPM encounters dramatic performance decrease, as to be accessed in Section 4.4.</p><p>The region locator is trained through cross-entropy loss with the self-supervision signal L as the ground truth, which is formulated by,</p><formula xml:id="formula_5">L R = ? g?T 1 i=L log(P (R i |g)),<label>(5)</label></formula><p>where 1 Ri=L returns 1 only when i equals the ground truth region label L and returns 0 in any other cases.</p><p>The region feature extractor is trained with the combination of cross-entropy loss and triplet loss, as illustrated in 3. Recall that the region feature extractor always generates p region features for any input image. It leads to a nontrivial problem during feature learning: only features of visible regions should be allowed to contribute to the training losses. With self-supervision signals V , we dynamically select the visible regions for feature learning.</p><p>The cross-entropy loss is commonly used in learning features for pedestrian under the IDE <ref type="bibr" target="#b31">[32]</ref> mode. We append a respective identity classifier i.e., IP i (f i )(i = 1, 2, ? ? ? , p) upon each region feature f i , to predict the identity of training images. The identity classifier consists of two sequential fully-connected layers and a Softmax function. The first fully-connected layers reduces the dimension of the input region feature, and the second one transforms the feature dimension to K (K is the total identities of training images). Then the cross-entropy loss is formulated by,</p><formula xml:id="formula_6">L ID = ? i?V 1 k=y log(sof tmax(IP i (f i ))),<label>(6)</label></formula><p>where k is the predicted identity and y is the ground truth label. With Eq. 6, self-supervision enforces focus on visible regions for learning region features through cross-entropy loss.</p><p>The triplet loss pushes the features from a same pedestrian close to each other and pulls the features from different pedestrians far away. Given a triplet of images, i.e., an anchor image I a , a positive image I p and a negative image I n , we define a region-selective triplet loss derived from the canonical one by,</p><formula xml:id="formula_7">L tri = [D ap ? D an + ?] + , D ap = i?(V a ?V p ) f a i ? f p i |V a ? V p | , D an = i?(V a ?V n ) f a i ? f n i |V a ? V n | ,<label>(7)</label></formula><p>where f a i , f p i and f n i are the region features from anchor image, positive image and negative image, respectively. V a , V p and V n are the visible region sets for anchor image, positive image and negative image, respectively. |?| denotes the operation of counting the elements of a set, i.e., the number of shared regions in the two compared images. ? is the margin for training triplet, and is set to 1 in our implementation.</p><p>With Eq. 7, self-supervision enforces focus on the shared regions when calculating the distances of two images.</p><p>The overall training loss is the sum of region prediction loss, the identity classification loss and the region-selective triplet loss, which is formulated by,</p><formula xml:id="formula_8">L = L R + L ID + L tri<label>(8)</label></formula><p>We also note that Eq. 4 and Eq. 7 share a similar pattern. Training with the modified triplet loss (Eq. 7) mimics the matching strategy (Eq. 4) and is thus specially beneficial (to be detailed in <ref type="table">Table 3</ref>). The difference is that, during training, the focus is enforced through "hard" visibility labels, while during testing, the focus is regularized through predicted "soft" visibility scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Datasets. We use four datasets, i.e., Market-1501 <ref type="bibr" target="#b32">[33]</ref>, DukeMTMC-reID <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref>, Partial-REID and Partial-iLIDS, to evaluate our method.</p><p>Market-1501 and DukeMTMC-reID are two large scale holistic re-ID dataset. The Market-1501 dataset contains 1,501 identities observed from 6 camera viewpoints, 19,732 gallery images and 12,936 training images detected by DPM <ref type="bibr" target="#b3">[4]</ref>. The DukeMTMC-reID dataset contains 1,404 identities, 16,522 training images, 2,228 queries, and 17,661 gallery images. We crop certain patches from the query images during testing stage to imitate the partial re-ID scenario and get a comprehensive evaluation of our method on largescale (synthetic) partial re-ID datasets. We note that few prior works on partial re-ID evaluated their methods on large-scale dataset, mainly because of low computing efficiency. Partial-REID <ref type="bibr" target="#b35">[36]</ref> and Partial-iLIDS <ref type="bibr" target="#b34">[35]</ref> are two commonly-used datasets for partial re-ID. Partial-REID contains 600 images and 60 identities, every one of which has 5 holistic images and 5 partial images. Partial-iLIDS is derived from iLIDS <ref type="bibr" target="#b23">[24]</ref>, which is collected in an airport and the lower-body of a pedestrian is frequently occluded by the luggage. Partial-iLIDS crops the non-occluded region from these images and get 238 images from 119 identities. Both Partial-REID and Partial-iLIDS offer only testing images. When evaluating our method on these two public datasets, we train VPM on the training set of Market-1501, for fair comparison with other competitive methods, including MTRC <ref type="bibr" target="#b14">[15]</ref>, AMC+SWM <ref type="bibr" target="#b35">[36]</ref>, DSR <ref type="bibr" target="#b6">[7]</ref>, and SFR <ref type="bibr" target="#b7">[8]</ref>. Implementation Details. Training VPM relies on the assumption that the original training images all contain holistic pedestrian and the pedestrian are tightly bounded by bounding boxes. On two holistic re-ID datasets, Market-1501 and DukeMTMC-reID, there do exist some images which contain either partial pedestrian or oversized bounding boxes. We consider these images as tolerable noises.</p><p>To generate the partial image for training VPM, we crop a patch from the holistic image with random area ratio ?. We set ? to be uniformly distributed between 0.5 and 1. VPM is not necessarily bounded with any specified crop strategy and we may consider the prior knowledge for optimization. We argue that choosing the detailed crop strategy according to the realistic condition is reasonable because partial re-ID is a realistic challenge, and the occlusion fashion is usually predictable. We also experimentally validate that choosing an appropriate crop strategy to imitate the confronted partial re-ID condition benefits the retrieval accuracy, as to be detailed in Section 4.3. That being said, VPM is still general in that it may adopt any crop strategy to conduct self-supervision.</p><p>VPM is trained with the combination of cross-entropy loss and triplet loss. We pre-train the model for 50 epochs with single cross-entropy loss because it helps VPM to converge faster and better. Then we append the triplet loss and fine-tune the model for another 80 epochs. In both the pre-training and the fine-tuning stages, we use standard Stochastic Gradient Descent (SGD) optimization strategy, initialize the learning rate as 0.1 and decay it to 0.01 after 30 epochs. During the fine-tuning stage, we construct each mini-batch with 64 images from 8 identities (8 images per identity) and use the hard mining strategy <ref type="bibr" target="#b8">[9]</ref> for deducing the triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on large-scale partial re-ID datasets</head><p>We evaluate the effectiveness of VPM with experiment on the synthetic partial datasets derived from two largescale re-ID datasets, Market-1501 and DukeMTMC-reID. We differ the ratio ? of the cropped patches from 0.5 to 1.0 during testing. For comparison with VPM, we implement a baseline which learns global feature through the combination of cross-entropy loss and triplet loss. We also implement a part-based feature learning method PCB <ref type="bibr" target="#b22">[23]</ref>. For fair comparison, we enhance PCB with an extra triplet loss during training, and achieve slightly higher performance than <ref type="bibr" target="#b22">[23]</ref>. The results are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>VPM significantly improves partial re-ID performance over the baseline. On Market-1501, VPM surpasses the baseline by +6.4%, +5.4%, +4.3%, +4.4%, +4.6% +6.2% rank-1 accuracy and +4.4%, +4.6%, +8.0%, + 8.6%, +10.9%, +13.1% mAP when ? is set from 0.5 to 1, respectively. The superiority of VPM against the baseline, which learns a global feature representation, is derived from two-fold benefit. On the one hand, VPM learns region-level features and benefits from fine-grained information. On the other hand, with visibility awareness, VPM conducts a region-level alignment and eliminates the distracting noises originated from unshared regions.</p><p>VPM increases the robustness of part features un-  . Impact of p on the partial-reID accuracy. We set p to 2,3,4,6 and 8, respectively. We use Market-1501 for training and differ the crop ratio ? during testing.</p><p>der partial re-ID scenario. Comparing VPM with PCB, a state-of-the-art part feature learning method for holistic person re-ID task, we observe that as ? decreases, the retrieval accuracy achieved by PCB dramatically drops (e.g., 0.9% rank-1 accuracy at ? = 0.5 ), implying that PCB is extremely vulnerable to the spatial misalignment in partial re-ID. By contrast, the retrieval accuracy achieved by VPM decreases much slower as ? decreases. We infer that VPM facilitates region-to-region comparison within shared regions of two images and thus gains strong robustness. We also notice that under ? = 1.0, i.e., the holistic person re-ID scenario, VPM achieves comparable retrieval accuracy with PCB.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, we use 6 pre-defined parts to construct VPM. Moreover, we analyze the impact of the part numbers p on Market-1501, with results shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. Under all settings of p and ?, VPM consistently surpasses the baseline, which further confirms the superiority of VPM. We also observe that larger p generally brings higher (rank-1) retrieval accuracy. Larger p allows VPM to learn the region-level features in finer granularity and thus benefits the discriminative ability, which is consistent with the observation in holistic person re-ID works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23]</ref>. Larger p also allows more accurate region alignment when comparing a partial person image against a holistic one. We suggest choosing p with the joint consideration of retrieval accuracy and computing efficiency, and set p = 6 in most of our experiments (if not specially mentioned).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the state of the art</head><p>We compare VPM with the state-of-the-art methods on two public datasets, i.e., Partial-REID and Partial-iLIDS. We train three different versions of VPM with different crop strategies for preparing training patches, i.e., top crop (the  <ref type="table" target="#tab_1">Table 2</ref>, from which two observations are drawn.</p><p>First, comparing three editions of VPM against each other, we find that the crop strategy matters to VPM. On Partial-iLIDS, all query images of which are cropped from the top side of holistic pedestrian images, VPM (Top) achieves the highest retrieval accuracy. On Partial-REID, which contains images cropped from different directions, VPM (Bilateral) achieves the highest retrieval accuracy. VPM (Bottom) always performs the worst due to two reasons. First, retaining the bottom regions severely deviates from the testing condition. Second, the bottom regions (mainly containing legs) inherently offers relatively weak discriminative clues. We note that when solving the problem of partial-reID, the realistic partial condition is usually estimable. We recommend analyzing the partial condition and choosing a similar crop strategy for training VPM. That being said, VPM is general in that it is able to cooperate with various crop strategies.</p><p>Second, given appropriate crop strategies, VPM achieves very competitive performance compared with the state of the art. On Partial-REID, VPM (Bilateral) surpasses the strongest competing method SFR by +10.6% Rank-1 accuracy. On Partial-iLIDS, VPM (Top) surpasses SFR by +3.3% Rank-1 accuracy. Even with no prior knowledge of partial condition on testing set, we may eclectically choose VPM (Bilateral), which considers both top and down occlusions and thus maintains stronger robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The importance of self-supervision</head><p>We conduct ablation study to analyze the impact of selfsupervision on VPM. We train four Malfunctioned VPM for comparison:</p><p>? MVPM-1 is trained as a normal VPM, but abandons the visibility awareness during testing, i.e., MVPM-1 concludes the overall distance with all region-level features, even if some regions are invisible.  <ref type="table">Table 3</ref>. Ablation study on VPM. "VPM (no triplet)" is trained with no triplet loss. On Market-1501, we only analyze the holistic person re-ID mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partial-iLIDS</head><p>? MVPM-2 abandons self-supervision on triplet loss during training, i.e., all region features equally contribute to deducing the triplet loss L tri .</p><p>? MVPM-3 abandons self-supervision on identification loss L ID during training, i.e., all region features are supervised by the training identity label through L ID .</p><p>? MVPM-4 abandons self-supervision on both triplet loss and identification loss.</p><p>Moreover, we also analyze the impact of two types of losses (cross-entropy loss and triplet loss) in training VPM. The results are summarized in <ref type="table">Table 3</ref>, from which we draw three observations. First, appending an extra triplet loss enhances the discriminative ability of the learned features, under both partial re-ID scenario (Partial-iLIDS) and holistic person re-ID scenario (Market-1501). This observation is consistent with prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29]</ref> on holistic person re-ID task.</p><p>Second, comparing "MVPM-1" with "VPM", we observe a dramatic performance decrease on Partial-iLIDS. Both models are trained in exactly the same procedure. The difference is that "MPVM-1" employs all the region features to conclude the overall distance, while VPM focuses on the shared regions between two images. On Market-1501, all the regions are visible and two models achieves very close retrieval accuracy. We thus infer that the visibility awareness is critical for VPM under partial re-ID scenario.</p><p>Third, comparing last three editions of MVPM with "VPM" as well as "MVPM-1", we observe further performance decreases on Partial-iLIDS. The last three editions abandon self-supervision to regularize the learning of region-level features (either on the cross-entropy loss or triplet loss or both). Learning features from invisible regions brings about larger sample noises. Consequentially, the learned region features are significantly compromised. We thus conclude that enforcing VPM to focus on visible regions through self-supervision is critical for learning region features. <ref type="figure">Figure 5</ref>. Region visualization. We train VPM with 3 ? 2 predefined regions. For each image, VPM discovers 6 regions with 6 probability maps, as detailed in Section 3.1. For better visualization, we assign each pixel to its closest region and achieve the partitioning effect. Images on the first and the second row are from (synthetic) Market-1501 and Partial-REID, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization of discovered regions</head><p>We visualize the regions discovered by VPM (the region locator, in particular) in <ref type="figure">Fig. 5</ref>. We use p = 3 ? 2 pre-defined regions to facilitate both horizontal and vertical visibility awareness. It is observed that VPM conducts adaptive partition with visibility awareness. Given holistic images (the first column), VPM successfully discovers all the 3 ? 2 regions. Given partial pedestrian images with horizontal occlusion (the second column), VPM favors the dominating regions (left regions in <ref type="figure">Fig. 5</ref>). Given partial pedestrian images with lower-body occluded (the last two columns), VPM roughly discovers 4 visible regions, and perceives that the bottom 2 regions are invisible. These observations confirm that VPM gains robust region-level visibility awareness and is capable to locate the visible regions through self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a region-based feature learning method, VPM for partial re-ID task. Given a set of pre-defined regions on the holistic pedestrian image, VPM learns to perceive which regions are visible on a partial image through self-supervision. VPM locates each region on the convolutional feature maps and then extracts regionlevel features. With visibility awareness, VPM compares two pedestrian images with focus on their shared regions and correspondingly suppresses the severe spatial misalignment in partial re-ID. Experimental results confirm that VPM surpasses both the global feature learning baseline and part-based convolutional methods, and the achieved performance is on par with the state of the art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Two challenges related to partial-re-ID and our solution with the proposed VPM. (a) aggravation of spatial misalignment, (b) distracting noises from unshared regions (the blue region on the left image) and (c) VPM locates visible regions on a given image and extracts region-level features. With visibility-awareness, VPM compares two images by focusing on their shared regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>VPM learns region-level features with auxiliary selfsupervision. Only features corresponding to visible regions contribute to the cross-entropy loss. Only features corresponding to shared regions contribute to the deducing of triplet loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Impact of p on the partial-reID accuracy. We set p to 2,3,4,6 and 8, respectively. We use Market-1501 for training and differ the crop ratio ? during testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between VPM, baseline and PCB. For VPM, we use p = 6 ? 1 pre-defined regions. For PCB, we adopt the code released by the authors and append an extra triplet loss, for fair comparison with VPM. On Market-1501, the extra triplet loss enables PCB to gain +5.6% mAP over the original 77.4% reported by the authors<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell>Dataset</cell><cell>?</cell><cell>R-1</cell><cell cols="3">baseline R-5 R-10 mAP R-1</cell><cell cols="3">PCB R-5 R-10 mAP R-1</cell><cell>VPM R-5 R-10 mAP</cell></row><row><cell></cell><cell cols="3">0.5 64.5 82.2 88.1</cell><cell>44.4</cell><cell>0.9</cell><cell>3.2</cell><cell>5.6</cell><cell>1.7</cell><cell>70.9 86.5 92.1</cell><cell>48.8</cell></row><row><cell></cell><cell cols="3">0.6 79.0 91.4 94.3</cell><cell>57.9</cell><cell>8.1</cell><cell cols="2">16.5 23.2</cell><cell>6.6</cell><cell>84.4 94.3 96.1</cell><cell>62.5</cell></row><row><cell>Market-1501</cell><cell cols="3">0.7 83.9 93.9 95.9</cell><cell cols="4">63.7 36.5 58.9 67.4</cell><cell>26.8 88.2 95.8 97.2</cell><cell>71.7</cell></row><row><cell></cell><cell cols="3">0.8 85.7 94.3 96.4</cell><cell cols="4">66.1 71.9 87.3 91.4</cell><cell>56.8 90.1 95.8 97.7</cell><cell>74.7</cell></row><row><cell></cell><cell cols="3">0.9 87.1 95.5 97.4</cell><cell cols="4">67.8 88.8 95.8 97.1</cell><cell>77.2 91.7 96.6 98.0</cell><cell>78.7</cell></row><row><cell></cell><cell cols="3">1.0 86.8 95.3 97.4</cell><cell cols="4">67.7 93.4 97.8 98.4</cell><cell>83.0 93.0 97.8 98.8</cell><cell>80.8</cell></row><row><cell></cell><cell cols="3">0.5 65.0 81.1 86.7</cell><cell>47.2</cell><cell>5.0</cell><cell cols="2">10.1 13.6</cell><cell>4.0</cell><cell>69.5 83.1 87.9</cell><cell>52.2</cell></row><row><cell></cell><cell cols="3">0.6 76.2 87.3 90.4</cell><cell cols="4">55.4 13.1 25.6 33.5</cell><cell>10.5 78.2 89.0 91.3</cell><cell>60.9</cell></row><row><cell cols="4">DukeMTMC-reID 0.7 76.3 87.3 90.6</cell><cell cols="4">90.6 35.9 57.0 65.4</cell><cell>28.4 80.3 89.5 92.0</cell><cell>63.1</cell></row><row><cell></cell><cell cols="3">0.8 76.3 88.3 91.9</cell><cell cols="4">58.8 64.0 82.6 87.7</cell><cell>52.3 80.3 89.3 92.4</cell><cell>63.5</cell></row><row><cell></cell><cell cols="3">0.9 77.0 88.1 91.7</cell><cell cols="4">59.0 81.6 90.4 93.0</cell><cell>70.3 81.7 90.9 93.1</cell><cell>70.7</cell></row><row><cell></cell><cell cols="3">1.0 76.2 87.3 91.2</cell><cell cols="4">58.6 84.1 92.4 94.5</cell><cell>73.2 83.6 91.7 94.2</cell><cell>72.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of VPM on Partial-REID and Partial-iLIDS. Three VPMs trained with different crop strategies are evaluated.</figDesc><table><row><cell>Methods</cell><cell>Partial-REID Partial-iLIDS R-1 R-3 R-1 R-3</cell></row><row><cell>MTRC [15]</cell><cell>23.7 27.3 17.7 26.1</cell></row><row><cell cols="2">AMC+SWM [36] 37.3 46.0 21.0 32.8</cell></row><row><cell>DSR [7]</cell><cell>50.7 70.0 58.8 67.2</cell></row><row><cell>SFR [8]</cell><cell>56.9 78.5 63.9 74.8</cell></row><row><cell>VPM (Bottom)</cell><cell>53.2 73.2 53.6 62.3</cell></row><row><cell>VPM (Top)</cell><cell>64.3 83.6 67.2 76.5</cell></row><row><cell cols="2">VPM (Bilateral) 67.7 81.9 65.5 74.8</cell></row><row><cell cols="2">top regions are always visible), bottom crop (the bottom re-</cell></row><row><cell cols="2">gions are always visible) and bilateral crop (top crop + bot-</cell></row><row><cell cols="2">tom crop). The results are presented in</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep spatial feature reconstruction for partial person re-identification: Alignmentfree approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1801.00881</idno>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Recognizing partial biometric patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1810.07399</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gokmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Partial face recognition: Alignment-free approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multicamera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Posedriven deep convolutional model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person reidentification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transitive invariance for selfsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">GLAD: Global-local-alignment descriptor for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hardaware point-to-set deep metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Person reidentification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person re-identification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 24th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2011</title>
		<meeting><address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago</title>
		<meeting><address><addrLine>Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
