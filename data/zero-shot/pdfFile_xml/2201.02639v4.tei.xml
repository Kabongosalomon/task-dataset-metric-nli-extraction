<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound ? Add a third of a cup of popcorn Now turn the heat on high Add a lid, and then *sizzling* *pouring sound* *lid clinking* jiggle it while it pops *jiggling, popcorn popping* w4</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Salehi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Allen Institute for Artificial Intelligence University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound ? Add a third of a cup of popcorn Now turn the heat on high Add a lid, and then *sizzling* *pouring sound* *lid clinking* jiggle it while it pops *jiggling, popcorn popping* w4</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: MERLOT Reserve learns multimodal neural script knowledge representations of video -jointly reasoning over video frames, text, and audio. Our model is pretrained to predict which snippet of text (and audio) might be hidden by the MASK. This task enables it to perform well on a variety of vision-and-language tasks, in both zero-shot and finetuned settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>As humans, we navigate a multimodal world, building a holistic understanding from all our senses. We introduce MERLOT Reserve, a model that represents videos jointly over time -through a new training objective that learns from audio, subtitles, and video frames. Given a video, we replace snippets of text and audio with a MASK token; the model learns by choosing the correct masked-out snippet. Our objective learns faster than alternatives, and performs well at scale: we pretrain on 20 million YouTube videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical results show that</head><p>MERLOT Reserve learns strong multimodal representations. When finetuned, it sets state-of-the-art on Visual Commonsense Reasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%, and 1.5% respectively. Ablations show that these tasks benefit from audio pretraining -even VCR, a QA task centered around images (without sound). Moreover, our objective enables out-of-the-box prediction, revealing strong multimodal commonsense understanding. In a fully zero-shot setting, our model obtains competitive results on four video tasks, even outperforming supervised approaches on the recently proposed Situated Reasoning (STAR) benchmark.</p><p>We analyze why audio enables better vision-language representations, suggesting significant opportunities for future research. We conclude by discussing ethical and societal implications of multimodal pretraining.</p><p>Recent papers propose extensions, like generating masked-out spans <ref type="bibr" target="#b21">[22]</ref> or text <ref type="bibr" target="#b47">[78,</ref><ref type="bibr" target="#b87">116]</ref>, but it is unclear whether they can outperform the VisualBerts on vision-language tasks like VCR <ref type="bibr" target="#b97">[126]</ref>. Another extension involves learning from text-to-speech audio in a captioning setting [62, 79]yet this lacks key supervision for environmental sounds and emotive speech.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><formula xml:id="formula_0">w4 [MASKed span] w1</formula><p>w2 w3 a1 a2 a3 a4 v1 v2 v3 v4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The world around us is dynamic. We experience and learn from it using all of our senses, reasoning over them temporally through multimodal script knowledge <ref type="bibr" target="#b70">[99,</ref><ref type="bibr" target="#b99">128]</ref>. Consider <ref type="figure">Figure 1</ref>, which depicts someone cooking popcorn. From the images and dialogue alone, we might be able to imagine what sounds of the scene are: the process might begin with raw kernels scattering in an empty, metallic pot, and end with the dynamic 'pops' of popcorn expanding, along with the jiggling of a metal around the stove.</p><p>Predicting this sound is an instance of learning from reentry: where time-locked correlations enable one modality to educate others. Reentry has been hypothesized by developmental psychologists to be crucial for how we as humans learn visual and world knowledge, much of it without need for an explicit teacher <ref type="bibr" target="#b59">[89,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b71">100</ref>]. Yet, we ask -can we build machines that likewise learn vision, language, and sound together? And can this paradigm enable learning neural script knowledge, that transfers to language-and-vision tasks, even those without sound?</p><p>In this work, we study these questions, and find that the answers are 'yes. <ref type="bibr">'</ref> We introduce a new model that learns self-supervised representations of videos, through all their modalities (audio, subtitles, vision). We dub our model MERLOT Reserve , henceforth Reserve for short.</p><p>Our model differs from past work that learns from audioimage pairs <ref type="bibr">[54,</ref><ref type="bibr" target="#b40">71]</ref>, from subtitled videos <ref type="bibr" target="#b76">[105,</ref><ref type="bibr" target="#b99">128]</ref>, or from static images with literal descriptions <ref type="bibr" target="#b77">[106,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b63">92]</ref>. Instead, we learn joint representations from all modalities of a video, using each modality to teach others. We do this at scale, training on over 20 million YouTube videos. We introduce a new contrastive masked span learning objective to learn script knowledge across modalities. It generalizes and outperforms a variety of previously proposed approaches (e.g. [29, <ref type="bibr" target="#b77">106,</ref><ref type="bibr" target="#b63">92,</ref><ref type="bibr" target="#b99">128]</ref>), while enabling audio to be used as signal. The idea is outlined in <ref type="figure">Figure 1</ref>: the model must figure out which span of text (or audio) was MASKed out of a video sequence. We combine our objective with a second contrastive learning approach, tailored to learning visual recognition from scratch: the model must also match each video frame to a contextualized representation of the video's transcript <ref type="bibr" target="#b99">[128]</ref>. Through ablations, we show that our framework enables rapid pretraining of a model and readily scales to 'large' transformer sizes (of 644M parameters).</p><p>Experimental results show that Reserve learns powerful representations, useful even for tasks posed over only a few of the studied modalities. For example, when finetuned on Visual Commonsense Reasoning <ref type="bibr" target="#b97">[126]</ref> (a vision+language task with no audio), it sets a new state-of-the-art, outperforming models trained on supervised image-caption pairs by over 5%. It does even better on video tasks: fine-tuning without audio, it outperforms prior work on TVQA <ref type="bibr" target="#b44">[75]</ref> by a margin of over 7% (and given TVQA audio, performance increases even further). Finally, audio enables 91.1% accuracy on Kinetics-600 <ref type="bibr" target="#b18">[19]</ref>. These performance improvements do not come at the expense of efficiency: our largest model uses one-fifths the FLOPs of a VisualBERT.</p><p>Reserve also performs well in zero-shot settings. We evaluate on four diverse benchmarks: Situated Reasoning (STAR) <ref type="bibr" target="#b90">[119]</ref>, EPIC-Kitchens <ref type="bibr" target="#b25">[26]</ref>, LSMDC-FiB <ref type="bibr" target="#b67">[96]</ref>, and MSR-VTT QA <ref type="bibr" target="#b91">[120]</ref>. These benchmarks require visual reasoning with respective emphasis on temporality, future prediction, and both social and physical understanding. With no fine-tuning or supervision, our model obtains competitive performance on each. Of note, it nearly doubles <ref type="bibr" target="#b94">[123]</ref>'s SoTA zero-shot accuracy on MSR-VTT QA, and it outperforms supervised approaches (like ClipBERT <ref type="bibr" target="#b43">[74]</ref>) on STAR.</p><p>Finally, we investigate why, and on which training instances audio-powered multimodal pretraining particularly helps. For instance, predicting audio rewards models for recognizing dynamic state changes (like cooked popcorn) and human communication dynamics (what are people's emotions and towards whom). Our model progressively learns these phenomena as pretraining progresses. These signals are often orthogonal to what snippets of text provide, which motivates learning from both modalities.</p><p>In summary, our key contributions are the following:</p><formula xml:id="formula_1">a.</formula><p>Reserve, a model for multimodal script knowledge, fusing vision, audio, and text. b. A new contrastive span matching objective, enabling our model to learn from text and audio self-supervision. c. Experiments, ablations, and analysis, that demonstrate strong multimodal video representations.</p><p>Overall, the results suggest that learning representations from all modalities -in a time-locked, reentrant manner -is a promising direction, and one that has significant space for future work. We release code and model checkpoints at rowanzellers.com/merlotreserve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work brings together two active lines of research. Joint representations of multiple modalities. Many language-and-vision tasks benefit from early fusion of the modalities <ref type="bibr" target="#b5">[6]</ref>. A family of 'VisualBERT' models have been proposed for this: typically, these use a supervised object detector image encoder backbone, and pretrain on images paired with literal captions <ref type="bibr" target="#b77">[106,</ref><ref type="bibr" target="#b46">77,</ref><ref type="bibr" target="#b50">81,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b95">124,</ref><ref type="bibr" target="#b43">74]</ref>. Cross-modal interactions are learned in part through a masked language modeling (mask LM) objective <ref type="bibr">[29]</ref>, where subwords are replaced with 'MASK', and models independently predict each subword conditioned on both images and unmasked tokens.</p><p>Perhaps closest to our work is MERLOT <ref type="bibr" target="#b99">[128]</ref>, which learns a joint vision-text model from web videos with automatic speech recognition (ASR). Through a combination of objectives (including a variant of mask LM), MERLOT established strong results on a variety of video QA benchmarks when finetuned. However, it lacks audio: it is limited to representing (and learning from) video frames paired with subtitles. Our proposed Reserve, which represents and learns from audio, outperforms MERLOT.</p><p>Co-supervision between modalities. A common pitfall when training a joint multimodal model is that complex inter-modal interactions can be ignored during learning, in favor of simpler intra-modal interactions [51, <ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">59]</ref>. For example, when using the aforementioned mask LM objective, models can ignore visual input completely in favor of text-text interactions <ref type="bibr" target="#b12">[13]</ref>; this issue is magnified when training on videos with noisy ASR text <ref type="bibr" target="#b99">[128]</ref>.</p><p>A line of recent work thus learns independent modalityspecific encoders, using objectives that cannot be shortcutted with simple intra-modal patterns. Models like CLIP learn image classification by matching images with their captions, contrastively <ref type="bibr" target="#b103">[132,</ref><ref type="bibr" target="#b63">92,</ref><ref type="bibr" target="#b32">63]</ref>. Recent work has explored this paradigm for matching video frames with their transcripts <ref type="bibr" target="#b92">[121]</ref>, with their audio signal <ref type="bibr" target="#b68">[97,</ref><ref type="bibr" target="#b85">114]</ref>, or both <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>; these works likewise perform well on single-modality tasks like audio classification and activity recognition. These independent encoders can be combined through late fusion <ref type="bibr" target="#b68">[97]</ref>, yet late fusion is strictly less expressive than our proposed joint encoding (early fusion) approach.</p><p>Our work combines both lines of research. We learn a model for jointly representing videos, through all their modalities, and train it using a new learning objective that enables co-supervision between modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model: Reserve</head><p>In this section, we present Reserve, including: our model architecture <ref type="bibr">(3.1)</ref>, new pretraining objectives <ref type="bibr">(3.2)</ref>, and pretraining video dataset <ref type="bibr">(3.3)</ref>. At a high level,</p><p>Reserve represents a video by fusing its constituent modalities (vision, audio, and text from transcribed speech) together, and over time. These representations enable both finetuned and zero-shot downstream applications.</p><p>More formally, we split a video V into a sequence of non-overlapping segments in time {s t }. Each segment has: a. A frame v t , from the middle of the segment, b. The ASR tokens w t spoken during the segment, c. The audio a t of the segment.</p><p>Segments default to 5 seconds in length; we discuss details of how we split videos into segments in Appendix C.</p><p>As the text w t was automatically transcribed by a model given audio a t , it is reasonable to assume that it contains strictly less information content. Thus, for each segment s t , we provide models with exactly one of text or audio. We will further mask out portions of the text and audio during pretraining, to challenge models to recover what is missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model architecture</head><p>An overview of Reserve is shown in <ref type="figure">Figure 2</ref>. We first pre-encode each modality independently (using a Transformer <ref type="bibr" target="#b81">[110]</ref> or images/audio; a BPE embedding table for text). We then learn a joint encoder to fuse all representations, together and over time.</p><p>Image encoder. We use a Vision Transformer (ViT; [34]) to encode each frame independently. We use a patch size of <ref type="bibr" target="#b15">16</ref> and apply a 2x2 query-key-value attention pool after the Transformer, converting an image of size H?W into a H/32?W/32 feature map of dimension d h .</p><p>Audio encoder. We split the audio in each segment a t into three equal-sized subsegments, for compatibility with the lengths at which we mask text (Appendix C). We use an Despite being derived from the audio, pretraining with text is still paramount: 1) in ?3.2 we discuss how jointly modeling audio+text prevents models from shortcutting pretraining objectives via surface correlations; 2) in ?4.2 we show that incorporating both transcripts and audio during fine-tuning improves performance; and 3) a textual interface to the model is required for downstream vision+language with textual inputs. Predict MASKed text and audio ^F igure 2: Reserve architecture. We provide sequencelevel representations of video frames, and either words or audio, to a joint encoder. The joint encoder contextualizes over modalities and segments, to predict what is behind MASK for audio a t and text w t . We supervise these predictions with independently encoded targets: a t from the audio encoder, and w t from a separate text encoder (not shown).</p><p>Audio Spectrogram Transformer to encode each subsegment independently <ref type="bibr">[47]</ref>. The three feature maps are concatenated; the result is of size 18?d h for every 5 seconds of audio.</p><p>Joint encoder. Finally, we jointly encode all modalities (over all input video segments) using a bidirectional Transformer. We use a linear projection of the final layer's hidden states for all objectives (e.g. w t and a t ).</p><p>Independently-encoded targets. We will supervise the joint encoder by simultaneously learning independentlyencoded 'target' representations for each modality. Doing this is straightforward for the image and audio encoders: we add a CLS to their respective inputs, and extract the final hidden state v t or a t at that position. For text, we learn a separate bidirectional Transformer span encoder, which computes targets w t from a CLS and embedded tokens of a candidate text span. This enables zero-shot prediction (4.4).</p><p>Architecture sizes. We consider two model sizes in this work, which we pretrain from random initialization:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Reserve-B, with a hidden size of 768, a 12-layer</head><p>ViT-B/16 image encoder, and a 12-layer joint encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Reserve-L, with a hidden size of 1024, a 24-layer</head><p>ViT-L/16 image encoder, and a 24-layer joint encoder.</p><p>We always use a 12-layer audio encoder, and a 4-layer text span encoder. Details are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive Span Training</head><p>We introduce contrastive span training, which enables learning across and between the three modalities. As shown in <ref type="figure">Figure 3</ref>, the model is given a sequence of video segments.  <ref type="figure">Figure 3</ref>: Contrastive span training. Given a video with all modalities temporally aligned, we MASK out a region of text and audio. The model must maximize its similarity only to an independent encoding of the text w t and audio a t .</p><p>For each one, we include the video frame, and then three 'subsegments' that are each either text or audio. The subdivided audio segments are encoded independently by the Audio Encoder, before being fused by the Joint Encoder. We train by replacing 25% of these text and audio subsegments with a special MASK token. The model must match the representation atop the MASK only with an independent encoding of its span. Our approach combines past success at matching images to their captions <ref type="bibr" target="#b63">[92,</ref><ref type="bibr" target="#b32">63]</ref> along with 'VisualBERT'-style prediction of independent tokens <ref type="bibr" target="#b77">[106,</ref><ref type="bibr" target="#b20">21]</ref> -though, crucially, we predict representations at a higher-level semantic unit than individual tokens. Our approach also enables the model to learn from both audio and text, while discouraging memorization of raw perceptual input, or tokens -which can harm representation quality <ref type="bibr" target="#b83">[112]</ref>.</p><p>Formally, we minimize the cross entropy between the MASKed prediction? t and its corresponding phrase representation w t , versus others in the batch W:</p><formula xml:id="formula_2">L mask?text = 1 |W| wt?W log exp(?? t ? w t ) w?W exp(?? t ? w) .<label>(1)</label></formula><p>We first L 2 -normalize w and?, and scale their dot product with a parameter ? <ref type="bibr" target="#b63">[92]</ref>. We then add this to its transposed version L text?mask , giving us our text-based loss L text . Analogously, we define L audio for audio, between the MASKed prediction? t and its target a t , versus others a in the batch. In addition to these masked text and audio objectives, we simultaneously train the model to match video frames with a contextualized encoding of the transcript. Here, the joint encoder encodes the entire video's transcript at once, extracting a single hidden representation per segmentv t . We use the same contrastive setup as Equation 1 to maximize the Following past work, we optimize ? and clip it at 100, which enables the model to 'warm-up' its emphasis placed on hard negatives <ref type="bibr" target="#b63">[92,</ref><ref type="bibr" target="#b84">113]</ref>.</p><p>In MERLOT <ref type="bibr" target="#b99">[128]</ref>, this objective was found to be critical for learning visual recognition from self-supervised videos. similarity of these vectors with the corresponding v t vectors from the frames, giving us a symmetric frame-based loss L frame . The final loss is the sum of the component losses:</p><formula xml:id="formula_3">L = L text + L audio + L frame .</formula><p>(2)</p><p>Avoiding shortcut learning. Early on, we observed that training a model to predict a perceptual modality (like audio or vision) given input from the same modality, led to shortcut learning -a low training loss, but poor representations. We hypothesize that this setup encourages models to learn imperceptible features, like the exact model of the microphone, or the chromatic aberration of the camera lens <ref type="bibr">[33]</ref>. We avoid this, while still using audio as a target, by simultaneously training on two kinds of masked videos:</p><p>i. Audio only as target. We provide only video frames and subtitles. The model produces representations of both audio and text that fill in MASKed blanks. ii. Audio as input. We provide the model video frames, and subtitles or audio at each segment. Because the model is given audio as an input somewhere, the model only produces representations for MASKed text.</p><p>Another issue is that YouTube's captions are not perfectly time-aligned with the underlying audio. During our initial exploration, models took ready advantage of this shortcut: for instance, predicting an audio span based on what adjacent (overlapping) words sound like. We introduce a masking algorithm to resolve this; details in Appendix C.</p><p>Pretraining setup. We train on TPU v3-512 accelerators; training takes 5 days for Reserve-B, and 16 days for Reserve-L. We made pretraining more efficient through several algorithmic and implementation improvements. Of note, we simultaneously train on written (web) text, which enables more text candidates to be used. We use a batch size of 1024 videos, each with N =16 segments (split into two groups of 8 segments each). We use AdamW <ref type="bibr" target="#b38">[69,</ref><ref type="bibr" target="#b49">80]</ref> to minimize Equation 2. More details and hyperparameters are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pretraining Dataset</head><p>Recent prior work on static images that demonstrates empirical improvements by increasing dataset size -all the way up to JFT-3B <ref type="bibr" target="#b39">[70,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b63">92,</ref><ref type="bibr" target="#b101">130]</ref>. The same pattern emerges in videos: prior work that has shown promising empirical improvements not only by scaling to 6 million videos/180M frames <ref type="bibr" target="#b99">[128]</ref>, but also by collecting a diverse set (i.e., going beyond instructional videos <ref type="bibr" target="#b29">[60]</ref>).</p><p>To this end, we introduce a new training dataset of 20 million English-subtitled YouTube videos, and 1 billion frames, called YT-Temporal-1B. At the same time, we take steps to protect user privacy, directing scraping towards public, large, and monetized channels. We detail our collection, preprocessing, and release strategy in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present model ablations (4.1.1), and show that a finetuned Reserve obtains state-of-the-art results on VCR (4.1.2), TVQA (4.2), and Kinetics-600 (4.3). We then show that our model has strong zero-shot capability, over four challenging zero-shot tasks (4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Visual Commonsense Reasoning (VCR)</head><p>We evaluate Reserve first through finetuning on VCR <ref type="bibr" target="#b97">[126]</ref>. Most competitive models for VCR are pretrained exclusively on images paired with captions, often with supervised visual representations (e.g. from an object detector). To the best of our knowledge, the only exception is MERLOT <ref type="bibr" target="#b99">[128]</ref>, which uses YouTube video frames and text as part of pretraining; no VCR model to date was pretrained on audio.</p><p>VCR Task. A model is given an image from a movie, and a question. The model must choose the correct answer given four multiple choice options (Q?A); it then is given four rationales justifying the answer, and it must choose the correct one (QA?R). The results are combined with a Q?AR metric, where a model must choose the right answer and then the right rationale, to get the question 'correct.'</p><p>Finetuning approach. We follow <ref type="bibr" target="#b99">[128]</ref>'s approach: 'drawing on' VCR's detection tags onto the image, and jointly finetuning on Q?A and QA?R. For both subproblems, we learn by scoring each Q?A (or QA?R) option independently. We pool a hidden representation from a MASK inserted after the text, and pass this through a newlyinitialized linear layer to extract a logit, which we optimize through cross-entropy (details in Appendix D.1.1.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Ablations: contrastive learning with audio helps.</head><p>While we present our final, state-of-the-art VCR performance in 4.1.2, we first use the corpus for an ablation study. We use the same architecture and data throughout, allowing applesto-apples comparison between modeling decisions. We start with a similar configuration to MERLOT <ref type="bibr" target="#b99">[128]</ref> and show that contrastive span training improves further, particularly when we add audio.</p><p>Contrastive Span helps for Vision+Text modeling. We start by comparing pretraining objectives for learning from YouTube ASR and video alone: a. Mask LM. This objective trains a bidirectional model by having it independently predict masked-out tokens. We make this baseline as strong as possible by using SpanBERT-style masking <ref type="bibr" target="#b33">[64]</ref>, where text spans are masked out (identical to our contrastive spans). Each span w is replaced by a MASK token, and we predict each of its subwords w i independently.</p><p>Like <ref type="bibr" target="#b33">[64]</ref>, we concatenate the MASK's hidden state with a position embedding for index i, pass the result through a two-layer MLP, and use tied embedding weights to predict w i .   <ref type="bibr">[27]</ref>. In this objective, we likewise mask text subsegments and extract their hidden states. The difference is that we sequentially predict tokens w i ? w, using a left-to-right language model (LM) with the same architecture details as our proposed span encoder.</p><p>Results are in <ref type="table" target="#tab_3">Table 1</ref>. Versus these approaches, our contrastive span objective boosts performance by over 2%, after one epoch of pretraining only on vision and text. We hypothesize that its faster learning is caused by encouraging models to learn concept-level span representations; this might not happen when predicting tokens individually <ref type="bibr" target="#b22">[23]</ref>.</p><p>Audio pretraining helps, even for the audio-less VCR:</p><p>d. Audio as target. Here, the model is only given video frames and ASR text as input. In addition to performing contrastive-span pretraining over the missing text spans, it does the same over the (held-out) audio span (Equation 2. This boosts VCR accuracy by 0.7%. e. Audio as input and target. The model does the above (for video+text input sequences), and simultaneously is given video+text+audio sequences, wherein it must predict missing text. This boosts accuracy by 1% in total. f. Sans strict localization. We evaluate the importance of our strict localization in time. Here, in addition to correct subsegments at the true position t as a correct match, we count adjacent MASKed out regions as well. An extreme version of this was proposed by <ref type="bibr">[49]</ref>, where a positive match can be of any two frames in a video. Yet even in our conservative implementation, performance drops slightly, suggesting localization helps.</p><p>Putting these all together, we find that contrastive span pretraining outperforms mask LM, with improved performance when audio is used both as input and target. For our flagship model, we report results in <ref type="table" target="#tab_3">Table 1</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">VCR Results</head><p>Encouraged by these results, we train our models for 10 epochs on YT-Temporal-1B. <ref type="figure">Figure 4</ref> demonstrates that finetuned VCR performance tracks with the number of pretraining epochs, as well as the validation loss. Finally, in <ref type="table" target="#tab_5">Table 2</ref>, we compare Reserve against the largest published models from the VCR leaderboard. Of note,</p><p>Reserve-L outperforms all prior work, by over 5% on Q?AR metric. It outperforms even large ensembles (e.g. 15 ERNIE-Large's) submitted by industry <ref type="bibr" target="#b95">[124]</ref>, though we do not show these on this table to focus on only single models.</p><p>Efficiency. The accuracy increase of Reserve is not simply due to compute. In fact, our Reserve-L requires one-fifth the FLOPs of detector-based systems, like UNITER-Large <ref type="bibr" target="#b20">[21]</ref>  <ref type="table">(Appendix B.</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3). Moreover, because</head><p>Reserve-L uses a pure ViT backbone versus MERLOT's ViT-ResNet hybrid, it uses fewer FLOPs than MERLOT, while scoring 7% higher. Meanwhile, Reserve-B outperforms 'base' detector-based models, while using less than one-tenth their FLOPs.</p><p>In terms of parameter count, Reserve-B is comparable to prior work. On VCR, including the vision stack,</p><p>Reserve-B has 200M finetunable parameters and performs similarly to the 378M parameter UNITER-Large.</p><p>Reserve-L has 644M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Finetuning on TVQA</head><p>Next, we use TVQA <ref type="bibr" target="#b44">[75]</ref> to evaluate our model's capacity to transfer to multimodal video understanding tasks. In The plot suggests that if we pretrained longer, VCR performance might continue to increase, though a confounding factor might be the learning-rate schedule. With access to compute beyond our current capacity, future work would be well-suited to consider this and other pre-training modifications.</p><p>Here, we use FLOPs as our key efficiency metric, as they are a critical bottleneck in model scaling <ref type="bibr" target="#b35">[66,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b101">130]</ref>. On the other hand, we argue that parameter count can be misleading -for instance, many Transformer parameters can be tied together with minimal performance loss <ref type="bibr" target="#b41">[72]</ref>. TVQA, models are given a video, a question, and five answer choices. The scenes come from American TV shows, and depict characters interacting with each other through dialogue -which past work represents through subtitles.</p><p>Audio-Subtitle Finetuning. To evaluate how much audio can help for TVQA, we finetune Reserve jointly between the 'Subtitles' and 'Audio' settings. Like on VCR, we consider one sequence per candidate: each contains video frame features, the question, the answer candidate, and a MASK token (from where we pool a hidden representation). During training, each sequence is duplicated: we provide one sequence with subtitles from the video, and for the other, we use audio. This lets us train a single model, and then test how it will do given subtitles, given audio, or given both (by averaging the two softmax predictions).</p><p>Results. We show TVQA results in <ref type="table" target="#tab_6">Table 3</ref>. With subtitles and video frames alone, our Reserve-B outperforms all prior work by over 3%. Combining subtitle-only and audio-only predictions performs even better, improving over 4% versus the prior state-of-the-art, MERLOT (and in turn over other models). The same pattern holds (with additional performance gains) as model size increases:</p><p>Reserve-L improves over prior work by 7.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Finetuning on Kinetics-600 Activity Recognition</head><p>Next, we use Kinetics-600 <ref type="bibr" target="#b18">[19]</ref> to compare our model's (finetuned) activity understanding versus prior work, including many top-scoring models that do not integrate audio. The task is to classify a 10-second video clip as one of 600 categories. We finetune Reserve jointly over two settings: vision only, and vision+audio.</p><p>Results. We show Kinetics-600 results on the validation set, in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Zero-Shot Experiments</head><p>Next, we show that our model exhibits strong zero-shot performance for a variety of downstream tasks. Our zero-shot interface is enabled by our contrastive span objective. For QA tasks that require predicting an option from a label space of short phrases, we encode this label space as vectors, and predict the closest phrase to a MASKed input. We consider:</p><p>i. Situated Reasoning (STAR) <ref type="bibr" target="#b90">[119]</ref>. This task requires the model to reason over short situations in videos, covering four axes: interaction, sequence, prediction, and feasibility. The model is given a video, a templated question, and 4 answer choices. We convert templated questions into literal statements (which are more similar to YouTube dialogue); the label space is the set of four options. ii. Action Anticipation in Epic Kitchens <ref type="bibr" target="#b25">[26]</ref>. Here, the goal is to predict future actions given a video clip, which requires reasoning temporally over an actor's motivations and intentions. The dataset has a long tail of rare action combinations, making zero-shot inference challenging (since we do not assume access to this prior). As such, prior work [46, 38] trains on the provided in-domain training set. To adapt Reserve to this task, we provide it a single MASK token as text input, and use as our label space of all combinations of verbs and nouns in the vocabulary (e.g. 'cook apple, cook avocado', etc.). iii. LSMDC <ref type="bibr" target="#b51">[82,</ref><ref type="bibr" target="#b67">96]</ref>. Models are given a video clip, along with a video description (with a MASK to be filled in). We compare it with the vocabulary used in prior work <ref type="bibr" target="#b99">[128]</ref>. iv. MSR-VTT QA <ref type="bibr" target="#b91">[120]</ref>. This is an open-ended video QA task about what is literally happening in a web video. We use GPT3 <ref type="bibr" target="#b15">[16]</ref>, prompted with a dozen (unlabelled) questions, to reword the questions into statements with MASKs. This introduces some errors, but minimizes domain shift. We use a label space of the top 1k options.</p><p>For these tasks, we use N =8 video segments (dilating time when appropriate), and provide audio input when possible. Details and prompts are in Appendix D. We compare against both finetuned and zeroshot models, including running CLIP <ref type="bibr" target="#b63">[92]</ref> on all tasks. CLIP is a strong model for zero-shot classification, particularly when encyclopedic knowledge about images is helpful; our comparisons showcase where multimodal script knowledge helps. Results. <ref type="table" target="#tab_9">Table 5</ref> shows our model performs competitively:</p><p>i. On STAR, it obtains state-of-the-art results, with performance gain when audio is included. Interestingly, Reserve-B outperforms its larger variant; we hypothesize that this is due to limited prompt searching around question templates. We qualitatively observed that Reserve-L sometimes excludes topically correct options if they sound grammatically strange (to it). ii. On EPIC-Kitchens, our model obtains strong results at correctly anticipating the verb and noun -despite the heavy-tailed nature of both distributions. It is worse on getting both right ('action'), we suspect that this might be due to priors (motifs) between noun and verb <ref type="bibr" target="#b100">[129]</ref>. These are easy to learn given access to training data, but we exclude these as we consider the zero-shot task. iii. On LSMDC, our model obtains strong results at fillingin-the-blank, likewise despite a heavy (unseen) frequency bias. Notably, it outperforms CLIP significantly, with CLIP often preferring templates that use visually-relevant words, even if they don't make sense as a whole. For instance, given a clip of a mailman, CLIP chooses 'the mailman smiles off,' versus 'the mailman takes off.' iv. Finally, our model performs well on MSR-VTT QA, outperforming past work that directly rewords subtitled instructional videos into video QA instances <ref type="bibr" target="#b94">[123]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Qualitative Analysis: Why does audio help?</head><p>What can Reserve learn from both text and audio? Three validation set examples are shown in <ref type="figure">Figure 5</ref>. The model is given the displayed text and video frames, and must stretch in the calf press and push your hands into the floor for more stretch i know this time we're holding it with the right leg *popcorn popping* and forth every now and then *exhausted laugh* why <ref type="figure">Figure 5</ref>: Exploring MASKed audio self-supervision. Shown are example videos from our validation set, with predictions from Reserve-B. During pretraining, our model progressively learns to pick up on audio-specific clues. It seems to recognize physical dynamics of cooking popcorn, matching the first row to its MASKed audio. Likewise, it seems to use social reasoning to match the second row to its audio. Both of these clues are orthogonal to what the subtitles provide.</p><p>match the MASK to the correct missing text and audio span (out of 48k total in the batch). The plots show Reserve-B's probability of correctly identifying the correct audio or text span, as it progresses through 10 epochs of pretraining.</p><p>Audio's supervisory signal. In the first two rows of <ref type="figure">Figure 5</ref>, audio provides orthogonal supervision to text:</p><p>1. In the first row, the MASKed audio contains the sound of popcorn pops slowing. By the final epoch, Reserve-B selects this specific auditory cue with 60% probability, over others (including from adjacent segments, at different stages of popping). Here, sound provides signal for joint vision-text understanding of the situation, as evidenced by its greater match probability. 2. The second row contains only the text 'why,' with the audio providing greatly more information -a femalepresenting speaker (shown in the next frame) laughs, astonished that the child (in the frame afterwards) might want a better relationship with their parents. 3. In the third row, matching performance is similar between modalities, possibly as the yogi is narrating over a (muted) video recording, and not adding much information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role of text.</head><p>Text is still a crucial complement to audio, in terms of the supervision it provides. Consider the second row:</p><p>Reserve-B learns to match the audio almost perfectly (perhaps reasoning that the speaker is shown in the next frame, and is laughing). In later epochs, its text-match probability increases: knowing that a 'why' question is likely to be asked is a valid social inference to make about this (tense) situation.</p><p>Learning through multimodal reentry. Developmental psychologists have hypothesized that human children learn by reentry: learning connections between all senses as they interact with the world <ref type="bibr">[35,</ref><ref type="bibr" target="#b71">100]</ref>. Using a held-out modality (like audio) might support learning a better world representation (from e.g. vision and text), by forcing models to abstract away from raw perceptual input. Our work suggests that reentry has potential for machines as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion, Limitations, Broader Impact</head><p>We introduced Reserve, which learns jointly through sound, language, and vision, guided through a new pretraining objective. Our model performs well in both finetuned and zero-shot settings, yet it has limitations. Our model only learns from 40-second long videos; relies on ASR models for subtitles, and can only match (not generate) text and audio. Still, we foresee broad possible societal impact of this line of work. Video-pretrained models might someday assist low vision or d/Deaf users <ref type="bibr" target="#b45">[76,</ref><ref type="bibr">48]</ref>. Yet, the same technology can have impacts that we authors consider to be negative, including surveillance, or applications that hegemonize social biases. We discuss these further in Appendix A: key dimensions include respecting user privacy during dataset collection, exploring biases in YouTube data, dual use, and energy consumption. We discuss our plan to release our model and data for research use so others can critically study this approach to learning script knowledge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Broader Impact Statement</head><p>In this paper, we have presented a model for learning multimodal neural script knowledge, through incorporation of audio as a first-class citizen alongside text and video frames. We argue that academic study of this learning paradigm is important, in part because it relates to how we as humans understand the world. We as humans process situations by perceiving through multiple modalities and interpreting the result holistically.</p><p>At the same time, the work and methodology that we outlined risks dual use. Like other large machine learning systems pretrained on web data, our system may reproduce harmful social biases present in its training data. While a variety of past work has studied risks of language-only pretraining <ref type="bibr" target="#b98">[127,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">61]</ref>, the video-centric pretraining that we explore in our work might have different benefits and risks. We discuss these below, along with how we worked to mitigate them through our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Privacy.</head><p>A significant risk with training on data at YouTube scale is protecting user privacy. We took several proactive steps to ensure this, that in turn build off prior work and community norms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b53">84,</ref><ref type="bibr" target="#b99">128]</ref>: a. We release only the video IDs for download, following prior work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b53">84]</ref>. Thus, if a user deletes a video off of YouTube, it becomes removed from YT-Temporal-1B as well, giving content creators a right to opt out of all uses of their videos. b. Building off of past work <ref type="bibr" target="#b99">[128]</ref>, we directed our data collection towards public and monetized channels. These channels are identifiable insofar as they contain more subscribers, and more videos. They include companies that have official accounts, including journalism outlets like the New York Times and Vox. They also include individuals for whom making public YouTube videos is their full time job. In either case, our use videos in question for research purposes can be seen as fair use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy (%) Model</head><p>Voice Image+Voice Image</p><p>Reserve-L 10.8 9.6 10.7 CLIP ViT-B/16 <ref type="bibr" target="#b63">[92]</ref> 86.0 <ref type="table">Table 6</ref>: Zero-shot person (face/voice) recognition accuracy on VoxCeleb2 <ref type="bibr" target="#b57">[87]</ref> and VGGFace2 <ref type="bibr" target="#b16">[17]</ref>, using different modalities. While Reserve can perform person recognition from several modalities, its performance is much lower than the recognition-optimized CLIP model in the image-toname setting. We hypothesize that this is due to a similarity between this setting and CLIP's pretraining data -news articles often include celebrity images, paired with their names.</p><p>Framing of privacy. Privacy is a nuanced topic with many societally, culturally, and generationally-specific interpretations. We took inspiration from Marwick and Boyd <ref type="bibr" target="#b52">[83]</ref>'s framework of networked privacy, which posits that users posting public videos might encode private information -enough so that their intended viewership (friends, possibly) can catch the gist, but not enough so as to leak private details like phone numbers to the world.</p><p>Through the lens of networked privacy, we see key differences between studying videos on a moderated platform, versus NLP work that trains models from the open web (e.g.</p><p>[29, <ref type="bibr" target="#b64">93,</ref><ref type="bibr" target="#b15">16]</ref>). When YouTube users upload videos, they tend to understand details of its privacy policy, beyond consenting to it <ref type="bibr" target="#b34">[65]</ref>. Likewise, YouTubers typically upload their own videos <ref type="bibr" target="#b73">[102]</ref>; the platform deters users from re-posting other users' content. These factors differ from text on the open web. Today, 'data brokers' post private details (like phone numbers) to the web for profit <ref type="bibr" target="#b24">[25]</ref>; concerningly, a study on language models suggests that models are vulnerable at memorizing this private information <ref type="bibr" target="#b17">[18]</ref>.</p><p>It is worth examining our research through other framings of privacy as well. For example, internet platforms profit off of user data, whereas users do not share equally in these profits <ref type="bibr">[37]</ref>. For this, and for the other reasons mentioned, we aim to release our model only for research-based use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Empirical study: can Reserve identify individual celebrities?</head><p>Inspired by work studying language model memorization of private information <ref type="bibr" target="#b17">[18]</ref>, we wish to empirically probe Reserve's ability to recognize individuals. Our goal during model development was not to optimize for this ability. Instead, our goal was to study models for multimodal script knowledge (what people might be doing in a situation over time, and why) instead of long-tailed visual recognition (including who those individuals are). These goals might trade off -for instance, our training data only has individuals' names when they are mentioned in ASR subtitles, a pairing that might be significantly noisier than images and text on the open web.</p><p>We study this capacity on the VoxCeleb2 and VGGFace2 datasets <ref type="bibr" target="#b57">[87,</ref><ref type="bibr" target="#b16">17]</ref>, where we created a test set of 120 celebrities, with 100 samples of each. We study these datasets not to promote them, but to establish a conservative upper-bound for the capacity of a model to recognize non-celebrities. We hypothesize that if Reserve struggles to select the right celebrity out of 120 predefined options, it would struggle much more at identifying random people (where the set of candidate names is much greater). We test this hypothesis over three zero-shot settings:</p><p>1. Voice to name. Given an audio clip sampled for a celebrity, we encode it with our model's audio encoder. We provide our model's joint encoder the text 'the sound of MASK', followed by the encoded audio. A blank image is provided. We extract the representation on top of the MASK, and choose the most similar celebrity name. 2. Image+voice to name. Here, we adopt the same format as 'Audio to name,' except we additionally encode an image of the celebrity's face in question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Image to name.</head><p>Here, Reserve encodes an image of the celebrity in question, and we provide it with text 'A picture of MASK.' No audio is provided. Using our model's joint encoder, we select the closest encoded celebrity name, out of all options. We use this format to compare to a CLIP model, which was trained on web images with captions <ref type="bibr" target="#b63">[92]</ref>. For the CLIP comparison, we use it to encode each image, and for all considered celebrity names, the sentence 'A picture of ${name}'. We choose the closest encoded sentence to the encoded image.</p><p>We show our results in <ref type="table">Table 6</ref>. In all modes, our model is less than 11% accurate at recognizing celebrities. Curiously, the accuracy drops given both the image and the voice, suggesting that the way we fused a celebrity's image and voice together might be outside the model's training distribution. These results are significantly lower than CLIP's 86% accuracy at classifying a person from their image.</p><p>In <ref type="figure" target="#fig_1">Figure 6</ref>, we investigate more into which celebrities our model is best at recognizing. Only a few celebrities are reliably classified; these tend to be very famous celebrities like Oprah Winfrey and Justin Bieber. Several sports players are recognized well (including Lebron James and Roger Federer), which could imply that our model learned their identities from watching sports replays or commentary. Most other celebrities are hardly recognized, whereas CLIP does well across the board.</p><p>Results summary. Together, these results show that while models like CLIP focus on encyclopedic knowledge  Reserve-L versus CLIP ViT-B/32 in the same 'imagetext' setting. Our model reliably recognizes A-list celebrities like Oprah Winfrey, very famous musicians (Justin Bieber) and sports players (LeBron James). However, it struggles on every other celebrity, particularly compared with CLIP. This suggests that our model primarily learns semantic as opposed to recognition-level encyclopedic knowledge. that results in strong zero-shot person recongition accuracy,</p><p>Reserve is not as effective as other models in memorizing particular celebrities-and, thus, perhaps not as effective as memorizing particular non-celebrities. These results suggest that Reserve's objectives and data might make it less of a concern to release privacy-wise, versus models trained on web images with captions.</p><p>As the rest of the paper emphasizes however, Reserve performs well on tasks with temporal understanding and commonsense reasoning as the primary goal. On a broader level, these results suggest that it is possible to learn strong models about temporal reasoning without person-level memorization, though more work is needed.</p><p>A.2. Biases in (pre)training data.</p><p>The 'knowledge' that our model learns should be viewed as situated within YouTube <ref type="bibr" target="#b36">[67]</ref>, which has numerous biases (that we will discuss next). Past work has made similar observations for language model pretraining on the open web <ref type="bibr" target="#b6">[7]</ref>. One of the root causes of such bias is learning objectives that encourage memorization of surface level cooccurences, rather than truly causal factors [56, <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b88">117]</ref>. Though it is possible that in the very long term, a paradigm of grounded learning might resolve some of these issues, the objectives in this work still likely reify biases that exist in the YouTube data.</p><p>Platform biases. Unlike many other pretraining efforts, that scrape data from the open internet (e.g. <ref type="bibr" target="#b64">[93,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b63">92]</ref>) which directly leads to toxic biases (e.g. [42, 32, 11]); we trained our model on YouTube, which is a moderated platform <ref type="bibr" target="#b72">[101]</ref>. Though the content moderation might perhaps reduce overtly 'toxic' content, social media platforms like YouTube still contain harmful microagressions <ref type="bibr" target="#b14">[15]</ref>, and alt-lite to alt-right content <ref type="bibr" target="#b66">[95]</ref>. Additionally, it should be mentioned that the content moderation on YouTube disproportionately filters out minoritized voices <ref type="bibr">[44]</ref>. Thus, despite us not using any word-based 'blocklist,' our model's pretraining data is still biased <ref type="bibr">[32]</ref>. Even without videos being explicitly removed, the 'YouTube algorithm' incentivizes the production of certain types of content over others <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b73">102]</ref>; e.g. people's roles in YouTube videos tend to be highly gendered <ref type="bibr" target="#b56">[86]</ref>, which might bias situation understanding <ref type="bibr" target="#b104">[133]</ref>.</p><p>Bias amplification. In this work, we pretrained a model primarily on ASR text, which is itself produced by another model. The automatic captions in YouTube are known to suffer from gender bias <ref type="bibr" target="#b78">[107]</ref>, which our model (like neural models generally) might in turn amplify <ref type="bibr" target="#b104">[133]</ref>. The transcriptions on YouTube are also likely poor at handling important identity markers, like pronouns. Already, text-only models like BERT struggle with pronouns like they/them and zi/zir; our reliance on ASR text makes our corpus likely worse in this regard <ref type="bibr">[28]</ref>. While past work, namely MERLOT <ref type="bibr" target="#b99">[128]</ref>, 'cleaned' this ASR text -through another large language model -we opted not to do so for this work due to computational expense. Though in that work, the ASR-denoisification was found to boost performance in VCR, it seems unlikely that it would solve this core issue of model bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Dual use.</head><p>Learning connections between video, audio, and textthough an important area of study as we have argued -can be used for undesirable applications, beyond what we have outlined under 'biases. <ref type="bibr">'</ref> We outline and discuss a few below.</p><p>Generating fake content. A concern for pretrained models is that they can generate fake content, that could be used by 'bad' actors for their ends <ref type="bibr" target="#b98">[127]</ref>. It should be noted that our model cannot explicitly 'generate' text, audio, or vision in a direct sense. Nonetheless, however, it is possible that a finetuned or expanded version of this model could be used for that purpose -and that our model would be more helpful to such an actor versus them training their own (perhaps larger) model from scratch. Surveillance. Our model might contain representations that enable it to be used in surveillance applications. As we note in Appendix A.1.1, our model's low performance on person recognition suggests that it might perform poorly recognition-focused applications. Still, one possibility is that a neural script knowledge could 'summarize' surveillance videos in some form (like identifying an activity of interest), without identifying the person(s).</p><p>We suspect (but cannot definitively prove) that the reporting bias of the YouTube data that it was trained on might make it poor for such a surveillance-focused task <ref type="bibr">[50]</ref>. Namely, most surveillance videos are sparse in nature -finding an activity of interest is like finding a 'needle in a haystack' <ref type="bibr" target="#b61">[91]</ref>. Though, some surveillance videos are inevitably posted on YouTube and then captioned, these disproportionately contain interesting events (like somebody's car crashing into a house). It is not clear whether our system could be easily adapted to such a sparse problem; the amount of work required suggests that it might be out-of-scope at least for low-skill actors. On the other hand, this broad research agenda, and perhaps all of computer vision for that matter, might enable large actors to do just that <ref type="bibr" target="#b105">[134]</ref>; which might not be addressable through purely technical solutions <ref type="bibr">[52]</ref>.</p><p>Harmful outcomes if deployed. Beyond the biases that our system possesses, some applications of our system -if deployed in production -could cause harm, particularly to groups already harmed by AI systems. Of note, linking someone's voice with their appearance is not always a good thing <ref type="bibr" target="#b65">[94]</ref>. Likely some of the key features that our model learns -though we did not teach it this explicitly -involve recognizing gender, and this is harmful especially to transgender individuals [55].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Energy consumption.</head><p>Our model cost a lot amount of energy to pretrain <ref type="bibr" target="#b74">[103]</ref>; roughly 3 weeks of time on a TPU v3-512. The total carbon footprint of our work was a net 8.23 tons of CO 2 equivalent, which is roughly 4.5% of the emissions of a jet plane flying round-trip from San Francisco to New York.</p><p>At the same time, it is possible that our model could save energy overall, when shared with researchers who build off of our system. Indeed, Reserve-B uses less energy than MERLOT <ref type="bibr" target="#b99">[128]</ref> (due to a smaller vision backbone, and smaller image sizes), MERLOT in turn is more efficient than past work which used expensive detector-based backbones (e.g. <ref type="bibr" target="#b77">[106,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b102">131]</ref>), that are made more expensive because some of their computational primitives (like non-maximum suppression) are difficult to make efficient on-device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Synthesis.</head><p>With these risks in mind, we release our video IDs, as well as Reserve's checkpoints, exclusively for research use. We believe that at this point in time, we as a field lack full knowledge of the privacy, bias, and dual-use risks of videobased models -though, we hope that our analysis in this section provides a good starting point. For instance, while the objectives that we have studied were designed to promote learning general neural script knowledge above encyclopedic memorization, they have not yet been tested in all possible cases. By opening our models to the research community, we hope to promote fundamental work in uncovering both promising aspects of these systems, alongside examining their risks. We hope to contribute to these lines of research as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model implementation details</head><p>In this section, we discuss at a more in-depth, technical level, how we implement certain aspects of Reserve, and other details (like its runtime in FLOPs). We discuss our use of rotary position encodings (B.1), how we set the sequence lengths for the model <ref type="figure">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Rotary position encoding</head><p>We use a rotary position encoding to model the relative location of input sequences <ref type="bibr" target="#b75">[104,</ref><ref type="bibr" target="#b9">10]</ref>. We chose this primarily CO 2 Calculation. It is also important to consider the location where these TPUs are located, as the renewables portion at each datacenter is not equal <ref type="bibr" target="#b58">[88]</ref>. Our TPUs were in the 'europe-west4' region, which uses on average 60% carbon-free energy, and a Grid Carbon intensity of 0.410 kgCO 2 eq / kWh. A single TPU v3 processor (with 8 cores over 2 chips) has a power average of 283 W, so after performing the math from <ref type="bibr" target="#b58">[88]</ref>, our training cost 20,000 kWh. This gives us a net 8.23 tons of CO 2 equivalent. It should be mentioned that this figure only covers the electricity usage given the chips (and the datacenter), not the raw materials involved in making these chips (which is significant <ref type="bibr" target="#b82">[111]</ref>). because we did not want to use absolute (additive) position embeddings, which would have to be added to the inputs of each encoder, and possibly at multiple levels in the hierarchy (e.g. for the joint encoder, the video segment index t would be needed as well).</p><p>The rotary encoding uses no parameters, and instead uses a kernel trick to allow the model to recover relative distances between key and query elements in a Transformer's attention head. This can be seen as 'rotating' pairs of elements; we apply the rotation to only the first half of each 64-dimensional head, and the second half is kept as is.</p><p>Multidimensional coordinates. We treat each token as having a 4-dimensional position of (h, w, , t), corresponding to the h, w coordinates in the image, the position in the text-sequence, and the segment index t. If a dimension is irrelevant to a modality (like h, w for text), we set it to 0. Thus, for our various encoders, we use the following coordinate schemes: As part of our implementation, we normalize the rotary coordinates. h, w are scaled to be in the range [?1/2, 1/2], such that text is implicitly 'in the center' of the image. Likewise, and t are scaled to be in the range of [0, 1]. The positions are used to compute relative distances, by using a kernel trick to rotate coordinates in the keys and values of each d h -sized Transformer attention head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Sequence lengths</head><p>We briefly remark on the sequence lengths used by parts of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a. Video Frame Encoder (ViT): Most YouTube videos are</head><p>widescreen (16x9). We thus used a widescreen resolution for our video frame encoder. It takes in patches of size 16x16, and we used a layout of 12 patches (in height) by 20 patches (in width). This corresponds to 192x320. Among other factors that are important are ensuring that TPUs do not execessively pad the sequence length <ref type="bibr" target="#b101">[130]</ref>. The sequence length is 241 in this case, as there is a CLS token, and it gets padded to 256. Attention pooling. As we note in the main text, after-wards we apply attention pooling in a 2x2 grid (ignoring the CLS token here). Similar to Transformer-style query,key,value attention <ref type="bibr" target="#b81">[110]</ref>, the query is the average of the vectors in the 2x2 grid; the keys and values are learned projections of the vectors. This gives us a H/32 by W/32 grid for the joint encoder (6 x 10). b. Audio Encoder. Our model independently encodes each 1.6 second of audio (a segment has three such 'subsegments'). We do this through spectrograms. Each window involves 1536 samples at a sample rate of 22500 Hz, and there are 588 samples 'hops' between windows. We chose these hyperparameters largely around efficiency. We found that the Discrete Fourier Transform is fastest if the window size is close to a multiple of 2. We used a small number of mel spectrogram bins (64) because we found that at that threshold, we could reconstruct the original sequence at an acceptable level using the Griffin-Lim algorithm, [53] which itself might be a lower bound on quality as neural methods trained for this purpose have been shown to do better <ref type="bibr" target="#b86">[115]</ref>.</p><p>In our implementation, we compute the spectrogram for an entire video segment (5 seconds) at once; this is of size 64 mel bins by 192 windows. During pretraining, we perform what is effectively a 'random crop' over the spectrogram: we extract three sequential 64x60 subspectrograms, for each audio subsegment. We constrain them to not overlap, which means that 12 (random) windows are held out. We note that our Audio Encoder AST is quite different from the one proposed by <ref type="bibr">[47]</ref>. Though it operates over spectrograms, we opted for a linear '1-dimensional' layout rather than a two-dimensional (image-like) one. We also did not pretrain our audio encoder on any supervised data (they used ImageNet and found, perhaps surprisingly, that it helped initialize the model). We used a patch size of 64 mel bins by 2 windows; the resulting (1D) sequence is of size 30. After adding a CLS token, the result is a sequence of length 31.</p><p>As we note in the main text, we apply attention pooling afterwards (for all elements except the CLS token), pooling by a factor of five to resize the length-30 sequence to a length of 6 'audio tokens.' c.  To better adapt our model to downstream tasks -particularly single-image tasks like VCR <ref type="bibr" target="#b97">[126]</ref>, where past work tends to use a resolution much higher than 192x320, after pretraining, we performed FixRes pretraining (for one epoch on Reserve-B, and one half epoch on Reserve-L <ref type="bibr" target="#b79">[108]</ref>. Here, we trained the model on larger images -simultaneously on 288x512 widescreen images (18 patches by 32 patches), and on 384x384 square images (24 patches on each side). The joint encoder, correspondingly, uses a sequence length of 1312.</p><p>During 10 epochs of pretraining, we used a cosine decay of the learning rate down to 0.02 its maximum. During FixRes pretraining afterwards, we warmed up the learning rate to 0.02x its peak, over the first 1/5th of an epoch, and afterwards used a cosine schedule to anneal it towards 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Efficiency metrics of our model</head><p>In <ref type="table" target="#tab_13">Table 7</ref>, we report efficiency metrics of Reserve, versus others. We calculate these metrics in the context of scoring a single VCR question and answer candidate. This requires encoding one image, and using 128 tokens for each question and answer combined (for all models). We compare against a UNITER <ref type="bibr" target="#b20">[21]</ref>, which is a representative Visual-BERT style model, along with MERLOT <ref type="bibr" target="#b99">[128]</ref>. Our models are far more efficient in terms of FLOPs, with Reserve-L being roughly on par with MERLOT, yet outperforming it by 6% in terms of VCR accuracy. We discuss key differences below: a. UNITER. We note that UNITER, like other VisualBERT models, uses a supervised object detection backbone <ref type="bibr" target="#b4">[5]</ref>.</p><p>We had intended to do a full epoch for Reserve-L, but our job got preempted, and the loss seemed to have already converged. This processes images using a ResNet 101 model <ref type="bibr" target="#b26">[57]</ref>, at a resolution of 600x800; the final ResNet 'C4' block is applied densely over the entire image to obtain objectdetection potentials everywhere in the image. Both factors greatly increase the FLOPs count. When computing UNITER's FLOPs count, we exclude operations like non-max suppression, which is an operation that is difficult to implement (and thus whose FLOP count might vary significantly depending on implementation). Our FLOPs count is thus a lower-bound. 36 detection regions are extracted, which is why the 'joint encoder' for UNITER is smaller than the equivalents for MERLOT and Reserve. b. MERLOT. This model has two key differences versus our Reserve. First, it uses a larger image resolution for VCR: 384x704, versus our 288x512. Second, it uses a hybrid ViT-ResNet50 backbone for encoding images. The backbone here is lighter weight than the object detection backbone of UNITER (in particular, the final 'C4' block is removed), and thus, as shown in <ref type="table" target="#tab_13">Table 7</ref>, though it uses more FLOPs than does our Reserve-L, it uses far fewer FLOPs than UNITER.</p><p>We choose flops as our primary comparison metric as past work shows that it is one of the key factors in model scaling <ref type="bibr" target="#b35">[66,</ref><ref type="bibr">34]</ref>. Parameters are arguably more fungible. For instance, in text-only representation learning, ALBERT <ref type="bibr" target="#b41">[72]</ref> demonstrates that it is possible to tie parameters together at all layers of a BERT-like transformer, reducing parameters by an order of magnitude (while not modifying compute), with a minimal performance drop. We did not do this for this work, as we wanted to use a more 'vanilla' Transformer architecture; however, it suggests that representation learning models with hundreds of millions of parameters might be FLOPs bound as opposed to parameter-bound.</p><p>Nonetheless, UNITER-Base has 154 million parameters, though some are frozen (86 million from their Transformer, 23 million from the word embedding layer, and then 44 million from their object detector <ref type="bibr" target="#b4">[5]</ref>). UNITER-Large has 378 million parameters (303 from their Transformer, 31 million from word embeddings, and 44 million from the same object detector. Meanwhile, MERLOT has 223M parameters. Versus our Reserve-B, 14 million extra parameters are due to a larger vocabulary, and 10 million parameters are due to a ResNet50 encoder -but these parameters have a disproportionate impact in FLOPs count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Full model hyperparameters</head><p>In <ref type="table" target="#tab_15">Table 8</ref>, we present full hyperparameters for our model. Among other details, we used AdamW as our optimizer, with ? 2 = 0.98 and = 1e ? 6. We increased the learning rate linearly to its peak value <ref type="table" target="#tab_7">(4e-4</ref>    <ref type="table">Table 9</ref>: Hyperparameters for finetuning on downstream tasks. Note that for Kinetics-600, we tried to mimic VATT's setup <ref type="bibr" target="#b1">[2]</ref>, including adopting their training-epoch regime and their data augmentation strategies. Our data augmentation strategies were much simpler for VCR and TVQA (random cropping, and for VCR sometimes horizontally flipping the image); we suspect that our VCR/TVQA results could be made higher if data augmentation was further explored.</p><p>number of warmup steps is lower than many other pretraining work; we note that all of our contrastive objectives involve learning a ? parameter, which functions as a secondary 'warmup. <ref type="bibr">'</ref> We did not use gradient clipping. We trained and evaluated in 16-bit bfloat16 precision wherever we could -casting all gradients to that precision as well, and saving the AdamW running mean and variance to be 16-bit as well. A few times during pretraining Reserve-L, we found that some values in gradients would be NaN. We addressed this by always setting NaN values to be 0. This seemed to address the symptoms of training instability -though sometimes the training loss would spike to roughly around the same loss as random initialization, it always converged back to slightly better than it was before the spike. We are not currently sure why this happens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Speed improvements during pretraining</head><p>We made several high-level algorithmic and engineering implementations to our implementation, which made pretraining run faster, and that we discuss here.</p><p>Duplicated video copies. As mentioned in the main text, we create two copies per each video -allowing us to learn separately how to handle audio as an input as well as how to learn from audio. We chose this in part because copying a video does not increase the total compute requried by a factor of two. Instead:</p><p>1. We use the image and audio encoders, to encode the underlying video frames and audio clips only once (for the two video copies), and then duplicate the encodings; this is far more efficient than encoding them both separately from scratch.</p><p>2. For the two video copies, we sampled two disjoint sets of masks (for which audio and text subsegments are replaced with MASK) at a 25% rate. This increases the pool of negative samples for contrastive learning, again increasing training efficiency.</p><p>Reducing memory usage. The memory usage of our Transformer implementation scales quadratically with sequence length, which could pose a problem since we operate on sequences of videos. We split the video into two groups of 8 segments, and encode each group separately by the joint encoder.</p><p>Vectorization. We vectorize all joint transformer inputs together into a single call. During this vectorization, we also encode the transcript (for the transcript-frame matching objective).</p><p>We note that this vectorization is incompatible with the Mask LM variant proposed by MERLOT <ref type="bibr" target="#b99">[128]</ref>. In this variant, which the authors called 'attention masking,' two transformer calls must happen sequentially -first, a language only encoder must encode the inputs and mark down (what is presumably) visually-grounded tokens; second, these tokens are masked for the joint encoder. We found that such an objective was unnecessary when pretraining under our contrastive span approach, which in turn enabled more efficient pretraining.</p><p>We discuss the exact pretraining data formatting technique that we used in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pretraining Data Formatting: alignment and masking</head><p>In this section, we discuss how we turn a video V into a (masked) list of segments {s t } for pretraining.</p><p>Recall that each segment contains a video frame v t , ASR tokens w t , and audio a t . We generate the list of segments by iterating through the video with a 5-second sliding window.</p><p>Audio and text subsegments for masking. We want audio to be used in part as a target for contrastive prediction. However, during early exploration we found that 5 seconds of audio could correspond to many BPE tokens; roughly Sometimes there are long 'pauses' in videos where nothing gets said. When this happens -if two segments in a row have fewer than 8 BPE tokens -we merge them 90% of the time, in effect 'fast-forwarding' the audio and still extracting a frame from the middle. We do this at most twice, so the total length is at most 15 seconds here (in effect, a 'playback' rate of 1x, 2x, or 3x). In roughly 90% of cases, the segments are 5 seconds of length. 15 on average. We use past work in language modeling as a guide <ref type="bibr" target="#b33">[64,</ref><ref type="bibr" target="#b64">93]</ref> and wanted an average span length of around 5 tokens. To get this, we split each audio segment into three equal subsegments, each with a duration of 1.66 seconds. We can then perform masked language modeling at the aligned subsegment level, where we mask out the text corresponding to an audio subsegment, and have the model (contrastively) predict the masked-out span of text, as well as the corresponding span of audio. We use a masking rate of 25%, which means that a quarter of the subsegments will be corrupted and replaced by a MASK token.</p><p>In theory, splitting the videos into (masked) segments ought to be straightforward. However, the key challenge that we ran into is that the YouTube caption timing information is unreliable. Problems might arise when we perform pretraining with both audio and text, on misaligned data. Suppose the model is given audio in segment s t?1 that ends with somebody saying the word 'pasta.' If the alignment between audio and text is off, the model might be able to cheat the desired task by simply predicting the word 'pasta' for segment s t -thereby turning the challenging maskedprediction task into an easier speech recognition task; we discuss this in more detail in Appendix C.1.</p><p>One way of addressing the timing issue would be to run our own ASR model over all videos, but we chose not to do this due to computational expense. Instead, we adopted two complementary strategies. First, we trained a lighweight regressor to refine the timing information (C.2); second, we mask audio and text conservatively, to minimize alignment errors (C.3). Finally, we discuss how we combine everything efficiently (in a vectorized way) in C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. YouTube Caption Timings</head><p>YouTube provides automatically generated captions for accessibility purposes, which include timing information on each word. In the subtitle encoding that we used (vtt), each word w contains a single timestamp t which corresponds to when the word should flash on-screen. The timings are mostly accurate, but we found two key issues: a. First, they show up on average roughly 0.1 seconds before each word is spoken, which we suspect might be for usability purposes (perhaps so that while the viewer is reading the caption, they hear the word). b. Second, with a single timestamp t for each word, it is difficult to infer about pauses. For each word w, we can use its timestamp t, and the timestamps of adjacent words, to loosely infer an interval [t s , t e ] around when the word is said. However, the interval is not tight. We can only infer that the word is being actively spoken for some subinterval [t s , t e ] such that t s ? t s ? t e ? t e .</p><p>Note that this is compounded with the first problem, the ground truth interval [ts, te] might not be fully contained in the provided interval [t s , t e ] This can lead to high absolute error (in terms of a difference between timesteps), when pauses occur. For example, suppose a speaker says a word, and then pauses. The interval given by the subtitles, [t s , t e ], might be rather large (possibly a few seconds), even though the actual word was spoken for a fraction of that time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Refining timing information</head><p>We trained a simple multilayer perceptron regressor to correct the timing information of YouTube transcripts. For data, we used 2000 videos with transcripts from YT-Temporal-180M, and also used Google Cloud's (highest quality, paid) ASR service to transcribe them. After aligning the words for these transcripts, this gave us tuples of the YouTube ASR word w, its provided interval [t s , t e ], and the 'ground truth' interval [t s , t e ]. Our modeling objective was then to predict the desired offsets with respect to the provided interval: ? s = t s ? t s and ? e = t e ? t e . We took a feature based approach.</p><p>For each input (w, t s , t e ), we used as features:</p><p>i. the length of w in characters, ii. the length of w in BPE tokens, iii. whether w is uppercase or not, iv. the number of vowels in w, v. the number of punctuation characters in w, vi. the value of t e ? t s .</p><p>We provided these features as input to the model, as well as the corresponding features for the next word, and the previous word. We z-normalized all features and used a two-layer multilayer perceptron, with a hidden size of 32 and RELU activations. We used a tanh activation at the end to bound the regression. The final predictions for ? s (analogously for ? e ) were then given by the following equation:</p><formula xml:id="formula_4">? s = c tanh(w ? h + b 1 ) + b 2<label>(3)</label></formula><p>where h is the hidden state, and with learnable parameters c, w, b 1 , and b 2 . The learned bounds mean that, no matter what the input, the model will never predict an offset of above c + b 2 (of which it learned for both parameters c ? 0.2 and b 2 ? 0.11, so the offsets can never be above 0.3 seconds). We trained our lightweight regression model using an L 1 loss, and used it to correct the timing on all of the transcripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Handling worst-case scenarios in masking, when alignment isn't perfect</head><p>The regressor that we described reduces the average timing error of a transcript, as a preprocessing step, but it is due to 'captions being shown before audio', the error here is typically small though (0.1 seconds).</p><p>When matching YouTube ASR to Google Cloud's ASR, we skipped words without an 'exact-match' alignment, as well as words that were over 0.25 seconds apart (i.e., where either ?s &gt; 0. <ref type="bibr" target="#b24">25</ref>  the best thing to do is really focus ? ? ? ? <ref type="figure">Figure 7</ref>: An overview of our masking strategy for dealing with sequences of video frames, ASR, and audio. We have noisy timing information for each word, so we can align the ASR text with audio spans of 1.6 seconds each, using three sub-segments of audio and text for each video frame. However, there exist alignment errors between the ASR and audio sub-segmentscertain words (and sub-words) have phonemes that are are in the wrong segment (like 'back' in w 1,1 is only partially said in the first sub-segment; the 'k' sound is said in the second. When audio is only a target, we address these by 'donating' tokens to predicted spans. When audio is only provided as input, we address this by sandwiching 'mask' tokens between text input (so alignment does not 'bleed' over). not perfect. Thankfully, however, we find that most of the remaining alignment errors are single words that are slightly misaligned. For instance, for three words w t , w t+1 , w t+2 , the audio corresponding to the time interval around w t might contain sound from w t+1 being spoken, but rarely w t+2 . We suspect this is primarily due to the difficulty inferring pauses: by definition, no other word can be said in a pause, so the errors are local.</p><p>We present a high level approach for masking audio and text, that in turn addresses these alignment issues (making it difficult for models to cheat). A diagram is in <ref type="figure">Figure 7</ref>.</p><p>Recall that in our framework, we only either go from 'vision and text ? text and audio' (VT?TA), or, 'vision, text, and audio ? text' (VTA?T). One of the reasons we did this is to avoid allowing a model to cheat by performing speaker identification (or even 'microphone identification'), which might be feasible if audio was given to the joint model as input. We can handle the two cases separately: a. Vision and text ? text and audio (VT?TA). Here, the text as input (to the joint encoder) might overlap with the audio we are trying to predict. Our solution here is thus to donate nearby tokens from the predicted span, to the input. Let the span that we are trying to predict (and that we will 'mask out') have a start time of t s and an ending time of t e . If the final token in the previous text span, if any, has a timestamp of greater than t s ?0.125, we move it to the predicted span; likewise, if the first token in the next text span has a timestamp of less than t e +0.125, we move it to the predicted span as well. b. Vision, text, and audio ? text (VTA?T). In this prediction task, models are given information from all modalities as input, and must predict masked-out text spans. Note that models are only given a single 'speech' modality -either text, or audio -at each timestep. What this means is that we can carefully choose which input subsegments to turn into 'audio subsegments,' and which to turn into 'text subsegments.' Our strategy is, given a masked out subsegment, to turn 80% of adjacent subsegments into 'text subsegments. <ref type="bibr">'</ref> We give an illustration of this in <ref type="figure">Figure 7</ref>, part 2. Here the word 'focus' is part of a 4,1 but also w 3,3 ). This might make w 3,3 ) overly easy to predict, if we gave the model a 4,1 as input. Our solution is thus to give the model text from w 3,2 ) and from w 4,1 ) as input; we are guaranteed that there is no misalignment overlap here between input and prediction spans. All of the other subsegments (not adjacent to one of the 25% that we mask out) will be provided as audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Putting it all together, along with web text</head><p>Finally, we discuss how we combine the various masking approaches into the prediction tasks outlined in the main text.</p><p>Each video has N = 16 video segments, and three subsegments of audio or text spans per segment. We consider two sub-problems for this video sequence:</p><p>i. in VT?TA, vision and text are provided as input, and the model must predict masked-out text and audio. These are done on top of separately-encoded MASK tokens and MASKAUDIO tokens, to enable the model to learn different predictions for each modality over two separate transformer 'columns.' ii. In VTA?T, vision, text and audio are provided as input, and models must predict masked-out text. Here, we use the term 'predict' as a shorthand for our contrastive objectivein which a model must match a context (a jointly-encoded MASK) to the exact missing span in question, where many negative contexts and spans are provided.</p><p>We use a masking rate of 25% for audio and text subsegments, and there are 3 subsegments per segment. This means that a single video instance gives us 48 ? 0.25=12 masked-out spans of text, for each of VT?TA and VTA?T, so 24 in total (as we use disjoint masked-out subsegments). Likewise, it gives us 12 masked-out spans of audio. If we scaled these to the whole batch of 1024 videos, we would have 12k audio span options and 24k text span options. This might suffice, but scaling up the pool of candidates boosts performance in a contrastive setting, as suggested from prior work (e.g. <ref type="bibr" target="#b63">[92]</ref>), and as our ablations <ref type="table" target="#tab_3">(Table 1</ref>) support as well. Thus, we do the following: a. Text candidates. We scale up the text candidates by simultaneously training the model on web text, from The Pile <ref type="bibr">[40]</ref>. The joint encoder -which can handle pooled video, pooled audio, and BPE-encoded text -is simultaneously given a sequence of web text, for each video that we have. By performing the span-contrastive objective with this piece of web text as well, we can not only teach the model about written (as opposed to spoken) language, but we can scale up the set of text candidates as well. Let each web-text sequence be of length L. We first divide it into fake regions that 'look like' the text subsegments in length. We do this by calculating the empirical length distribution of the text subsegments, and then using this (categorical) distribution to sample a sequence of sub-segment lengths 1 , . . . , K . We clip the sampled sequence, such that i i = L. Next, we mask the fake subsegments. During pretraining, we use text sequences of length L = 800, but a model sequence length of only 640. Because we are masking spans and not individual tokens, the text sequences 'shrink' when we mask them. We extract exactly 38 masked-out spans, which corresponds to around 25% of total text. Finally, we combine the target spans that we took from the webtext sequence, with the target spans from the video. We note that sometimes -especially in a videotext spans might be empty. Not every 1.6 second slice of a video has someone speaking. We thus try to not use these empty spans in our contrastive objective. For each video (which is paired with text for implementation reasons) we select the 'best' 48 text spans out of the (38+24) options -penalizing empty spans, and choosing spans from videos 4x as often. These 'best 48' text spans, as well as the pooled contexts that they were paired with, will be used in the contrastive objective. Aggregating over the entire batch of 1024 videos (and 1024 web text sequences), this gives us 49152 text spans as candidates, for the all-pairs symmetric softmax between text spans and contexts. b. Audio candidates. For each video, we note that we have exactly 12 pooled MASKAUDIO tokens, where the model is trying to predict the corresponding audio span. One option would be be to just use those 12 corresponding audio spans as the targets, aggregate these over the batch, and do a symmetric-cross-entropy loss. However, we can do even better for free. Note that for the VTA?T direction, we might have to encode many of the audio spans anyways, using the lower level audio encoder (which simultaneously extracts a CLS representation and a sequence-level pooled representation). To simplify implementation, we encode all 48 audio spans per video. We can use these audio spans as candidates. Thus, we do the following when computing the loss over audio prediction. We aggregate all 12288 contexts from the MASKAUDIO tokens in the batch, and we aggregate all 49152 candidate audio spans. We perform an all-pairs dot product between these two sets, and use it to compute</p><p>The empirical distribution for each length, in order from a length of 1 to 15, is [0.03, 0.05, 0.08, 0.11, 0.13, 0.13, 0.12, 0.10, 0.07, 0.05, 0.03, 0.02, 0.01, 0.006, 0.003]. a symmetric cross-entropy loss over both directions. We did not encounter any trouble using the same temperature for both directions (even though for one direction, there are 12288 options, and for the other, there are 49152).</p><p>The combination of these design decisions provide more 'hard negatives' for the model during training. We also found that they worked well to reduce wasted computation on a TPU. For each video, the joint transformer uses one L = 640 length sequence for transcript-frame matching, two length-L sequences for the VT?TA direction (as we break it up into two groups of 8 frames each), two length L sequences for the VTA?T direction, and finally one length-L sequence of text. These sequences can all be vectorized together, and the total batch size is 6? the number of videos. This is helpful because using an even-numbered batch size reduces wasted computation on a TPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Downstream Task Implementation Details</head><p>In this section, we present information for how we adapted Reserve on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Setup for finetuned tasks</head><p>For adapting Reserve in a finetuned setting, we take the following approach. We use a linear warmup of the learning rate over the first half of the first epoch, with a linear decay thereafter to 0. To find the learning rate, we did a small grid search generally centered around 1e-5. Our full hyperparameters are shown in <ref type="table" target="#tab_15">Table 8</ref>.</p><p>When finetuning (and pretraining), we did not use any dropout to make implementation simpler. Instead, as a way to apply regularization, we used the same L 2 penalty as in pretraining (a weight decay of 0.1), but with respect to the pretrained weights. This idea was used in <ref type="bibr" target="#b89">[118]</ref> among other works, and although it often tends to underperform dropout <ref type="bibr" target="#b42">[73]</ref>, it is simple to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.1 Visual Commonsense Reasoning</head><p>As mentioned in the main text, VCR considers two subtasks: Q?A, where models are given a question and must choose the right answer given four options; and QA?R, where models are given a question (and the right answer) and must select the right rationale.</p><p>In our setup for this task, we treat it as a four-way classification problem, extracting a single score from each answer or rationale candidate. An example Q?A is:</p><p>What is going to happen next? answer: person2 is going to say how cute person4's children are. MASK</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An example QA?R:</head><p>What is going to happen next? person2 is going to say how cute person4's children are. rationale: It looks like person4 is showing the photo to person2, and person2 will want to be polite. MASK We extract representations from the MASK position (which are of dimension d h ), score them with a newly-initialized d h ?1 weight matrix, and optimize scores with softmax-cross entropy.</p><p>Both VCR subtasks use only a single image. We also followed past work in 'drawing on' the provided detection tags to the image <ref type="bibr" target="#b99">[128]</ref>. These are unambiguous references to entities that are then referred to in the question, answer, and rationale. For example, text might reference a 'person1', which corresponds to an image region. When drawing on these detection tags, we do so in a deterministic way -for example, 'person1' always gets the same box color. We determine the box color by hashing the object's ID (in this case, 'person1') and using that to determine the hue. The model learns the connection between boxes with different hues, and the names, during finetuning.</p><p>We randomly flip images left or right, so long as there is no instance of the word 'left' or 'right' in the question, answer, or rationale candidates. We did no other data augmentation (other than randomly resizing images to between 100% to 110% of the network's size).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.2 TVQA</head><p>TVQA provides models with a video, a question, and five answer candidates; we represent this as five distinct sequences for the model to score (one per candidate). The version of TVQA that we used also gives models annotations for the time region in the video that is being referenced. It is not clear that only using this region would provide enough context to be able to understand what is going on -enough to answer correctly. Thus, for each question, we extract 35 seconds of video around the provided time region. We then provided the model with two numbers corresponding to the time region, relative to the cropped time interval. For example, if the provided timestamp annotation is [t 0 , t 1 ], we use the following region:</p><formula xml:id="formula_5">t c = (t 0 + t 1 ) 2 (4) t s = t c ? 17.5 (5) t e = t c + 17.5<label>(6)</label></formula><p>The location of [t 0 , t 1 ] in relative coordinates is then:</p><formula xml:id="formula_6">t r 0 = t 0 ? t s t e ? t s<label>(7)</label></formula><formula xml:id="formula_7">t r 1 = t 1 ? t s t e ? t s<label>(8)</label></formula><p>We provide models with t r 0 and t r 1 , multiplied by 100 and casted to an integer. Thus, an example TVQA instance might look like:</p><p>This text input corresponds to the first 'segment' of a video; to it we append subtitles (or audio representations) from seven segments from the provided TVQA video (with accompanying frames).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.3 Kinetics-600</head><p>We evaluate Reserve on Activity Recognition over the Kinetics-600 dataset <ref type="bibr" target="#b18">[19]</ref>. Here, the model has to classify a short 10-second video clip into a mutually-exclusive set of 600 categories, like 'assembling bicycle' or 'alligator wrestling'. We consider performing this task in a finetuned setting, so as to better compare to prior work. We format each example by extracting 4 video frames from the clip (sampled uniformly), and extracting 6 audio subsegments (totalling 10 seconds of audio). The model processes these inputs along with a MASK token, where we extract a vector representation. We initialize the 600-way classification layer with the activations of our Text Span Encoder, over the names of the 600 categories.</p><p>We finetune the model jointly over two settings: a setting where audio is provided, and a setting where no audio is provided, to allow us to investigate both settings. We tried to closely follow VATT's finetuning approach <ref type="bibr" target="#b1">[2]</ref>, including their exact data augmentation settings. We used a batch size of 64 videos (that we process simultaneously 'with audio' and 'without audio'). We used the same image augmentation code as VATT <ref type="bibr" target="#b1">[2]</ref>, and finetuned for 15 epochs. We used a learning rate of 5e-6 for Reserve-L and 1e-5 for Reserve-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Setup and prompting for Zero-shot tasks</head><p>Here, we discuss how we set up various tasks for Reserve in a fully zero-shot setting. In addition to evaluating Reserve, we also evaluate CLIP <ref type="bibr" target="#b63">[92]</ref> in the same zero-shot setting. CLIP is not pretrained on videos, and it cannot jointly encode text. For each task, we construct CLIP's label space by taking our prompt and substituting in each possible answer option. We average together the logits over all frames, and take a softmax, giving us a distribution over the task-specific label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.1 Zero-shot Action Anticipation on EPIC-Kitchens</head><p>We study the task of action anticipation from the EPIC-Kitchens dataset <ref type="bibr" target="#b25">[26]</ref>, a large egocentric video dataset with 700 unscripted and untrimmed videos of cooking activities. In action anticipation, a model must predict a future action that comes ? a seconds after a given video clip. The observed segments are of arbitrary length; we follow prior work <ref type="bibr" target="#b25">[26]</ref> and set ? a = 1.</p><p>The model tries to choose the correct noun and verb that happens next, given a list of predefined options for each. We report results on each category using the class-mean top-5 recall.  <ref type="table" target="#tab_3">Table 10</ref>:</p><p>Reserve gets competitive results on EPIC Kitchen Action Anticipation challenge with zero-shot, over methods from prior work.</p><p>Zero-shot inference approach. We directly evaluate the pretrained Reserve on action anticipation to verify the knowledge learned during pre-training. All prior work reported on the official leaderboard use supervision from the in-domain training set, which we do not use at all <ref type="bibr">[46,</ref><ref type="bibr">38]</ref>.</p><p>For each action segment, we sample at most N = 8 image frames and their associated audio, with fixed time interval t = 2.0 preceding it and ending ? a seconds before the start of the action. We append a MASK token as the sole text input (at the last frame, after audio is optionally included). We create short phrases out of all candidate nouns and verbs, and use that as our label space to simultaneously predict them both. We compute the score for each verb and noun independently by averaging their scores, over all labels for which they appear.</p><p>Results. We show the full zero-shot action anticipation results in <ref type="table" target="#tab_3">Table 10</ref>. We also show our results on the test set here for our best performing model ( Reserve-L, with audio provided). It gets competitive results on verb and noun prediction -with only 1.6% and 3.3% lower compared to the challenge winner method AVT+ <ref type="bibr">[46]</ref>, which is fully supervised and use additional object-level annotations. On Unseen Kitchen and Tail Classes, our model outperforms AVT+ on noun and verb. Overall, audio significantly improves the results -Reserve-L (+audio) outperforms</p><p>Reserve-L with an average 3.0%, which suggests that it is useful for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.2 Zero-shot Situated Reasoning</head><p>Next, we evaluate on situated reasoning (STAR) <ref type="bibr" target="#b90">[119]</ref> which requires the model to capture the knowledge from surrounding situations and perform reasoning accordingly. STAR dataset includes four types of questions, including interaction, sequence, prediction, and feasibility. A model is given a video clip, a templated question, and 4 answer choices.</p><p>Zero-shot inference approach. For each video clip, we sample N = 8 image frames uniformly from the video, we also optionally include the video's sound.</p><p>To reduce domain shift between YouTube data -where people don't typically ask visual questions, and where ASR typically does not insert question marks -we convert the question-answer pair into a statement. We did so using the question-answer templates provided by the author, with the answer replaced by a MASK. For example, "Q: What did the person do with the bottle? -A: Put down." will be converted to "The person MASK the bottle.".</p><p>We put the converted statement into the first frame and use the four candidate answers as a unique label space (that differs from example to example). Like with EPIC-Kitchens, we also evaluate how much audio can help by masking the audio inputs.</p><p>Results. We show our zero-shot STAR results in Table 5 in the main text. Our base model outperforms all supervised prior work by 3.7%. The model with audio performs better, with average 1.1% improvement. Interestingly, Reserve-L is worse than Reserve-B, we suspect the reason is Reserve-L is sensitive to grammar details. Given the previous example, we note that while 'Put down' is a valid answer that might make sense both semantically and syntactically, a different answer 'pick up' might be flagged by some English speakers as being ungrammatical: the instantiated template would then be 'the person pick up the bottle.' We noticed instances of the larger model paying greater attention to these syntax-level details, even though they were not the focus of the task. It does suggest, however, that additional prompting (or label space augmentation) could resolve these issues and increase performance even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.3 Zero-shot LSMDC</head><p>We evaluate our model on Movie Fill-in-the-Blank <ref type="bibr" target="#b67">[96,</ref><ref type="bibr" target="#b51">82]</ref> task, which based on descriptive audio description for the visually impaired. Given a movie clip and an aligned description with a blank in it, the task is to fill in the blank with the correct word. Following <ref type="bibr" target="#b51">[82]</ref>, we report prediction accuracy in test set of 30,354 examples from 10K movie clips.</p><p>Zero-shot Inference approach. We sample N = 8 video segments uniformly over the movie clip, and extract the audio and middle frame of each segment. We replace the 'blank' token in each description with a MASK token, and provide it (as text-based input) to the model at its final segment. For the other segments, we optionally provide the model with audio; for all segments, we provide the associated image frame. We use the vocabulary set in the LSMDC dataset as our label space (for what the 'missing word' might be).</p><p>Results. Our results are shown in <ref type="table" target="#tab_9">Table 5</ref> in the main text. Our model obtains 31% when audio is included, which outperforms human text-only performance (30.2 %) <ref type="bibr" target="#b51">[82]</ref>, predicted by human annotators. A supervised LSTM obtains 34.4% in this text-only setting <ref type="bibr" target="#b51">[82]</ref> which suggests that there is a certain textual bias in this task, which our model cannot learn (as it is zero-shot). This also suggests that state-ofthe-art supervised models exploit patterns in this vocabulary distribution.</p><p>Without such an advantage, our model performs well, outperforming CLIP (2%) by a large margin. This suggests that jointly reasoning over both the visual situation, and the linguistic context of the provided sentence, is helpful for zero-shot performance on LSMDC fill-in-the-blank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2.4 Zero-shot MSRVTTQA</head><p>Finally, we evaluate our model on MSR VTT-QA, a questionanswering task over videos <ref type="bibr" target="#b91">[120]</ref>. We provide a model with N = 8 video segments sampled uniformly from the video clip, and extract an image from each one. For the first seven segments, we optionally include audio extracted from that point; at the last segment, we insert a converted version of the question, along with a MASK. We compare the similarity of that hidden state to the top 2000 most common answers, similar to past work <ref type="bibr" target="#b99">[128]</ref>.</p><p>Similar to STAR, we convert the questions into statements to minimize drift away from the pretraining distribution. We use GPT3 prompted with several examples for this. Our exact prompt is the following:</p><p>Input: what shows a person killing animals in a green forest?</p><p>Output: _ shows a person killing animals in a green forest. Then, given a new question ${question}, GPT3 generates a converted output, wherein we can replace it's underscore with a MASK. GPT3 works well at this conversion, though sometimes it generates a sentence where inserting the 'correct answer' feels gramatically strange. For example, the question 'how many women talk in a bedroom?' suggests any integer might be a reasonable answer. On the other hand, '_ women talk in a bedroom' implies that 'one' is not a valid answer (since 'women' is plural). We note that the errors caused by this conversion technique are specific to English grammar, and so if such a question-conversion approach was done in other languages, there could be more (or less) errors that directly result.</p><p>Our results are shown in <ref type="table" target="#tab_9">Table 5</ref>. Of note, our model through automatic question-conversion outperforms Just Ask <ref type="bibr" target="#b94">[123]</ref>, which performs an analogous (supervised-guided) question conversion on all its YouTube transcripts, before pretraining. Our model also outperforms CLIP, which cannot naturally handle dynamic situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Dataset Collection</head><p>In this section, we discussed how we curated data for YT-Temporal-1B. We had several goals in mind. We wanted to use only public-facing data, which motivated our choice of YouTube as it is a public platform that users understand is public <ref type="bibr" target="#b34">[65]</ref>. We wanted to use this platform to examine to what extent we can learn multimodal neural script knowledge from web data alone.</p><p>Our data collection strategy in this work was informed by past work, notably MERLOT <ref type="bibr" target="#b99">[128]</ref>. That paper found that increasing the diversity and scale of a video corpus both allowed for better learned representations. At the same time, the data collected by MERLOT (YT-Temporal-180M) has issues. Of note, the authors' scraping strategies -to prioritize monetized content -also led to a lot of U.S. local news being in that corpus (roughly 30% of all data). Local news might be problematic to learn from, particularly in that quantity, due to its numerous biases (e.g. racist coverage on 'crime' <ref type="bibr">[45,</ref><ref type="bibr">31,</ref><ref type="bibr">30,</ref><ref type="bibr" target="#b27">58]</ref>). Our goal was to expand the dataset in both diversity and size to 20 million videos, while having less local news and without scraping private content.</p><p>High level approach. We adopt a similar dataset collection strategy as in MERLOT <ref type="bibr" target="#b99">[128]</ref>. In the first phase, we identify a candidate set of videos ID to download. In the second phase, we open each video ID in YouTube and apply several filtering steps that go from inexpensive to expensive. The filtering steps allow us to exit early and possibly avoid downloading the video if the video seems unsuitable for our purpose from the title, description, and captions alone.</p><p>For a Datasheet <ref type="bibr">[41]</ref>, please see the MERLOT paper <ref type="bibr" target="#b99">[128]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Candidate video IDs</head><p>For MERLOT's YT-Temporal-180M, the bulk of the video IDs were identified by applying breadth-first-search on YouTube channels from HowTo100M <ref type="bibr" target="#b55">[85]</ref> and VLOG <ref type="bibr">[36]</ref>. Each channel often links to other channels, and given a channel it is inexpensive to obtain a list of all its videos using the youtube-dl Python package.</p><p>In this paper, we considered numerous approaches to search for diverse, visually grounded videos. We ended up using an approach where we used YouTube's recommended videos algorithm to suggest similar videos to YT-Temporal-180M. We went through all non-news and non-sports videos YT-Temporal-180M, and opened each video up in YouTube. For each other video that YouTube recommended, we retrieved its channel ID -giving us access to not just that video, but all other videos. This approach yielded 2 million channels, with 200 million videos among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Filtering video IDs by channel</head><p>Given this (large) list of channels, each with many videos, we took steps to filter it further. We used the python cld3 library to remove channels whose titles might not be in English. We then finetuned, and used, a language model to identify channels likely to have visually grounded videos, which we describe next.</p><p>In more detail, we selected 2000 videos, and asked workers on Mechanical Turk to rate their level of groundedness, their genre, and whether they had explicit content or not. The questions we asked are shown in <ref type="figure" target="#fig_4">Figure 8</ref>. We annotated 2k videos under this schema, and trained a model to predict the annotations given video metadata.</p><p>For model training, we used a slightly different setting to what we gave the crowdworkers. We trained a model to predict the labels, given a formatted list of 5 video titles from the same channel. During training, we made the weaksupervision assumption that all videos from a channel have exactly the same rating (as the video we annotated). This enabled us to collect 84k examples from our 2k annotations. The model we chose was T5-base model <ref type="bibr" target="#b64">[93]</ref>, which generates the labels left-to-right in text form (and which we converted automatically to a structured representation).</p><p>We then used this model to identify channels that seem especially promising. For each channel with at least 5 videos, we randomly sampled 8 sets of length-5 videos, and used the finetuned T5 model to classify them. We filtered out any channel that had at least 25% of likely non-English or  irrelevant-English videos, any channel that had at least 25% of slideshows, and any channel that likely had racist or sexist content.</p><p>One side benefit of this model is that it allowed us to estimate our videos' genre breakdown before downloading them. We found 1% Gaming videos, 11% News videos, 20% How-To videos, 20% 'chatting' videos, 5% sports videos, 5% Music videos, 3% Movies/Drama videos, 4% Documentary videos, and 31% Miscellaneous. The Gaming videos were then filtered out.</p><p>We used the classification model to create a budget for how many videos to download from each channel; with the aim to download more videos from likely more-grounded channels. Using the answers to Q3 (from <ref type="figure" target="#fig_4">Figure 8</ref>), we gave each channel 1 point for likely having 'a variety of objects', 2 points for 'a variety of actions', and 0.5 points for 'a variety of scenes.' We subtracted 3 points if it was likely to be a slideshow. (Likely-racist or sexist channels were already filtered out.) We then z-normalized and softmaxed the channel scores, and used the result as the channel-level budgets. Any channel with an aggregate 'interestingness' score of 1 standard deviation above the mean would then have a budget of 8x larger than the mean. We clipped the channel-level budgets to include at most 500 videos per <ref type="figure">Figure 9</ref>: An image prompt used in zero-shot audio classification. Here, "the sound of" is always inserted, and the word "birds" is one of the labels in ESC50 <ref type="bibr" target="#b60">[90]</ref>. We consider one image prompt for each label in ESC50 (or whichever dataset we are using).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>channel.</head><p>This process (finally!) gave us 30 million YouTube video IDs that were likely to be high-quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Filtering videos from their metadata</head><p>Last, we filtered and downloaded these videos using a filtering approach similar to <ref type="bibr" target="#b99">[128]</ref>. We first retrieved the video metadata and used it to filter out 'gaming' videos. We then retrieved the video's transcript, and filtered out any video without a 'dense' span of spoken words -defined as an interval of 30 seconds where at least 50 words are spoken. Additionally, we used the Python package cld3 to filter out any transcript with a probability of less than 80% of being English. Last, we used a hidden feature in the YouTube API to download four thumbnails of the video. Using the image classification model from <ref type="bibr" target="#b99">[128]</ref>, we filtered out videos whose four thumbnails had an average cosine similarity of above 85%, or that contained fewer than 1 object from COCO.</p><p>Unlike <ref type="bibr" target="#b99">[128]</ref>, we did not use a sequence-to-sequence model to 'translate' spoken text to text that appears more stylistically like written English (i.e., by adding capitalization and punctuation, and removing filler words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Experiments and Exploration</head><p>In this section, we briefly include additional experiments, showcasing our model's performance on specific tasks that do not necessarily require multimodal script knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Zero-shot Audio classification</head><p>We evaluate Reserve on the task of zero-shot audio classification, to study to what extent its learned audio representations can directly predict text-based labels. We conduct this evaluation on environmental sounds from ESC50 <ref type="bibr" target="#b60">[90]</ref>, urban sounds from US8K <ref type="bibr" target="#b69">[98]</ref>, and (as part of the privacyminded exploration in Appendix A) celebrity voices from VoxCeleb2 <ref type="bibr" target="#b57">[87]</ref>.  <ref type="table" target="#tab_3">Table 11</ref>: Zero-shot audio classification accuracies (%) on ESC50 <ref type="bibr" target="#b60">[90]</ref>, US8K <ref type="bibr" target="#b69">[98]</ref>, and VoxCeleb2 <ref type="bibr" target="#b57">[87]</ref>. We compare our model with AudioClip [54], which was pretrained on supervised data from AudioSet <ref type="bibr">[43]</ref>. Our Reserve performs well across the board, especially when given both the image and the text as a prompt -demonstrating its OCR capability.</p><p>We consider the format where we encode an audio input into a CLS level representation, and retrieve the most-similar label given a set of encoded options. We encode the audio input with our encoder, which takes in as input audio clips of length at most 1.6 seconds. For shorter audio clips (like many sounds in ESC50), we repeat them in time until their length is at least 1.6 seconds. For longer audio clips, we encode multiple CLS representations and then average the resulting vectors.</p><p>We consider the following ways to encode the labels:</p><p>a. Text-only. Inspired by the prompt 'a photo of', which is used in CLIP's zero-shot image classification task <ref type="bibr" target="#b63">[92]</ref>, we give Reserve's joint encoder a blank image, with associated tokens the sound of ${label}. We do this once for each label, giving us a single 'target' vector for each possible label in the dataset. b. Image-only. Inspired by YouTube videos of sound effects , we created image-only prompts that suggest a sound (of the target class) is playing in the background. An example is shown in <ref type="figure">Figure 9</ref>. We encode each image with our joint encoder, and do this once for each label. We note that for VoxCeleb2, we use face images of celebrities rather than this image-based prompt, due to our interest in exploring whether models can perform person-level recognition due to the privacy issue (Appendix A.1.1). c. Image and text. Here, we combine both of the above options: encoding one input for each label, using both the image and text prompt.</p><p>For each prompt, we append the token 'MASKAUDIO' and extract the hidden state from there, as our final representation for that label. We present our results in <ref type="table" target="#tab_3">Table 11</ref>. The results show, possibly surprisingly, that Reserve can perform optical character recognition over image prompts like <ref type="figure">Figure 9</ref> -For instance, youtu.be/VmgKryu4__k. given just the image, its accuracy on ESC50 is higher than given just text. Its accuracy on ESC50 and US8K improves further when given both an image and text.</p><p>These results are slightly different for VoxCeleb2, which emphasizes long-tail recognition of people -something that might be more encyclopedic than semantic, and that we did not wish to optimize in this work. There, when given an image of a celebrity's face, it demonstrates some capacity at linking it with one of their audio clips -a capacity that decreases if prompted with additional text. We suspect that this is due to interpreting the given text as spoken, for example, Justin Bieber himself saying 'the sound of Justin Bieber.' On all celebrities, Reserve struggles versus recognition-focused models like CLIP <ref type="bibr" target="#b63">[92]</ref> (Appendix A.1.1).</p><p>Overall, our model displays strong audio understanding ability. In comparison, AudioCLIP [54] (which is supervised on human-annotated labels from AudioSet [43]), performs 16% higher on ESC50, and 6.4% higher on US8K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Additional Qualitative Analysis</head><p>In <ref type="figure" target="#fig_5">Figure 10</ref>, we include an additional figure of examples, of the same format as <ref type="figure">Figure 5</ref>. The examples are chosen randomly -not by how much Reserve improved at retrieving their audio or text spans over the course of training.</p><p>we'll be seasoning it with this homemade adobo seasoning i couldn't find it and then i'm gonna take these two chicken breasts and i'm gonna and a repaint there at one stage other alden renovated or restored it other than just some running maintenance we've created to pay their paper and have the student go ...</p><p>... i'll have a appear we use the same rubric little writing rubrics for our writing units we create our ... corrections anything you'd like to share well ... differently in using rubrics developing rub ... student learning so when you were sharing with your partner's did you think of any ways to involve students</p><p>[MASK] clear and specific ... than student socioeconomic characteristics in addition to the power of feedback we *female-presenting voice, with a U.S. southern accent; using a phone microphone* dice them up and *male-presenting narrator's voice, with an Australian accent* one particular year we *male-presenting voice introducing the speaker (Paula Andrews); cheesy synth music in the background* Know that expectations that are Reserve-B over the course of pretraining. Match performance increases over time. The audio prediction in the first row is perhaps made easier by the speaker's australian accent. The audio prediction in the second row is perhaps easier due to the lecture-video setting. In the third row, both audio and text span prediction improves, with text being slightly favored in the end. This might be in part because of the truncation we do on audio (Section C.3) -the audio span is shorter than the text span of 'dice them up and' so as to not leak information, making prediction more challenging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>VoxCeleb2 results per-celebrity, comparing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>B.2), measure the model's computational footprint (B.3), list hyperparameters (B.4 ), and discuss several training strategies (B.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>a.</head><label></label><figDesc>Video Frame Encoder (ViT): just the h, w coordinates of the image; so (h, w, 0, 0). b. Audio Encoder: Only the 1-D position of the patch in the spectrogram: (0, 0, , 0). c. Text Span Encoder: Only the 1-D position of the token in the input: (0, 0, , 0). d. Joint encoder: Here, we use all coordinates. Inputs from the video frame encoder have coordinates (h, w, 0, t), where t is their segment index. The text and (pooled) audio inputs are merged, and they each have coordinates (0, 0, , t), where here is the absolute position in the entire sequence (across segments).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Video annotation. We had workers on Mechanical Turk annotate 2000 videos in our dataset with this questionnaire, allowing us to then train a model to identify suitable channels for our purpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>MASKed audio self-supervision on different examples. Similar to Figure 5, we show predictions from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>inputs for segment t</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">wt</cell><cell>at</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">Joint Encoder (Transformer)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">for all modalities and timesteps</cell><cell></cell><cell></cell><cell></cell></row><row><cell>vt</cell><cell>1,1 1,2</cell><cell>?</cell><cell>H/32, W/32</cell><cell>MASK Text</cell><cell>MASK Audio</cell><cell>while</cell><cell cols="2">? or</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>?</cell><cell>at</cell></row><row><cell></cell><cell cols="3">Image Encoder</cell><cell cols="4">Word Embed</cell><cell cols="5">Audio Encoder</cell></row><row><cell></cell><cell cols="2">(ViT)</cell><cell></cell><cell></cell><cell cols="2">(BPE)</cell><cell></cell><cell></cell><cell></cell><cell>(AST)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>vt</cell><cell></cell><cell></cell><cell cols="4">while it pops MASK wt</cell><cell>at</cell><cell cols="3">popping* popcorn *jiggling,</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of our contrastive span objective. It outperforms prior work in a Vision+Text setting, with a 1% boost when audio is added. Our full setup, adding written text, improves another 1%. denotes part of our full model.</figDesc><table /><note>b. VirTex</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :state-of-the-art leader- board performance on VCR. We</head><label>2</label><figDesc>Reserve gets</figDesc><table><row><cell></cell><cell></cell><cell cols="2">TVQA (acc; %)</cell></row><row><cell></cell><cell>Model</cell><cell cols="2">Val Test</cell></row><row><cell></cell><cell>Human [75]</cell><cell>-</cell><cell>89.4</cell></row><row><cell></cell><cell>MERLOT [128]</cell><cell cols="2">78.7 78.4</cell></row><row><cell>Subtitles</cell><cell>MMFT-BERT [109] Kim et al [68] Reserve-B Reserve-L</cell><cell cols="2">73.5 72.8 76.2 76.1 82.5 -85.9 85.6</cell></row><row><cell>Audio</cell><cell>Reserve-B Reserve-L</cell><cell cols="2">81.3 -85.6 84.8</cell></row><row><cell>Both</cell><cell>Reserve-B Reserve-L</cell><cell cols="2">83.1 82.7 86.5 86.1</cell></row><row><cell>compare it with</cell><cell></cell><cell></cell></row><row><cell>the largest submitted single models, including image-</cell><cell></cell><cell></cell></row><row><cell>caption models that utilize heavy manual supervision</cell><cell></cell><cell></cell></row><row><cell>(e.g. object detections and captions).</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Reserve</figDesc><table><row><cell>gets state-of-the-art</cell></row><row><cell>results on TVQA by over 7%, versus prior</cell></row><row><cell>work (that cannot make use of audio).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .1.7% when</head><label>4</label><figDesc>Reserve improves by</figDesc><table><row><cell>it can</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Reserve</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Situated Reasoning (STAR)</cell><cell cols="3">EPIC-Kitchens</cell><cell cols="2">LSMDC</cell><cell>MSR-VTT QA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(test acc; %)</cell><cell cols="3">(val class-mean R@5; %)</cell><cell cols="2">(FiB test %)</cell><cell>(test acc %)</cell></row><row><cell></cell><cell>Model</cell><cell>Interaction Sequence</cell><cell>Prediction Feasibility Overall</cell><cell>Verb</cell><cell>Noun</cell><cell>Action</cell><cell>Acc</cell><cell></cell><cell>top1</cell><cell>top5</cell></row><row><cell></cell><cell>Supervised SoTA</cell><cell cols="2">ClipBERT [74] 39.8 43.6 32.3 31.4 36.7</cell><cell cols="3">AVT+ [46] 28.2 32.0 15.9</cell><cell>52.9</cell><cell cols="2">MERLOT [128] 43.1</cell></row><row><cell></cell><cell>Random</cell><cell cols="2">25.0 25.0 25.0 25.0 25.0</cell><cell cols="3">6.2 2.3 0.1</cell><cell>0.1</cell><cell></cell><cell>0.1 0.5</cell></row><row><cell></cell><cell>CLIP (VIT-B/16) [92]</cell><cell cols="2">39.8 40.5 35.5 36.0 38.0</cell><cell cols="3">16.5 12.8 2.3</cell><cell>2.0</cell><cell></cell><cell>3.0 11.9</cell></row><row><cell>zero-shot</cell><cell>CLIP (RN50x16) [92] Just Ask (ZS)[123] Reserve-B Reserve-L Reserve-B (+audio) Reserve-L (+audio)</cell><cell cols="2">39.9 41.7 36.5 37.0 38.7 44.4 40.1 38.1 35.0 39.4 42.6 41.1 37.4 32.2 38.3 44.8 42.4 38.8 36.2 40.5 43.9 42.6 37.6 33.6 39.4</cell><cell cols="3">13.4 14.5 2.1 17.9 15.6 2.7 15.6 19.3 4.5 20.9 17.5 3.7 23.2 23.7 4.8</cell><cell>2.3 26.1 26.7 29.1 31.0</cell><cell></cell><cell>2.3 9.7 2.9 8.8 3.7 10.8 4.4 11.5 4.0 12.0 5.8 13.6</cell></row><row><cell>gets state-of-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the-art results on Kinetics-600 by</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.5% versus standard approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(that cannot make use of audio).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Zero shot results. On STAR, Reserve obtains state-of-the-art results, outperforming finetuned</cell></row><row><cell>video models. It performs well on EPIC-Kitchens (verb and noun forecasting), along with LSMDC, despite their</cell></row><row><cell>long-tail distributions. On MSR-VTT QA, it outperforms past work on weakly-supervised video QA. Further, it</cell></row><row><cell>outperforms CLIP (that cannot handle dynamic situations), and benefits from audio when given.</cell></row><row><cell>[2] which learns to represent audio independently from</cell></row><row><cell>vision (and so cannot early-fuse them), along with the larger</cell></row><row><cell>MTV-Huge model [122] by 1.5%.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>to pour these over top of ... that i've melted these are just the wilton candy melts and i'm ... try it anyway what ... this is a lot of popcorn so i don't know how this is gonna work this into a ... ... ... so now what ... so that's mainly why i turned the burner off ...</figDesc><table><row><cell>[MASK] it quits</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>popping i don't</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>want to burn this</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>hadn't no control over but i knew that i could control how my room looked what</cell><cell>i ate what i wore i kind of embraced ...</cell><cell>that that's all i had control ...</cell><cell>over [MASK] are you</cell><cell>shaking your head like there's always room for improvement that i like even if you're at like the best</cell><cell>you always want to get a better relationship with your parents got it i</cell><cell>just i feel like it's so i had kids when i was 20 by the time i was 22 i had both</cell><cell>... my kids i go oh that ...</cell></row><row><cell>weight on the legs and get more stretch in the calves in these 45 ...</cell><cell>because the next one is slightly ...</cell><cell>alright shake out your arms and your legs if you need forth a</cell><cell>single lick down dog where we ... [MASK]</cell><cell>leg extended completely straight and heel on the floor</cell><cell>left leg bent and place on top of right</cell><cell>... ... on the right leg for the maximum</cell><cell></cell></row></table><note>going</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>[ 27 ]</head><label>27</label><figDesc>Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11162-11173, 2021. 5 [28] Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Arjun Subramonian, Jeff M Phillips, and Kai-Wei Chang. Harms of gender exclusivity and challenges in non-binary representation in language technologies. arXiv preprint arXiv:2108.12084, 2021. 16 [29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. 2020. 7, 25 [39] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020. 6 [40] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. 23 [41] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume? III, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010, 2018. 27 [42] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356-3369, 2020. 16 [43] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and humanlabeled dataset for audio events. In Proc. IEEE ICASSP 2017, New Orleans, LA, 2017. 29 [44] Tarleton Gillespie. Content moderation, ai, and the question of scale. Daniel Gordon, Kiana Ehsani, Dieter Fox, and Ali Farhadi. Watching the world go by: Representation learning from unlabeled videos. arXiv preprint arXiv:2003.07990, 2020. 5 [50] Jonathan Gordon and Benjamin Van Durme. Reporting bias and knowledge acquisition. In Proceedings of the 2013 workshop on Automated knowledge base construction, pages 25-30. ACM, 2013. 16 [51] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, volume 1, page 9, 2017. 2 [52] Ben Green. Good" isn't good enough. In Proceedings of the AI for Social Good workshop at NeurIPS, 2019. 16 [53] Daniel Griffin and Jae Lim. Signal estimation from modified short-time fourier transform. IEEE Transactions on acoustics, speech, and signal processing, 32(2):236-243, 1984. 18 [54] Andrey Guzhov, Federico Raue, J?rn Hees, and Andreas Dengel. Audioclip: Extending clip to image, text and audio. arXiv preprint arXiv:2106.13043, 2021. 2, 29 [55] Foad Hamidi, Morgan Klaus Scheuerman, and Stacy M Branham. Gender recognition or gender reductionism? the social implications of embedded gender recognition systems.</figDesc><table><row><cell>arXiv preprint [30] Travis L Dixon. Crime news and racialized beliefs: Un-arXiv:1810.04805, 2018. 2, 5, 14 derstanding the relationship between local news viewing and perceptions of african americans and crime. Journal of Communication, 58(1):106-125, 2008. 27 [31] Travis L Dixon and Daniel Linz. Overrepresentation and We provide the following materials in the appendix: ? A full broader impact statement (Section A) ? Details about our model architecture (Section B) ? Details about how we provide video data into the model, including how we align the modalities and perform the masking (Section C ? Details about how we adapted our model to downstream tasks (Section D) ? Details about how we collected data (Section E) [49] Abstract [45] Franklin D Gilliam Jr, Shanto Iyengar, Adam Simon, and Big Data &amp; Society, 7(2):2053951720943234, 2020. 16 Oliver Wright. Crime in black and white: The violent, scary world of local news. Harvard International Journal of press/politics, 1(3):6-23, 1996. 27 [46] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. arXiv preprint arXiv:2106.02036, 2021. 7, 25 [47] Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio spectrogram transformer. arXiv preprint arXiv:2104.01778, 2021. 3, 18 [48] Steven M Goodman, Ping Liu, Dhruv Jain, Emma J McDon-nell, Jon E Froehlich, and Leah Findlater. Toward user-driven sound recognizer personalization with people who are d/deaf or hard of hearing. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 5(2):1-23, 2021. 8 ? Additional experiments (Section F)</cell></row></table><note>underrepresentation of african americans and latinos as lawbreakers on television news. Journal of communication, 50(2):131-154, 2000. 27 [32] Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, and Matt Gardner. Docu- menting the english colossal clean crawled corpus. CoRR, abs/2104.08758, 2021. 16 [33] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper- vised visual representation learning by context prediction. In Proceedings of the IEEE international conference on computer vision, pages 1422-1430, 2015. 4 [34] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3, 4, 6, 19 [35] Gerald M Edelman. Neural darwinism: selection and reen- trant signaling in higher brain function. Neuron, 10(2):115- 125, 1993. 1, 8 [36] David F Fouhey, Wei-cheng Kuo, Alexei A Efros, and Jitendra Malik. From lifestyle vlogs to everyday interactions. In CVPR, 2018. 27 [37] Christian Fuchs. An alternative view of privacy on facebook. Information, 2(1):140-165, 2011. 14 [38] Antonino Furnari and Giovanni Maria Farinella. Rolling- unrolling lstms for action anticipation from first-person video. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),In Proceedings of the 2018 chi conference on human factors in computing systems, pages 1-13, 2018. 16 [56] Donna Haraway. Situated knowledges: The science question in feminism and the privilege of partial perspective. Feminist studies, 14(3):575-599, 1988. 16</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Text Span Encoder: We operate on spans that are at most of length 15, with an additional CLS token. Its length is thus 16. Joint encoder. Let L be the number of text or pooled audio tokens given to the model per segment, on average; we set L=20. Let T be the number of video segments. Then, the</figDesc><table><row><cell></cell><cell cols="3">GFlops, from</cell><cell>VCR</cell></row><row><cell>Model</cell><cell>Image Encoder</cell><cell>Joint Encoder</cell><cell>Total</cell><cell>Q?AR Acc(%)</cell></row><row><cell>UNITER-Base[21]</cell><cell>1766</cell><cell cols="2">28 1794</cell><cell>58.2</cell></row><row><cell cols="2">UNITER-Large[21] 1767</cell><cell cols="2">99 1867</cell><cell>62.8</cell></row><row><cell>MERLOT [128]</cell><cell>236</cell><cell>67</cell><cell>303</cell><cell>65.1</cell></row><row><cell>Reserve-B Reserve-L</cell><cell cols="2">99 176 165 46</cell><cell>146 341</cell><cell>62.6 71.5</cell></row></table><note>d.joint model's sequence length is T ?(L+W/32?H/32). We set T =8 (8 video segments given to the model at a time) and used a H=192 by W =320 resolution. Our total sequence length was thus 640.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell>Efficiency metrics of our model versus others,</cell></row><row><cell>measured in terms of (giga) floating point operations required</cell></row><row><cell>to process a single image, question, and answer candidate</cell></row><row><cell>on VCR. We compare with the overall VCR performance on</cell></row><row><cell>the combined Q?AR metric. Our Reserve family of</cell></row><row><cell>models are significantly more efficient than prior work, with</cell></row><row><cell>Reserve-L being roughly on par with MERLOT [128]</cell></row><row><cell>in terms of FLOPs, yet improving accuracy by over 6%.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Architecture details, and pretraining hyperparameters, for both model sizes.</figDesc><table><row><cell></cell><cell></cell><cell>Base</cell><cell>Large</cell></row><row><cell></cell><cell>Batch Size</cell><cell></cell><cell>32</cell></row><row><cell>VCR</cell><cell>Training Epochs Image Size Learning Rates Tried</cell><cell cols="2">5 288?512 1e-5, 2e-5, 3e-5 8e-6, 1e-5, 1.2e-5</cell></row><row><cell></cell><cell>Learning Rate</cell><cell>2e-5</cell><cell>8e-6</cell></row><row><cell></cell><cell>Batch Size</cell><cell></cell><cell>32</cell></row><row><cell>TVQA</cell><cell>Training Epochs Image Size Learning Rates Tried</cell><cell cols="2">3 288?512 5e-6, 1e-5 5e-6, 1e-5</cell></row><row><cell></cell><cell>Learning Rate</cell><cell></cell><cell>5e-6</cell></row><row><cell>Kinetics-600</cell><cell>Batch Size Training Epochs Image Size Learning Rate Data Augmentation</cell><cell>1e-5</cell><cell>64 15 288?512 5e-6 From [2]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>23.2 23.7 4.8 20.3 21.0 5.9 22.7 21.6 4.0</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Overall</cell><cell>Unseen Kitchen</cell><cell>Tail Classes</cell></row><row><cell>Model</cell><cell cols="3">Verb Noun Act Verb Noun Act Verb Noun Act</cell></row><row><cell>RULSTM [38]</cell><cell cols="3">27.8 30.8 14.0 28.8 27.2 14.2 19.8 22.0 11.1</cell></row><row><cell>AVT+ (TSN) [46]</cell><cell cols="3">25.5 31.8 14.8 25.5 23.6 11.5 18.5 25.8 12.6</cell></row><row><cell>AVT+ [46]</cell><cell cols="3">28.2 32.0 15.9 19.5 23.9 11.9 21.1 25.8 14.1</cell></row><row><cell cols="4">Chance CLIP (VIT-B/16) [92] CLIP (RN50x16) [92] Reserve-B Reserve-L Reserve-B (+audio) 20.9 17.5 3.7 15.5 20.1 4.3 20.7 14.5 3.2 6.4 2.0 0.2 14.4 2.9 0.5 1.6 0.2 0.1 13.3 14.5 2.0 12.3 8.4 2.1 14.3 14.3 1.7 16.5 12.8 2.2 13.4 7.0 1.2 17.1 12.6 2.5 17.9 15.6 2.7 11.0 15.7 4.4 18.0 12.7 2.0 15.6 19.3 4.5 14.1 18.4 3.4 14.7 18.5 4.4 RULSTM [38] 25.3 26.7 11.2 19.4 26.9 9.7 17.6 16.0 7.9 Reserve-L (+audio) Test Validation AVT+ [46] 25.6 28.8 12.6 20.9 22.3 8.8 19.0 22.0 10.1</cell></row><row><cell cols="4">Reserve-L (+audio) 24.0 25.5 5.8 22.7 26.4 7.0 23.7 24.2 4.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Input: who does a man ask to go on a date?Output: a man asks _ to go on a date. Input: what are three people sitting on?Output: three people are sitting on _.</figDesc><table><row><cell>Input: ${question}</cell></row><row><cell>Output:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>{VIDEO} Q1. How would you describe the role of English speech in the video? a. This video doesn't have spoken English, or if it does, it's irrelevant to what's going on in the video. b. This video has English speech that describes, or adds onto, the visual content. Q2. Select at least one genres of the video: Select if any of the following are true: a. A variety of objects are interacted with. b. A variety of actions are performed. c. A variety of scenes are performed. d. This video is a slideshow. e. This video contains racist or sexist content..</figDesc><table><row><cell>$a. Gaming</cell></row><row><cell>b. News</cell></row><row><cell>c. How-to</cell></row><row><cell>d. Chatting</cell></row><row><cell>e. Sports</cell></row><row><cell>f. Music</cell></row><row><cell>g. Movies / Drama</cell></row><row><cell>h. Documentary</cell></row><row><cell>i. Miscellaneous</cell></row><row><cell>Q3.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">to 28 What is Janice Holding on to after Chandler sends Joey to his room? Chandler's tie. MASK[subtitles or audio]</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">We were unable to find a better text based prompt than this, as we found that they often biased the model towards linguistically relevant words; however, we suspect that such a prompt does exist.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers, as well as Jae Sung Park, Oren Etzioni, Gabriel Ilharco, and Mitchell Wortsman for feedback on this work. Thanks also to Zak Stone and the Google Cloud TPU team for providing access to the TPU machines used for conducting experiments. Thanks to James Bradbury and Skye Wanderman-Milne for help with JAX on TPUs. Thanks to the AI2 ReVIZ team, including Jon Borchardt and M Kusold, for help with the demo. This work was funded by DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI. Last, but not least, thanks to the YouTubers whose work and creativity helps machines to learn about the multimodal world. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Ha?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>V?ayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">Youtube-8m: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">VATT: transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linagzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11178</idno>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Selfsupervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16228</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reitter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05054</idno>
		<title level="m">Fusion of detected objects in text for visual question answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmargaret</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Climbing towards NLU: On meaning, form, and understanding in the age of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rotary embeddings: A relative revolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Online; accessed</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abeba</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vinay Uday Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kahembwe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01963</idno>
		<title level="m">Multimodal datasets: misogyny, pornography, and malignant stereotypes</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Anxiety, panic and self-optimization: Inequalities and the youtube algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convergence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Data efficient masked language modeling for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.02040</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simran</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Breitfeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-?CNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-?CNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE international conference on automatic face &amp; gesture recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Extracting training data from large language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07805</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">A short note about kinetics-600. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Children&apos;s language learning: An interactionist perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Child Psychology and Psychiatry and Allied Disciplines</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="54" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">important dependences between the image features and words/phrases in the description could be explained away by the dependencies among words/phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tweet</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="4069" to="4082" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The limits of transparency: Data brokers and commodification. new media &amp; society</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Crain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">White news: Why local news programs don&apos;t cover people of color. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Heider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Does my multimodal model learn cross-modal interactions? it&apos;s harder to tell than you might think! In EMNLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A case study on combining ASR and visual features for generating instructional video captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Five sources of bias in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<idno>e12432, 2021. 14</idno>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Largescale representation learning from visually grounded untranscribed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pretraining by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">my data just goes everywhere:&quot; user mental models of the internet and implications for privacy and security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruogu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dabbish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Fruchter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Kiesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Symposium On Usable Privacy and Security ({SOUPS} 2015)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Truth from the machine: artificial intelligence and the materialization of identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Os</forename><surname>Keyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zo?</forename><surname>Hitzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mwenza</forename><surname>Blell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interdisciplinary Science Reviews</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-supervised pre-training and contrastive representation learning for multiple-choice video qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyeong</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inho</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="13171" to="13179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="491" to="507" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V 16</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cross-modal learning for audio-visual video parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Lamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayaprakash</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04598</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mixout: Effective regularization to finetune large-scale pretrained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheolhyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tvqa: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Computer vision for assistive technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vx2text: End-to-end learning of video-based text generation from multimodal inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7005" to="7015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Opt: Omni-perception pre-trainer for cross-modal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longteng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z?ia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00249</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ViL-BERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher Joseph</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Networked privacy: How teenagers negotiate context in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">E</forename><surname>Marwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danah</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New media &amp; society</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Exploring the gender divide on youtube: An analysis of the creation and reception of vlogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heather</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerri</forename><surname>Donnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janice</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Communication Journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08612</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">The origins of intelligence in children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Piaget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><forename type="middle">Trans</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Avishai Hendel, and Shmuel Peleg. Clustered synopsis of surveillance video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarit</forename><surname>Ratovitch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<title level="m">Sixth IEEE international conference on advanced video and signal based surveillance</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Nonbinary: Memoirs of Gender and Identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Rajunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott Duane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Columbia University Press</publisher>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Auditing radicalization pathways on youtube</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Manoel Horta Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ottoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Virg?lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wagner</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meira</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on fairness, accountability, and transparency</title>
		<meeting>the 2020 conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angie</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09199</idno>
		<title level="m">Avlnet: Learning audio-visual language representations from instructional videos</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Scripts, plans, and knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 4th International Joint Conference on Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc. 1</publisher>
			<date type="published" when="1975" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
	<note>?CAI&apos;75</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The development of embodied cognition: Six lessons from babies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial life</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="13" to="29" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Platform capitalism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Srnicek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Watching YouTube</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strangelove</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>University of Toronto press</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09864</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">VideoBERT: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">LXMERT: Learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Gender and dialect bias in youtube&apos;s automatic captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachael</forename><surname>Tatman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matth?s</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Mmftbert: Multimodal fusion transformer with bert encodings for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aisha</forename><surname>Urooj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4648" to="4660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Drawing a chip environmental profile: environmental indicators for the semiconductor industry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur?lie</forename><surname>Villard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lelah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Brissaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cleaner Production</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Understanding the behaviour of contrastive loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="2495" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Multimodal selfsupervised learning of general audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12807</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10135</idno>
		<title level="m">Towards endto-end speech synthesis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Disembodied machine learning: On the illusion of objectivity in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeerak</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smarika</forename><surname>Lulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11974</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Neural domain adaptation for biomedical question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Star: A benchmark for situated reasoning in real-world videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoubin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
	<note>Xiangnan He, and Yueting Zhuang</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian Metze Luke Zettlemoyer Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14084</idno>
		<title level="m">Videoclip: Contrastive pre-training for zero-shot video-text understanding</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Multiview transformers for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04288</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00451</idno>
		<title level="m">Just ask: Learning to answer questions from millions of narrated videos</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Ernie-vil: Knowledge enhanced vision-language representations through scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?i</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16934</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Defending against neural fake news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Merlot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02636</idno>
		<title level="m">Multimodal neural script knowledge models</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?uan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinvl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00529</idno>
		<title level="m">Revisiting visual representations in vision-language models</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langlotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00747</idno>
		<title level="m">Contrastive learning of medical visual representations from paired images and text</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Men also like shopping: Reducing gender bias amplification using corpus-level constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Big other: surveillance capitalism and the prospects of an information civilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoshana</forename><surname>Zuboff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Technology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Input: when is a girl performing? Output: a girl is performing at _. Input: what is a cartoon doing? Output: a cartoon is _. Input: how many women talk in a bedroom? Output: _ women talk in a bedroom. Input: what a man playing while dancing with others? Output: a man is playing _ while dancing with others</title>
	</analytic>
	<monogr>
		<title level="m">Input: where is a flag hoisted? Output: a flag is hoisted in _. Input: who talks to another man on the couch? Output: _ talks to another man on the couch. Input: what does a teenage girl try to get at a public restroom? Output: a teenage girl tries to get _ at a public restroom</title>
		<imprint/>
	</monogr>
	<note>Input: what is a car being driven through? Output: a car is being driven through _. Input: who are running across screen? Output: _ are running across screen. Input: when do the models walk as the audience watches? Output: the models walk as the audience watches at _</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
