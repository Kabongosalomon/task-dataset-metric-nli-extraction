<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">TAL</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">TAL</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">TAL</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">TAL</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further taskspecific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during "identity fine-tuning". We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentencelevel tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transfer learning with pretrained Masked Language Models (MLMs) such as BERT <ref type="bibr" target="#b16">(Devlin et al., 2019)</ref> and <ref type="bibr">RoBERTa (Liu et al., 2019)</ref> has been widely successful in NLP, offering unmatched performance in a large number of tasks <ref type="bibr">(Wang et al., 2019a)</ref>. Despite the wealth of semantic knowledge stored in the MLMs <ref type="bibr" target="#b58">(Rogers et al., 2020)</ref>, they do not produce high-quality lexical and sentence embeddings when used off-the-shelf, without further</p><formula xml:id="formula_0">dist( , ) &lt; dist( / , / /?) f(x 1 ) f(x 1 ) f(x 1 ) f(x 1 ) f(x 2 ) f(x 3 )</formula><p>BERT/ RoBERTa BERT/ RoBERTa weight sharing ( random span masking ) multiple dropout layers</p><formula xml:id="formula_1">x 1x 1 f(x 1 ) f(x 1 ) f(x 2 ) f(x 3 ) f(x 4 )</formula><p>f(x 5 ) <ref type="figure">Figure 1</ref>: Illustration of the main concepts behind the proposed self-supervised Mirror-BERT method. The same text sequence can be observed from two additional "views": 1) by performing random span masking in the input space, and/or 2) by applying dropout (inside the BERT/RoBERTa MLM) in the feature space, yielding identity-based (i.e., "mirrored") positive examples for fine-tuning. A contrastive learning objective is then applied to encourage such "mirrored" positive pairs to obtain more similar representations in the embedding space relatively to negative pairs. task-specific fine-tuning <ref type="bibr" target="#b20">(Feng et al., 2020;</ref><ref type="bibr" target="#b35">Li et al., 2020)</ref>. In fact, previous work has shown that their performance is sometimes even below static word embeddings and specialised sentence encoders <ref type="bibr" target="#b15">(Cer et al., 2018)</ref> in lexical and sentence-level semantic similarity tasks <ref type="bibr" target="#b56">(Reimers and Gurevych, 2019;</ref><ref type="bibr" target="#b67">Vuli? et al., 2020b;</ref><ref type="bibr" target="#b38">Litschko et al., 2021)</ref>. In order to address this gap, recent work has trained dual-encoder networks on labelled external resources to convert MLMs into universal language encoders. Most notably, Sentence-BERT (SBERT, <ref type="bibr" target="#b56">Reimers and Gurevych 2019)</ref> further trains BERT and RoBERTa on Natural Language Inference <ref type="bibr">(NLI, Bowman et al. 2015;</ref><ref type="bibr" target="#b72">Williams et al. 2018</ref>) and sentence similarity data <ref type="bibr" target="#b14">(Cer et al., 2017)</ref> to obtain high-quality universal sentence embeddings. Recently, SapBERT  self-aligns phrasal representations of the same meaning using synonyms extracted from the UMLS <ref type="bibr" target="#b9">(Bodenreider, 2004)</ref>, a large biomedical knowledge base, obtaining lexical embeddings in the biomedical domain that reach state-of-the-art (SotA) performance in biomedical entity linking tasks. However, both SBERT and SapBERT require annotated (i.e., human-labelled) data as external knowledge: it is used to instruct the model to produce similar representations for text sequences (e.g., words, phrases, sentences) of similar/identical meanings.</p><p>In this paper, we fully dispose of any external supervision, demonstrating that the transformation of MLMs into universal language encoders can be achieved without task-labelled data. We propose a fine-tuning framework termed Mirror-BERT, which simply relies on duplicating and slightly augmenting the existing text input (or their representations) to achieve the transformation, and show that it is possible to learn universal lexical and sentence encoders with such "mirrored" input data through self-supervision (see <ref type="figure">Fig. 1</ref>). The proposed Mirror-BERT framework is also extremely efficient: the whole MLM transformation can be completed in less than one minute on two 2080Ti GPUs.</p><p>Our findings further confirm a general hypothesis from prior work <ref type="bibr" target="#b8">Ben-Zaken et al., 2020;</ref>) that finetuning exposes the wealth of (semantic) knowledge stored in the MLMs. In this case in particular, we demonstrate that the Mirror-BERT procedure can rewire the MLMs to serve as universal language encoders even without any external supervision. We further show that data augmentation in both input space and feature space are key to the success of Mirror-BERT, and they provide a synergistic effect.</p><p>Contributions. 1) We propose a completely selfsupervised approach that can quickly transform pretrained MLMs into capable universal lexical and sentence encoders, greatly outperforming offthe-shelf MLMs in similarity tasks across different languages and domains. 2) We investigate the rationales behind why Mirror-BERT works at all, aiming to understand the impact of data augmentation in the input space as well as in the feature space. We release our code and models at https: //github.com/cambridgeltl/mirror-bert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mirror-BERT: Methodology</head><p>Mirror-BERT consists of three main parts, described in what follows. First, we create positive pairs by duplicating the input text ( ?2.1). We then further process the positive pairs by simple data augmentation operating either on the input text or on the feature map inside the model ( ?2.2). Finally, we apply standard contrastive learning, 'attracting' the texts belonging to the same class (i.e., positives) while pushing away the negatives ( ?2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training Data through Self-Duplication</head><p>The key to success of dual-network representation learning <ref type="bibr" target="#b27">(Henderson et al., 2019;</ref><ref type="bibr" target="#b56">Reimers and Gurevych, 2019;</ref><ref type="bibr" target="#b31">Humeau et al., 2020;</ref><ref type="bibr">Liu et al., 2021, inter alia)</ref> is the construction of positive and negative pairs. While negative pairs can be easily obtained from randomly sampled texts, positive pairs usually need to be manually annotated. In practice, they are extracted from labelled task data (e.g., NLI) or knowledge bases that store relations such as synonymy or hypernymy (e.g., PPDB, <ref type="bibr" target="#b51">Pavlick et al. 2015;</ref><ref type="bibr">BabelNet, Ehrmann et al. 2014;</ref><ref type="bibr">WordNet, Fellbaum 1998;</ref><ref type="bibr">UMLS)</ref>.</p><p>Mirror-BERT, however, does not rely on any external data to construct the positive examples. In a nutshell, given a set of non-duplicated strings X , we assign individual labels (y i ) to each string and build a dataset D = {(x i , y i )|x i ? X , y i ? {1, . . . , |X |}}. We then create self-duplicated training data D simply by repeating every element in D. In other words, let X = {x 1 , x 2 , . . .}. We then have D = {(x 1 , y 1 ), (x 2 , y 2 ), . . .} and D = {(x 1 , y 1 ), (x 1 , y 1 ), (x 2 , y 2 ), (x 2 , y 2 ), . . .} where x 1 = x 1 , y 1 = y 1 , x 2 = x 2 , y 2 = y 2 , . . .. In ?2.2, we introduce data augmentation techniques (in both input space and feature space) applied on D . Each positive pair (x i , x i ) yields two different points/vectors in the encoder's representation space (see again <ref type="figure">Fig. 1</ref>), and the distance between these points should be minimised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>We hypothesise that applying certain 'corruption' techniques to (i) parts of input text sequences or (ii) to their representations, or even (iii) doing both in combination, does not change their (captured) meaning. We present two 'corruption' techniques as illustrated in <ref type="figure">Fig. 1</ref>. First, we can directly mask parts of the input text. Second, we can erase (i.e., dropout) parts of their feature maps. Both techniques are rather simple and intuitive: (i) even when masking parts of an input sentence, humans can usually reconstruct its semantics; (ii) dropping a small subset of neurons or representation dimen-  sions, the representations of a neural network will not drift too much.</p><formula xml:id="formula_2">v1 == v1 f(x1) f(x1) f(x2) f(x3) f(x5) f(x4) dist(f(x1), f(x1)) &lt; dist(f(x1/x1), f(x2/x3/?))</formula><p>Input Augmentation: Random Span Masking. The idea is inspired by random cropping in visual representation learning <ref type="bibr" target="#b28">(Hendrycks et al., 2020)</ref>. In particular, starting from the mirrored pairs (x i , y i ) and (x i , y i ), we randomly replace a consecutive string of length k with [MASK] in either x i or x i . The example <ref type="figure" target="#fig_0">(Fig. 2)</ref> illustrates the random span masking procedure with k = 5.</p><p>Feature Augmentation: Dropout. The random span masking technique, operating directly on text input, can be applied only with sentence/phraselevel input; word-level tasks involve only short strings, usually represented as a single token under the sentence-piece tokeniser. However, data augmentation in the feature space based on dropout, as introduced below, can be applied to any input text.</p><p>Dropout <ref type="bibr" target="#b60">(Srivastava et al., 2014)</ref> randomly drops neurons from a neural net during training with a probability p. In practice, it results in the erasure of each element with a probability of p. It has mostly been interpreted as implicitly bagging a large number of neural networks which share parameters at test time <ref type="bibr" target="#b10">(Bouthillier et al., 2015)</ref>. Here, we take advantage of the dropout layers in BERT/RoBERTa to create augmented views of the input text. Given a pair of identical strings x i and x i , their representations in the embedding space slightly differ due to the existence of multiple dropout layers in the BERT/RoBERTa architecture <ref type="figure">(Fig. 6</ref>). The two data points in the embedding space can be seen as two augmented views of the same text sequence, which can be leveraged for fine-tuning. <ref type="bibr">1</ref> It is possible to combine data augmentation via random span masking and featuure augmentation via dropout; this variant is also evaluated later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning</head><p>Let f (?) denote the encoder model. The encoder is then fine-tuned on the data constructed in ?2.2. <ref type="bibr">1</ref> The dropout augmentations are naturally a part of the BERT/RoBERTa network. That is, no further actions need to be taken to implement them. Note that random span masking is applied on only one side of the positive pair while dropout is applied on all data points. Given a batch of data D b , we leverage the standard InfoNCE loss <ref type="bibr" target="#b50">(Oord et al., 2018)</ref> to cluster/attract the positive pairs together and push away the negative pairs in the embedding space:</p><formula xml:id="formula_3">L b = ? |D b | i=1 log exp(cos(f (xi), f (xi))/? ) x j ?N i exp(cos(f (xi), f (xj))/? ) . (1)</formula><p>? denotes a temperature parameter; N i denotes all negatives of x i , which includes all x j , x j where i = j in the current data batch (i.e., |N i | = |D b | ? 2). Intuitively, the numerator is the similarity of the self-duplicated pair (the positive example) and the denominator is the sum of the similarities between x i and all other strings besides x i (the negatives). 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>Evaluation Tasks: Lexical. We evaluate on domain-general and domain-specific tasks: word similarity and biomedical entity linking (BEL). For the former, we rely on the Multi-SimLex evaluation set <ref type="bibr" target="#b65">(Vuli? et al., 2020a)</ref>: it contains human-elicited word similarity scores for multiple languages. For the latter, we use NCBI-disease (NCBI, Dogan et al. 2014), BC5CDR-disease, BC5CDR-chemical (BC5-d, BC5-c, , AskAPatient <ref type="bibr" target="#b37">(Limsopatham and Collier, 2016)</ref> and COMETA (stratified-general split, <ref type="bibr" target="#b7">Basaldella et al. 2020)</ref> as our evaluation datasets. The first three datasets are in the scientific domain (i.e., the data have been extracted from scientific papers), while the latter two are in the social media domain (i.e., extracted from online forums discussing health-related topics). We report Spearman's rank correlation coefficients (?) for word similarity; accuracy @1/@5 is the standard evaluation measure in the BEL task.</p><p>Evaluation Tasks: Sentence-Level. Evaluation on the intrinsic sentence textual similarity (STS) task is conducted on the standard SemEval 2012-2016 datasets <ref type="bibr" target="#b3">(Agirre et al., 2012</ref><ref type="bibr" target="#b4">(Agirre et al., , 2013</ref><ref type="bibr" target="#b1">(Agirre et al., , 2014</ref><ref type="bibr" target="#b0">(Agirre et al., , 2015</ref><ref type="bibr" target="#b2">(Agirre et al., , 2016</ref>, STS Benchmark (STS-b, <ref type="bibr" target="#b14">Cer et al. 2017)</ref>, SICK-Relatedness (SICK-R, <ref type="bibr" target="#b43">Marelli et al. 2014)</ref> for English; STS SemEval-17 data is used for Spanish and Arabic <ref type="bibr" target="#b14">(Cer et al., 2017)</ref>, and we also evaluate on Russian STS. <ref type="bibr">3</ref> We report Spearman's ? rank correlation. Evaluation in the question-answer entailment task is conducted on QNLI <ref type="bibr" target="#b55">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b70">Wang et al., 2019b)</ref>. It contains 110k English QA pairs with binary entailment labels. 4</p><p>Evaluation Tasks: Cross-Lingual. We also assess the benefits of Mirror-BERT on cross-lingual representation learning, evaluating on cross-lingual word similarity (CLWS, Multi-SimLex is used) and bilingual lexicon induction (BLI). We rely on the standard mapping-based BLI setup <ref type="bibr" target="#b6">(Artetxe et al., 2018)</ref>, and training and test sets from <ref type="bibr" target="#b23">Glava? et al. (2019)</ref>, reporting accuracy @1 scores (with CSLS as the word retrieval method, <ref type="bibr" target="#b34">Lample et al. 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mirror-BERT: Training</head><p>Resources. For finetuning (general-domain) lexical representations, we use the top 10k most frequent words in each language. For biomedical name representations, we randomly sample 10k names from the UMLS. In sentence-level tasks, for STS, we sample 10k sentences (without labels) from the training set of the STS Benchmark; for Spanish, Arabic and Russian, we sample 10k sentences from the Wiki-Matrix dataset <ref type="bibr" target="#b59">(Schwenk et al., 2021)</ref>. For QNLI, we sample 10k sentences from its training set.</p><p>Training Setup and Details. The hyperparameters of word-level models are tuned on SimLex-999 <ref type="bibr" target="#b30">(Hill et al., 2015)</ref>; biomedical models are tuned on COMETA (zero-shot-general split). Sentencelevel models are tuned on the dev set of STS-b. ? in Eq.</p><p>(1) is 0.04 (biomedical and sentence-level models); 0.2 (word-level). Dropout rate p is 0.  <ref type="table">Table 1</ref>: Word similarity evaluation on Multi-SimLex. "BERT" denotes monolingual BERT models in each language (see the Appendix). "mBERT" denotes multilingual BERT. Bold and underline denote highest and second-highest scores per column, respectively. scientific language social media language dataset? model? NCBI BC5-d BC5-c AskAPatient COMETA @1 @5 @1 @5 @1 @5 @1 @5 @1 @5  rate of k = 5, while k = 2 for biomedical phraselevel models; we do not employ span masking for word-level models (an analysis is in the Appendix).</p><p>All lexical models are trained for 2 epochs, max token length is 25. Sentence-level models are trained for 1 epoch with a max sequence length of 50. All models use AdamW <ref type="bibr" target="#b42">(Loshchilov and Hutter, 2019)</ref> as the optimiser, with a learning rate of 2e-5, batch size of 200 (400 after duplication). In all tasks, for all 'Mirror-tuned' models, unless noted otherwise, we create final representations using [CLS], instead of another common option: mean-pooling (mp) over all token representations in the last layer <ref type="bibr" target="#b56">(Reimers and Gurevych, 2019)</ref>. 5 6 4 Results and Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Lexical-Level Tasks</head><p>Word Similarity (Tab. 1). SotA static word embeddings such as fastText <ref type="bibr" target="#b47">(Mikolov et al., 2018)</ref> typically outperform off-the-shelf MLMs on word similarity datasets <ref type="bibr" target="#b65">(Vuli? et al., 2020a)</ref>. However, our results demonstrate that the Mirror-BERT procedure indeed converts the MLMs into much stronger word encoders. The Multi-SimLex results on 8 languages from Tab. 1 suggest that the fine-   tuned +Mirror variant substantially improves the performance of base MLMs (both monolingual and multilingual ones), even beating fastText in 5 out of the 8 evaluation languages. <ref type="bibr">7</ref> We also observe that it is essential to have a strong base MLM. While Mirror-BERT does offer substantial performance gains with all base MLMs, the improvement is more pronounced when the base model is strong (e.g., EN, ZH).</p><p>Biomedical Entity Linking (Tab. 2). The goal of BEL is to map a biomedical name mention to a controlled vocabulary (usually a node in a knowledge graph). Considered a downstream application in BioNLP, the BEL task also helps evaluate and compare the quality of biomedical name representations: it requires pairwise comparisons between the biomedical mention and all surface strings stored in the biomedical knowledge graph.</p><p>The results from Tab. 2 suggest that our +Mirror transformation achieves very strong gains on top of the base MLMs, both BERT and PubMedBERT <ref type="bibr" target="#b73">(Gu et al., 2020)</ref>. We note that PubMedBERT is a current SotA MLM in the biomedical domain, and performs significantly better than BERT, both before and after +Mirror fine-tuning. This highlights the necessity of starting from a domain-specific model when possible. On scientific datasets, the self-supervised PubMedBERT+Mirror model is 7 Language codes: see the Appendix for a full listing. very close to SapBERT, which fine-tunes PubMed-BERT with more than 10 million synonyms extracted from the external UMLS knowledge base. However, in the social media domain, PubMed-BERT+Mirror still cannot match the performance of knowledge-guided SapBERT. This in fact reflects the nature and complexity of the task domain. For the three datasets in the scientific domain (NCBI, BC5-d, BC5-c), strings with similar surface forms tend to be associated with the same concept. On the other hand, in the social media domain, semantics of very different surface strings might be the same. <ref type="bibr">8</ref>    Question-Answer Entailment <ref type="figure">(Fig. 4)</ref>. The results indicate that our +Mirror fine-tuning consistently improves the underlying MLMs. The RoBERTa+Mirror variant even shows a slight edge over the supervised SBERT model (.709 vs. .706).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cross-Lingual Tasks</head><p>We observe huge gains across all language pairs in CLWS (Tab. 5) and BLI (Tab. 6) after running the Mirror-BERT procedure. For language pairs that involve Hebrew, the improvement is usually smaller. We suspect that this is due to mBERT itself containing poor semantic knowledge for Hebrew. This finding aligns with our prior argument that a strong base MLM is still required to obtain prominent gains from running Mirror-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Further Discussion and Analyses</head><p>Running Time. Input Data Size <ref type="figure" target="#fig_3">(Fig. 5)</ref>. In our main experiments in ?4.1- ?4.3, we always use 10k examples for Mirror-BERT tuning. In order to assess the importance of the fine-tuning data size, we run a relevant analysis for a subset of base MLMs, and on a subset of English tasks. In particular, we evaluate the following: (i) BERT, Multi-SimLex (EN) (word-level); (ii) PubMedBERT, COMETA (biomedical phrase-level); (iii) RoBERTa, STS12 (sentence-level). The results indicate that the performance in all tasks reaches its peak in the region of 10k-20k examples and then gradually decreases, with a steeper drop on the the word-level task. 10 11</p><p>Random Span Masking + Dropout? (Tab. 7).</p><p>We conduct our ablation studies on the English STS tasks. First, we experiment with turning off dropout, random span masking, or both. With both techniques turned off, we observe large performance drops of RoBERTa+Mirror and BERT+Mirror (see also the Appendix). Span masking appears to be the more important factor: its absence causes a larger decrease. However, the best performance is achieved when both dropout and random span masking are leveraged, suggesting a synergistic effect when the two augmentation techniques are used together.   <ref type="figure">Figure 6</ref>: Under controlled dropout, if two strings are identical, they will have an identical set of dropout masks throughout the encoding process.</p><p>other augmentation types work as well? Recent work points out that pretrained MLM are heavily overparameterised and most Transformer heads can be pruned without hurting task performance <ref type="bibr" target="#b64">(Voita et al., 2019;</ref><ref type="bibr">Kovaleva et al., 2019;</ref><ref type="bibr" target="#b45">Michel et al., 2019)</ref>.  propose a drophead method: it randomly prunes attention heads at MLM training as a regularisation step. We thus evaluate a variant of Mirror-BERT where the dropout layers are replaced with such dropheads: 12 this results in even stronger STS performance, cf. Tab. 7. In short, this hints that the Mirror-BERT framework might benefit from other data and feature augmentation techniques in future work. 13</p><p>Regularisation or Augmentation? (Tab. 8).</p><p>When using dropout, is it possible that we are simply observing the effect of adding/removing regularisation instead of the augmentation benefit? To answer this question, we design a simple probe that attempts to disentangle the effect of regular-12 Drophead rates for BERT and RoBERTa are set to the default values of 0.2 and 0.05, respectively. <ref type="bibr">13</ref> Besides the drophead-based feature space augmentation, in our side experiments, we also tested input space augmentation techniques such as whole-word masking, random token masking, and word reordering; they typically yield performance similar or worse to random span masking. We also point to very recent work that explores text augmentation in a different context <ref type="bibr" target="#b73">(Wu et al., 2020;</ref>. We leave a thorough search of optimal augmentation techniques for future work.  isation versus augmentation; we turn off random span masking but leave the dropout on (so that the regularisation effect remains). However, instead of assigning independent dropouts to every individual string (rendering each string slightly different), we control the dropouts applied to a positive pair to be identical. As a result, it holds f (x i ) = f (x i ), when x i ? x i , ?i ? {1, ? ? ? , |D|}. We denote this as "controlled dropout". In Tab. 8, we observe that, during the +Mirror fine-tuning, controlled dropout largely underperforms standard dropout and is even worse than not using dropout at all. As the only difference between controlled and standard dropout is the augmented features for positive pairs in the latter case, this suggests that the gain from +Mirror indeed stems from the data augmentation effect rather than from regularisation.</p><p>Mirror-BERT Improves Isotropy? <ref type="figure" target="#fig_4">(Fig. 7)</ref>. We argue that the gains with Mirror-BERT largely stem from its reshaping of the embedding space geometry. Isotropy (i.e., uniformity in all orientations) of the embedding space has been a favourable property for semantic similarity tasks <ref type="bibr" target="#b5">(Arora et al., 2016;</ref><ref type="bibr" target="#b49">Mu and Viswanath, 2018)</ref>. However, Ethayarajh (2019) shows that (off-the-shelf) MLMs' representations are anisotropic: they reside in a narrow cone in the vector space and the average cosine similarity of (random) data points is extremely high. Sentence embeddings induced from MLMs without fine-tuning thus suffer from spatial anistropy <ref type="bibr" target="#b35">(Li et al., 2020;</ref><ref type="bibr" target="#b62">Su et al., 2021)</ref>. Is Mirror-BERT then improving isotropy of the embedding space? <ref type="bibr">14</ref> To investigate this claim, we inspect (1) the distributions of cosine similarities and (2) an isotropy score, as defined by <ref type="bibr" target="#b49">Mu and Viswanath (2018)</ref>.</p><p>First, we randomly sample 1,000 sentence pairs from the Quora Question Pairs (QQP) dataset. In 14 Some preliminary evidence from Tab. 7 already leads in this direction: we observe large gains over the base MLMs even without any positive examples, that is, when both span masking and dropout are not used (i.e., it always holds xi = xi and f (xi) = f (xi)). During training, this leads to a constant numerator in Eq. (1). In this case, learning collapses to the scenario where all gradients solely come from the negatives: the model is simply pushing all data points away from each other, resulting in a more isotropic space.  <ref type="figure" target="#fig_4">Fig. 7</ref>, we plot the distributions of pairwise cosine similarities of BERT representations before ( <ref type="figure" target="#fig_4">Figs. 7a and 7b</ref>) and after the +Mirror tuning <ref type="figure" target="#fig_4">(Fig. 7c)</ref>. The overall cosine similarities (regardless of positive/negative) are greatly reduced and the positives/negatives become easily separable. We also leverage a quantitative isotropy score (IS), proposed in prior work <ref type="bibr" target="#b5">(Arora et al., 2016;</ref><ref type="bibr" target="#b49">Mu and Viswanath, 2018)</ref>, and defined as follows:</p><formula xml:id="formula_4">IS(V) = minc?C v?V exp(c v) maxc?C v?V exp(c v) (2)</formula><p>where V is the set of vectors, 15 C is the set of all possible unit vectors (i.e., any c so that c = 1) in the embedding space. In practice, C is approximated by the eigenvector set of V V (V is the stacked embeddings of V). The larger the IS value, more isotropic an embedding space is (i.e., a perfectly isotropic space obtains the IS score of 1). IS scores in Tab. 9 confirm that the +Mirror finetuning indeed makes the embedding space more isotropic. Interestingly, with both data augmentation techniques switched off, a naive expectation is that IS will increase as the gradients now solely come from negative examples, pushing apart points in the space. However, we observe the increase of IS only for word-level representations. This hints at more complex dynamics between isotropy and gradients from positive and negative examples, where positives might also contribute to isotropy in some settings. We will examine these dynamics more in future work. <ref type="bibr">16</ref> Learning New Knowledge or Exposing Available Knowledge? Running Mirror-BERT for more epochs, or with more data (see <ref type="figure" target="#fig_3">Fig. 5</ref>) does not re-15 V comprises the corresponding text data used for Mirror-BERT fine-tuning (10k items for each task type). <ref type="bibr">16</ref> Introducing positive examples also naturally yields stronger task performance, as the original semantic space is better preserved. <ref type="bibr" target="#b21">Gao et al. (2021)</ref> provide an insightful analysis on the balance of learning uniformity and alignment preservation, based on the method of <ref type="bibr" target="#b71">Wang and Isola (2020)</ref>.  .393 + Mirror (random string, lr 5e-5) .481  sult in performance gains. This hints that, instead of gaining new knowledge from the fine-tuning data, Mirror-BERT in fact 'rewires' existing knowledge in MLMs <ref type="bibr" target="#b8">(Ben-Zaken et al., 2020)</ref>. To further verify this, we run Mirror-BERT with random 'zero-semantics' words, generated by uniformly sampling English letters and digits, and evaluate on (EN) Multi-SimLex. Surprisingly, even these data can transform off-the-shelf MLMs into effective word encoders: we observe a large improvement over the base MLM in this extreme scenario, from ? =0.267 to 0.481 (Tab. 10). We did a similar experiment on the sentence-level and observed similar trends. However, we note that using the actual English texts for fine-tuning still performs better as they are more 'in-domain' (with further evidence and discussions in the following paragraph).</p><p>Selecting Examples for Fine-Tuning. Using raw text sequences from the end task should be the default option for Mirror-BERT fine-tuning since they are in-distribution by default, as semantic similarity models tend to underperform when faced with a domain shift <ref type="bibr" target="#b76">(Zhang et al., 2020)</ref>. In the generaldomain STS tasks, we find that using sentences extracted from the STS training set, Wikipedia articles, or NLI datasets all yield similar STS performance after running Mirror-BERT (though optimal hyperparameters differ). However, porting BERT+Mirror trained on STS data to QNLI results in AUC drops from .674 to .665. This suggests that slight or large domain shifts do affect task performance, further corroborated by our findings from fine-tuning with fully random strings (see before).</p><p>Further, <ref type="figure">Fig. 8</ref> shows a clear tendency that more frequent strings are more likely to yield good task performance. There, we split the 100k most frequent words from English Wikipedia into 10 equally sized fine-tuning buckets of 10k examples each, and run +Mirror fine-tuning on BERT with each bucket. In sum, using frequent in-domain examples seems to be the optimal choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Self-supervised text representations have a large body of literature. Here, due to space constraints, we provide a highly condensed summary of the most related work. Even prior to the emergence of large pretrained LMs (PLMs), most representation models followed the distributional hypothesis <ref type="bibr" target="#b26">(Harris, 1954)</ref> and exploited the co-occurrence statistics of words/phrases/sentences in large corpora <ref type="bibr">(Mikolov et al., 2013a,b;</ref><ref type="bibr" target="#b52">Pennington et al., 2014;</ref><ref type="bibr" target="#b33">Kiros et al., 2015;</ref><ref type="bibr" target="#b29">Hill et al., 2016;</ref><ref type="bibr" target="#b41">Logeswaran and Lee, 2018)</ref>. Recently, DeCLUTR <ref type="bibr" target="#b22">(Giorgi et al., 2021)</ref> follows the distributional hypothesis and formulates sentence embedding training as a contrastive learning task where span pairs sampled from the same document are treated as positive pairs. Very recently, there has been a growing interest in using individual raw sentences for selfsupervised contrastive learning on top of PLMs. <ref type="bibr" target="#b73">Wu et al. (2020)</ref> explore input augmentation techniques for sentence representation learning with contrastive objectives. However, they use it as an auxiliary loss during full-fledged MLM pretraining from scratch <ref type="bibr" target="#b57">(Rethmeier and Augenstein, 2021)</ref>. In contrast, our post-hoc approach offers a lightweight and fast self-supervised transformation from any pretrained MLM to a universal language encoder at lexical or sentence level. <ref type="bibr" target="#b13">Carlsson et al. (2021)</ref> use two distinct models to produce two views of the same text, where we rely on a single model, that is, we propose to use dropout and random span masking within the same model to produce the two views, and demonstrate their synergistic effect. Our study also explores word-level and phrase-level representations and tasks, and to domain-specialised representations (e.g., for the BEL task).</p><p>SimCSE <ref type="bibr" target="#b21">(Gao et al., 2021)</ref>, a work concurrent to ours, adopts the same contrastive loss as Mirror-BERT, and also indicates the importance of data augmentation through dropout. However, they do not investigate random span masking as data augmentation in the input space, and limit their model to general-domain English sentence representations only, effectively rendering SimCSE a special case of the Mirror-BERT framework. Other concurrent papers explore a similar idea, such as Self-Guided Contrastive Learning <ref type="bibr" target="#b32">(Kim et al., 2021)</ref>, ConSERT , and BSL , inter alia. They all create two views of the same sentence for contrastive learning, with different strategies in feature extraction, data augmentation, model updating or choice of loss function. However, they offer less complete empirical findings compared to our work: we additionally evaluate on (1) lexical-level tasks, (2) tasks in a specialised biomedical domain and (3) cross-lingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed Mirror-BERT, a simple, fast, selfsupervised, and highly effective approach that transforms large pretrained masked language models (MLMs) into universal lexical and sentence encoders within a minute, and without any external supervision. Mirror-BERT, based on simple unsupervised data augmentation techniques, demonstrates surprisingly strong performance in (wordlevel and sentence-level) semantic similarity tasks, as well as on biomedical entity linking. The large gains over base MLMs are observed for different languages with different scripts, and across diverse domains. Moreover, we dissected and analysed the main causes behind Mirror-BERT's efficacy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Training Details</head><p>Most Frequent 10k/100k Words by Language. The most frequent 10k words in each language were selected based on the following list: https://github.com/oprogramador/ most-common-words-by-language. The most frequent 100k English words in Wikipedia can be found here: https://gist.github.com/h3xx/ 1976236.</p><p>[CLS] or Mean-Pooling? For MLMs, the consensus in the community, also validated by our own experiments, is that mean-pooling performs better than using [CLS] as the final output representation. However, for Mirror-BERT models, we found [CLS] (before pooling) generally performs better than mean-pooling. The exception is BERT on sentence-level tasks, where we found mean-pooling performs better than <ref type="bibr">[CLS]</ref>. In sum, sentence-level BERT+Mirror models are finetuned and tested with mean-pooling while all other Mirror-BERT models are fine-tuned and tested with <ref type="bibr">[CLS]</ref>. We also tried representations after the pooling layer, but found no improvement. Training Stability. All task results are reported as averages over three runs with different random seeds (if applicable). In general, fine-tuning is very stable and the fluctuations with different random seeds are very small. For instance, on the sentencelevel task STS, the standard deviation is &lt; 0.002. On word-level, standard deviation is a bit higher, but is generally &lt; 0.005. Note that the randomly dropout rate? 0.05 0.1 * 0.2 0.3 0.4 <ref type="bibr">BERT + Mirror .740 .743 .748 .748 .731 RoBERTa + Mirror .755 .753 .737 .694 .677</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Mean-Vector l 2 -Norm (MVN)</head><p>To supplement the quantitative evidence already suggested by the Isotropy Score (IS) in the main paper, we additionally compute the mean-vector l 2norm (MVN) of embeddings. In the word embedding literature, mean-centering has been a widely studied post-processing technique for inducing better semantic representations. <ref type="bibr" target="#b49">Mu and Viswanath (2018)</ref> point out that mean-centering is essentially increasing spatial isotropy by shifting the centre of the space to the region where actual data points reside in. Given a set of representation vectors V, we define MVN as follows:</p><formula xml:id="formula_5">MVN(V) = v?V v |V| 2 .</formula><p>(3)</p><p>The lower MVN is, the more mean-centered an embedding is. As shown in Tab. 14, MVN aligns with the trends observed with IS. This further confirms our intuition that +Mirror tuning makes the space more isotropic and shifts the centre of space close to the centre of data points. Very recently, <ref type="bibr" target="#b12">Cai et al. (2021)</ref> defined more metrics to measure spatial isotropy. <ref type="bibr" target="#b54">Rajaee and Pilehvar (2021)</ref> also used Eq. (2) for analysing sentence embedding's isotropiness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Evaluation Dataset Details</head><p>All datasets used and links to download them can be found in the code repository provided. The Russian STS dataset is provided by https://github.com/deepmipt/ deepPavlovEval.</p><p>The Quora Question Pair (QQP) dataset is downloaded at https://www.kaggle.com/c/ quora-question-pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Pretrained Encoders</head><p>A complete listing of URLs for all used pretrained encoders is provided in Tab. 15. For monolingual MLMs of each language, we made the best effort to select the most popular one (based on download counts). For computational tractability of the large number of experiments conducted, all models are BASE models (instead of LARGE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Full Tables</head><p>Here, we provide the complete sets of results. In these tables we include both MLMs w/ features extracted using both mean-pooling ("mp") and</p><p>[CLS] ("CLS").</p><p>For </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Number of Model Parameters</head><p>All BERT/RoBERTa models in this paper have ?110M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Hyperparameter Optimisation</head><p>Tab. 21 lists the hyperparameter search space. Note that the chosen hyperparameters yield the overall best performance, but might be suboptimal on any single setting (e.g. different base model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Software and Hardware Dependencies</head><p>All our experiments are implemented using Py-Torch 1.7.0 and huggingface.co transformers 4.4.2, with Automatic Mixed Precision (AMP) 17 turned on during training. Please refer to the GitHub repo for details. The hardware we use is listed in Tab. 22.       <ref type="table" target="#tab_3">Table 21</ref>: Hyperparameters along with their search grid. * marks the values used to obtain the reported results. The hparams are not always optimal in every setting but generally performs (close to) the best.</p><p>hardware specification RAM 128 GB CPU AMD Ryzen 9 3900x 12-core processor ? 24 GPU NVIDIA GeForce RTX 2080 Ti (11 GB) ? 2 <ref type="table" target="#tab_3">Table 22</ref>: Hardware specifications of the used machine. When encountering out-of-memoery error, we also used a second server with two NVIDIA GeForce RTX 3090 (24 GB).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An example of input data augmentation via random span masking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>As the same vector goes through the same dropout layer separately, the outcomes are independent. Consequently, two fully identical strings fed to the single BERT/RoBERTa model yield different representations in the MLM embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>BERT (area = 0.545) BERT+Mirror (area = 0.674) RoBERTa+Mirror (area = 0.709) SBERT (area = 0.706) Figure 4: Unsupervised QNLI: ROC curves and AOC scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The Mirror-BERT procedure is extremely time-efficient. While fine-tuning on NLI (SBERT) or UMLS (SapBERT) data can take hours, Mirror-BERT with 10k positive pairs completes the conversion from MLMs to universal language encoders within a minute on two NVIDIA RTX 2080Ti GPUs. On average, 10-20 seconds is needed for 1 epoch of the Mirror-BERT procedure. The impact of the number of fine-tuning "mirrored" examples (x-axis) on the task performance (yaxis). The scores across tasks are not directly comparable, and are based on different evaluation metrics ( ?3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Cosine similarity distribution over 1k sentence pairs sampled from QQP. Blue and orange mean positive and negative similarities, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(en) word freq. distribution BERT+Mirror (w/ words of different freq.) Figure 8: Blue: words in Multi-SimLex (EN) follow a long-tail distribution. Yellow: BERT+Mirror trained with frequent words tend to perform better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Econ [MASK] Paul Krugman mainly works on trade models.</figDesc><table><row><cell>x1</cell><cell>random masking</cell><cell>x1</cell></row><row><cell></cell><cell>multiple</cell><cell></cell></row><row><cell>dropout</cell><cell>dropout layers</cell><cell></cell></row><row><cell>BERT/</cell><cell></cell><cell>BERT/</cell></row><row><cell>RoBERTa</cell><cell>weight</cell><cell>RoBERTa</cell></row><row><cell></cell><cell>sharing</cell><cell></cell></row><row><cell>(v1) != dropout(v1)</cell><cell></cell><cell></cell></row></table><note>x1: Economist Paul Krugman mainly works on trade models.x1:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.528 .560 .447 .409 .428 .435 .488 .396 .461   BERT  .267 .020 .106 .220 .398 .202 .177 .217 .201  + Mirror .556 .621 .308 .538 .639 .365 .296 .444 .471   mBERT .105 .130 .094 .101 .261 .109 .095 .087 .123  + Mirror .358 .341 .134 .097 .501 .210 .332 .141 .264    </figDesc><table><row><cell>lang.?</cell><cell>EN</cell><cell>FR</cell><cell>ET</cell><cell>AR</cell><cell>ZH</cell><cell>RU</cell><cell>ES</cell><cell>PL</cell><cell>avg.</cell></row><row><cell>1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sentence-level models use a random span masking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>fastText</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>SapBERT.920 .956 .935 .960 .965 .982 .705 .889 .659 .779   BERT  .676 .770 .815 .891 .798 .912 .382 .433 .404 .477  + Mirror  .872 .921 .921 .949 .957 .971 .555 .695 .547 .647   PubMedBERT .778 .869 .890 .938 .930 .946 .425 .496 .468 .532  + Mirror  .909 .948 .930 .962 .958 .979 .590 .750 .603 .713    </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Biomedical entity linking (BEL) evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>English STS. *We were able to reproduce the scores reported in the original Sentence-BERT (SBERT, Reimers and Gurevych 2019) paper. However, we found mean-pooling over all tokens (including padded ones) yield better performance (.754 vs .749). We thus report the stronger baseline.</figDesc><table><row><cell>model?, lang.?</cell><cell>ES</cell><cell>AR</cell><cell>RU</cell><cell>avg.</cell></row><row><cell>BERT</cell><cell>.599</cell><cell>.455</cell><cell>.552</cell><cell>.533</cell></row><row><cell>+ Mirror</cell><cell>.709</cell><cell>.669</cell><cell>.673</cell><cell>.684</cell></row><row><cell>mBERT</cell><cell>.610</cell><cell>.447</cell><cell>.616</cell><cell>.558</cell></row><row><cell>+ Mirror</cell><cell>.755</cell><cell>.594</cell><cell>.692</cell><cell>.680</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>STS evaluation in other languages.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>This also suggests that Mirror-BERT adapts PubMedBERT to a very good surface-form encoder for biomedical names, but dealing with more difficult synonymy relations (e.g. as found in the social media) does need external knowledge.9 4.2 Sentence-Level Tasks English STS (Tab. 3). Regardless of the base model (BERT/RoBERTa), applying +Mirror fine-lang.? EN-FR EN-ZH EN-HE FR-ZH FR-HE ZH-HE avg.</figDesc><table><row><cell>mBERT .163</cell><cell>.118</cell><cell>.071</cell><cell>.142</cell><cell>.104</cell><cell>.010</cell><cell>.101</cell></row><row><cell>+ Mirror .454</cell><cell>.385</cell><cell>.133</cell><cell>.465</cell><cell>.163</cell><cell>.179</cell><cell>.297</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Cross-lingual word similarity results.</figDesc><table><row><cell cols="3">lang.? EN-FR EN-IT</cell><cell>EN-RU</cell><cell>EN-TR</cell><cell>IT-FR</cell><cell>RU-FR</cell><cell>avg.</cell></row><row><cell>BERT</cell><cell>.014</cell><cell>.112</cell><cell>.154</cell><cell>.150</cell><cell>.025</cell><cell>.018</cell><cell>.079</cell></row><row><cell cols="2">+ Mirror .458</cell><cell>.378</cell><cell>.336</cell><cell>.289</cell><cell>.417</cell><cell>.345</cell><cell>.371</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>BLI results.</figDesc><table /><note>tuning greatly boosts performance across all En- glish STS datasets. Surprisingly, on average, RoBERTa+Mirror, fine-tuned with only 10k sen- tences without any external supervision, is on-par with the SBERT model, which is trained on the merged SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) datasets, containing 570k and 430k sentence pairs, respectively. Spanish, Arabic and Russian STS (Tab. 4). The results in the STS tasks on other languages, which all have different scripts, again indicate very large gains, using both monolingual language-specific BERTs and mBERT as base MLMs. This confirms that Mirror-BERT is a language-agnostic method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Ablation study: (i) replacing dropout with drophead; (ii) the synergistic effect of dropout and random span masking in the English STS tasks.</figDesc><table><row><cell></cell><cell>== v 1v1</cell><cell></cell></row><row><cell>controlled</cell><cell cols="2">controlled</cell></row><row><cell>dropout</cell><cell>dropout</cell><cell></cell></row><row><cell>dropout(</cell><cell>) == dropout( v 1v1</cell><cell>)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>; dropout .648 ? .086 random span masking ; controlled dropout .452 ? .110</figDesc><table><row><cell>model configuration (MLM=RoBERTa)</cell><cell>? on STS12</cell></row><row><cell>random span masking ; dropout</cell><cell>.562</cell></row><row><cell>random span masking</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Probing the impact of dropout.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>IS of word, phrase, and sentence-level models.</figDesc><table><row><cell>model</cell><cell>?</cell></row><row><cell>fastText</cell><cell>.528</cell></row><row><cell>BERT-CLS</cell><cell>.105</cell></row><row><cell>BERT-mp</cell><cell>.267</cell></row><row><cell>+ Mirror</cell><cell>.556</cell></row><row><cell>+ Mirror (random string)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>: Running Mirror-BERT with a set of 'zero-</cell></row><row><cell>semantics' random strings. Evaluation is conducted on</cell></row><row><cell>Multi-SimLex (EN).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics. Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4365-4374, Hong Kong, China. Association for Computational Linguistics.</figDesc><table><row><cell cols="2">A Language Codes</cell></row><row><cell cols="2">EN English</cell></row><row><cell>ES</cell><cell>Spanish</cell></row><row><cell>FR</cell><cell>French</cell></row><row><cell>PL</cell><cell>Polish</cell></row><row><cell>ET</cell><cell>Estonian</cell></row><row><cell>FI</cell><cell>Finnish</cell></row><row><cell cols="2">RU Russian</cell></row><row><cell>TR</cell><cell>Turkish</cell></row><row><cell>IT</cell><cell>Italian</cell></row><row><cell cols="2">ZH Chinese</cell></row><row><cell cols="2">AR Arabic</cell></row><row><cell cols="2">HE Hebrew</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Language abbreviations used in the paper.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 12 :</head><label>12</label><figDesc>Average ? across STS tasks with different dropout rates. * default dropout rate for all models in other experiments.</figDesc><table><row><cell cols="2">random span mask rate? 2</cell><cell>5  *</cell><cell>10</cell><cell>15</cell><cell>20</cell></row><row><cell>BERT + Mirror</cell><cell cols="5">.741 .743 .720 .690 .616</cell></row><row><cell>RoBERTa + Mirror</cell><cell cols="5">.750 .753 .757 .743 .706</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 13 :</head><label>13</label><figDesc>Avg. ? across STS tasks with different random span masking rates. * default mask rates for all models in other experiments.</figDesc><table><row><cell>sampled training sets are fixed across all experi-</cell></row><row><cell>ments, and changing the training corpus for each</cell></row><row><cell>run might lead to larger fluctuations.</cell></row><row><cell>C Details of Mirror-BERT Trained on</cell></row><row><cell>Random Strings</cell></row><row><cell>We pointed out in the main text that BERT+Mirror</cell></row><row><cell>trained on random strings can outperform MLMs</cell></row><row><cell>by large margins. With standard training configu-</cell></row><row><cell>rations, BERT improves from .267 (BERT-mp) to</cell></row><row><cell>.393 with +Mirror. When learning rate is increased</cell></row><row><cell>to 5e-5, the MLM fine-tuned with random strings</cell></row><row><cell>performs only around 0.07 lower than the standard</cell></row><row><cell>BERT+Mirror model fine-tuned with the 10k most</cell></row><row><cell>frequent English words.</cell></row><row><cell>D Dropout and Random Span Masking</cell></row><row><cell>Rates</cell></row><row><cell>Dropout Rate (Tab. 12). The performance trends</cell></row><row><cell>conditioned on dropout rates are generally the same</cell></row><row><cell>across word-level, phrase-level and sentence-level</cell></row><row><cell>fine-tuning. Here, we use the STS task as a ref-</cell></row><row><cell>erence point. BERT prefers larger dropouts (0.2</cell></row><row><cell>&amp; 0.3) and is generally more robust. RoBERTa</cell></row><row><cell>prefers a smaller dropout rate (0.05) and its perfor-</cell></row><row><cell>mance decreases more steeply with the increase of</cell></row><row><cell>the dropout rate. For simplicity, as mentioned in</cell></row><row><cell>the main paper, we use the default value of 0.1 as</cell></row><row><cell>the dropout rate for all models.</cell></row><row><cell>Random Span Masking Rate (Tab. 13). Interest-</cell></row><row><cell>ingly, the opposite holds for random span masking:</cell></row><row><cell>RoBERTa is more robust to larger masking rates k,</cell></row><row><cell>and is much more robust than BERT to this hyper-</cell></row><row><cell>parameter.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14 :</head><label>14</label><figDesc>Full table for MVN and IS of word-, phrase-, and sentence-level models. Higher is better, that is, more isotropic with IS, while the opposite holds for MVN (lower scores mean more isotropic representation spaces).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>full multilingual word similarity results, view Tab. 16. For full Spanish, Arabic and Russian STS results, view Tab. 3. For full cross-lingual word similarity results, view Tab. 18. For full BLI results, view Tab. 19. For full ablation study results, view Tab. 20. For full MVN and IS scores, view Tab. 14.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 15 :</head><label>15</label><figDesc>A listing of HuggingFace &amp; fastText URLs of all pretrained models used in this work. .528 .560 .447 .409 .428 .435 .488 .396 .461 BERT-CLS .105 .050 .160 .210 .277 .177 .152 .257 .174 BERT-mp .267 .020 .106 .220 .398 .202 .177 .217 .201 + Mirror .556 .621 .308 .538 .639 .365 .296 .444 .471 mBERT-CLS .062 .046 .074 .047 .204 .063 .039 .051 .073 mBERT-mp .105 .130 .094 .101 .261 .109 .095 .087 .123 + Mirror .358 .341 .134 .097 .501 .210 .332 .141 .264</figDesc><table><row><cell>language?</cell><cell>EN</cell><cell>FR</cell><cell>ET</cell><cell>AR</cell><cell>ZH</cell><cell>RU</cell><cell>ES</cell><cell>PL</cell><cell>avg.</cell></row></table><note>fastText</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 16 :</head><label>16</label><figDesc>Word similarity evaluation on Multi-SimLex (Spearman's ?).</figDesc><table><row><cell>model?, lang.?</cell><cell>ES</cell><cell>AR</cell><cell>RU</cell><cell>avg.</cell></row><row><cell>BERT-CLS</cell><cell cols="4">.526 .308 .470 .435</cell></row><row><cell>BERT-mp</cell><cell cols="4">.599 .455 .552 .535</cell></row><row><cell>+ Mirror</cell><cell cols="4">.709 .669 .673 .684</cell></row><row><cell>mBERT-CLS</cell><cell cols="4">.421 .326 .430 .392</cell></row><row><cell>mBERT-mp</cell><cell cols="4">.610 .447 .616 .558</cell></row><row><cell>+ Mirror</cell><cell cols="4">.755 .594 .692 .680</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 17 :</head><label>17</label><figDesc>Full Spanish, Arabic and Russian STS evaluation. Spearman's ? correlation reported.</figDesc><table><row><cell>lang.?</cell><cell>EN-FR</cell><cell>EN-ZH</cell><cell>EN-HE</cell><cell>FR-ZH</cell><cell>FR-HE</cell><cell>ZH-HE</cell><cell>avg.</cell></row><row><cell>mBERT-CLS</cell><cell>.059</cell><cell>.053</cell><cell>.032</cell><cell>.042</cell><cell>.024</cell><cell>.050</cell><cell>.043</cell></row><row><cell>mBERT-mp</cell><cell>.163</cell><cell>.118</cell><cell>.071</cell><cell>.142</cell><cell>.104</cell><cell>.010</cell><cell>.101</cell></row><row><cell>+ Mirror</cell><cell>.454</cell><cell>.385</cell><cell>.133</cell><cell>.465</cell><cell>.163</cell><cell>.179</cell><cell>.297</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 18 :</head><label>18</label><figDesc>Full cross-lingual word similarity evaluation on Multi-SimLex (Spearman's ?).</figDesc><table><row><cell>lang.?</cell><cell>EN-FR</cell><cell>EN-IT</cell><cell>EN-RU</cell><cell>EN-TR</cell><cell>IT-FR</cell><cell>RU-FR</cell><cell>avg.</cell></row><row><cell>BERT-CLS</cell><cell>.045</cell><cell>.049</cell><cell>.108</cell><cell>.109</cell><cell>.046</cell><cell>.068</cell><cell>.071</cell></row><row><cell>BERT-mp</cell><cell>.014</cell><cell>.112</cell><cell>.154</cell><cell>.150</cell><cell>.025</cell><cell>.018</cell><cell>.079</cell></row><row><cell>+ Mirror</cell><cell>.458</cell><cell>.378</cell><cell>.336</cell><cell>.289</cell><cell>.417</cell><cell>.345</cell><cell>.371</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 19 :</head><label>19</label><figDesc>Full Bilingual Lexicon Induction results (accuracy reported). "EN-FR" means en mapped to FR. model configuration?, dataset? STS12 STS13 STS14 STS15 STS16 STS-b SICK-R avg.</figDesc><table><row><cell>BERT + Mirror</cell><cell>.674</cell><cell>.796</cell><cell>.713</cell><cell>.814</cell><cell>.743</cell><cell>.764</cell><cell>.703</cell><cell>.744</cell></row><row><cell>-dropout</cell><cell>.646</cell><cell>.770</cell><cell>.691</cell><cell>.800</cell><cell>.726</cell><cell>.745</cell><cell>.701</cell><cell>.726 ?.018</cell></row><row><cell>-random span masking</cell><cell>.641</cell><cell>.775</cell><cell>.684</cell><cell>.777</cell><cell>.737</cell><cell>.749</cell><cell>.658</cell><cell>.717 ?.027</cell></row><row><cell>-dropout &amp; random span masking</cell><cell>.587</cell><cell>.695</cell><cell>.617</cell><cell>.688</cell><cell>.683</cell><cell>.674</cell><cell>.614</cell><cell>.651 ?.093</cell></row><row><cell>RoBERTa + Mirror</cell><cell>.648</cell><cell>.819</cell><cell>.732</cell><cell>.798</cell><cell>.780</cell><cell>.787</cell><cell>.706</cell><cell>.753</cell></row><row><cell>-dropout</cell><cell>.619</cell><cell>.795</cell><cell>.706</cell><cell>.802</cell><cell>.777</cell><cell>.727</cell><cell>.698</cell><cell>.732 ?.021</cell></row><row><cell>-random span masking</cell><cell>.616</cell><cell>.786</cell><cell>.689</cell><cell>.766</cell><cell>.743</cell><cell>.756</cell><cell>.663</cell><cell>.717 ?.036</cell></row><row><cell>-dropout &amp; random span masking</cell><cell>.562</cell><cell>.730</cell><cell>.643</cell><cell>.744</cell><cell>.752</cell><cell>.708</cell><cell>.638</cell><cell>.682 ?.071</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 20 :</head><label>20</label><figDesc>Full table for the synergistic effect of dropout and random span masking in sentence similarity tasks. , 3, 5} ? in Eq. (1) {0.03, 0.04 * , 0.05, 0.07, 0.1, 0.2</figDesc><table><row><cell>hyperparameters</cell><cell>search space</cell></row><row><cell>learning rate</cell><cell>{5e-5, 2e-5  *  , 1e-5}</cell></row><row><cell>batch size</cell><cell>{100, 200  *  , 300}</cell></row><row><cell>training epochs</cell><cell>{1</cell></row></table><note>* , 2** , 0.3}</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also experimented with another state-of-the-art contrastive learning scheme proposed by. There, hard triplet mining combined with multi-similarity loss (MS loss) is used as the learning objective. InfoNCE and triplet mining + MS loss work mostly on par, with slight gains of one variant in some tasks, and vice versa. For simplicity and brevity, we report the results only with InfoNCE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">github.com/deepmipt/deepPavlovEval 4 We follow the setup of<ref type="bibr" target="#b35">Li et al. (2020)</ref> and adapt QNLI to an unsupervised task by computing the AUC scores (on the development set, ?5.4k pairs) using 0/1 labels and cosine similarity scores of QA embeddings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For 'non-Mirrored' original MLMs, the results with mp are reported instead; they produce much better results than using [CLS]; see the Appendix.6  All reported results are averages of three runs. In general, the training is very stable, with negligible fluctuations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">For instance, HCQ and Plaquenil refer to exactly the same concept on online health forums: Hydroxychloroquine. 9 Motivated by these insights, in future work we will also investigate a combined approach that blends self-supervision and external knowledge, which could also be automatically mined<ref type="bibr" target="#b61">(Su, 2020;</ref><ref type="bibr" target="#b63">Thakur et al., 2021)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">We suspect that this is due to the inclusion of lowerfrequency words into the fine-tuning data: embeddings of such words typically obtain less reliable embeddings<ref type="bibr" target="#b53">(Pilehvar et al., 2018)</ref>.11  For word-level experiments, we used the top 100k words in English according to Wikipedia statistics. For phrase-level experiments, we randomly sampled 100k names from UMLS. For sentence-level experiments we sampled 100k sentences from SNLI and MultiNLI datasets (as the STS training set has fewer than 100k sentences).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">https://pytorch.org/docs/stable/amp. html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the reviewers and the AC for their considerate comments. We also thank the LTL members and Xun Wang for insightful feedback. FL is supported by Grace &amp; Thomas C.H. Chan Cambridge Scholarship. AK and IV are supported by the ERC Grant LEXICAL (no. 648909) and the ERC PoC Grant MultiConvAI (no. 957356). NC kindly acknowledges grant-in-aid funding from ESRC (grant number ES/T012277/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S15-2045</idno>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>English, Spanish; Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
	<note>SemEval-2015 task 2: Semantic textual similarity</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1081</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">*SEM 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity</title>
		<meeting>the Main Conference and the Shared Task: Semantic Textual Similarity<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A latent variable model approach to PMI-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00106</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="385" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1073</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia. As</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="789" to="798" />
		</imprint>
	</monogr>
	<note>sociation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">COMETA: A corpus for medical entity linking in the social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Basaldella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.253</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3122" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">BitFit: Simple parameter-efficient fine-tuningfor transformer-based masked languagemodel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Ben-Zaken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dropout as data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<idno>abs/1506.08700</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Isotropy in the contextual embedding space: Clusters and manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic re-tuning with contrastive tension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelia</forename><surname>Amaru Cuba Gyllensten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Ylip??</forename><surname>Gogoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Hellqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Universal sentence encoder for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ncbi disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu</title>
		<meeting>the 2019 Conference Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>BERT: Pre-training of deep bidirectional transformers for language understanding</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representing multilingual data as linked data: the case of BabelNet 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maud</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Cecconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Philip Mccrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1006</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Languageagnostic bert sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2007.01852</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2104.08821</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeCLUTR: Deep contrastive learning for unsupervised textual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvald</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bader</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.72</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="879" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How to (properly) evaluate crosslingual word embeddings: On strong baselines, comparative analyses, and some misconceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Litschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="710" to="721" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3090" to="3104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Jianfeng Gao, and Hoifung Poon. 2020. Domainspecific language model pretraining for biomedical natural language processing. ArXiv preprint, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<idno type="DOI">https:/www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520</idno>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training neural response selection for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawe?</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Coope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrk?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1536</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5392" to="5404" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SimLex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00237</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="OpenRe-view.net" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa</title>
		<meeting><address><addrLine>Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-guided contrastive learning for BERT sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.197</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2528" to="2540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Antonio Torralba, and Sanja Fidler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
	<note>Skip-thought vectors</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the sentence embeddings from pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.733</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9119" to="9130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction. Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">https:/academic.oup.com/database/article-abstract/doi/10.1093/database/baw068/2630414</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Normalising medical concepts in social media texts by learning semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1096</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1014" to="1023" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluating multilingual text encoders for unsupervised cross-lingual retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Litschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-72113-8_23</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 43rd European Conference on Information Retrieval (ECIR 2021)</title>
		<meeting>43rd European Conference on Information Retrieval (ECIR 2021)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="342" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-alignment pretraining for biomedical entity representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Basaldella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.334</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4228" to="4238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/1907.11692</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note type="report_type">OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Coco-lm: Correcting and contrasting text sequences for language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/2102.08473</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="14014" to="14024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Proceedings of a meeting held</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">All-but-thetop: Simple and effective postprocessing for word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-2070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Card-660: Cambridge rare word dataset -a reliable benchmark for infrequent word representation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Mohammad Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1169</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1391" to="1401" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A cluster-based approach for improving isotropy in contextual embedding space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rajaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.73</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A primer on contrastive pretraining in language processing: Methods, lessons learned and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Rethmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno>abs/2102.12982</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A primer in BERTology: What we know about how BERT works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00349</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Wiki-Matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1351" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Simbert: Integrating retrieval and generation into bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Whitening sentence representations for better semantics and faster retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyiwen</forename><surname>Ou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/2103.15316</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.28</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="296" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1580</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5797" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-SimLex: A largescale evaluation of multilingual and crosslingual lexical semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulla</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Petti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eden</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00391</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="847" to="897" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">LexFit: Lexical fine-tuning of pretrained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glava?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5269" to="5283" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Probing pretrained language models for lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Litschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.586</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7222" to="7240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<imprint>
			<pubPlace>Felix Hill, Omer</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Clear: Contrastive learning for sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno>abs/2012.15466</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">ConSERT: A contrastive framework for self-supervised sentence representation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanmeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.393</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5065" to="5075" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Bootstrapped unsupervised sentence representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.402</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5168" to="5180" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">An unsupervised sentence embedding method by mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Scheduled DropHead: A regularization method for transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.178</idno>
		<ptr target="https://huggingface.co/bert-base-uncased" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1971" to="1980" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics. model URL fastText</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename></persName>
		</author>
		<ptr target="https://huggingface.co/roberta-basemBERThttps://huggingface.co/bert-base-multilingual-uncasedTurkishBERTdbmdz/bert-base-turkish-uncasedItalianBERTdbmdz/bert-base-italian-uncasedFrenchBERThttps://huggingface.co/camembert-baseSpanishBERThttps://huggingface.co/dccuchile/bert-base-spanish-wwm-uncasedRussianBERThttps://huggingface.co/DeepPavlov/rubert-base-casedChineseBERThttps://huggingface.co/bert-base-chineseArabicBERThttps://huggingface.co/aubmindlab/bert-base-arabertv02PolishBERThttps://huggingface.co/dkleczek/bert-base-polish-uncased-v1EstonianBERThttps://huggingface.co/tartuNLP/EstBERT" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
