<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><forename type="middle">A</forename><surname>Tailor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">L</forename><surname>Opolka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Invenia Labs</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Samsung AI Center</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DO WE NEED ANISOTROPIC GRAPH NEURAL NETWORKS?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Common wisdom in the graph neural network (GNN) community dictates that anisotropic models-in which messages sent between nodes are a function of both the source and target node-are required to achieve state-of-the-art performance. Benchmarks to date have demonstrated that these models perform better than comparable isotropic models-where messages are a function of the source node only. In this work we provide empirical evidence challenging this narrative: we propose an isotropic GNN, which we call Efficient Graph Convolution (EGC), that consistently outperforms comparable anisotropic models, including the popular GAT or PNA architectures by using spatially-varying adaptive filters. In addition to raising important questions for the GNN community, our work has significant real-world implications for efficiency. EGC achieves higher model accuracy, with lower memory consumption and latency, along with characteristics suited to accelerator implementation, while being a drop-in replacement for existing architectures. As an isotropic model, it requires memory proportional to the number of vertices in the graph (O(V )); in contrast, anisotropic models require memory proportional to the number of edges (O(E)). We demonstrate that EGC outperforms existing approaches across 6 large and diverse benchmark datasets, and conclude by discussing questions that our work raise for the community going forward. Code and pretrained models for our experiments are provided at https://github.com/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) have emerged as an effective way to build models over arbitrarily structured data. For example, they have successfully been applied to computer vision tasks: GNNs can deliver high performance on point cloud data <ref type="bibr" target="#b32">(Qi et al., 2017)</ref> and for feature matching across images <ref type="bibr" target="#b37">(Sarlin et al., 2020)</ref>. Recent work has also shown that they can be applied to physical simulations . Code analysis is another application domain where GNNs have found success <ref type="bibr" target="#b20">(Guo et al., 2020;</ref><ref type="bibr" target="#b0">Allamanis et al., 2017)</ref>.</p><p>In recent years, the research community has devoted significant attention to building more expressive, and better performing, models to process graphs. Efforts to benchmark GNN models, such as Open Graph Benchmark , or the work by <ref type="bibr" target="#b15">Dwivedi et al. (2020)</ref>, have attempted to more rigorously quantify the relative performance of different proposed architectures. One common conclusion-explicitly stated by <ref type="bibr" target="#b15">Dwivedi et al. (2020)</ref>-is that anisotropic 1 models, in which messages sent between nodes are a function of both the source and target node, are the best performing models. By comparison, isotropic models, where messages are a function of the source node only, achieve lower accuracy, even if they have efficiency benefits over comparable anisotropic models. Intuitively, this conclusion is satisfying: anisotropic models are inherently more expressive, hence we would expect them to perform better in most situations. Our work provides a surprising challenge to this wisdom by providing an isotropic model, called Efficient Graph Convolution <ref type="bibr">(EGC)</ref>, <ref type="figure">Figure 1</ref>: Many GNN architectures (e.g. GAT <ref type="bibr" target="#b44">(Veli?kovi? et al., 2018)</ref>, PNA <ref type="bibr" target="#b14">(Corso et al., 2020)</ref>) incorporate sophisticated message functions to improve accuracy (left). This is problematic as we must materialize messages, leading to O(E) memory consumption and OPs to calculate messages; these dataflow patterns are also difficult to optimize for at the hardware level. This work demonstrates that we can use simple message functions, requiring only O(V ) memory consumption (right) and improve performance over existing GNNs.</p><formula xml:id="formula_0">Isotropic b f ( b ) a f ( a ) c f ( c ) d f ( d )</formula><p>that outperforms comparable anisotropic approaches, including the popular GAT <ref type="bibr" target="#b44">(Veli?kovi? et al., 2018)</ref> and PNA <ref type="bibr" target="#b14">(Corso et al., 2020)</ref> architectures.</p><p>In addition to providing a surprising empirical result for the community, our work has significant practical implications for efficiency, as shown in <ref type="figure">Figure 1</ref>. As EGC is an isotropic model achieving high accuracy, we can take advantage of the efficiency benefits offered by isotropic models without having to compromise on model accuracy. We have seen memory consumption and latency for state-of-the-art GNN architectures increase to O(E) in recent years, due to state-of-the-art models incorporating anisotropic mechanisms to boost accuracy. EGC reduces the complexity to O(V ), delivering substantial real-world benefits, albeit with the precise benefit being dependent on the topology of the graphs the model is applied to. The reader should note that our approach can also be combined with other approaches for improving the efficiency of GNNs. For example, common hardware-software co-design techniques include quantization and pruning <ref type="bibr" target="#b39">(Sze et al., 2020)</ref> could be combined with this work, which proposes an orthogonal approach of improving model efficiency by improving the underlying architecture design. We also note that our approach can be combined with graph sampling techniques <ref type="bibr" target="#b52">(Zeng et al., 2019;</ref><ref type="bibr" target="#b21">Hamilton et al., 2017;</ref><ref type="bibr" target="#b6">Chen et al., 2018a)</ref> to improve scalability further when training on graphs with millions, or billions, of nodes.</p><p>Contributions (1) We propose a new GNN architecture, Efficient Graph Convolution (EGC), and provide both spatial and spectral interpretations for it. (2) We provide a rigorous evaluation of our architecture across 6 large graph datasets covering both transductive and inductive use-cases, and demonstrate that EGC consistently achieves better results than strong baselines. (3) We provide several ablation studies to motivate the selection of the hyperparameters in our model. <ref type="formula" target="#formula_7">(4)</ref> We demonstrate that our model simultaneously achieves better parameter efficiency, latency and memory consumption than competing approaches. Code and pre-trained models for our experiments (including baselines) can be found at https://github.com/shyam196/egc. At time of publication, EGC has also been upstreamed to PyTorch Geometric <ref type="bibr" target="#b16">(Fey &amp; Lenssen, 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">HARDWARE-SOFTWARE CO-DESIGN FOR DEEP LEARNING</head><p>Several of the popular approaches for co-design have already been described in the introduction: quantization, pruning, and careful architecture design are all common for CNNs and Transformers <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref>. In addition to enabling better performance to be obtained from general purpose processors such as CPUs and GPUs, these techniques are also essential for maximizing the return from specialized accelerators; while it may be possible to improve performance over time due to improvements in CMOS technology, further improvements plateau without innovation at the algorithmic level <ref type="bibr" target="#b17">(Fuchs &amp; Wentzlaff, 2019)</ref>. As neural network architecture designers, we cannot simply rely on improvements in hardware to make our proposals viable for real-world deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GRAPH NEURAL NETWORKS</head><p>Many GNN architectures can be viewed as a generalization of CNN architectures to the irregular domain: as in CNNs, representations at each node are built based on the local neighborhood using parameters that are shared across the graph. GNNs differ as we cannot make assumptions about the the size of the neighborhood, or the ordering. One common framework used to define GNNs is the message passing neural network (MPNN) paradigm <ref type="bibr" target="#b18">(Gilmer et al., 2017)</ref>. A graph G = (V, E) has node features X ? R N ?F , adjacency matrix A ? R N ?N and optionally D-dimensional edge features E ? R E?D . We define a function ? that calculates messages from node u to node v, a differentiable and permutation-invariant aggregator ?, and an update function ? to calculate representations at layer l + 1: h</p><formula xml:id="formula_1">(i) l+1 = ?(h (i) l , ? j?N (i) [?(h (i) l , h (j)</formula><p>l , e ij )]). Propagation rules for baseline architecture are provided in <ref type="table" target="#tab_0">Table 1</ref>, with further details supplied in <ref type="table">Table 5</ref> in the Appendix.</p><p>Relative Expressivity of GNNs Common wisdom in the research community states that isotropic GNNs are less expressive than anisotropic GNNs; empirically this is well supported by benchmarks. <ref type="bibr" target="#b4">Brody et al. (2022)</ref> prove that GAT models can be strictly more expressive than isotropic models.  also discuss the relative expressivity of different classes of GNN layer, and argue that convolutional (also known as isotropic) models are well suited to problems leveraging homophily 2 in the input graph. They further argue that attentional, or full message passing, models are suited to handling heterophilous problems, but they acknowledge the resource consumption and trainability of these architectures may be prohibitive-especially in the case of full message passing.</p><p>Scaling and Deploying GNNs While GNNs have seen success across a range of domains, there remain challenges associated with scaling and deploying them. Graph sampling is one approach to scaling training for large graphs or models which will not fit in memory. Rather than training over the full graph, each iteration is run over a sampled sub-graph; approaches vary in whether they sample node-wise <ref type="bibr" target="#b21">(Hamilton et al., 2017)</ref>, layer-wise <ref type="bibr" target="#b6">(Chen et al., 2018a;</ref><ref type="bibr" target="#b23">Huang et al., 2018)</ref>, or subgraphs <ref type="bibr" target="#b52">(Zeng et al., 2019;</ref><ref type="bibr" target="#b12">Chiang et al., 2019)</ref>. Alternatively, systems for distributed GNN training have been proposed <ref type="bibr" target="#b24">(Jia et al., 2020)</ref> to scale training beyond the limits of a single accelerator. Some works have proposed architectures that are designed to accommodate scaling: graph-augmented MLPs, such as SIGN <ref type="bibr" target="#b34">(Rossi et al., 2020)</ref>, are explicitly designed as a shallow architecture, as all the graph operations are done as a pre-processing step. Other work includes applying neural architecture search (NAS) to arrange existing GNN layers <ref type="bibr" target="#b54">(Zhao et al., 2020)</ref>, or building quantization techniques for GNNs <ref type="bibr" target="#b40">(Tailor et al., 2021)</ref>. Finally, a recent work has shown that using memoryefficient reversible residuals  for GNNs  enables us to train far deeper and larger GNN models than before, thereby progressing the state-of-the-art accuracy.</p><p>Why Are Existing Approaches Not Sufficient? It is worth noting that many of these approaches have significant limitations that we aim to address with our work. Sampling methods are often ineffective when applied to many problems which involve model generalization to unseen graphs-a common use-case for GNNs. We evaluated a variety of sampling approaches and observed that even modest sampling levels, which provide little benefit to memory or latency, cause model performance to decline noticeably. In addition, these methods do not accelerate the underlying GNN, hence they may not provide any overall benefit to inference latency. There is also no evidence that we are aware of that graph-augmented MLPs perform adequately when generalizing to unseen graphs; indeed, they are known to be theoretically less expressive than standard GNNs <ref type="bibr" target="#b7">(Chen et al., 2021)</ref>. We also investigated this setup, and found that these approaches do not offer competitive accuracy with state-of-the-art approaches. Experiment details and results, along with further discussion of the limitations of existing work, is provided in Appendix B.</p><p>In summary, our work on efficient GNN architecture design is of interest to the community for two reasons: firstly, it raises questions about common assumptions, and how we design and evaluate GNN models; secondly, our work may enable us to scale our models further, potentially yielding improvements in accuracy. In addition, for tasks where we need to generalize to unseen graphs, such as code analysis or point cloud processing, we reduce memory consumption and latency, thereby enabling us to deploy our models to more resource-constrained devices than before. We note that efficient architecture design can be usefully combined with other approaches including sampling, quantization, and pruning, where appropriate.</p><formula xml:id="formula_2">x (i) w (i) = ?x (i) + b ? 1 ? 2 ? 3 ?</formula><p>Run separate graph filters parameterised by ? b over graph Apply per-node weightings w (i) to each basis filter </p><formula xml:id="formula_3">h (i) 1 h (i) 2 h (i) 3 y (i)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR ARCHITECTURE: EFFICIENT GRAPH CONVOLUTION (EGC)</head><p>In this section we describe our approach, and delay theoretical analysis to the next section. We present two versions: EGC-S(ingle), using a single aggregator, and EGC-M(ulti) which generalizes our approach by incorporating multiple aggregators. Our approach is visualized in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ARCHITECTURE DESCRIPTION</head><p>For a layer with in-dimension of F and out-dimension of F we use B basis weights ? b ? R F ?F . We compute the output for node i by calculating combination weighting coefficients w (i) ? R B per node, and weighting the results of each aggregation using the different basis weights ? b . The output for node i is computed in three steps. First, we perform the aggregation with each set of basis weights ? b . Second, we compute the weighting coefficients w (i) = ?x (i) + b ? R B for each node i, where ? ? R B?F and b ? R B are weight and bias parameters for calculating the combination weighting coefficients. Third, the layer output for node i is the weighted combination of aggregation outputs:</p><formula xml:id="formula_4">y (i) = B b=1 w (i) b j?N (i) ?(i, j)? b x (j)<label>(1)</label></formula><p>where ?(i, j) is some function of nodes i and j, and N (i) denotes the in-neighbours of i. A popular method pioneered by GAT <ref type="bibr" target="#b44">(Veli?kovi? et al., 2018)</ref> to boost representational power is to represent ? using a learned function of the two nodes' representations. While this enables anisotropic treatment of neighbors, and can boost performance, it necessarily results in memory consumption of O(E) due to messages needing to be explicitly materialized, and complicates hardware implementation for accelerators. If we choose a representation for ? that is not a function of the node representationssuch as ?(i, j) = 1 to recover the add aggregator used by GIN <ref type="bibr" target="#b47">(Xu et al., 2019)</ref>, or ?(i, j) = 1/ deg(i)deg(j) to recover symmetric normalization used by GCN (Kipf &amp; Welling, 2017)then we can implement our message propagation phase using sparse matrix multiplication (SpMM), and avoid explicitly materializing each message, even for the backwards pass. In this work, we assume ?(i, j) to be symmetric normalization as used by GCN unless otherwise stated; we use this normalization as it is known to offer strong results across a variety of tasks; more formal justification is provided in section 4.2.</p><p>Adding Heads as a Regularizer We can extend our layer through the addition of heads, as used in architectures such as GAT or Transformers <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref>. These heads share the basis weights, but apply different weighting coefficients per head. We find that adding this degree of freedom aids regularization when the number of heads (H) is larger than B, as bases are discouraged from specializing (see section 5.3), without requiring the integration of additional loss terms into the optimization-hence requiring no changes to code for downstream users. To normalize the output dimension, we change the basis weight matrices dimensions to F H ?F . Using as the concatenation operator, and making the use of symmetric normalization explicit, we obtain the EGC-S layer:</p><formula xml:id="formula_5">y (i) = H h=1 B b=1 w (i) h,b j?N (i)?{i} 1 deg(i)deg(j) ? b x (j)<label>(2)</label></formula><p>EGC works by combining basis matrices. This idea was proposed in R-GCN <ref type="bibr" target="#b38">(Schlichtkrull et al., 2018)</ref> to handle multiple edge types; <ref type="bibr" target="#b48">Xu et al. (2021)</ref> can be viewed as a generalization of this approach to point cloud analysis. In this work we are solving a different problem to these works: we are interested in designing efficient architectures, rather than new ways to handle edge information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BOOSTING REPRESENTATIONAL CAPACITY</head><p>Recent work by <ref type="bibr" target="#b14">Corso et al. (2020)</ref> has shown that using only a single aggregator is sub-optimal: instead, it is better to combine several different aggregators. In Equation <ref type="formula" target="#formula_5">(2)</ref> we defined our layer to use only symmetric normalization. To improve performance, we propose applying different aggregators to the representations calculated by ? b x (j) . The choice of aggregators could include different variants of summation aggregators e.g. mean or unweighted addition, as opposed to symmetric normalization that was proposed in the previous section. Alternatively, we can use aggregators such as stddev, min or max which are not based on summation. It is also possible to use directional aggregators proposed by <ref type="bibr" target="#b2">Beaini et al. (2021)</ref>, however this enhancement is orthogonal to this work. If we have a set of aggregators A, we can extend Equation <ref type="formula" target="#formula_5">(2)</ref> to obtain our EGC-M layer:</p><formula xml:id="formula_6">y (i) = H h=1 ??A B b=1 w (i) h,?,b j?N (i)?{i} ? b x (j)<label>(3)</label></formula><p>where ? is an aggregator. With this formulation, we are reusing the same messages we have calculated as before-but we are applying several aggregation functions to them at the same time.</p><p>Aggregator Fusion It would appear that adding more aggregators would cause latency and memory consumption to grow linearly. However, this is not true in practice. Firstly, since sparse operations are typically memory bound in practice, we can apply extra aggregators to data that has already arrived from memory with little latency penalty. EGC can also efficiently inline the nodewise weighting operation at inference time, thereby resulting in relatively little memory consumption overhead. The equivalent optimization is more difficult to apply successfully to PNA due to the larger number of operations per-node that must be performed during aggregation, caused by scaling functions being applied to each aggregation, before all results are concatenated and a transformation applied. More details, including profiling and latency measurements, can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INTERPRETATION AND BENEFITS</head><p>This section will explain our design choices, and why they are better suited to the hardware. We emphasize that our approach does not directly correspond to attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SPATIAL INTERPRETATION: NODE-WISE WEIGHT MATRICES</head><p>In our approach, each node effectively has its own weight matrix. We can derive this by re-arranging eq. (2) by factorizing the ? b terms out of inner sum:</p><formula xml:id="formula_7">y (i) = H h=1 ? (i) h Varying per Node ? ? j?N (i)?{i} 1 deg(i)deg(j) x (j) ? ? Computable via SpMM<label>(4)</label></formula><p>In contrast, GAT shares weights, and pushes complexity into the message calculation phase by calculating per-message weightings. MPNN <ref type="bibr" target="#b18">(Gilmer et al., 2017)</ref> and PNA <ref type="bibr" target="#b14">(Corso et al., 2020)</ref> further increase complexity by explicitly calculating each message-resulting in substantial latency overhead due to the number of dense operations increasing by roughly |E| |V | . Specifically, we have:</p><formula xml:id="formula_8">y (i) GAT = H h=1 ? Shared Weights ? ? j?N (i)?{i} ? h,i,j x (j) ? ? Calculated Message Weighting y (i) PNA = U (x (i) , j?N (i) M (x (i) , x (j) ) Explicit Message Calculation )</formula><p>From an efficiency perspective we observe that our approach of using SpMM has better characteristics due to it requiring only O(V ) memory consumption-no messages must be explicitly materialized to use SpMM. We note that although it is possible to propagate messages for GAT with SpMM, there is no way to avoid storing the weightings during training as they are needed for backpropagation, resulting in O(E) memory consumption. We also note that fusing the message and aggregation steps for certain architectures may be possible at inference time, but this is a difficult pattern for hardware accelerators to optimize for.</p><p>Relation To Attention Our method is not directly related to attention, which relies upon pairwise similarity mechanisms, and hence results in a O(E) cost when using the common formulations. Alternatives to attention-based Transformers proposed by <ref type="bibr" target="#b45">Wu et al. (2019a)</ref> are a closer analogue to our technique, but rely upon explicit prediction of the per-timestep weight matrix. This approach is not viable for graphs, as the neighborhood size is not constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SPECTRAL INTERPRETATION: LOCALISED SPECTRAL FILTERING</head><p>We can also interpret our EGC-S layer through the lens of graph signal processing <ref type="bibr" target="#b36">(Sandryhaila &amp; Moura, 2013)</ref>. Many modern graph neural networks build on the observation that the convolution operation for the Euclidean domain when generalised to the graph domain has strong inductive biases: it respects the structure of the domain and preserves the locality of features by being an operation localised in space. Our method can be viewed as a method of building adaptive filters for the graph domain. Adaptive filters are a common approach when signal or noise characteristics vary with time or space; for example, they are commonly applied in adaptive noise cancellation. Our approach can be viewed as constructing adaptive filters by linearly combining learnable filter banks with spatially varying coefficients.</p><p>The graph convolution operation is typically defined on the spectral domain as filtering the input signal x ? R N on a graph with N nodes with a filter g ? parameterized by ?. This requires translating between the spectral and spatial domain using the Fourier transform. As on the Euclidean domain, the Fourier transform on the graph-domain is defined as the basis decomposition with the orthogonal eigenbasis of the Laplace operator, which for a graph with adjacency matrix</p><formula xml:id="formula_9">A ? R N ?N is defined as L = D?A, where D is the diagonal degree matrix with D ii = N j=1 A ij . The Fourier transform of a signal x ? R N then is F(x) = U x, where L = U?U , with orthogonal eigenvector-matrix U ? R N ?N and diagonal eigenvalue-matrix ? ? R N ?N . The result of a signal x filtered by g ? is y = g ? (L)x = Ug ? (?)U x</formula><p>where the second equality holds if the Taylor expansion of g ? exists.</p><p>Our approach corresponds to learning multiple filters and computing a linear combination of the resulting filters with weights depending on the attributes of each node locally. The model therefore allows applying multiple filters for each node, enabling us to obtain a spatially-varying frequency response, while staying far below O(E) in computational complexity. Using a linear combination of filters, the filtered signal becomes y =</p><formula xml:id="formula_10">B b=1 w b g ? b (L)x, where w b ? R N</formula><p>are the weights of filter b for each of the N nodes in the graph. If we parameterize our filter using first-order Chebyshev polynomials as used by Kipf &amp; Welling (2017) our final expression for the filtered signal becomes</p><formula xml:id="formula_11">Y = B b=1 w b (D ? 1 2?D ? 1 2 )X? b , where? = A + I N</formula><p>is the adjacency matrix with added self-loops andD is the diagonal degree matrix of? as defined earlier. This justifies the symmetric normalization aggregator we chose in Equation <ref type="formula" target="#formula_5">(2)</ref>. <ref type="bibr" target="#b11">Cheng et al. (2021)</ref> proposed an approach for localized filtering. However, their approach does not generalize to unseen topologies or scale to large graphs as it requires learning the coefficients of several filter matrices S k of size N ? N . Our approach does not suffer from these constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>Propagation Rule Memory</p><formula xml:id="formula_12">ZINC (MAE ?) CIFAR (Acc. ?) MolHIV (ROC-AUC ?) Code-V2 (F1 ?) Arxiv (Acc. ?) Unseen Graph Unseen Graph Unseen Graph Unseen Graph Transductive Node Regression Classification Classification Classification Classification GCN y (i) = ? j 1 ? deg(i)deg(j) x (j) O(V )</formula><p>0.459 ? 0.006 55.71 ? 0.38 76.14 ? 1.29 0.1480 ? 0.0018 71.92 ? 0.21</p><formula xml:id="formula_13">GIN y (i) = f?[(1 + )x (i) + j x (j) ] O(V )</formula><p>0.387 ? 0.015 55.26 ? 1.53 76.02 ? 1.35 0.1481 ? 0.0027 67.33 ? 1.47</p><p>GraphSAGE</p><formula xml:id="formula_14">y (i) = ?1x (i) + j ?2x (j) O(V )</formula><p>0.468 ? 0.003 65.77 ? 0.31 75.97 ? 1.69 0.1453 ? 0.0028 71.73 ? 0.26</p><formula xml:id="formula_15">GAT y (i) = ?i,i?x (i) + j ?i,j?x (j) O(E)</formula><p>0.475 ? 0.007 64.22 ? 0.46 77.17 ? 1.37 0.1513 ? 0.0011 * 71.81 ? 0.23</p><p>GATv2</p><formula xml:id="formula_16">y (i) = ?i,i?x (i) + j ?i,j?x (j) O(E)</formula><p>0.447 ? 0.015 67.48 ? 0.53 77.15 ? 1.55 0.1537 ? 0.0022 * 71.87 ? 0.43</p><formula xml:id="formula_17">MPNN-Sum y (i) = U (x (i) , j M (x (i) , x (j) )) O(E)</formula><p>0.381 ? 0.005 65.39 ? 0.47 75.19 ? 3.57 0.1470 ? 0.0017 * 66.11 ? 0.56</p><formula xml:id="formula_18">MPNN-Max y (i) = U (x (i) , maxjM (x (i) , x (j) )) O(E)</formula><p>0.468 ? 0.002 69.70 ? 0.55 77.07 ? 1.37 0.1552 ? 0.0022 * 71.02 ? 0.21 </p><formula xml:id="formula_19">PNA y (i) = U (x (i) , j M (x (i) , x (j) )) O<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PROTOCOL</head><p>We primarily evaluate our approach on 5 datasets taken from recent works on GNN benchmarking. We use ZINC and CIFAR-10 Superpixels from Dwivedi et al. <ref type="formula" target="#formula_5">(2020)</ref> and Arxiv, MolHIV and Code from Open Graph Benchmark . These datasets cover a wide range of domains, cover both transductive and inductive tasks, and are larger than datasets which are typically used in GNN works. We use evaluation metrics and splits specified by these papers. Baseline architectures chosen reflect popular general-purpose choices <ref type="bibr" target="#b26">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b47">Xu et al., 2019;</ref><ref type="bibr" target="#b21">Hamilton et al., 2017;</ref><ref type="bibr" target="#b44">Veli?kovi? et al., 2018;</ref><ref type="bibr" target="#b18">Gilmer et al., 2017)</ref>, along with the state-of-the-art PNA <ref type="bibr" target="#b14">(Corso et al., 2020)</ref> and GATv2 <ref type="bibr" target="#b4">(Brody et al., 2022)</ref> architectures.</p><p>In order to provide a fair comparison we standardize all parameter counts, architectures and optimizers in our experiments. All experiments were run using Adam <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2014)</ref>. Further details on how we ensured a fair evaluation can be found in the appendix.</p><p>We do not use edge features in our experiments as for most baseline architectures there exist no standard method to incorporate them. We do not use sampling, which, as explained in Section 2.2, is ineffective for 4 datasets; for the remaining dataset, Arxiv, we believe it is not in the scientific interest to introduce an additional variable. This also applies to GraphSAGE, where we do not use the commonly applied neighborhood sampling. All experiments were run 10 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MAIN RESULTS</head><p>Our results across the 5 tasks are shown in <ref type="table" target="#tab_0">Table 1</ref>. We draw attention to the following observations:</p><p>? EGC-S is competitive with anisotropic approaches. We outperform GAT(v1) and MPNN-Sum on all benchmarks, despite our resource efficiency. The clearest exception is MPNN-Max on CIFAR &amp; Code, where the max aggregator provides a stronger inductive bias. We observe that GATv2 improves upon GAT, but does not clearly outperform EGC.</p><p>? EGC-M outperforms PNA. The addition of multiple aggregator functions improves performance of EGC to beyond that obtained by PNA. We hypothesize that our improved performance over PNA is related to PNA's reliance on multiple degree-scaling transforms. While this approach can boost the representational power of the architecture, we believe that it can result in a tendency to overfit to the training set.</p><p>? EGC performs strongly without running out of memory. We observe that EGC is one of only three architectures that did not exhaust the VRAM of the popular Nvidia 1080/2080Ti GPUs, with 11GB VRAM, when applied to Arxiv: we had to use an RTX 8000 GPU with 48GB VRAM to run these experiments. PNA, our closest competing technique accuracywise, exhausted memory on the Code benchmark as well. Detailed memory consumption figures are provided in <ref type="table">Table 4</ref>.  <ref type="figure">Figure 3</ref>: Study over the number of heads (H) and bases <ref type="bibr">(B)</ref>. Study run on ZINC dataset with EGC-S. Metric is MAE (mean ? standard deviation): lower is better. We study keeping the total parameter count constant, and fixing the hidden dimension. Each experiment was tuned individually. Setting B &gt; H does not necessarily improve performance due to the risk of overfitting, and forces the usage of a smaller hidden dimension to retain a constant parameter count.</p><p>Overall, EGC obtains the best performance on 4 out of the 5 main datasets; on the remaining dataset (MolHIV), EGC is the second best architecture. This represents a significant achievement: our architecture demonstrates that we do not need to choose between efficiency and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ADDITIONAL STUDIES</head><p>How Should Heads and Bases be Chosen? To understand the trade-off between the number of heads (H) and bases (B), we ran an ablation study on ZINC using EGC-S; this in shown in <ref type="figure">Figure 3</ref>.</p><p>The relationship between these parameters is non-trivial. There are several aspects to consider: (1) increasing H and B means that we spend more of our parameter budget to create the combinations, which reduces hidden dimension-as shown in <ref type="figure">Figure 3</ref>. This is exacerbated if we use multiple aggregators: our combination dimension must be HB|A|.</p><p>(2) Increasing B means we must reduce the hidden size substantially, since it corresponds to adding more weights of size F H ? F . (3) Increasing H allows us to increase hidden size, since each basis weight becomes smaller. We see in <ref type="figure">Figure 3</ref> that increasing B beyond H does not yield significant performance improvements: we conjecture that bases begin specializing for individual heads; by sharing, there is a regularizing effect, like observed in <ref type="bibr" target="#b38">Schlichtkrull et al. (2018)</ref>. This regularization stabilizes the optimization and we observe lower trial variance for smaller B.</p><p>We advise B = H or B = H 2 . We find H = 8 to be effective with EGC-S; for EGC-M, where more parameters are spent on combination weights, we advise setting H = 4. This convention is applied consistently for <ref type="table" target="#tab_0">Table 1</ref>; full details are supplied in the appendix and code.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Should The Combination Weightings (w) Be Activated?</head><p>Any activation function will shrink the space the per-node weights ? (i) h can lie in, hence we would expect it to harm performance; this is verified in <ref type="table" target="#tab_3">Table 2</ref>. Activating w may improve training stability, but we did not observe to be an issue in our experiments. Another issue is that different aggregators result in outputs with different means and variances <ref type="bibr" target="#b40">(Tailor et al., 2021)</ref>, hence they need to be scaled by different factors to be combined.</p><p>Applying EGC to Large-Scale Heterogeneous Graphs We evaluated EGC on the heterogeneous OGB-MAG dataset, containing 2M nodes and 21M edges. On a homogeneous version of the graph, we exceed the baselines' performance by 1.5-2%; we experiment with both symmetric normalization (EGC-S) and the mean aggregators to demonstrate that the mechanism utilized by EGC is effective, regardless of which aggregator provides the stronger inductive bias for the dataset. Our architecture can be expanded to handle different edge types, yielding the R-EGC architecture which improves performance over R-GCN by 0.9%. We expect that accuracy can be further improved by using sampling techniques to regularize the optimization, or using pretrained embeddings; however, adding these techniques makes comparing the results more difficult as it is known that sampling techniques can affect each architecture's performance differently .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">MEMORY AND LATENCY BENCHMARKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test   <ref type="table">Table 4</ref>: Memory and latency statistics for parameter-normalized models (used in table 1) on Arxiv. Note that EGC-M memory consumption and latency can be reduced with aggregator fusion at inference time.</p><p>We now assess our model's resource efficiency. For CPU measurements we used an Intel Xeon Gold 5218 and for GPU we used an Nvidia RTX 8000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregator Fusion</head><p>We evaluated aggregator fusion across several topologies on both CPU and GPU. For space reasons, we leave full details to Appendix D. In summary, we observe that using 3 aggregators over standard SpMM incurs an additional overhead of only 14%, enabling us to improve model performance without excessive computational overheads at inference time.</p><p>End-to-End Latency We provide latency and memory statistics for parameter-normalized models in <ref type="table">Table 4</ref>. We draw attention to how slow and memory-intensive the O(E) models are for training and inference. The reader should note that the inference latency and memory consumption for MPNN and PNA rises by 6-7? relative to EGC, corresponding to the large |E| |V | ratio for Arxiv. EGC-M offers substantially lower latency and memory consumption than its nearest competitor, PNA, however the precise benefit will be dataset-dependent. Extended results can be found in Appendix E, including results for accuracy-normalized models on Arxiv, which further demonstrate EGC's resource efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND CONCLUSION</head><p>How Surprising Are Our Results? We observed that it was possible to design an isotropic GNN that is competitive with state-of-the-art anisotropic GNN models on 6 benchmarks. This result contradicts common wisdom in the GNN community. However, our results may be viewed as part of a pattern visible across the ML community: both the NLP and computer vision communities have seen works indicating that anisotropy offered by Transformers may not be necessary <ref type="bibr">(Tay et al., 2021;</ref><ref type="bibr" target="#b29">Liu et al., 2022)</ref>, consistent with our observations. It is worth asking why we observe our results, given that they contradict properties that have been theoretically proven. We believe that there are a variety of reasons, but the most important is that most real world datasets do not require the theoretical power these more expressive models provide to achieve good results. In particular, many real world datasets are homophilous: therefore simplistic approaches, such as EGC, can achieve high performance. This implies that the community should consider adding more difficult datasets to standard benchmarks, such as those presented in <ref type="bibr" target="#b28">Lim et al. (2021)</ref> and .</p><p>Our proposed layer, EGC, can be used as a drop-in replacement for existing GNN layers, and achieves better results across 6 benchmark datasets compared to strong baselines, with substantially lower resource consumption. Our work raises important questions for the research community, while offering significant practical benefits with regard to resource consumption. We believe the next step for our work is incorporation of edge features e.g. through line graphs <ref type="bibr" target="#b10">(Chen et al., 2020b)</ref> or topological message passing <ref type="bibr" target="#b3">(Bodnar et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Propagation Rule Memory Notes <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref> </p><formula xml:id="formula_20">y (i) = ? j?N (i)?{i} 1 ? deg(i)deg(j) x (j) O(V )</formula><p>Formally defined for undirected graphs with self-loops; motivated by graph signal processing.</p><p>GIN <ref type="bibr" target="#b47">(Xu et al., 2019)</ref> </p><formula xml:id="formula_21">y (i) = f?[(1 + )x (i) + j?N (i) x (j) ] O(V )</formula><p>f is a learnable function, typically parameterized as an MLP or linear layer; may be fixed or learned.</p><p>GraphSAGE <ref type="bibr" target="#b21">(Hamilton et al., 2017)</ref> </p><formula xml:id="formula_22">y (i) = ?1x (i) + j?N (i) ?2x (j) O(V )</formula><p>typically parameterized as mean or max.</p><p>GAT <ref type="bibr" target="#b44">(Veli?kovi? et al., 2018)</ref> </p><formula xml:id="formula_23">y (i) = ?i,i?x (i) + j?N (i) ?i,j?x (j) O(E)</formula><p>Attention coefficients calculated using:</p><formula xml:id="formula_24">?i,j = exp(LeakyReLU(a [?x (i) ?x (j) ])) k?N (i)?{i} exp(LeakyReLU(a [?x (i) ?x (k) ])) .</formula><p>Common to define multiple attention heads and concatenate.</p><p>GATv2 <ref type="bibr" target="#b4">(Brody et al., 2022)</ref>  </p><formula xml:id="formula_25">y (i) = ?i,i?x (i) + j?N (i) ?i,j?x (j) O(E)<label>Similar</label></formula><formula xml:id="formula_26">y (i) = U (x (i) , j?N (i) M (x (i) , x (j) , eij))</formula><p>O(E) U, M typically defined as linear layers acting on concatenated features; may be any valid aggregator, typically sum or max.</p><p>PNA <ref type="bibr" target="#b14">(Corso et al., 2020)</ref> </p><formula xml:id="formula_27">y (i) = U (x (i) , j?N (i) M (x (i) , x (j) , eij))</formula><p>O(E) Similar to MPNN, but with defined to use 4 aggregators (mean, standard deviation, max, and min) scaled by 3 different functions of node degree, resulting in 12 different aggregations by default. <ref type="table">Table 5</ref>: Propagation rules for general-purpose GNN architectures we compare against in this work; rules are provided using node-wise formulations. We evaluate against popular architectures, and a recent proposal that has achieved state-of-the-art performance, PNA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A FURTHER EXPERIMENT DETAILS A.1 ENSURING FAIRNESS</head><p>We expand on our experimental protocol, with a particular focus on describing measures that we took to ensure that the results we report are not unfairly biased towards EGC.</p><p>For EGC-S, we use H = 8 and B = 4, as implied by our ablation for all experiments, with the single exception of OGB-Code, where we use H = B = 8. The benefit of using a smaller set of bases is that we can increase the hidden dimension, but this is not viable in this case since most of the 11M parameters in the model correspond to the token read-out layers, which quickly increases as the model hidden dimension grows. As shown by section 5.3, if we cannot increase the hidden dimension, it is better to increase the bases. This is the only exception we make.</p><p>For EGC-M, we use H = B = 4 for all experiments. The main challenge is aggregator selection, and this remains a major challenge for our work. We were unable to find a satisfactory technique for automated discovery of aggregator choices, hence we rely on heuristics to find them. We restrict ourselves to use 3 aggregators for each model (yielding 35 possible choices). In order to determine the aggregators, we use two heuristics: (1) aggregators should be "diverse" and (2) aggregators should be chosen based on inductive bias for task. Using these two rules, we try up to 3 possible choices of aggregators; all choices considered are shown in <ref type="table">Table 6</ref>. We note that while some choices do improve performance, our conclusions are not invalidated; it is also worth noting that it is likely that better aggregator choices can be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 CLUSTER DETAILS</head><p>Most of our experiments were run on several machines in our SLURM cluster using Intel CPUs and NVIDIA GPUs. Each machine was running Ubuntu 18.04. The GPU models in our cluster were RTX 2080Ti and GTX 1080Ti. High-memory experiments were run on V100s in our cluster and an RTX 8000 virtual machine we had access to. 0.1578 ? 0.0021 <ref type="table">Table 6</ref>: Possible aggregators tried for EGC-M. Up to 3 combinations (from a possible 35) were tried, as we limited ourselves to always using 3 aggregators. Our conclusions do not change, and it is likely that better results can be found with more optimal aggregator choices.  <ref type="table">Table 7</ref>: Results of applying graph-augmented MLPs (GA-MLPs) to tasks requiring generalization to unseen graphs. We see that the performance is broadly similar to the corresponding GCN modeland far weaker than EGC-S, which uses the same aggregator. We also considered using the add aggregation on ZINC, and achieved 0.444 ? 0.019. By comparison, the equivalent GIN model (which uses the add aggregation) achieved 0.387 ? 0.015.</p><formula xml:id="formula_28">Model ZINC (MAE ?) CIFAR (Acc. ?) MolHIV (ROC-AUC ?) Code-V2 (F1 ?) GA-MLP 0.510 ? 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B LIMITATIONS OF EXISTING APPROACHES</head><p>As explained in the related work, existing approaches to improving GNN efficiency have severe limitations. In this section we elaborate upon them, and provide experimental evidence where necessary.</p><p>We first examine graph-augmented MLPs (GA-MLPs); to our knowledge there are few experimental results assessing the performance of these models when they are applied to problems requiring generalization to unseen graphs. In the literature they are generally applied to large scale node classification benchmarks, such as those found in the OGB benchmark suite <ref type="bibr" target="#b46">(Wu et al., 2019b;</ref><ref type="bibr" target="#b34">Rossi et al., 2020)</ref>. It is known that these models are theoretically less expressive than standard GNNs, however their performance when applied to node classification datasets has been acceptable.</p><p>We consider models of the form:</p><formula xml:id="formula_29">Y = Readout(MLP( 4 k=0 S k XW))<label>(5)</label></formula><p>We use up to the 4th power of the diffusion operator S to emulate the depth of the corresponding GNNs, which all use 4 layers. We set S to use symmetric normalization, as used by GCN and EGC-S; we also consider setting S to the adjacency matrix on ZINC, to emulate the operations used by GIN. The results are provided in <ref type="table">Table 7</ref>. We see that the GA-MLP models offer similar (but often worse) performance than the corresponding GNN baselines. The models are not competitive with approaches such as GAT or MPNN, and is outperformed by a wide margin by EGC-S. Achieving</p><formula xml:id="formula_30">Experiment ZINC (MAE ?) CIFAR (Acc. ?) MolHIV (ROC-AUC ?) Code-V2 (F1 ?)</formula><p>EGC-S 0.364 ? 0.020 66.92 ? 0.37 77.44 ? 1.08 0.1528 ? 0.0025 EGC-S + DropEdge (p = 0.1) 0.468 ? 0.007 66.37 ? 0.28 77.41 ? 1.32 0.1553 ? 0.0021 EGC-S + DropEdge (p = 0.5) 0.629 ? 0.023 64.60 ? 0.58 75.33 ? 0.82 0.1527 ? 0.0019 EGC-S + GraphSAINT Node Sampler (p = 0.1) 0.631 ? 0.012 61.37 ? 0.75 73.65 ? 1.41 0.1461 ? 0.0027 <ref type="table">Table 8</ref>: Results of applying sampling approaches to EGC-S. We do not enable sampling at test time-hence these approaches do not offer any test time reductions to computation. Sampling, in principle, is applicable to any underlying GNN: our conclusions will transfer to other underlying GNNs. We see that DropEdge <ref type="bibr" target="#b33">(Rong et al., 2020)</ref> with a low drop probability (p = 0.1) can aid model performance; however, setting p this low does not significantly reduce memory consumption or computation. Increasing p to 0.5 does reduce resource consumption noticeably, but results in noticeable degradation to model performance.</p><p>state-of-the-art performance with GA-MLPs does not appear to be possible, at least with our current understanding of these models.</p><p>We now proceed to investigate sampling, in which each training step does not run over the entire graph, but over some sampled subgraph. These methods have seen great popularity when applied to tasks such as node classification, and they are able to deliver sampling ratios in excess of 20?, hence yielding noticeable improvements to memory consumption (the primary limitation with large scale training). However, we note that the benefit of these methods has not been examined carefully for many graph topologies; while they have been shown to be effective for many "small-world" graph topologies-which arise in many graphs-it is not the case that all graphs fall into this category. For example, molecule graphs would not fit this category.</p><p>We first assess the sampling strategy from GraphSAINT <ref type="bibr" target="#b52">(Zeng et al., 2019)</ref> by applying it to EGC-S models. For our experiments, we use the node-centric sampler. The results are presented in <ref type="table">Table 8</ref>; we disable sampling at test time. Even with a relatively low dropping probability of 10% (i.e. 90% of nodes are retained), the model performance degradation is severe. We note that the computational savings achieved are modest when using this drop probablility. We also observed similar results when using the edge-centric sampler from GraphSAINT.</p><p>We also attempted a different approach for sampling proposed by DropEdge <ref type="bibr" target="#b33">(Rong et al., 2020)</ref>; as before, we apply it to EGC-S models. In this simple scheme, elements of the adjacency matrix are dropped; this scheme was shown to be effective for training deep graph networks, but it is also useful for reducing the computational footprint of models, since it effectively reduces the number of messages that have to be computed. We also provide results in <ref type="table">Table 8</ref>. The results are significantly better than we observe when using GraphSAINT's sampling strategies: at 10% drop probabilities we even see an improvement in some cases, due to the regularization effect. However, once we increase the drop probability to levels where we would observe a noticeable reduction in computational demand, we observe that model performance declines. In summary, while DropEdge is a more effective strategy for sampling on many inductive tasks, it is not beneficial as a method to reduce the computational burden.</p><p>Finally, we discuss the limitations of other approaches proposed in the literature. Quantization is an approach that is typically applied at inference time; mixed-precision approaches can be applied at training time, however care must be taken for GNNs to avoid biasing the gradients <ref type="bibr" target="#b40">(Tailor et al., 2021)</ref>. Additionally, while neural architecture search (NAS) may be useful to find memory-efficient models if the search objective is set appropriately, they suffer from some limitations-primarily search time and memory consumption. Finally, approaches such as reversible residuals  are useful to architecture design, they do not tackle issues such as high peak memory usage induced by the message passing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C APPROACHES FOR HARDWARE ACCELERATION OF GNNS</head><p>The reader should note that most existing work for GNN hardware acceleration focuses on supporting only a subset of GNNs: specifically, they tend to only support models that can be implemented using SpMM. Approaches in the literature falling into this area include <ref type="bibr" target="#b9">Chen et al. (2020a)</ref>, <ref type="bibr" target="#b49">Yan et al. (2020)</ref>, <ref type="bibr" target="#b53">You et al. (2021)</ref> and <ref type="bibr" target="#b53">Zhang et al. (2021)</ref>. It is possible to add greater flexibility to the accelerator to support more expressive message passing schemes, however this necessarily implies greater complexity. As Amdahl's law <ref type="bibr" target="#b1">(Amdahl, 1967)</ref> implies, increasing flexibility is likely to reduce peak performance, while increasing silicon area requirements. Therefore, aiming for the simplest primitive (as we do with EGC) is the most sensible approach to obtain hardware acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D AGGREGATOR FUSION</head><p>Algorithm 1 Aggregator Fusion with aggregators A. This method is a modification of the Compressed Sparse Row (CSR) SpMM algorithm, where we maximize re-use of matrix B. Maximizing re-use enables us to obtain significantly better accuracy with minimal impact on memory and latency. For simplicity, pseudocode assumes H = B = 1. This version demonstrates how we can remove memory overheads at inference time. The naive approach of performing each aggregation sequentially would cause a linear increase in latency with respect to |A|. However, a key observation to note is that we are memory-bound: the bottleneck with sparse operations is waiting for the data to arrive from memory. This observation applies to both GPUs and CPUs, and justified through profiling. Using a profiler on a GTX 1080Ti we observed that SpMM using the Reddit graph <ref type="bibr" target="#b21">Hamilton et al. (2017)</ref> with feature sizes of 256 achieved just 1.2% of the GPU's peak FLOPS, with 88.5% of stalls being caused by unmet memory dependencies. The fastest processing order performs as much work as possible with data that has already been fetched from memory, rather than fetching it multiple times. This concept is illustrated in Algorithm 1 in the appendix. We can perform all aggregations as a lightweight modification to the standard compressed sparse row (CSR) SpMM algorithm.</p><formula xml:id="formula_31">Input: CSR A ? R N ?N , Dense B ? R N ?F , Combination weightings w ? R N ?|A| Output: Dense C ? R N ?F for i = 0 to A.</formula><p>The second observation we make is that storing the results of all aggregations is unnecessary at inference time. Note that the CSR SpMM algorithm processes each row in the output matrix sequentially: rather than storing the aggregations for every row, instead we should store only the weighted results. This approach not only reduces memory consumption, but also latency as we improve the effectiveness of our cache and reduce memory system contention. In practice, this optimization is especially important when performing inference on topologies which have more frequent periods where the processing has become compute-bound, since we reduce contention between load and store units Corporation. This is also demonstrated in Algorithm 1.</p><p>We evaluated aggregator fusion across four different topologies, on both CPU and GPU; our results can be found in <ref type="table">Table 9</ref>. We assumed all operations are 32-bit floating point, and that we were using three aggregators: summation-based, max, and min; these aggregators match those used for EGC-M Code. Our benchmarks were conducted on a batch of 10k graphs from the ZINC and Code datasets, Arxiv, and the popular Reddit dataset <ref type="bibr" target="#b21">(Hamilton et al., 2017)</ref>, which is one of the largest graph datasets commonly evaluated on in the GNN literature. Our SpMM implementation on GPU is based on <ref type="bibr" target="#b50">Yang et al. (2018)</ref>. Code for the kernels are provided in our repo.  <ref type="table">Table 9</ref>: Inference latency (mean and standard deviation) for CSR SpMM, used by GCN/GIN, and aggregator fusion. Assuming a feature dimension of 256 and H = B = 1 per Algorithm 1. We observe that aggregator fusion results in an increase of 34% in the worse case; in contrast, the naive implementation has a worst case increase of 466%. Also included are timings for dense multiplication with a square weight matrix; we observe that sparse operations dominate latency measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Train Epoch Time (s) Test Epoch Time (s) Peak Train Memory (MB) , we see that the trends we observed in for Arxiv broadly remain. It is worth noting again that EGC-M is far more efficient both latency and memory-wise than PNA.</p><p>As expected, our technique optimizing for input re-use achieves significantly lower inference latency than the naive approach to applying multiple aggregators. While the naive approach results in a mean increase in latency of 331%, our approach incurs a mean increase of only 14% relative to ordinary SpMM, used by GCN and GIN. The increase is topology dependent, with larger increases in latency being observed for topologies which are less memory-bound. We also provide timings for dense matrix multiplication (i.e. X?) to justify our focus on optimizing sparse operations in this work: CSR SpMM operation is 4.7? slower (geomean) than the corresponding weight multiplication. We believe further optimizations of the operations used by architecture are achievable through the use of auto-tuning frameworks e.g. TVM <ref type="bibr" target="#b8">(Chen et al., 2018b)</ref>, but this lies beyond the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E LATENCY AND MEMORY CONSUMPTION ON OTHER DATASETS</head><p>In the evaluation, we assessed the memory consumption and latency for the parameter-normalized models on Arxiv. In this section, we consider a similar exercise for OGB Code models. The results are provided in <ref type="table" target="#tab_0">Table 10</ref>. Our conclusions remain broadly similar, with EGC-M offering clear improvements to memory consumption, latency, and parameter efficiency relative to PNA. EGC-S is superior to GAT, with similar inference latency, better model performance, and noticeably lower memory consumption. We train models using batch size 128; the reader should note that the memory consumption figures can vary between runs (even for the same model), since graphs vary in the number of nodes and edges.</p><p>So far we have not demonstrated that our approach is more efficient than the baselines on Arxiv, which is the only dataset where EGC-M and PNA are not the best performing. To demonstrate that we are more efficient we must show that we achieve lower memory consumption and latency for a given accuracy-level-i.e. we must consider models that are accuracy-normalized. We evaluate increasing the parameter count for baseline models until they achieve the same accuracy as EGC-S;</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Visual representation of our EGC-S layer. In this visualization we have 3 basis filters (i.e. B = 3), which are combined using per-node weightings w. This simplified figure does not show the usage of heads, or multiple aggregators, as used by EGC-M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results (mean ? standard deviation) for parameter-normalized models run on 5 datasets. Details of the specific aggregators chosen per dataset and further experimental details can be found in the supplementary material. Results marked with * ran out of memory on 11GB 1080Ti and 2080Ti GPUs. EGC obtains best performance on 4 of the tasks, with consistently wide margins.</figDesc><table><row><cell></cell><cell></cell><cell>E)</cell><cell>0.320 ? 0.032 70.21 ? 0.15</cell><cell>79.05 ? 1.32</cell><cell cols="2">*  0.1570 ? 0.0032  *  71.21 ? 0.30</cell></row><row><cell>EGC-S (Ours)</cell><cell>Equation 2</cell><cell>O(V )</cell><cell>0.364 ? 0.020 66.92 ? 0.37</cell><cell>77.44 ? 1.08</cell><cell>0.1528 ? 0.0025</cell><cell>72.21 ? 0.17</cell></row><row><cell>EGC-M (Ours)</cell><cell>Equation 3</cell><cell>O(V )</cell><cell>0.281 ? 0.007 71.03 ? 0.42</cell><cell>78.18 ? 1.53</cell><cell>0.1595 ? 0.0019</cell><cell>71.96 ? 0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Activating the combination weightings w harms performance. Run on ZINC; lower is better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>EGC can be applied applied to large scale heterogeneous graphs, outperforming R-GCN by 0.9%.</figDesc><table><row><cell>Model</cell><cell>Peak Training</cell><cell>GPU Training</cell><cell>GPU Inference</cell></row><row><cell></cell><cell>Memory (MB)</cell><cell>Latency (ms)</cell><cell>Latency (ms)</cell></row><row><cell>GCN</cell><cell>1905</cell><cell>159.8 ? 4.6</cell><cell>35.2 ? 0.1</cell></row><row><cell>GIN</cell><cell>1756</cell><cell>155.2 ? 3.9</cell><cell>35.2 ? 0.1</cell></row><row><cell>GraphSAGE</cell><cell>1352</cell><cell>113.9 ? 5.4</cell><cell>25.1 ? 0.4</cell></row><row><cell>GAT</cell><cell>10841</cell><cell>324.3 ? 1.2</cell><cell>84.7 ? 0.3</cell></row><row><cell>GATv2</cell><cell>14124</cell><cell>341.8 ? 0.5</cell><cell>129.2 ? 0.2</cell></row><row><cell>MPNN-Sum</cell><cell>14323</cell><cell>768.2 ? 0.8</cell><cell>230.7 ? 0.3</cell></row><row><cell>MPNN-Max</cell><cell>14623</cell><cell>797.8 ? 0.9</cell><cell>258.2 ? 0.4</cell></row><row><cell>PNA</cell><cell>14533</cell><cell>892.7 ? 1.1</cell><cell>305.7 ? 0.5</cell></row><row><cell>EGC-S</cell><cell>2430</cell><cell>177.7 ? 2.2</cell><cell>37.3 ? 0.1</cell></row><row><cell>EGC-M</cell><cell>4068</cell><cell>220.9 ? 0.6</cell><cell>42.2 ? 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>rows ? 1 do for jj = A.row pointer[i] to A.row pointer[i + 1] do j = A.column index[jj] Init temp arrays of length F per aggregator a ij = A.values[jj] {May be faster to interleave these calls:} for ? ? A do process row ? (a ij , B[i, :], temp ? )</figDesc><table><row><cell>end for</cell></row><row><cell>end for</cell></row><row><cell>{Can be generalized to H, B &gt; 1:}</cell></row><row><cell>C[i, :] = ??A w[i, ?] ? temp ? [:] end for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>? 0.00 0.423 ? 0.023 0.055 ? 0.013 0.074 ? 0.010 2.36 ? 0.00 13.66 ? 0.12 1.74 ? 0.01 2.36 ? 0.02 CSR SpMM 29.08 ? 0.12 1.943 ? 0.040 0.760 ? 0.021 0.315 ? 0.006 186.44 ? 0.05 19.88 ? 0.10 5.56 ? 0.02 3.39 ? 0.01 Naive Fusion 88.25 ? 0.20 8.680 ? 0.094 2.631 ? 0.016 1.482 ? 0.010 595.91 ? 0.13 112.52 ? 0.22 23.81 ? 0.06 19.09 ? 0.05 + Faster Ordering 40.05 ? 0.10 4.592 ? 0.054 1.303 ? 0.037 0.772 ? 0.008 214.60 ? 0.10 29.38 ? 0.12 7.63 ? 0.03 4.78 ? 0.02 + Store Weighted Result Only 38.63 ? 0.22 1.752 ? 0.018 0.952 ? 0.026 0.278 ? 0.003 208.22 ? 0.13 26.64 ? 0.09 6.75 ? 0.03 4.38 ? 0.02</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CPU (Xeon Gold 5218)</cell><cell></cell><cell></cell><cell>GPU (RTX 8000)</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Reddit / s</cell><cell>Code / s</cell><cell>Arxiv / s</cell><cell>ZINC / s</cell><cell>Reddit / ms</cell><cell>Code / ms</cell><cell>Arxiv / ms</cell><cell>ZINC / ms</cell></row><row><cell>Weight Matmul</cell><cell>0.07</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Latency and memory results for parameter-normalized models on OGB Code-V2. Despite having a lower |E| |V | ratio of 2.75 relative to Arxiv (13.67)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Homophily means that if two nodes are connected, then they have high similarity</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the UK's Engineering and Physical Sciences Research Council (EP-SRC) with grant EP/S001530/1 (the MOA project) and the European Research Council (ERC) via the REDIAL project (Grant Agreement ID: 805194). FLO acknowledges funding from the Huawei Studentship at the Department of Computer Science and Technology of the University of Cambridge.</p><p>The authors would like to thank Ben Day, Javier Fernandez-Marques, Chaitanya Joshi, Titouan Parcollet, Petar Veli?kovi?, and the anonymous reviewers who have provided comments on earlier versions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>The method described in this paper is generic enough that it can be applied to any problem GNNs are applied to. The ethical concerns associated with this work are related to enabling more efficient training and deployment of GNNs. This may be positive (drug discovery) or negative (surveillance), but these concerns are inherent to any work investigating efficiency for machine learning systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>We have supplied the code required to regenerate our results, along with the hyperparameters required. In addition, we supply pre-trained models. Resources associated with this paper can be found at https://github.com/shyam196/egc.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 11</ref><p>: Latency and memory statistics for accuracy-normalized models on Arxiv. To achieve the same accuracy as EGC-S, we must boost the size of the baseline models. We observe that EGC-S offers noticeable reductions to both memory consumption and latency for a given accuracy level.</p><p>we also gave these models an extra advantage over our method by increasing the hyperparameter search budget. The results are shown in <ref type="table">Table 11</ref>, where we observe that EGC-S is more efficient once we are comparing models achieving the same accuracy.</p><p>The reader should note that GAT (but not GATv2) can be implemented to reduce memory consumption by noting that the left and right halves of the attention vector can be computed separately, and added together as appropriate. We use an optimized implementation of GAT for our experiments (from PyTorch Geometric); we refer the reader to the implementation for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F GENERALIZING TO HETEROGENEOUS GRAPHS</head><p>Our R-EGC model is similar to the baseline R-GCN model included in the OGB repository. The OGB model deviates from the standard definition of R-GCN <ref type="bibr" target="#b38">(Schlichtkrull et al., 2018</ref>) since it handles different node types, not just edge types. The baseline model has weights to generate messages for each relation-type, and a weight matrix to update each individual node type. This corresponds to:</p><p>where ? corresponds to the node type of node i, and R represents the set of relation types. Note that the mean aggregator is used.</p><p>We deviate from the baseline by using a single set of basis weights. Instead, we use a different weighting calculation layers (w (i) = ?x + b) per node and relation type.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Khademi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Validity of the single processor approach to achieving large scale computing capabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amdahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="page" from="483" to="485" />
		</imprint>
	</monogr>
	<note>spring joint computer conference</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Directional graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saro</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>L?tourneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Weisfeiler and lehman go topological: Message passing simplicial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><forename type="middle">Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Otter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Mont?far</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How attentive are graph attention networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaked</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=F72ximsx7C1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veli?kovi?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On graph neural networks versus graph-augmented {mlp}s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=tiqI7w64JG2" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tvm: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;18</title>
		<meeting>the 13th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;18<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="579" to="594" />
		</imprint>
	</monogr>
	<note>USENIX Association. ISBN 9781931971478</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Rubik: A Hierarchical Architecture for Efficient Graph Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abanti</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12495</idno>
		<idno>arXiv: 2009.12495</idno>
		<ptr target="http://arxiv.org/abs/2009.12495" />
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph convolution with low-rank learnable local filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Intel? 64 and ia-32 architectures optimization reference manual</title>
		<imprint/>
	</monogr>
	<note>Intel Corporation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Principal Neighbourhood Aggregation for Graph Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05718</idno>
		<idno>arXiv: 2004.05718</idno>
		<ptr target="http://arxiv.org/abs/2004.05718" />
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<idno>arXiv: 2003.00982</idno>
		<ptr target="http://arxiv.org/abs/2003.00982" />
	</analytic>
	<monogr>
		<title level="j">Benchmarking Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<ptr target="https://github.com/pyg-team/pytorch_geometric" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The accelerator wall: Limits of chip specialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shuo Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08366</idno>
		<title level="m">Pre-training code representations with data flow</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05343</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving the accuracy, scalability, and performance of graph neural networks with roc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="187" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<title level="m">New benchmarks for learning on nonhomophilous graphs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<title level="m">Trevor Darrell, and Saining Xie. A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bandit samplers for training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning meshbased simulation with graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03409</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09405</idno>
		<title level="m">Learning to simulate complex physics with graph networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1644" to="1656" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4938" to="4947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient processing of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Architecture</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="341" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Degree-Quant: Quantization-Aware Training for Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Tailor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Fernandez-Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jai</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Are pre-trained convolutions better than pre-trained transformers</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The clrs algorithmic reasoning benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><forename type="middle">Puigdom?nech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Dashevskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pay less attention with lightweight and dynamic convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<idno>arXiv: 1810.00826</idno>
		<ptr target="http://arxiv.org/abs/1810.00826" />
		<imprint>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<title level="m">Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">HyGCN: A GCN Accelerator with Hybrid Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongrui</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA47549.2020.00012</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2020-02" />
			<biblScope unit="page" from="2378" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Design principles for sparse matrix multiplication on the gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aydin</forename><surname>Buluc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Gcod: Graph convolutional network acceleration via dedicated algorithm and accelerator co-design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th IEEE International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>HPCA 2022</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">G-cos: Gnnaccelerator co-search towards both better accuracy and efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyan</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Probabilistic dual network architecture search on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mullins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateja</forename><surname>Jamnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
