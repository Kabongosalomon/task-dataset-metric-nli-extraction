<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Two-stage Deep Network for High Dynamic Range Image Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Sharif</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Rigel-IT</orgName>
								<address>
									<settlement>Bangladesh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rizwan</forename><forename type="middle">Ali</forename><surname>Naqvi</surname></persName>
							<email>rizwanali@sejong.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Sejong University</orgName>
								<address>
									<region>South Korea</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mithun</forename><surname>Biswas</surname></persName>
							<email>mithun.bishwash.cse@ulab.edu.bd</email>
							<affiliation key="aff0">
								<orgName type="department">Rigel-IT</orgName>
								<address>
									<settlement>Bangladesh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjun</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">FS Solution</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Two-stage Deep Network for High Dynamic Range Image Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mapping a single exposure low dynamic range (LDR) image into a high dynamic range (HDR) is considered among the most strenuous image to image translation tasks due to exposure-related missing information. This study tackles the challenges of single-shot LDR to HDR mapping by proposing a novel two-stage deep network. Notably, our proposed method aims to reconstruct an HDR image without knowing hardware information, including camera response function (CRF) and exposure settings. Therefore, we aim to perform image enhancement task like denoising, exposure correction, etc., in the first stage. Additionally, the second stage of our deep network learns tone mapping and bit-expansion from a convex set of data samples. The qualitative and quantitative comparisons demonstrate that the proposed method can outperform the existing LDR to HDR works with a marginal difference. Apart from that, we collected an LDR image dataset incorporating different camera systems. The evaluation with our collected realworld LDR images illustrates that the proposed method can reconstruct plausible HDR images without presenting any visual artefacts. Code available : https://github. com/sharif-apu/twostageHDR_NTIRE21. * Corresponding author image consider among the most prominent solution to address the shortcomings of its multi-shot counterparts. However, a single-shot HDR recovery always remains a challenging task as it aims to recover significantly higher pixelwise information than a legacy LDR image (i.e., 8-bit image) <ref type="bibr" target="#b8">[9]</ref>. Most notably, such LDR to HDR mapping has to incorporate dynamic bit-expansion, noise suppression, and estimation of CRF without having any additional information from the neighbour frames.</p><p>In the recent past, several methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14]</ref> have attempted to reconstruct HDR images from single-shot LDR input by leveraging the convolutional neural networks (CNNs). Typically, these deep methods learn to hallucinate the CRF and perform bit-expansion from a convex set of data samples <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9]</ref>. Notably, the hardware-related information, explicitly the CRF is proprietary property of the original equipment manufacturer (OEMs) and mostly remains undisclosed. Therefore, addressing the single-shot LDR to HDR mapping with a single-stage deep network with pre/post-processing operation can result in inaccurate CRF estimation along with quantization. Subsequently, such HDR mapping methods can end up with visual artefacts in real-world scenarios <ref type="bibr" target="#b20">[21]</ref>.</p><p>In this paper, we propose a two-stage learning-based deep method to tackle the challenging single-shot HDR reconstruction. The proposed method comprises a two-stage deep network and learns from a convex set of single-shot 8-bit LDR images to reconstruct 16-bit HDR images comprehensively (please see <ref type="figure">Fig. 1</ref>). Here, the first stage of the proposed method performs the basic enhancements task like exposure correction <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b4">5]</ref>, denoising <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">36]</ref>, etc., and the second stage recovers the 16-bit HDR image, including the tone mapping <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref>. Notably, we encouraged our network to directly learn to reconstruct HDR images without explicitly estimating hardware-related information like CRF and bit-expansion. Hence, our method incorporates a significantly simple training process and does not require any handcrafted processing. We studied our network with real-world LDR images to confirm the feasibility in unknown data samples.</p><p>Our contributions are as follows: 1 arXiv:2104.09386v1 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to numerous hardware limitations, digital cameras are susceptible to capture a limited range of luminance. Subsequently, such hardware deficiencies drive most standalone devices to capture over/under-exposed images with implausible perceptual quality <ref type="bibr" target="#b20">[21]</ref>. To counter such inevitable consequences, typically, digital camera leverage multiple LDR shoots with different exposure settings <ref type="bibr" target="#b6">[7]</ref>. Regrettably, such multi-shot LDR to HDR recovery is also far from the expectation and can incorporate limitations, including producing ghost artefacts in dynamic scenes captured with hand-held cameras <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Contrarily, recovering HDR images from a single-shot <ref type="figure">Figure 1</ref>: Single-shot LDR to HDR reconstruction obtained by the proposed two-stage deep method. The proposed network intends to map an 8-bit LDR input into a 16-bit HDR image. However, for better visualization, we normalized the reconstructed images and compared them with their inputs. In each pair, the top section illustrates the LDR input, and the bottom segment shows the corresponding HDR output.</p><p>? A two-stage deep network to reconstruct 16-bit HDR images from 8-bit LDR inputs.</p><p>? Comparison with state-of-the-art methods and outperform them in both objective and subjective measurement.</p><p>? Collection of an LDR image dataset and extensively study the proposed method's feasibility in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>LDR to HDR image reconstruction has been largely investigated in the last couple of years. The following subsection discusses some of the previous work on this topic, and for simplicity of the presentation, we categories those methods into learning and non-learning based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Non-learning Based Methods</head><p>Inverse tone-mapping <ref type="bibr" target="#b1">[2]</ref>, additionally known as Expansion operators(EOs), broadly used for LDR to HDR image reconstruction, has been studied for the last couple of decades. Nevertheless, this technique's difficulty persists as it lacks to produce details of the missing portion of the image. Hereabouts, concerning single image HDR reconstruction, we discuss some existing EOs techniques. EOs is commonly formulated mathematically as:</p><formula xml:id="formula_0">L e = f (L d ), wheref : [0, 255] ? R +<label>(1)</label></formula><p>Here, L e indicates the produced HDR content from LDR inputs, which is denoted as L d . f(.) indicates the expansion function, which takes LDR content as input.</p><p>Inverse tone mapping, along with global operators, mainly used in the early time of solving this LDR to HDR conversion problem. Landis <ref type="bibr" target="#b15">[16]</ref>, one of the earliest to solve this problem, used a linear function to all the images' pixels. A gamma function has been used in Bist et al. <ref type="bibr" target="#b2">[3]</ref> paper, where the gamma curve is defined with the help of the human visual system's characteristics. Maisa et al. <ref type="bibr" target="#b24">[25]</ref> proposed a global method that expands the content based on image properties determined by an image key. All the above methods are categorized as the global method <ref type="bibr" target="#b0">[1]</ref>.</p><p>An analytical method coupled with an expand map is typically applied in the local method to expand LDR content to HDR. A median-cut <ref type="bibr" target="#b5">[6]</ref> method was used in Banterle et al. <ref type="bibr" target="#b1">[2]</ref> paper to find the areas with high luminance.</p><p>Later they generated an expand map using an inverse operator to extend the luminance range in the high luminance areas. To maintain the contrast, Rempel et al. <ref type="bibr" target="#b29">[30]</ref> further used an expand map calculated by a gaussian filter and an edge-stopping function.</p><p>Some other methods were proposed to tackle this issue where user interaction was added in most of them. Didyk et al. <ref type="bibr" target="#b7">[8]</ref> used a semiautomatic classifier to detect the high luminance and other saturated areas. Wang et al. <ref type="bibr" target="#b37">[37]</ref> proposed an impainting-based method where textures are recovered by transferring details from the user's specific selected region. However, these above techniques solve LDR to HDR conversion problem and produce satisfactory outcomes only when well-behaved inputs are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning Based Methods</head><p>Learning-based image to image translation like image enhancement showed great promises in the past decade. Considering their success in different domains of image manipulation, recent LDR to HDR studies have incorporated deep learning in their respective solutions. In recent work, Endo et al. <ref type="bibr" target="#b9">[10]</ref> propose an auto-encoder to generate HDR images from multi-exposure LDR images. Lee et al. <ref type="bibr" target="#b17">[18]</ref> sequentially bracketed LDR exposures and utilized a CNN to reconstruct an HDR image. Later, Lee et al. <ref type="bibr" target="#b18">[19]</ref> proposed a recursive conditional generative adversarial network (GAN) <ref type="bibr" target="#b10">[11]</ref> and combined an L1-norm to reconstruct the HDR images. Yu-Lun et al. <ref type="bibr" target="#b20">[21]</ref> intended to learn reverse camera pipeline for HDR reconstruction from a single input. Notably, all of these deep methods incorporate complicated training manoeuvre and handcrafted pre/postprocessing operations.</p><p>Apart from these approaches, a few novel methods propose to learn LDR to HDR directly through a single-stage deep network. For example, Eilertsen et al. <ref type="bibr" target="#b8">[9]</ref> propose to utilize a U-Net architecture <ref type="bibr" target="#b31">[31]</ref> to estimate the overexposed region of an image and combines it with underexposed pixels of the LDR inputs. In another way, Marnerides et al. <ref type="bibr" target="#b23">[24]</ref> proposed a multi-branch CNN to extract features from the input LDR and fuse the output of each branch to expand the bit values of LDR images. Similarly, Zeeshan et al. <ref type="bibr" target="#b13">[14]</ref> proposed a recurrent neural network to learn single-shot LDR to HDR from training pairs. The existing straightforward deep networks learn CRF and bit-expansion with a single-stage network, which can easily misinterpret the reconstruction network to produce visual artefacts.</p><p>Unlike the existing works, the proposed method does not include any additional pre/post-processing operation. Our proposed method directly learns an 8-bit LDR to 16-bit HDR mapping with a novel deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The proposed method aims to recover 16-bit HDR images from single-shot LDR inputs. This section describes the process of network design, optimization, and implementation strategies in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Design</head><p>We consider the single-shot LDR to HDR formation as an image to image translation task. Therefore, the proposed deep network aims to recover 16-bit HDR images as F : I L ? I H . Where mapping function (F) learns to generate a 16-bit image (I H ) from an 8-bit LDR image (I L ) comprehensively from a convex set of training samples. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates the overview of the proposed method.</p><p>As <ref type="figure" target="#fig_0">Fig. 2</ref> depicts, the proposed method comprises a twostage deep network to map an input LDR input to an HDR image. The stages of the proposed deep method aim to perform as follows:</p><p>? Stage I: Learns basic operation like exposure correction, denoising, contrast correction, gamma correction, etc.</p><p>? Stage II: Learns tone mapping, bit-expansion, and recover 16-bit HDR images from the output of stage-I.</p><p>Stage-I design. Typically, the LDR images illustrate numerous shortcomings like over/under exposure, over/desaturation, sensor noises, etc. Stage-I of the proposed method aims to perform such image enhancement tasks before reconstructing the HDR images. Here, the network maps the input LDR input (I L ) as I H ? [0, M ] H?W ?3 . Here, H and W represent the height and width of I H . The maximum value of M can be perceived as M = 255. However, we normalized the value of M by dividing 255 to accelerate the training process. We design our stage-I as a stacked CNN and comprises a single convolutional operations (i.e., as input and output layer) with multiple Residual Dense Attention blocks (RDAB). To perceive a deeper architecture, we emphatically selected the frequency of RDAB in stage-I as RDAB as n = 2.</p><p>Stage-II design. Stage-II of the proposed method aims to reconstruct the final 16-bit HDR images by learning tone mapping and bit expansion. Here, it takes the output of the stage-I I H as input and maps it as I H ? [0, K] H?W ?3 . It is noteworthy that the output range of I H has been stored in a 16-bit image format. Therefore, the maximum value of K can be K = 65535. Apart from that, this stage shares a similar network architecture as its predecessor. However, due to reduce the trainable parameter, we set the frequency of RDAB in stage-II as n = 1.</p><p>Residual Dense Attention Block. To accelerate our learning process, we develop a novel block combining a residual dense block <ref type="bibr" target="#b41">[41]</ref> and a spatial attention module <ref type="bibr" target="#b39">[39]</ref>, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Notably, the spatial attention modules in the newly developed RDAB allowed us to leverage spatial attention along with residual feature propagation to mitigate visual artefacts. For a given input X, an RDAB aims to output the feature map (X ) as:</p><formula xml:id="formula_1">X = R(X) + S(X)<label>(2)</label></formula><p>R(?) and S(?) present the function of residual dense attention block and spatial attention block. We added the output of S(?) along with R(?) to learn a long-distance feature inter-dependency while performing HDR mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization</head><p>The stages of the proposed method have been optimized with dedicated loss functions. Based on their dedicated role, we set the objective functions to maximize the performance.</p><p>Stage-I optimization. Typically, the deep networks have to employ a reconstruction loss to minimize the objective loss <ref type="bibr" target="#b33">[33]</ref>. This study utilizes an L1-norm as a base reconstruction loss <ref type="bibr" target="#b32">[32]</ref>, which can be derived as follows:</p><formula xml:id="formula_2">L R1 = I G8 ? I H 1<label>(3)</label></formula><p>Here, I H and I G8 present the output obtained from stage-I and reference 8-bit image.</p><p>Due to the presence of extensive sensor noises in the LDR inputs, the generated images through the deep model can suffer from structural distortion. To avoid such unexpected structural degradation, we leveraged an SSIM loss <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b42">42]</ref> as structure loss and derived as follow:  We used a multi-scale variant of SSIM-loss during training.</p><formula xml:id="formula_3">L S = SSIM I G8 , I H<label>(4)</label></formula><p>Apart from the L1 and SSIM loss, we utilized a GAN based loss in this study. Here, the GAN-based loss aims to improve the texture in the reconstructed images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">38]</ref> and derived as follows:</p><formula xml:id="formula_4">L G = ? t log D(I H , I G8 )<label>(5)</label></formula><p>The total loss of the stage-I can be derived as :</p><formula xml:id="formula_5">L S1 = L R1 + L S + 1e ? 4.L G<label>(6)</label></formula><p>Stage-II optimization. Similar to stage-I, we develop another dedicated loss function to maximize the performance of stage II. Here, the objective reconstruction loss of stage-II has obtained as follow:</p><formula xml:id="formula_6">L R2 = I G ? I H 1<label>(7)</label></formula><p>Here, I H and I G generated 16-bit HDR image and corresponding reference 16-bit image.</p><p>We combined a perceptual colour loss (PCL) <ref type="bibr" target="#b34">[34]</ref> along with the L1 loss to optimize stage-II. Here, the PCL aims to guide the network to avoid any colour degradation while mapping the given 8-bit images into a 16-bit HDR image <ref type="bibr" target="#b34">[34]</ref>. The PCL can be derived as follows:</p><formula xml:id="formula_7">L C = ?E I G , I H<label>(8)</label></formula><p>Here, ?E represents the CIEDE2000 colour difference between generated image and the reference image <ref type="bibr" target="#b21">[22]</ref>.</p><p>The total loss of stage-II can be summarized as follows:</p><formula xml:id="formula_8">L S2 = L R2 + L C<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>Both stages of the proposed method comprise a similar network architecture. The input layer of both stages aims to map an arbitrary image with a dimension of H ? W ? 3 into a feature map Z = H ? W ? 64, where H and W represent the height and width of the input image. Contrarily, each network's output layer generates images as I R * = H ? W ? 3. The convolution operations of stage-I and stage-II comprises a kernel= 3 ? 3, a stride=1, padding=1, and activated by a ReLU activation.</p><p>Apart from the stage-I and stage-II networks, the proposed method also utilizes a discriminator for estimating the adversarial loss. Here, we adopted a well-established variant of the generative adversarial network (GAN) known as conditional GAN (cGAN) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b19">20]</ref> to obtain a stable training phase. Our discriminator's goal has set to maximize E X,Y log D X, Y . The network comprises eight consecutive convolutional layers with a kernel size of 3 ? 3 and activated with a swish function. The feature depth of these convolutional layers has started from 64 channels. In every (2n ? 1) th layer, the architecture expands its feature depth and reduce the spatial dimension by a factor of 2. The final output of the discriminator obtained with another convolution operation comprising a kernel = 1 ? 1 and activated by a sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head><p>We perform dense experiments to study the feasibility of the proposed study in a different scenario. This section details the results obtained from the experiments for LDR to HDR reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>We studied our method with images from the HdM HDR dataset <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. The used dataset comprised a set of 1289 scenes (i.e., long, medium, short exposure LDR images, and 16-bit HDR ground-truth) captured with two Alexa Arri cameras. For this study, we used 1,000 image sets for training and the rest for the testing. We extracted a total of 7,551 image patches and made image sets for exploiting supervised training. Each patch set comprised randomly extracted images patches of LDR input, 16-bit and 8-bit ground truth images. It is worth noting, we obtained the 8-bit reference images by clipping and normalizing the 16bit ground truth images. <ref type="figure" target="#fig_2">Fig. 4</ref> depicts the sample image patches that we used extracted from the HdM HDR dataset, which we used for training only. Apart from that, we evaluated our method with higher resolution images in the later stages.</p><p>The proposed solution is implemented with the PyTorch framework <ref type="bibr" target="#b28">[29]</ref>. Additionally, the networks were optimized with an Adam optimizer <ref type="bibr" target="#b14">[15]</ref> , where the hyperparameters were tuned as ? 1 = 0.9, ? 2 = 0.99, and learning rate = 5e-4. We trained our model for 25 epochs with a constant batch size of 8. It took around 24 hours to converge our model. We conducted our experiments on a machine comprises of an AMD Ryzen 3200G central processing unit (CPU) clocked at 3.6 GHz, a random-access memory of 16 GB, and An Nvidia Geforce GTX 1060 (6GB) graphical processing unit (GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with state-of-the-art methods</head><p>We compared our methods with three different state-ofthe-art single-shot LDR to HDR works: i) HDRCNN <ref type="bibr" target="#b8">[9]</ref>, ii) ExpandNet <ref type="bibr" target="#b23">[24]</ref>, and iii) FHDR <ref type="bibr" target="#b13">[14]</ref>. It is worth noting, none of these methods has been specially designed for generating 16-bit HDR images, as we aim to learn in this study. However, to keep the evaluation process as fair as possible, we studied each state-of-the-art model with the same dataset we used to investigate our proposed method. We trained these single-shot HDR reconstruction networks with pairs of reference 16-bit images and input LDR images. Also, each method was studied with their suggested hyperparameters until they converge with the given data samples. We evaluated each deep method with the same testing samples and summarized the performance with peak-signal-tonoise-ratio (PSNR) and ?-PSNR metrics <ref type="bibr" target="#b27">[28]</ref>. Here, we compute the ?-PSNR as per the suggestions of <ref type="bibr" target="#b27">[28]</ref> and employed a compression factor ? = 5000, normalizing percentile = 99, and a tanh function for maintaining the [0, 1] range.  <ref type="table">Table 1</ref>: Quantitative comparison between the proposed method and existing learning-based single-shot LDR to HDR methods. The proposed method outperforms the stateof-the-art methods in both evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Quantitative evaluation. <ref type="table">Table 1</ref> illustrates the quantitative comparisons between the deep methods. It can be seen that our two-stage HDR reconstruction method outperforms the existing deep methods in both evaluation metrics with a marginal score. It scores 34.29 dB in PSNR and 32.66 dB in ?-PSNR metrics, which is almost 3dB and 8dB higher in PSNR and ?-PSNR metrics than the lowest-performing deep network (i.e., HDRCNN <ref type="bibr" target="#b8">[9]</ref>). It LDR (input) HDRCNN <ref type="bibr" target="#b8">[9]</ref> FHDR <ref type="bibr" target="#b13">[14]</ref> ExpandNet <ref type="bibr" target="#b23">[24]</ref> Our <ref type="figure">Figure 5</ref>: Quantitative comparison between proposed method and existing learning-based single-shot LDR to HDR methods.</p><p>is worth noting the HDRCNN [9] model leverage a VGG-16 backbone <ref type="bibr" target="#b35">[35]</ref> in its architecture. Typically, such pretrained VGG-16 backbone networks aim to enhance the details while performing any image to image translations task. We found that the VGG-16 backbone of HDRCNN boosts the sensor noise of LDR inputs while detail enhancement. Also, the 16-bit expansion boosts up these noises further in final reconstruction and resist the HDRCNN to perform a satisfactory performance as its counterparts.</p><p>Qualitative comparison. Apart from the quantitative comparison, we perform a qualitative evaluation to perform the subjective measurement between the different singleshot LDR to HDR reconstruction methods. <ref type="figure">Fig. 5</ref> illustrates reconstructed HDR images obtained through the different deep models. We normalized and clipped the 16bit HDR outputs for better visualization. The visual comparison grasps the consistency of quantitative comparison. Moreover, our two-stage deep method reconstructs cleaner HDR images with natural colour consistency. It maintains the details in the complicated overexposed regions comparing to its counterparts. Overall, the proposed method can recover plausible HDR image from an LDR input without producing any visually disturbing artefacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We studied the feasibility and the contribution of our two-stage method with sophisticated experiments. Specifically, we trained and evaluated our stages separately to verify the feasibility of a two-stage model for LDR to HDR reconstruction. Here, we used challenging single-shot LDR images from the HdM HDR dataset to perform the quantitative and qualitative evaluation.</p><p>Quantitative evaluation. <ref type="table">Table.</ref> 2 illustrates the performance of each stage of the proposed method on the HdM HDR dataset. Here, the PSNR and ?-PSNR calculated over 289 image pairs. We arbitrarily selected an LDR image from the three exposure shoots and paired it with the  <ref type="table">Table 2</ref>: Ablation study of the proposed method. We performed a quantitative evaluation with images from the HdM HDR dataset.</p><p>ground truth image for performing the evaluation. The ablation study illustrates that each stage of the proposed method contributes to the final HDR reconstruction. The individual stages of the proposed method can not achieve the height in evaluation metrics as their two-stage variants. We observed a tendency of underfitting in one-stage variants due to the significantly lesser number of trainable parameters (please see sec. 4.5 for detail). Qualitative evaluation. <ref type="figure">Fig. 6</ref> illustrates the visual comparison between different variants of the proposed study. Results have been visualized by applying a normalizing factor on 16-bit HDR images. It can be visible that the proposed two-stage model can reconstruct visually cleaner and plausible images among all models. Despite sharing similar network configurations, the single-stage networks struggle to reach the height of their two-stage variants. Particularly, estimating CRF, bit-expansion with image enhancement misinterpreted them to produce visual artefacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Method generalization</head><p>The key motivation of our proposed works is to obtain satisfactory results on diverse LDR images. Therefore, we studied the feasibility of our proposed method with a substantial amount of LDR samples captured with different hardware. To obtain this, we collected an LDR dataset incorporating numerous camera hardware, including DSLR (i.e., Canon Rebel T3i) and smartphone cameras (i.e., Sam-LDR (Input)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-I (Visualized)</head><p>Stage-II (Visualized) Stage-I + Stage-II (Visualized) <ref type="figure">Figure 6</ref>: Qualitative evaluation on different variants of the proposed method. The proposed two-stage variants can reconstruct better HDR images than its one-stage variants.</p><p>sung Galaxy Note 8, Xiaomi MI A3, iPhone 6s, etc.). We collected a total of 52 LDR images using these devices. Depending on the hardware types (i.e., DSLR or smartphone), we capture images by applying the following strategies:</p><p>? DSLR: To capture LDR images with DSLR, we mostly used auto exposure settings and captured a total of 25 LDR shots with such configuration. Notably, we choose stochastic lighting conditions like middy sun, low-light condition, high-contrast lighting condition, and sunset as shooting environments. Which allowed us to cover the most challenging shooting environments from real-world environments.</p><p>? Smartphone: Smartphone photography has gained significant popularity over the last decades. Therefore, we included images capture with different smartphone cameras in our LDR dataset. Typically, due to the shortcoming of smaller sensor size <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">34]</ref>, smartphone OEMs shipped their devices with the ability to produce HDR images. However, such default HDR settings do not fit well with our target applications. Thus, we used a third-party camera app known as Open Camera for capturing the LDR images with different smartphones. We disabled the HDR mode, including HDR contrast enhancement from the default settings of the application. Apart from that, we kept the exposure setting in auto mode and captured a total of 27 LDR images in tricky lighting conditions similar to the DSLR setup.</p><p>The collected images presented into a unified dataset and resampled into 2048 ? 1080 ? 3 resolution. Later, we inference the resampled LDR images with our proposed method and summarized the results with a visual and blind-fold user study.</p><p>Visual Results. <ref type="figure" target="#fig_3">Fig. 7</ref> depicts the real-world LDR to HDR mapping obtained by the proposed method. Our method maps an 8-bit LDR image into the 16-bit HDR image. However, due to better visualisation, we clipped and normalised the 16-bit images into an 8-bit format. Despite the clipping process, it can be observable that the proposed method can handle LDR images captured with the diverse camera hardware without explicitly knowing the CRF and exposure settings. Also, the proposed method does not require any additional pre/post-processing operations.</p><p>User study. A blind-fold user study has been performed to summarise the preferences of random users on our HDR reconstruction. We conducted the evaluation process on 50 users, where users were age between <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">60]</ref>. We showed ten random image pairs to each participating user, where image pairs comprised an LDR input and our reconstructed HDR image (clipped and normalized). Later, we allowed the users to pick an image from the image pairs as their personal preference. We conducted the user study anonymously, and the information related to the evaluation process remained secret to the participating users. We summarized the unbiased user opinion with a mean opinion score (MOS). <ref type="table">Table.</ref> 3 shows the MOS obtained by conducting our user preference study. The proposed single-shot HDR reconstruction method outperforms LDR images in blindfold testing by a substantial margin. Also, the user study reveals the feasibility of the proposed two-stage deep network in HDR image reconstruction for consumer-grade camera systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MOS ? LDR (Input) 1.2 HDR (Reconstructed) 3.8 <ref type="table">Table 3</ref>: A user study on LDR image(input) and HDR image(output). Higher MOS indicates better user preference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussion</head><p>The proposed method is developed to participate in NTIRE 2021 High Dynamic Range Challenge (Track 1 Single Frame) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. In the final competition, we secured the top five position with our fully convolutional solution. Our method scored 30.99 and 32.84 respectively in PSNR and ?-PSNR metrics <ref type="bibr" target="#b27">[28]</ref>. The proposed method comprises 834,476 trainable parameters (555,655 for stage-I and 278,821 for stage-II). Despite train with images patches, our model can be inference with any dimensioned images. Our model takes around 1.10 seconds to successfully inference an image dimensioned of 1900 ? 1060 ? 3. As the proposed method doesn't require any pre/post-processing, the inference times are meant to remain contents with the same hardware settings. Subsequently, the simplicity of the proposed method made the solution convenient for real-world deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This study proposed a two-stage learning-based method for single-shot LDR to HDR mapping without explicitly calculating camera hardware related information. Here, stage-I of the proposed method learns to perform the basic image manipulation techniques like exposure correction, denoising, brightness correction comprehensively. Additionally, stage-II focuses on tone mapping and bitexpansion to output 16-bit HDR images. We evaluated and compared our proposed approach with the state-of-theart single-shot HDR reconstruction methods. Both qualitative and quantitative comparison evident that the proposed method can outperform the existing deep methods with a substantial margin. Apart from that, we also collected a set of LDR images captured with the different camera hardware. The study with our newly collected dataset reveals that the proposed method can handle the real-world LDR samples without producing any visual artefacts. It has planned to extend the proposed method for multi-shot HDR reconstruction in a future study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed method. The proposed method comprises a two-stage deep network. Stage-I aims to perform image enhancement task such as denoising, exposure correction, etc. Stage-II of the proposed method intends to perform tone mapping and bit-expansion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The residual dense attention block comprises a residual dense block and a spatial attention block. The stages of the proposed method leverage this residual dense attention block to accelerate the learning process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Example of image patches used for training. Top row: LDR image (Input), middle row: reference image (8bit), bottom row: reference image (16-bit).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Real-world LDR to HDR reconstruction obtained by the proposed study. Top row: LDR images, bottom row: reconstructed HDR images (visualized)</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Sejong University Faculty Research Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Advanced high dynamic range imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Artusi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Chalmers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Banterle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th international conference on Computer graphics and interactive techniques in Australasia and Southeast Asia</title>
		<meeting>the 4th international conference on Computer graphics and interactive techniques in Australasia and Southeast Asia</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tone expansion using lighting style aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cambodge</forename><surname>Bist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Cozot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Madec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Ducloux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="86" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A review of image denoising algorithms, with a new one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="490" to="530" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overexposure correction via exposure and scene information disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurui</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A median cut algorithm for light probe sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2008 classes</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recovering high dynamic range radiance maps from photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2008 classes</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhancement of bright video features for hdr displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Didyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hdr image reconstruction from a single exposure using deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyorgy</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rafa?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep reverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Mitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dslr-quality photos on mobile devices with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Replacing mobile camera isp with a single deep learning model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="536" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukul</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanmuganathan</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fhdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11463</idno>
		<title level="m">Hdr image reconstruction from a single ldr image using feedback network</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Production-ready global illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>Landis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="93" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluation of tone mapping operators using a high dynamic range display</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ledda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Chalmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Troscianko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Seetzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="640" to="648" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep chain hdri: Reconstructing a high dynamic range image from a single low dynamic range image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk-Ju</forename><surname>Gwon Hwan An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49913" to="49924" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep recursive hdri: Inverse tone mapping using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk-Ju</forename><surname>Gwon Hwan An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="596" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Importance-weighted conditional adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cangning</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">113404</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single-image hdr reconstruction by learning to reverse the camera pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Lun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Lung</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1651" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The development of the cie 2000 colour-difference formula: Ciede2000. Color Research &amp; Application: Endorsed by Inter-Society Color Council, The Colour Group (Great Britain), Canadian Society for Color, Color Science Association of Japan, Dutch Society for the Study of Color, The Swedish Colour Centre Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihua</forename><surname>M Ronnier Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rigg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="340" to="350" />
		</imprint>
		<respStmt>
			<orgName>Colour Society of Australia, Centre Fran?ais de la Couleur</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Color correction for tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radoslaw</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Tomaszewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Expandnet: A deep convolutional neural network for high dynamic range expansion from low dynamic range content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetris</forename><surname>Marnerides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bashford-Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hatchett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Debattista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic range expansion based on image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belen</forename><surname>Masia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Gutierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ntire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ntire Hdr Challange Code</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">NTIRE 2021 challenge on high dynamic range imaging: Dataset, methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>P?rez-Pellitero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibi</forename><surname>Catley-Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ale?</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">PyTorch Framework code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<idno>2021-02-14. 5</idno>
		<ptr target="https://pytorch.org/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Rempel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seetzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<imprint>
			<pubPlace>Wolfgang Heidrich, Lorne</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ldr2hdr: on-the-fly reverse tone mapping of legacy video and photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepisp: Toward learning an end-to-end image processing pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="912" to="923" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning medical image denoising with deep dynamic residual attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rizwan</forename><forename type="middle">Ali</forename><surname>Sma Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mithun</forename><surname>Naqvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2192</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond joint demosaicking and denoising: An image processing pipeline for a pixel-bin image sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rizwan</forename><forename type="middle">Ali</forename><surname>Sma Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mithun</forename><surname>Naqvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Worksh., 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning on image denoising: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunwei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lunke</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">High dynamic range image hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lvdi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rendering Techniques</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic exposure correction of consumer photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="771" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
