<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S-Net: A Scalable Convolutional Neural Network for JPEG Compression Artifact Reduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Advanced Digital Technology and Instrument</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<addrLine>No. 38 Zheda Road</addrLine>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Physical Science and Technology</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<addrLine>No.24 South Section 1, Yihuan Road</addrLine>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Advanced Digital Technology and Instrument</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<addrLine>No. 38 Zheda Road</addrLine>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Zhejiang Provincial Key Laboratory for Network Multimedia Technologies d The State Key Laboratory of Industrial Control Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowu</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Zhejiang University Embedded System Engineering Research Center</orgName>
								<address>
									<country>Ministry of Education of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">S-Net: A Scalable Convolutional Neural Network for JPEG Compression Artifact Reduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>compression artifact reduction</term>
					<term>encoder-decoder model</term>
					<term>scalable convolutional neural network</term>
					<term>depth evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have used deep residual convolutional neural networks (CNNs) for JPEG compression artifact reduction. This study proposes a scalable CNN called S-Net. Our approach effectively adjusts the network scale dynamically in a multitask system for real-time operation with little performance loss. It offers a simple and direct technique to evaluate the performance gains obtained with increasing network depth, and it is helpful for removing redundant network layers to maximize the network efficiency. We implement our architecture using the Keras framework with the TensorFlow backend on an NVIDIA K80 GPU server. We train our models on the DIV2K dataset and evaluate their performance on public benchmark datasets. To validate the generality and universality of the proposed method, we created and utilized a new dataset, called WIN143, for over-processed images evaluation. Experimental results indicate that our proposed approach outperforms other CNN-based methods and achieves state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image restoration for reducing lossy compression artifacts has been well-studied, especially for the JPEG compression standard <ref type="bibr" target="#b0">1</ref> . JPEG is a popular lossy image compression standard because it can achieve high compression ratio with only minimal reduction in visual quality. The JPEG compression standard divides an input image into 88 ? blocks and performs discrete cosine transform (DCT) on each block separately. The 64 DCT coefficients thus obtained are quantized based on standard quantization tables that are adjusted with different quality factors. Losses such as blackness, ringing artifacts, and blurring artifacts are mainly introduced by quantizing the DCT coefficients.</p><p>Recently, deep-neural-network-based approaches <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3</ref> have been used to significantly improved the JPEG compression artifact reduction performance in terms of peak signal-to-noise ratio (PSNR). However, these methods have several limitations. First, most existing methods <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5</ref> focus on the construction performance of grayscale images and try to restore each channel separately when applied to color images. However, this will introduce palpable chromatic aberrations in the reconstructed image. Second, many methods <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4</ref> based on the ResNet 6 architecture try to optimize the performance by increasing the number of residual blocks in networks. Although this is an effective optimization method, determining the exact depth for maximizing the network performance without traversing all depths remains challenging. Third, as found for super-resolution convolutional neural network (SRCNN) <ref type="bibr" target="#b6">7</ref> , CNN-based image restoration algorithms can be used for encoding, transforming, and decoding. Most existing CNN-based algorithms use only one decoder at the end of the network; alternatively, they use a stack of layers at the tail of the network as a decoder. This is called a columnar architecture.</p><p>However, we believe that a columnar architecture contains too many layers between the input layer and the loss layer. Increased network depth could make it much harder for training layers around the bottom, although residual connections <ref type="bibr" target="#b7">8</ref> or some excellent optimizers <ref type="bibr" target="#b8">9</ref> could mitigate this problem. Therefore, if we use only some layers of the transforming part, the results could degrade considerably. To solve these problems, first, we use color image pairs to train the network to restore color images directly. Second, we present a symmetric encoder-decoder model to finish the encoding and decoding tasks independently. Third, based on the greedy theory, we propose a scalable CNN called S-Net to maximize the performance of each convolutional layer in the network. We also prove that it is helpful to evaluate the influence of depth on the network performance.</p><p>We trained our models on the newly provided DIV2K 10 dataset and evaluated them on public benchmark datasets (LIVE1 11 and BSDS500 12 ). The proposed architecture achieved stateof-the-art performance on all datasets in terms of PSNR and structural similarity index (SSIM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep convolutional networks (DCNs) are trained for image restoration tasks by converting an input image into a feature space and building nonlinear mappings between the features of the input and the target images. To exploit error back-propagation, groups of convolutional filters that construct DCNs are learned during the training procedure so that they can be used for convoluting an input image for a specific image restoration task. SRCNN is the first DCN-based image restoration method that has been proposed for image super-resolution (SR). It uses bicubic interpolation to up-sample the low-resolution image and train a three-layer CNN to restore the up-sampled image. Based on SRCNN, ARCNN 2, 13 , a four-layer CNN, was proposed to reduce JPEG compression artifacts. However, because ARCNN does not use a pooling layer and a fully connected layer, the result deteriorated with increasing network depth, and it was difficult to guarantee convergence unless the weights of the convolutional layers were initialized carefully. Furthermore, because DCNs show promise for high-level computer vision tasks <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15</ref> , many DCN-based algorithms for image restoration tasks tried to improve the network construction based on high-level computer vision algorithms. Ledig et al. <ref type="bibr" target="#b15">16</ref> used a generative adversarial network (GAN) <ref type="bibr" target="#b16">17</ref> to reconstruct high-resolution images by the bicubic interpolation of downsampled low-resolution images. Li et al. <ref type="bibr" target="#b17">18</ref> used GAN to solve the image dehazing problem.</p><p>Other studies applied inception modules <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19</ref> to image SR. Shi et al. <ref type="bibr" target="#b19">20</ref> introduced a dilated convolutional layer <ref type="bibr" target="#b20">21</ref> and improved the inception module in an SR network. connections to restore a noisy image. Dong et al. <ref type="bibr" target="#b22">23</ref> improved RED-Net by adding a batchnormalization layer <ref type="bibr" target="#b23">24</ref> and a rectified linear unit (ReLU) <ref type="bibr" target="#b24">25</ref> layer to the output of each convolutional layer and deconvolutional layer <ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27</ref> to learn the intrinsic representations and successfully solved the image restoration and image classification problems using the same pretrained networks. However, these two approaches based on a convolutional autoencoder did not provide an explicit presentation of the relationship between the whole network and each encoderdecoder pair in it. By contrast, symmetrically connected convolutional layer pairs were more likely to be deformed residual structures rather than encoder-decoder pairs. Lim et al. <ref type="bibr" target="#b7">8</ref> proposed a very deep network with 32 residual blocks for single-scale SR (EDSR) and an even deeper network with 80 residual blocks for multiscale SR (MDSR).</p><p>In general, DCN-based methods for image restoration tasks like compression artifact reduction (AR) and SR focus on improving performance by increasing the number of parameters in the network. However, larger-scale networks always incur higher computational costs and longer training procedures that may sometimes be unaffordable. Higher costs are also incurred to evaluate whether the best performance is achieved by the network. This study focuses on maximizing the network performance by using fewer parameters and minimizing the training procedure time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>This section describes the proposed network architecture and implementation. First, we propose a convolutional encoder-decoder architecture to extract features from an input image and recover the input image from the feature domain. Then, we introduce the characterization of the greedy loss architecture for building a scalable CNN. Finally, we discuss the implementation of our proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Symmetric Convolutional Encoder-Decoder Model</head><p>A traditional autoencoder constructed by a multilayer full-connection network is a classical unsupervised machine learning algorithm for dimension reduction and feature extraction. Some SR reconstruction algorithms <ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29</ref> use an autoencoder to learn the sparse representation of image patches and to try to refine image patches in the sparse representation domain. Considering the relative position information of pixels in image patches, we modified the autoencoder by replacing fully connected layers with convolutional layers and built a convolutional encoderdecoder architecture. <ref type="figure" target="#fig_3">Figure 2</ref> shows the construction of our convolutional encoder-decoder model. The encoder and decoder blocks are formulated by a series of combinations of a convolutional layer and an activation layer. Let X be the input; the encoder and decoder blocks are expressed as:</p><formula xml:id="formula_0">1 max(0, ) 0 () max(0, ( ) ) 0 ii i i i i W X B i X W X B i ? ? ? ? ? ? ? ? ? ? ? ? ?<label>(1)</label></formula><p>where i W and i B respectively represent the parameters of the i th convolutional filters and bias, and ? is the convolution operation.</p><p>The two blocks have the same number of layers and a symmetric convolutional kernel size. Moreover, unlike in the case of the encoder block, we tried to formulate a decoder block with transposed convolutional layers <ref type="bibr" target="#b29">30</ref> instead of convolutional layers. However, although the relationship between convolution and transposed convolution seems like a key-lock relationship, this change does not result in any improvement in the datasets in our benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Greedy Loss Architecture</head><p>Although increasing the network depth is a simple way to improve performance, a deeper network does not always result in better performance. It is difficult to ensure the appropriate depth that maximizes the network performance without testing various depths. To solve this problem, we propose a greedy loss architecture to maximize the performance of each convolutional unit in the network. We use the encoder-decoder architecture to translate inputs from the image domain to the feature domain and to ensure that each output of a convolutional unit is limited to a fixed feature domain. We connect the decoder and loss layer after the output of each convolutional unit and hope that each unit can map the JEPG compressed features to the original features. Convolutional units at different depths clearly receive different gradients from backpropagation; specifically, shallower ones receive more gradients than deeper ones, and this could be helpful for optimizing the performance of shallower layers. On the other hand, the greedy loss architecture constrains the output of each convolutional unit; this is conductive for avoiding gradient explosion <ref type="bibr" target="#b18">19</ref> when training large-scale convolutional networks. Furthermore, it is feasible to determine the network performance with different depths without training the whole network repeatedly by analyzing the loss from each convolutional unit with fixed depth. Section 4 discusses the experimental evaluation of the network performance.  <ref type="figure" target="#fig_5">Figure 3</ref> shows an overview of our network architecture.</p><p>Unlike other deep neural networks in which all parts have to operate in combination in order for the network to function, these three parts construct a scalable convolutional neural network in which the operation of even one part enables the network to function normally. For example, if part of the nonlinear feature mapper is removed, the network can give a comparatively good result. Moreover, even if the nonlinear feature mapper is completely removed, the symmetrically encoder-decoder pairs can still give quite improved outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Architecture</head><p>The encoder and decoder are both formulated using two convolutional layers with a ReLU activation layer. <ref type="table" target="#tab_0">Table 1</ref> lists the construction of the encoder and decoder. Because the second convolutional layer of the decoder is connected to the loss layer, its number of channels depends on the channel size of the input/output images. A nonlinear feature mapper is formulated by a series of shortcut connections. Because these shortcut connections in low-level image processing using DCNs are always constructed using only convolutional layers, we call these fullconvolutional shortcut connections as convolutional units. We denote these convolutional units</p><formula xml:id="formula_1">as 1 CU , 2 CU , ?, L CU ,</formula><p>where L is the total number of convolutional units. All these convolutional units have the same structure, and they are formulated by convolutional layers and activation layers.</p><p>There are active discussions about the problem of deeper networks or wider networks <ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32</ref> .</p><p>The creators of ResNet preferred deeper networks and tried to make the network as thin as possible in order for it to have only a few parameters. Some recently proposed DCN-based methods for image restoration adopted this strategy to construct their networks. However, the wide ResNet (WRN) <ref type="bibr" target="#b32">33</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Convolutional Unit</head><p>Residual connections have been widely and successfully used in many image restoration algorithms <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35</ref> . A convolutional unit constructed with residual connection is the basic component of a network, and it plays an important role in the network performance. Here, we compare two widely used convolutional units: classic residual structure, the simplest residual connection structure in which the residual branch is constructed using a convolutional layer and a ReLU activation layer, and advanced residual structure, first proposed by Peng et al. <ref type="bibr" target="#b35">36</ref> for boundary refining in image segmentation and which shows great performance for image restoration tasks <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35</ref> . Although the batch-normalization layer is a basic component in the residual connection structure, it has been found that removing them from the network can improve the network performance for image SR tasks <ref type="bibr" target="#b7">8</ref> . We found that this modification is also effective for AR tasks and therefore applied this modification to our network. <ref type="figure" target="#fig_6">Figure 4</ref> shows the configuration of both structures. The residual branch of the advanced residual structure contains two convolutional layers and a ReLU activation layer. <ref type="table" target="#tab_1">Table 2</ref> lists the parameters of our networks constructed using these two different convolutional units.</p><p>Several researchers replaced the ReLU layer in networks with a parametric rectified linear unit (PReLU) 2, 5, 37 layer. PReLU imports a learnable parameter ? to restrict the negative output,</p><p>whereas ReLU compulsively cuts the negative output to zero.</p><formula xml:id="formula_2">0 () 0 xx PR x xx ? ? ? ? ? ? ?<label>(2)</label></formula><p>We tried using PReLU layers to replace ReLU layers in our network. However, doing so did not result in any improvement; instead, it increased the computational and memory costs. Therefore, we use only ReLU as the activation function in our following experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Training</head><p>PSNR is the most universal evaluation indicator for image restoration tasks. It can be represented as follows:</p><formula xml:id="formula_3">10= 10log ( ( , )) PSNR MSE X Y (3)</formula><p>where Y is the target image; X , the restored image; and MSE, the mean square error. To maximize the PSNR of the reconstructed image, we use the MSE loss as the loss function of each metric in the greedy loss architecture. The loss weight of each metric is equated to others, and the sum of the weights of all metrics is one. The final training loss function is</p><formula xml:id="formula_4">11 1 ( ) ( ; ) NM ii j ij XY MN ?? ?? ? ? ? ? ??<label>(4)</label></formula><p>where N is the number of samples in a training batch; M, the number of metrics; and j ? , the output of the j th metric in the greedy loss architecture.</p><p>We used the adaptive moment estimation (Adam) 9 method as the optimizer during the training procedure. Adam is a recently proposed optimization algorithm that has been proved to be effective for training very deep networks. We used the default parameters (beta_1 = 0.9, beta_2 = 0.999, epsilon = ) <ref type="bibr" target="#b8">9</ref> as specified in the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The <ref type="formula" target="#formula_2">DIV2K</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>For training, we extracted 48?48 RGB image patches from training images in the DIV2K dataset with random steps from 37 to 62 as the input. Although the JPEG compression algorithm is applied to each 8?8 patch, taking a random step to avoid integral multiples of eight can significantly enhance the network performance, as in the case of DDCN 4 . The initial learning rate was set to 10 -4 at the start of the training procedure, and subsequently halved after every set of 10 4 batch updates until it was below10 -6 . All network models for different convolutional units were trained with 2?10 5 batch updates. Considering the limitation of computational resources, the batch size was set to 16. We first trained models with JPEG quality of 40 (QF40) to evaluate the performance of different convolutional units. Then, we fine-tuned the pre-trained QF40 models for JPEG qualities of 10 (QF10) and 20 (QF20) with initial learning rate of 1?10 -5 and 4?10 4 batch updates. During fine-tuning, the learning rate was also halved after every set of <ref type="bibr">10 4</ref> batch updates until it reached 10 -6 or below. We performed the experiments using the Keras framework with a TensorFlow backend on an NVIDIA K80 GPU server. Training of the QF40 model took five days and two days was required to fine-tune the QF20 and QF10 models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Greedy Loss Architecture Performance Evaluation</head><p>We measured the PSNR and SSIM with only the y-channel considered, and used standard MATLAB library functions for the evaluations. We trained the network models with the convolutional unit of the advanced residual structure using the columnar architecture and greedy loss architecture for the JPEG quality of 40. For fairness, these two models were trained with the same image patches and same learning rate during the whole training procedure. <ref type="figure" target="#fig_7">Figure 5</ref> shows the results in terms of PSNR and SSIM for the LIVE1 dataset. We compared the performances of the two network models after 2?10 5 batch updates on the LIVE1 dataset. Although the greedy approach may lead to a local optimum, the model trained using the greedy loss architecture showed better performance in terms of both PSNR and SSIM compared with the model trained using the columnar architecture. Furthermore, the proposed model showed better performance for intermediate outputs than the columnar architecture. <ref type="figure" target="#fig_8">Figure 6</ref> shows the reconstructed images.</p><p>The greedy loss architecture significantly improved the consistency of color accuracy and texture sharpness in different metrics. With some convolutional units removed from the network, the result obtained using the greedy loss architecture was obviously more stable than that obtained using the columnar architecture. We also compared S-Net with columnar architecture CNNs with 1-8 convolutional units.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of Different Convolutional Units</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparisons to State-of-the-Art</head><p>We compared our models with the state-of-the-art models ARCNN 2 , DDCN <ref type="bibr" target="#b3">4</ref>   <ref type="table" target="#tab_4">Table 3</ref>, our model constructed with convolutional units of the advanced residual structure showed significant improvements compared to the other methods for all public benchmark datasets. <ref type="figure" target="#fig_21">Figure 9</ref> shows some qualitative results for the BSDS500 dataset. We also compared the computational efficiency of the proposed method with those of other state-of-the-art methods. All algorithms were implemented on a K80 GPU server with a single GPU core. We measured the computational efficiency in terms of million color pixels per second (MCP/s). However, because there is a significant difference between the qualities of the images reconstructed by ARCNN and other state-of-the-art methods, ARCNN is not included in this comparison although it is quite fast. <ref type="figure" target="#fig_0">Figure 10</ref> shows the computational efficiencies. S-Net with one and two convolutional units shows improved image quality and computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Extensional Performance Evaluation on the WIN143 Dataset</head><p>However, we noticed that the images in both LIVE1 and BSDS500 are typically everyday scenes.</p><p>Further, few shooting skills or post processing technologies were utilized when getting these images. We call images acquired like this normal images. However, because we believe that these limitations may not thoroughly show the generality and universality of algorithms, as a supplement, we created an extensional dataset, called WIN143, to evaluate the algorithm performance on specially acquired or post processed images. The WIN143 dataset contains 143 desktop wallpapers with a resolution of 1920?1080 that are always used in the Windows 10 operating system. The images in WIN143 were collected from the internet and are specially shot or carefully post processed, or even generated by computer graphics technologies. Here, we call images such as these over-processed images. Compared to daily shot images, the over-processed images always get higher contrast and saturation, and their complexity and unexpected changes are obviously enhanced. The extensional experiment on the WIN143 dataset was conducted with a JPEG quality of 20. Because the original resolution of the images in the WIN143 dataset was too large for them to be placed in memory, we reduced the image height and width by half using the bicubic interpolation algorithm. The performance comparison results in terms of PSNR and SSIM are shown in <ref type="table" target="#tab_5">Table 4</ref>. The performance difference between ARCNN and S-Net is significantly magnified that 28 of 143 images restored by ARCNN get even worse results in terms of PSNR while S-Net remains good performances as before. <ref type="figure" target="#fig_0">Figure 11</ref> shows the qualitative results for the WIN143 dataset. The images restored by S-Net have higher color and intensity accuracy, especially in the dark and the smooth areas. Further, S-Net is better able to distinguish the true textures and fake textures created by JPEG compression. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This study investigated the effects of increased network depth on network performance and proposed a scalable CNN called S-Net for JPEG compression artifact reduction. By applying a symmetric convolutional encoder-decoder model and a greedy loss architecture, S-Net dynamically adjusts the network depth. We proved that this greedy theory-based architecture does not sink into a local optimum and achieves better results than a specifically trained network under the same conditions. Furthermore, the proposed architecture is also helpful for discovering the minimal network scale with maximum possible network performance. With the greedy loss architecture, the evaluation results for the depth of the network were quickly obtained after training once, whereas several training sessions had to be applied with the conventional architecture. We compared our approach with other state-of-the-art algorithms on public benchmark datasets and achieved top ranking. We also created an over-processed image dataset, called WIN143, using images obtained from the internet. The results of extensional performance evaluation on the WIN143 dataset successfully validated the generality and universality of the proposed algorithm.</p><p>Instrumentation, Zhejiang University. His major research interests include embedded systems, networking multimedia systems, and electronic instrumentation systems.          </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caption List</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 11</head><p>Comparison of our model with state-of-the-art methods for QF20 JPEG compression artifact reduction on the WIN143 dataset. <ref type="table" target="#tab_0">Table 1</ref> Construction of convolutional encoder-decoder model. <ref type="table" target="#tab_1">Table 2</ref> Size of parameters of proposed architectures. <ref type="table" target="#tab_4">Table 3</ref> Comparison of our approaches with existing methods on public benchmark datasets.</p><p>Boldface indicates the best performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Comparison of JPEG compression (QF40) artifact reduction results of existing and proposed methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For</head><label></label><figDesc>image restoration tasks, very deep CNNs can only operate well with residual connections and effective optimizers. Svoboda et al. 3 used an 8-layer CNN and introduced a skip connection to learn Sobel features between a JPEG compressed image and the original image. Because JPEG compression artifacts are introduced by quantizing DCT coefficients, DDCN 4 uses the DCT domain and trains a network with both the image and the DCT domains to learn the difference between the JPEG compressed image and the original image. DDCN uses 30 residual blocks, with 10 each used for the pixel domain branch, DCT domain branch, and aggregation. Because SR has shown success with DCNs, CAS-CNN 5 imported stepped convolutional layers and deconvolutional layers (also called up-sample layers) and transformed the compression artifact reduction problem into an SR problem. Mao et al. 22 proposed RED-Net for image restoration. RED-Net uses a series of encoder-decoder pairs with symmetric skip</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc>Constructions of convolutional encoder-decoder model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>For example, if the encoder block contains M layers and the convolutional kernel size of the k th ( {1, 2,3... } kM ? ) layer is kk ss ? , then the convolutional kernel size of the k th layer in the decoder</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3</head><label>3</label><figDesc>Overview of proposed network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4</head><label>4</label><figDesc>was developed based on the rationale that deeper networks can lead to less gradient flowing and fewer convolutional units can result in useful intern representations being learned. Further, the performance also suffers significantly from this very deep structure when the depth of the network is dynamically scaled. Motivated by this observation, we designed our S-Net with larger width and shallower depth. We set L = 8, and all convolutional layers in the convolutional units comprise filters with a kernel size of 3?3 and a channel size of 256. Structures of different convolutional units: (a) classic residual structure and (b) advanced residual structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5</head><label>5</label><figDesc>Performance comparison of models with convolutional unit of advanced residual structure trained using greedy loss architecture and columnar architecture on LIVE1 dataset with JPEG quality of 40: (a) PSNR and (b) SSIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6</head><label>6</label><figDesc>Comparison of details of images reconstructed by models with convolutional unit of advanced residual structure trained using greedy loss architecture and columnar architecture on LIVE1 dataset (JPEG quality = 40).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7</head><label>7</label><figDesc>Comparison of the performance of metrics in S-Net using columnar architecture CNNs with same network scale on LIVE1 dataset with convolutional unit of advanced residual block (JPEG quality = 40): (a) PSNR and (b) SSIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 Fig. 8</head><label>78</label><figDesc>shows the obtained results. The performance of metric two in S-Net is very close to that of the columnar architecture CNN with two convolutional units. The subsequent metrics in our model all show better performance than the columnar architecture CNNs for the same network scale. These results also show that scalable CNN is effective for evaluating the performance improvement with increased network depth. The results for columnar architecture CNNs show that the performance of the nonlinear feature mapper with four convolutional units is close to eight, and the decrease in PSNRs from eight to four is less than 0.1 dB. Furthermore, the performance did not significantly improve for more than six convolutional units. The results of the metrics in S-Net are identical. This indicates that the greedy loss architecture in S-Net is effective for evaluating the improvement achieved with increased network scale. Comparison of performance of models with different convolutional units on LIVE1 dataset (JPEG quality = 40): (a) PSNR and (b) SSIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8</head><label>8</label><figDesc>shows quantitative evaluation results of network models with different convolutional units on the LIVE1 dataset. The model trained with an advanced residual block showed significant improvement compared to that of the model trained with a classic residual block. The advanced residual block model provides average enhancement of 0.20 dB for each metric with the trade-off of double the number of parameters in the nonlinear feature mapper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10</head><label>10</label><figDesc>Comparison of computational efficiency of our model with those of other state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 1</head><label>1</label><figDesc>Comparison of JPEG compression (QF40) artifact reduction results of existing and proposed methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 2</head><label>2</label><figDesc>Constructions of convolutional encoder-decoder model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 3</head><label>3</label><figDesc>Overview of proposed network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 4</head><label>4</label><figDesc>Structures of different convolutional units: (a) classic residual structure and (b) advanced residual structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 5</head><label>5</label><figDesc>Performance comparison of models with convolutional unit of advanced residual structure trained using greedy loss architecture and columnar architecture on LIVE1 dataset with JPEG quality of 40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 6</head><label>6</label><figDesc>Comparison of details of images reconstructed by models with convolutional unit of advanced residual structure trained using greedy loss architecture and columnar architecture on LIVE1 dataset (JPEG quality = 40) : (a) PSNR and (b) SSIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 7</head><label>7</label><figDesc>Comparison of the performance of metrics in S-Net using columnar architecture CNNs with same network scale on LIVE1 dataset with convolutional unit of advanced residual block (JPEG quality = 40) : (a) PSNR and (b) SSIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 8</head><label>8</label><figDesc>Comparison of performance of models with different convolutional units on LIVE1 dataset (JPEG quality = 40) : (a) PSNR and (b) SSIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 9</head><label>9</label><figDesc>Comparison of our model with state-of-the-art methods for QF10 JPEG compression artifact reduction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 10</head><label>10</label><figDesc>Comparison of computational efficiency of our model with those of other state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Construction of convolutional encoder-decoder model. Building a Scalable Convolutional Neural Network 3.3.1 Overview Our network architecture can be divided into three parts: encoder, decoder and nonlinear feature mapper. The encoder uses JPEG compressed image patches as inputs and translates them to the feature domain. A nonlinear feature mapper learns a nonlinear functions to map features from anamorphic image patches to uncompressed image features. The decoder translates mapped features back to the image domain.</figDesc><table><row><cell>Conv Layer</cell><cell>Filter Size</cell></row><row><cell>Encoder</cell><cell>Decoder</cell></row><row><cell>1</cell><cell></cell></row><row><cell>2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Size of parameters of proposed architectures.</figDesc><table><row><cell>Architecture</cell><cell cols="2">Number of Parameters</cell><cell>Total</cell><cell></cell></row><row><cell></cell><cell>Classic</cell><cell>Advanced</cell><cell>Classic</cell><cell>Advanced</cell></row><row><cell>Encoder</cell><cell>0.58M</cell><cell>0.58M</cell><cell>-</cell><cell>-</cell></row><row><cell>Decoder</cell><cell>0.58M</cell><cell>0.58M</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc><ref type="bibr" target="#b9">10</ref> dataset has been recently proposed in the NTIRE2017 challenge for single-image SR<ref type="bibr" target="#b9">10</ref> . The dataset consists of 1000 2K-resolution images, of which 800 are training images, 100 are validation images, and the remaining 100 are testing images. Because the testing dataset was prepared for image SR and the ground truth has not been not released, we could not compare performances using this dataset. Instead, we evaluated the performance of our proposed method</figDesc><table /><note>and compared it with other state-of-the-art methods on other known datasets.LIVE1 11 and BSDS500 12 are two known datasets that are frequently used to validate the performance of proposed approaches in reducing JPEG compressed artifacts. LIVE1 is a publicly released dataset that contains 29 images for image quality assessment. BSDS500 is a publicly released database for image segmentation that contains 200 training images, 100 validation images, and 200 test images. Quantitative evaluations are conducted on the 29 images in the LIVE1 dataset and the 200 test images in the BSDS500 dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, and CAS-CNN 5 on the BSDS500 and LIVE1 datasets. Because only ARCNN provides open source code and a pre-trained model, the results for ARCNN were obtained from experiments conducted by ourselves, while the results for DDCN and CAS-CNN were obtained from the reports presented in corresponding papers. Further, no qualitative comparison could be carried out for DDCN and CAS-CNN. As shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Comparison of our approaches with existing methods on public benchmark datasets. Boldface indicates the best performance.</figDesc><table><row><cell>Dataset</cell><cell>JPEG</cell><cell></cell><cell></cell><cell cols="3">Average PSNR (dB)/SSIM</cell></row><row><cell></cell><cell>Quality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>JPEG</cell><cell>ARCNN</cell><cell>CAS-CNN</cell><cell>DDCN</cell><cell>Metric1</cell><cell>Metric2</cell><cell>Metric8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(proposed)</cell><cell>(proposed)</cell><cell>(proposed)</cell></row><row><cell></cell><cell>40</cell><cell cols="3">32.93/0.9255 33.63/0.9306 34.10/0.937</cell><cell>-/-</cell><cell cols="2">34.41/0.9402 34.48/0.9410 34.61/0.9422</cell></row><row><cell>LIVE1</cell><cell>20</cell><cell cols="3">30.62/0.8816 31.40/0.8886 31.70/0.895</cell><cell>-/-</cell><cell cols="2">32.05/0.9034 32.13/0.9046 32.26/0.9067</cell></row><row><cell></cell><cell>10</cell><cell cols="3">28.36/0.8116 29.13/0.8232 29.44/0.833</cell><cell>-/-</cell><cell cols="2">29.67/0.8415 29.75/0.8435 29.87/0.8467</cell></row><row><cell>BSDS50 0</cell><cell>40 20 10</cell><cell cols="2">32.89/0.9257 33.55/0.9296 30.61/0.8811 31.28/0.8854 28.39/0.8098 29.10/0.8198</cell><cell>-/--/--/-</cell><cell cols="3">34.27/0.9389 34.27/0.9394 34.33/0.9401 34.45/0.9413 31.88/0.8996 31.97/0.9017 32.04/0.9028 32.15/0.9047 29.59/0.8381 29.64/0.8391 29.71/0.8410 29.82/0.8440</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Comparison of our approaches with existing methods on the WIN143 datasets. Boldface indicates the best performance.</figDesc><table><row><cell>Items</cell><cell></cell><cell></cell><cell cols="2">Average PSNR (dB)/SSIM</cell><cell></cell></row><row><cell></cell><cell>JPEG</cell><cell>ARCNN</cell><cell>Mertic1 (proposed)</cell><cell>Metric2 (proposed)</cell><cell>Metric8 (proposed)</cell></row><row><cell>WIN143</cell><cell>32.95/0.9033</cell><cell>33.09/0.9106</cell><cell>34.38/0.9220</cell><cell>34.47/0.9232</cell><cell>34.61/0.9250</cell></row><row><cell>public benchmark improvement</cell><cell>-</cell><cell>+0.69/+0.0046</cell><cell>+1.37/+0.0195</cell><cell>+1.44/+0.0219</cell><cell>+1.55/+0.0238</cell></row><row><cell>WIN143 improvement</cell><cell>-</cell><cell>+0.14/+0.0073</cell><cell>+1.43/+0.0187</cell><cell>+1.52/+0.0199</cell><cell>+1.66/+0.0217</cell></row></table><note>* Here, public benchmark refers to the combination of BSDS500 and LIVE1 datasets; improvement refers to the PSNR and SSIM improvements compared to JPEG compressed images.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Comparison of our approaches with existing methods on the WIN143 datasets. Boldface indicates the best performance.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Fig. 11Comparison of our model with state-of-the-art methods for QF20 JPEG compression artifact reduction on the WIN143 dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by Fundamental Research Funds for the Central Universities, China.</p><p>The authors would like to thank the anonymous reviewers for their valuable comments that have helped to significantly improve this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The JPEG still picture compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.1145/103085.103089</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the Acm</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="xviii" to="xxxiv" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep convolution networks for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02778</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compression Artifacts Removal Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hradis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Wscg</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building Dual-Domain Representations for Compression Artifacts Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CAS-CNN: A deep convolutional neural network for image compression artifact suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn.2017.7965927</idno>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks. IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="752" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2017.151</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition Workshops</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2017.150</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1110" to="1121" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition Workshops</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.819861</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contour Detection and Hierarchical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.161</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compression Artifacts Reduction by a Deep Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.73</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision. IEEE</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.73</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz? R</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.19</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing System</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial dehaze mapping nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2017.11.021</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Single Image Super-Resolution with Dilated Convolution based Multi-Scale Information Learning Inception Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2017.8296427</idno>
		<idno type="arXiv">arXiv:1707.07128</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Image restoration using convolutional autoencoders with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08921</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09119</idno>
		<title level="m">Learning Deep Representations Using Convolutional Auto-encoders with Symmetric Skip Connections</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Deconvolution Network for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.178</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1495" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coupled Deep Autoencoder for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2015.2501373</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="37" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Coupled Autoencoder Network with Joint Regularizations for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3007669.3007717</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Internet Multimedia Computing and Service</title>
		<meeting>the International Conference on Internet Multimedia Computing and Service</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="114" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2011.6126474</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2018" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Scaling learning algorithms towards AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="321" to="359" />
		</imprint>
	</monogr>
	<note>Large-scale Kernel Machines</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273496.1273556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Wide Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="DOI">10.5244/c.30.87</idno>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273496.1273556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.35</idno>
		<idno type="arXiv">arXiv:1612.02177</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Large Kernel Matters--Improve Semantic Segmentation by Global Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.189</idno>
		<idno type="arXiv">arXiv:1703.02719</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.123</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Bolun Zheng is a PhD candidate of Zhejiang University and received his BSc degree from</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">His current research interests include image restoration, image processing and computer vision</title>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Hangzhou, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Zhejiang University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">She is currently participate in a project of FPGA watermark image acceleration processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Rui Sun is an undergraduate student of Sichuan University in Chengdu</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">respectively. Currently, he is an associate professor in the Institute of Advanced Digital Technologies and Instrumentation, Zhejiang University. His major research interests include VLSI-based high-performance computing, networking multimedia systems</title>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Hangzhou, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xiang Tian received his BSc and PhD degrees from Zhejiang University</orgName>
		</respStmt>
	</monogr>
	<note>and parallel processing</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Currently, he is a professor and the director of the Institute of Advanced Digital Technologies</title>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>Hangzhou, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Yaowu Chen received his PhD from Zhejiang University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
