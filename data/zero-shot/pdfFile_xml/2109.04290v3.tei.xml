<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Cheng</surname></persName>
							<email>chengxing03@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hezheng</forename><surname>Lin</surname></persName>
							<email>linhezheng@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wu</surname></persName>
							<email>wuxiangyu@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<email>yangfan@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Shen</surname></persName>
							<email>shendong@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MMU KuaiShou Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Employing large-scale pre-trained model CLIP to conduct video-text retrieval task (VTR) has become a new trend, which exceeds previous VTR methods. Though, due to the heterogeneity of structures and contents between video and text, previous CLIP-based models are prone to overfitting in the training phase, resulting in relatively poor retrieval performance. In this paper, we propose a multi-stream Corpus Alignment network with single gate Mixture-of-Experts (CAMoE) and a novel Dual Softmax Loss (DSL) to solve the two heterogeneity. The CAMoE employs Mixture-of-Experts (MoE) to extract multi-perspective video representations, including action, entity, scene, etc., then align them with the corresponding part of the text. In this stage, we conduct massive explorations towards the feature extraction module and feature alignment module, and conclude an efficient VTR framework. DSL is proposed to avoid the oneway optimum-match which occurs in previous contrastive methods. Introducing the intrinsic prior of each pair in a batch, DSL serves as a reviser to correct the similarity matrix and achieves the dual optimal match. DSL is easy to implement with only one-line code but improves significantly. The results show that the proposed CAMoE and DSL are of strong efficiency, and each of them is capable of achieving State-of-The-Art (SOTA) individually on various benchmarks such as MSR-VTT, MSVD, and LSMDC. Further, with both of them, the performance is advanced to a great extent, surpassing the previous SOTA methods for around 4.6% R@1 in MSR-VTT. The code will be available soon at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction Motivation</head><p>The primary issue limiting VTR task presently is the heterogeneity between different modals, reflected in both structures and contents.</p><p>The heterogeneity of structures. This mainly lies in the impossibility of directly aligning the words in sentences with corresponding video frames <ref type="bibr" target="#b17">(Jin et al. 2021</ref>). Singlestream or two-stream structures are applied to treat text and video as two independent parts for early or late fusion, which ignore the internal relevancy between frames and words, resulting in that the models require massive data to reach decent performance. In this paper, we assume that texts can be parsed into separate sentences with distinct aspects of information. Though directly aligning a word with a frame is unachievable, guiding the model to learn how to align cross modal information is possible. Referring to the example in <ref type="figure">Fig.2</ref>, the video is paired with the sentence "a boy is performing for an audience.", where "boy", "performing", "audience" are the keywords and can be categorized as "entity", "action", "entity" accordingly. We design several experts to learn corresponding representations independently. In addition, a gating module is employed to measure their importance score and then strengthen the representation of the fusion expert. Such innovation brings little parameters and computations increment and surpassing the previous Stateof-The-Art (SOTA) method on various benchmarks.</p><p>Previous work has taken a similar approach, either by simple part-of-speech tagging or by wielding multi-dimensional features on the video. HGR <ref type="bibr" target="#b5">(Chen et al. 2020</ref>) and HCGC <ref type="bibr" target="#b17">(Jin et al. 2021)</ref> hypothesize that a text can be constructed into a hierarchical semantic graph structure, where lie sentence, action, entity embedding in the top, second, third level node, respectively. T2VLAD <ref type="bibr" target="#b36">(Wang, Zhu, and Yang 2021)</ref> extracts features from the aspects of scene and action, and performs similarity matching with the representations of each local token and the global sentence, while HiT  conducts cross-matching between featurelevel and semantic-level embedding. But they don't simultaneously decompose the video and text to conduct deep alignment, from where we proposed the multi-stream multi-task <ref type="bibr" target="#b32">(Ruder 2017</ref>) architecture, as shown in <ref type="figure">Fig.2</ref> The heterogeneity of contents and dual optimal-match hypothesis. Another important contribution of this paper is the proposed problem that semantic and visual modals usually express in a different range of content. The example shown in <ref type="figure">Fig.1</ref>, denotes the comparisons of the process calculating the final probability matrix for Video-to-Text retrieval. Although each video describes specific and explicit content, the corresponding text can be unspecific and fuzzy, which harms model training. The original method conducts the softmax for every single retrieval, ignoring the potential cross-retrieval information and leading to a confusing result. To solve this, we propose the dual optimal-match hypothe- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dot</head><p>Revised scaling similarity matrix <ref type="figure">Figure 1</ref>: A diagram of the heterogeneity of contents and Dual Softmax loss. The highlighted block denotes the maximum value in each row. The sentence "A woman is decorating her finger nail." describes broad content and can be paired with all videos painting on nails, so it is inferred with the maximum score for each row in the original similarity matrix. Considering the diagonal scores denote the ground truth and should be highlighted, a prior probability matrix is calculated in the cross direction. With the dot product of the prior and the original similarity, the diagonal part achieves the optimal. sis based on the discovered phenomenon that when a Textto-Video or Video-to-Text pair reaches the optimal match, the symmetric Video-to-Text or Text-to-Video score should be the highest. With this hypothesis, the corresponding DSL is designed to revise the predicted similarity score, significantly improving the performance. It introduces a prior probability matrix calculated in the cross direction to adjust the original scaling similarity matrix. By the dot product of the prior probability matrix and original scaling similarity matrix, we can filter the hard case with a high Video-to-Text similarity score but a low Text-to-Video similarity score. Referring to <ref type="figure">Fig.1</ref>, the final probability matrix's maximum is adjusted to the diagonal, which indicates DSL's positive effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>Our main contributions can be concluded as:</p><p>? We propose a visual-semantic data decomposing and multi-task constructing scheme for VTR task, and it is capable of being extended to other cross-modal tasks, such as image text generation, image text retrieval, image caption <ref type="bibr" target="#b21">Li et al. 2020;</ref><ref type="bibr" target="#b17">Zhang et al. 2021)</ref>. And some corresponding foundational explorations have been enforced.</p><p>? We state the problem of the contents heterogeneity in VTR for the first time and a corresponding dual optimalmatch hypothesis is proposed, where massive undiscovered works can be done in the future to perfect the video text retrieval task.</p><p>? The proposed CAMoE and Dual Softmax loss primarily specialized for contents heterogeneity advance the SOTA to a new level. We claim that CAMoE is a novel and promising architecture that can serve as the future crossmodal large-scale pre-training model. And DSL is one of the efficient ways to significantly improve the performance with eligible cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Video-text retrieval <ref type="bibr" target="#b33">(Sivic and Zisserman 2003;</ref><ref type="bibr">Yu, Kim, and Kim 2018;</ref><ref type="bibr">Zhu and Yang 2020;</ref><ref type="bibr" target="#b20">Lei et al. 2021;</ref><ref type="bibr" target="#b12">Gabeur et al. 2020;</ref><ref type="bibr" target="#b9">Dzabraev et al. 2021;</ref><ref type="bibr" target="#b27">Mithun et al. 2018;</ref><ref type="bibr" target="#b1">Arnab et al. 2021)</ref>, as a task evolved from image-text retrieval <ref type="bibr" target="#b10">(Faghri et al. 2017;</ref><ref type="bibr" target="#b11">Frome et al. 2013;</ref><ref type="bibr" target="#b13">Gong et al. 2014;</ref><ref type="bibr" target="#b15">Gu et al. 2018)</ref>, though developed for years, still largely follows the single-stream or two-stream architecture <ref type="bibr" target="#b9">(Dzabraev et al. 2021;</ref><ref type="bibr">Zhu and Yang 2020;</ref><ref type="bibr" target="#b20">Lei et al. 2021;</ref><ref type="bibr" target="#b12">Gabeur et al. 2020;</ref><ref type="bibr" target="#b10">Fang et al. 2021;</ref><ref type="bibr" target="#b24">Luo et al. 2021</ref>). For singlestream network, raw text and video frames are input into the network directly, and the cross-modal information is fused simultaneously. The two-stream network employs the separate text and video embedding extractors firstly and then matching the cross-modal embedding with specific fusion networks.</p><p>If dividing according to the ideas improving the performance, previous methods can be roughly inducted into two types: alignment-based and embedding-based methods.</p><p>The alignment-based method aims to decompose the video and text into a somehow regular structure to facilitate calculating the similarity. In the early practice, Le et al. <ref type="bibr" target="#b19">(Le et al. 2016</ref>) uses bag-of-visual word model with geometric verification to search for shots With the query location. <ref type="bibr" target="#b26">Markatopoulou et al. (Markatopoulou et al. 2017</ref>) encode the decomposed query into related semantic concepts and then conduct concept matching with specific videos. Recently, HGR <ref type="bibr" target="#b5">(Chen et al. 2020</ref>) is proposed to divide the sentence into three parts: events, action, and entity, with the hypothesis that all the captions own a relatively fixed hierarchical graph form. HCGC <ref type="bibr" target="#b17">(Jin et al. 2021)</ref> adopts the same sentence resolving strategy and introduces the hierar-chical cross-modal graph consistency learning into embedding space.</p><p>Unlike the alignment-based scheme, which is to a largely dependent on the regulation set by the designer, the methods of matching video and text features directly in the embedding space have begun to emerge with the development of big data and large models in recent years. MMT <ref type="bibr" target="#b12">(Gabeur et al. 2020</ref>) puts forward the completed two-stream transformer framework solving video text retrieval for the first time. MDMMT <ref type="bibr" target="#b9">(Dzabraev et al. 2021</ref>) considers that the action word in the text is the key to encode effectively. Consequently, several excellent pre-training models pre-trained on various datasets are adopted to replace the original video encoder in MMT, and it concludes that CLIP performs best. CLIP4Clip <ref type="bibr" target="#b24">(Luo et al. 2021)</ref>, the previous SOTA, proposes three different similarity calculators to model temporal dependency between video frames. CAMoE reported in this paper reaches a consensus of adopting CLIP as pre-training parameters with MDMMT and CLIP4Clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>In this section, we will introduce the overall architecture of our approach and elaborate on the submodules within it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Architecture</head><p>In general, we convert the video and text into three-stream outputs according to the designed rules and conduct the consistency learning as <ref type="figure">Fig.2</ref>.</p><p>For the semantic side, the nouns and verbs of the text are selected out with the pre-trained part-of-speech tagging (POS) models <ref type="bibr" target="#b34">(Toutanova et al. 2003;</ref><ref type="bibr" target="#b29">Ratnaparkhi 1996)</ref>, and sequentially transformed into nouns sentence and verbs sentence by sentence generation strategy (SGS). Then we adopt Bert <ref type="bibr" target="#b6">(Devlin et al. 2018)</ref> pre-trained by CLIP (Radford et al. 2021) to encode them into semantic representations. Though we have tried to add a scene sentence to strengthen the background information, little texts can be extracted from scene parts. Too much irrelevant noise input may be detrimental for training.</p><p>Referring to the visual side, for fair comparison and keeping the efficiency, we adopt only Vit <ref type="bibr" target="#b8">(Dosovitskiy et al. 2020)</ref> pre-trained by CLIP <ref type="bibr" target="#b28">(Radford et al. 2021)</ref> as the bottom feature extractor, which is the same as the previous SOTA method. Fusion, entity, and action experts are specially designed to learn distinct semantic matching from the bottom features. Note that the three tasks are separate but subordinative. A gate module <ref type="bibr" target="#b25">(Ma et al. 2018</ref>) is added to integrate entity and action representations with fusion ones, which improves fusion expert's performance.</p><p>After acquiring visual and semantic representations, they are matched in a similarity calculator, which leads to the loss value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence generation strategy.</head><p>As shown in <ref type="figure" target="#fig_1">Fig.3</ref>, we test three different sentence generation strategies (SGS), named recombining keywords (RKW), averaging keywords embedding (AKWE), and masking unconsidered words (MUW), respectively. For RKW, the keywords are recombined into a sentence and then encoded by Bert, in which cls-token embedding represents the output. AKWE ignores no words, instead attentions the whole sentence, and adopts the average of keywords' output token embedding. To not change the sentence structure and keep networks from attention to all words, which will lead to overfitting, MUW takes another way. It masks all the non-keywords and then represents with cls-token, which is described as following:</p><formula xml:id="formula_0">s t = Bert(M ask(S))<label>(1)</label></formula><p>Where s t denotes the semantic representations of task t, which is designed as sentence matching, entity matching, and action matching. S denotes the input sentence.</p><p>Visual Frames Aggregation Scheme.</p><p>We uniformly sample C frames for each video in batch B, and encode them into x ? R B?C?d with a dimension d.</p><p>We adopt three visual frames aggregation schemes and apply them to different experts or gates for various purposes, as </p><formula xml:id="formula_1">v = C i x i (2)</formula><p>Where v denotes the aggregation module output. ? Squeeze-and-Excitation attention (se attention) <ref type="bibr">(Hu,</ref> Shen, and Sun 2018) conducts average pooling firstly, and then calculate the importance score of each frame following feed forward networks(FFN) and sigmoid function:</p><formula xml:id="formula_2">scores = Sigmoid(F F N (AV (x))) (3) v = scores ? x<label>(4)</label></formula><p>Where scores, Sigmoid, F F N , and AV denote importance scores, sigmoid function, FFN, and average pooling operation. ? Self-attention <ref type="bibr" target="#b35">(Vaswani et al. 2017</ref>) projects each frame feature into key K, query Q, value V and then utilizes the matching degree of K and Q as the projection scores of V .</p><formula xml:id="formula_3">K, Q, V = F F N (x + p) (5) v = F F N (Sof tmax( QK T ? d K )V )<label>(6)</label></formula><p>Where p denotes the position embedding, and d K represents the dimension of K Experts and Gating network.</p><p>We employ different frame aggregation schemes for foreign experts and gates. Specifically, when the gating network and fusion expert adopt se attention, entity expert, and action expert adopt self-attention, the proposed CAMoE performs best. Relevant explorations are exhibited in Ablation Studies.</p><p>Since there are somehow subordinate relationships among the three tasks, we find the output of fusion expert v F will  <ref type="figure">Figure 2</ref>: An overview of the proposed CAMoE. SGS denotes the sentence generation strategy, which is explained in <ref type="figure" target="#fig_1">Fig.3</ref> A boy is performing for an audience.  result in the overfitting of entity matching and action matching when the gate is added to these two tasks. This may be because that fusion expert is paired with the whole sentences, such abundant information will make the other learning tasks too simple. So only fusion expert adopts a gating module:</p><formula xml:id="formula_4">u v g F = 3 i=1 g(x) i e i (x) (7) v g E = v E (8) v g A = v A<label>(9)</label></formula><p>where x denotes the input of mixture-of-gate, g and e mean gate and expert network. v g F , v g E , and v g A represent the visual input of the fusion, entity, and action matching unit. The gate network is composed of aggregation networks, a single layer perceptron, and a softmax layer calculating the importance scores among experts:</p><formula xml:id="formula_5">g(x) = sof tmax(W p ? AGG(x))<label>(10)</label></formula><p>where W p ? R d?E represents the projection matrix, and AGG denotes the aggregation networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss function.</head><p>The proposed Dual Softmax loss is based on symmetric cross-entropy loss. Every text and video are calculated the similarity with other videos or texts, which should be maximum in terms of the ground truth pair. The original symmetric cross-entropy loss is as below:</p><formula xml:id="formula_6">L v2t t = ? 1 B B i log exp(l?sim(vi,si)) B j=1 exp(l?sim(vi,sj ))<label>(11)</label></formula><formula xml:id="formula_7">L t2v t = ? 1 B B i log exp(l?sim(vi,si)) B j=1 exp(l?sim(vj ,si))<label>(12)</label></formula><formula xml:id="formula_8">L = t L v2t t + L t2v t<label>(13)</label></formula><p>Where t denotes sentence matching, entity matching, action matching, respectively. i and j denote the sample index in the batch. l denotes a logit scaling parameter. We adopt a uniform one for all experiments in this paper, which is the same as CLIP. sim represents the cosine similarity function:</p><formula xml:id="formula_9">sim(v i , s i ) = vi?si ||vi||?||si||<label>(14)</label></formula><p>As for the DSL, a prior are introduced to revise the similarity score:</p><formula xml:id="formula_10">L v2t t = ? 1 B B i log exp(l?sim(vi,si)?P r v2t i,i ) B j=1 exp(l?sim(vi,sj )?P r v2t i,j ) (15) L t2v t = ? 1 B B i log exp(l?sim(vi,si)?P r t2v i,i ) B j=1</formula><p>exp(l?sim(vj ,si)?P r t2v j,i ) (16) Where P r v2t , P r t2v denotes the prior matrix for Video-to-Text and Text-to-Video task, temp represents a temperature hyper-parameter to smooth the gradients. </p><p>The specific process is as the diagram shown in <ref type="figure" target="#fig_3">Fig.4</ref> Softmax, dim=1 Dot</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax, dim=0</head><p>Original similarity matrix Revised similarity matrix Sharpen result It is worth noting that the practical difference lies in the introduced prior calculated in the cross direction. Multiplying the prior with the original similarity matrix imposes an efficient constraint and can help to filter those single side match pairs. As a result, DSL highlights the one with both great Text-to-Video and Video-to-Text probability, conducting a more convincing result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Settings</head><p>Datasets We conduct the experiments on the benchmarks of MSR-VTT <ref type="bibr" target="#b37">(Xu et al. 2016)</ref>, MSVD(Chen and Dolan 2011), and LSMDC <ref type="bibr" target="#b30">(Rohrbach, Rohrbach, and Schiele 2015)</ref>.</p><p>? MSR-VTT We employ the MSR-VTT <ref type="bibr" target="#b37">(Xu et al. 2016</ref>) as the primary dataset, the most frequently researched object in video text retrieval. This dataset consists of 10000 videos, each 10 to 32s in length and 20 items in cation. To be precise, not all the cations are paired with the whole content of the corresponding video. Some may describe only a short clip, which makes the task more difficult. We report the result on 1K-A split <ref type="bibr" target="#b12">(Gabeur et al. 2020</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric.</head><p>We conform to the standard metric settings as <ref type="bibr" target="#b24">(Luo et al. 2021)</ref>, which reports Recall at rank K (R@K), median rank (MdR) and mean rank (MnR). Generally, the higher R@K and lower MdR, MnR signify better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details.</head><p>We uniformly sample 16 frames for each video. The dimension of visual and semantic embedding is 512. Bert, Vit (ViT-B/32), and logit scaling l are the same with CLIP. The learning rate of Bert and Vit are set to be 1e-7, and other parameters' are 1e-4. The optimizer and scheduler are Adam(Kingma and Ba 2014) and warmup <ref type="bibr" target="#b14">(Goyal et al. 2017</ref>). The sentence generation strategy adopts MUW. Visual frames aggregation scheme takes se attention for gate and fusion expert, self-attention for entity expert and action expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>? MSR-VTT Referring to <ref type="table">Table.</ref>1, firstly, we can conclude that the models based on CLIP usually perform better than others. It isn't surprising due to its powerful generalization ability after learning from massive data. The proposed CAMoE automatically parses the original single task into multi-tasks and surpasses previous SOTA in various metrics with a standard loss function. If adopting the Dual Softmax loss, it achieves a higher SOTA. The R@1 is of approximate 2.8% and 6.4 % increments, respectively. It's interesting that Dual Softmax loss imposes a more important effect on Video-to-Text than on Text-to-Video, which is consistent with our hypothesis that texts' descriptions can be unspecific and be matched with several videos. As shown in <ref type="figure">Fig.1 (in Appendix)</ref>, the advantages of CAMoE mainly reflect on alleviating overfitting. The main reason is that the extra two match units provide more complicated but critical targets and prevent the model from falling into local optimum by learning unprofitable words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? MSVD</head><p>On the test dataset of MSVD as shown in <ref type="figure">Fig.2</ref>, the purely model-optimized CAMoE increases the Text-to-Video R@1 SOTA to 46.9 and lowers the prediction mean rank by 0.2%. And Dual Softmax plays a more significant role by improving R@1 by 2.9% over previous SOTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? LSMDC</head><p>The result on LSMDC is shown in <ref type="figure" target="#fig_1">Fig.3</ref>. LSMDC differs from other datasets adopted in this paper, where it contains the most videos, and each video pairs with only one  <ref type="bibr" target="#b12">(Gabeur et al. 2020)</ref> 24   Ablation Studies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Design</head><p>Since the CAMoE architecture belongs to multi-tasks with different sentence input, we compare it with the single task and multi-tasks inputting the same captions. Moreover, gates added to all tasks are also explored. As shown in <ref type="table">Table.</ref>4, multi-task with distinct input and single gate outperforms others, which indicates the efficiency of experts learning information from particular aspects. Combining the gate with all experts improves little, this may be because that fusion expert, which learns much better, will lead the other experts struck in local optimum through gating module. So we do not recommend introducing gates to all tasks that own subordinate relationships. Sentence Generation Strategy and Visual Frames Aggregation Scheme <ref type="table">Table 5</ref>: Ablation study for the sentence generation strategy and visual frames aggregation scheme on MSR-VTT. RKW and AKWE are described in <ref type="figure" target="#fig_1">Fig.3</ref>. mean pooling, se attention, self-attention denote adopting the scheme for all gates and experts. CAMoE employs MUW for sentence generation, se attention for gate and fusion experts, self-attention for entity and action experts. The experimental results in <ref type="table">Table.</ref>5 are in line with our hypothesis. RKW destroys the original sentence organization, AKWE inputs the whole sentence and results in overfitting, which is contrary to our of intention making professional expert learn specific features.</p><p>As for the frames integration methods, although there is consensus that the capacity of self-attention, which is exactly appropriate for entity and action experts who urge for complicated embedding space transformation, is more advanced than that of mean pooling and se attention, it does not always perform best on limited dataset due to the additionally introduced parameters and computational effort. Se attention only increases a very small number of parameters, allowing the model to automatically learn to attention to the keyframes in each video. Finally, we conclude that when the gate and fusion experts adopt the se attention, entity and action experts employ the self-attention, the proposed method performs best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss optimization</head><p>Referring to the result in <ref type="table" target="#tab_4">Table.1, Table.2, and Table.</ref>3, Dual Softmax loss makes excellent progress in all metrics for all benchmarks. To further prove the generalization of Dual Softmax, we also test it on other methods such as CLIP, FROZEN, and CLIP4Clip. Referring to <ref type="table">Table.</ref>6, Dual Softmax provides a fantastic improvement for all models, and even more significant increments with lower based recall scores. For the example of CLIP V2T-R@1, around 10 points of gain is given, but as for CLIP4Clip, whose original R@1 exceeds 40%, the increase drops down to 4.9 %. For a method with weak generalization, the local optimum caused by content heterogeneity may be one of the most significant reasons. And we infer that it is still a valuable problem to explore for future works. Quantitative Analysis and Visualization Expert Importance Analysis It will be interesting to figure out how much each experts' information occupies the results and the accuracy each expert can reach alone. So we test the metrics from embedding space produced by each expert. Referring to <ref type="table">Table.</ref>7, the fusion expert matches with the whole sentence and performs best. Entity expert reaches about 27%, while action expert reaches 8.4% for T2V and 4.2% for V2T, which is the worst. In addition, the average weights of the three experts calculated by the gate on the MSR-VTT test dataset are 0.63, 0.29, and 0.08, which indicates the two extra experts make an impact. We infer the reason for the action expert's lower accuracy and weight is that the CLIP we adopt is pre-trained on the image-text pair dataset, which is more equipped with entity information. So, the future solutions that can largely improve this task may rely on large-scale video-text pretraining or replace the uniform feature extractor of CAMoE architecture with the professional entity, action extractors. This indicates that CAMoE is one of the most excellent solutions with little computation increments and can serve as a pretrain architecture for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual Softmax Visualization</head><p>To vividly illustrate the role of Dual Softmax, we compare the inferred probability matrix of Dual Softmax with that of the previous one as <ref type="figure" target="#fig_5">Fig.5</ref>. The visualization indicates that Dual Softmax improves in two aspects:</p><p>? Filtering the outliers.</p><p>? Sharpening the crucial and confidence points.</p><p>We reason that the constraints of cross direction correct partial borderline scores by introducing a prior probability matrix. For instance, when calculating the Video-to-Text matrix, a video can be paired with multiple unspecific and generalized texts. If introducing the prior Text-to-Video probability matrix, the sample with a high Video-to-Text similarity score but low Text-to-Video probability will be ignored, which filters the outliers and then leads to sharpening the convincing points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Loss</head><p>Dual Softmax Loss Scaling similarity matrix </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we identify the data heterogeneity of structure and content in the field of video-text retrieval. For the structure heterogeneity, a multi-stream corpus alignment architecture is proposed and reaches SOTA. We suggest the future work focus on extending CAMoE's uniform feature extractor to professional extractors or adopting CAMoE as largescale pre-training architecture. To solve the error caused by the confusing sentence that may match with over one video, Dual Softmax loss is proposed based on the dual optimalmatch hypothesis and surprisingly achieved significant improvement with little extra training burden, which indicates its wide range of application scenarios in the industry and academia.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Demonstration of Preventing Overfitting</head><p>As shown in <ref type="figure">Fig.6</ref>, we compare the loss evolution of single and multi-task in both training and test dataset. For the single task, its training loss drops rapidly to a very low level and is always less than test loss. However, the results turn to be reversed for multi-task designed in this paper, the training loss always maintains a relatively high value and larger than the one for test. These indicate that multi-task can indeed prevent the model from overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Visualization of Experts' Weights for Different Videos</head><p>To further verify whether the gating module has learned to assign appropriate weights for diverse videos and whether the extra two experts work, the weights for videos are tested as shown in <ref type="figure" target="#fig_7">Fig.7</ref>. The weights projected from the gate do vary a lot from video to video. From the first four exhibitions, where entity expert plays a crucial role, we can conclude that entity expert is relatively more critical when entity objects are more conspicuous, which means: ? The contents, styles, and perspectives of the frames are fairly consistent.</p><p>? There may exist homogeneous entities, which cannot exhibit various behaviors.</p><p>Referring to the last four exhibitions, though action expert still occupies a lower weight, the results are much more larger than the average value which is 0.08. We have explained that the lower action expert weight may be caused by CLIP pre-training, and may conclude that action expert imposes an impact for the following two situations:</p><p>? There are apparent entities in the video that can perform specific behaviors. They are most likely people. ? The overall contents and perspectives are quite different, and it may be tough to distinguish specific actions from one frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Experiments</head><p>We enforced the experiments on other Three datasets:</p><p>? MSR-VTT full, different from the 1k-A split reported in the text body, splits the whole dataset as 7k for training and 3k for test. And all the captions of test data are token into consideration. ? DiDeMo(Anne <ref type="bibr" target="#b0">Hendricks et al. 2017</ref>) contains about 10000 videos range from 12s to 429s. We conform the tradition to concatenate all captions of a video into its text query as previous works <ref type="bibr" target="#b24">(Luo et al. 2021;</ref>. ? Activitynet(Caba Heilbron et al. 2015) is comprised of 20000 videos, whose descriptions are concatenated into one query. The corresponding splits are the same as <ref type="bibr" target="#b24">(Luo et al. 2021;</ref>. The results show that the proposed method is of great generalization and can achieve SOTA in various datasets. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig.3. ? Mean pooling averages all frame features from a video directly:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The diagrams of three sentence generation strategies (SGS) and three visual frames aggregation schemes (VFAS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The diagram of DSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The comparison of original inference method and Dual Softmax.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) The loss-epoch diagram of the single task (b) The loss-epoch diagram of the multi-task Figure 6: The comparison of loss evolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>The visualization of experts' weights for different videos</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>performing for an audience. Bert Boy audience</head><label></label><figDesc></figDesc><table><row><cell>Output</cell></row><row><cell>Bert</cell></row><row><cell>Boy, audience.</cell></row><row><cell>A boy is</cell></row><row><cell>(a) The SGS of RKW</cell></row></table><note>Mean Output (b) The SGS of AKWE A boy is performing for an audience.Bert [mask] boy [mask] [mask] [mask] [mask] audience [mask] Output (c) The SGS of MUW??? Mean Pooling</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b30">Rohrbach, and Schiele 2015;</ref><ref type="bibr" target="#b31">Rohrbach et al. 2017)</ref> contains 118081 videos and equal captions extracted from 202 movies with a split of 109673, 7408, and 1000 as the train, validation, and test set. Every video is selected from movies ranging from 2 to 30 seconds.</figDesc><table><row><cell>? LSMDC(Rohrbach,</cell></row><row><cell>),</cell></row><row><cell>in which 9k videos and 18w captions are used to train</cell></row><row><cell>and another 1k videos as the test set.</cell></row><row><cell>? MSVD(Chen and Dolan 2011) is composed of 1970</cell></row><row><cell>videos with a split of 1200, 100, and 670 as the train,</cell></row><row><cell>validation, and test set, respectively. Each video is paired</cell></row><row><cell>with approximate 40 captions and ranges from 1 to 62</cell></row><row><cell>seconds.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Experimental results of comparison with previous excellent methods on MSR-VTT dataset.</figDesc><table><row><cell></cell><cell></cell><cell>Text-to-Video Retrieval</cell><cell cols="3">Video-to-Text Retrieval</cell></row><row><cell></cell><cell>Model</cell><cell cols="4">R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR</cell></row><row><cell></cell><cell cols="2">Collaborative Experts(Liu et al.) 20.9 48.8 62.4 6.0 28.2 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Others</cell><cell>MMT</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">: Experimental results of comparison with previous</cell></row><row><cell cols="5">excellent methods on MSVD dataset. All metrics are mea-</cell></row><row><cell cols="2">sured for Text-to-Video Retrieval.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">R@1 R@5 R@10 MnR</cell></row><row><cell cols="2">Collaborative Expert 19.9</cell><cell>49.0</cell><cell>63.8</cell><cell>23.1</cell></row><row><cell>FROZEN</cell><cell>33.7</cell><cell>64.7</cell><cell>76.3</cell><cell>-</cell></row><row><cell>CLIP</cell><cell>37</cell><cell>64.1</cell><cell>73.8</cell><cell>-</cell></row><row><cell>CLIP4Clip</cell><cell>46.2</cell><cell>76.1</cell><cell>84.6</cell><cell>10.0</cell></row><row><cell>CAMoE</cell><cell>46.9</cell><cell>76.1</cell><cell>85.5</cell><cell>9.8</cell></row><row><cell>CAMoE+DSL</cell><cell>49.8</cell><cell>79.2</cell><cell>87.0</cell><cell>9.4</cell></row><row><cell cols="5">caption. So present neural networks generally perform</cell></row><row><cell cols="5">poorly, while CAMoE still improves 0.9% in R@1. With</cell></row><row><cell cols="5">Dual Softmax loss, the improvement turns to be 4.3%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Experimental results of comparison with previous</cell></row><row><cell cols="5">excellent methods on LSMDC dataset. All metrics are mea-</cell></row><row><cell cols="3">sured for Text-to-Video Retrieval.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">R@1 R@5 R@10 MnR</cell></row><row><cell>CLIP</cell><cell>11.3</cell><cell>22.7</cell><cell>29.2</cell><cell>-</cell></row><row><cell>FROZEN</cell><cell>15.0</cell><cell>30.8</cell><cell>39.8</cell><cell>-</cell></row><row><cell>MDMMT</cell><cell>18.8</cell><cell>38.5</cell><cell>47.9</cell><cell>58.0</cell></row><row><cell>CLIP4Clip</cell><cell>21.6</cell><cell>41.8</cell><cell>49.8</cell><cell>58.0</cell></row><row><cell>CAMoE</cell><cell>22.5</cell><cell>42.6</cell><cell>50.9</cell><cell>56.5</cell></row><row><cell cols="2">CAMoE+DSL 25.9</cell><cell>46.1</cell><cell>53.7</cell><cell>54.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for the architecture design on MSR-VTT. MTAC denotes multi-task with all captions input.</figDesc><table><row><cell></cell><cell cols="4">Text-to-Video Video-to-Text</cell></row><row><cell>method</cell><cell cols="4">R@1 R@5 R@1 R@5</cell></row><row><cell cols="2">single task 43.1</cell><cell>70.8</cell><cell>41.8</cell><cell>70.2</cell></row><row><cell>MTAC</cell><cell>43.8</cell><cell>71.5</cell><cell>42.6</cell><cell>71.3</cell></row><row><cell cols="2">multi-gate 43.5</cell><cell>71.0</cell><cell>43.3</cell><cell>71.6</cell></row><row><cell>CAMoE</cell><cell>44.6</cell><cell>72.6</cell><cell>45.1</cell><cell>72.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of Dual Softmax for various methods. T2V and V2T denote Text-to-Video and Video-to-Text.</figDesc><table><row><cell>Method</cell><cell>Loss</cell><cell cols="2">Original loss</cell><cell>DSL</cell></row><row><cell></cell><cell></cell><cell cols="4">T2V-R@1 V2T-R@1 T2V-R@1 V2T-R@1</cell></row><row><cell>CLIP</cell><cell></cell><cell>31.2</cell><cell>27.2</cell><cell>35.6</cell><cell>37.2</cell></row><row><cell cols="2">FROZEN</cell><cell>31.0</cell><cell>-</cell><cell>45.5</cell><cell>-</cell></row><row><cell cols="2">CLIP4Clip</cell><cell>44.5</cell><cell>42.7</cell><cell>47.0</cell><cell>47.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Separate test for each Expert.</figDesc><table><row><cell>Expert</cell><cell>Loss</cell><cell cols="2">Original loss</cell><cell>DSL</cell></row><row><cell></cell><cell></cell><cell cols="3">T2V-R@1 V2T-R@1 T2V-R@1 V2T-R@1</cell></row><row><cell cols="2">Entity</cell><cell>27.7</cell><cell>26.9</cell><cell>31.1(+3.4) 30.5(+3.6)</cell></row><row><cell cols="2">Action</cell><cell>8.4</cell><cell>4.2</cell><cell>8.9(+0.5) 4.9(+0.7)</cell></row><row><cell cols="2">Fusion</cell><cell>44.6</cell><cell>45.1</cell><cell>47.3(+2.7) 49.1(+4.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Yu, Y.; Kim, J.; and Kim, G. 2018. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV), 471-487. Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J. 2021. VinVL: Making Visual Representations Matter in Vision-Language Models. CVPR 2021. Zhu, L.; and Yang, Y. 2020. Actbert: Learning global-local video-text representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 8746-8755.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>The experiments on MSR-VTT full, DiDeMo, and Activitynet.</figDesc><table><row><cell></cell><cell cols="4">Text-to-Video Video-to-Text</cell></row><row><cell>Dataset</cell><cell cols="4">R@1 R@5 R@1 R@5</cell></row><row><cell cols="2">MSR-VTT full 48.8</cell><cell>75.6</cell><cell>50.3</cell><cell>74.6</cell></row><row><cell>DiDeMo</cell><cell>43.8</cell><cell>71.4</cell><cell>45.5</cell><cell>71.2</cell></row><row><cell>Activitynet</cell><cell>51.0</cell><cell>77.7</cell><cell>49.9</cell><cell>77.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<title level="m">Vivit: A video vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00650</idno>
		<title level="m">Frozen in Time: A Joint Video and Image Encoder for Endto-End Retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10638" to="10647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13290</idno>
		<title level="m">CogView: Mastering Text-to-Image Generation via Transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mdmmt: Multidomain multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Komkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petiushko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3354" to="3363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<idno>arXiv:2106.11097</idno>
	</analytic>
	<monogr>
		<title level="m">CLIP2Video: Mastering Video-Text Retrieval via Image CLIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Devise: A deep visualsemantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A multiview embedding space for modeling internet images, tags, and their semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="233" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7181" to="7189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical Cross-Modal Graph Consistency Learning for Video-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1114" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Renoust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-N</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klinkigt</surname></persName>
		</author>
		<idno>NII-HITACHI-UIT at TRECVID 2016</idno>
		<title level="m">TRECVID</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-andlanguage learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Hit: Hierarchical transformer with momentum contrast for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15049</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv preprint arxiv:1907.13487</idno>
		<title level="m">Use What You Have: Video retrieval using representations from collaborative experts</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling task relationships in multi-task learning with multi-gate mixture-of-experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1930" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Query and keyframe representations for ad-hoc video search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Markatopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2017 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="407" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A maximum entropy model for partof-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on empirical methods in natural language processing</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The long-short story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="209" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="120" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1470" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10054</idno>
		<title level="m">Global-Local Sequence Alignment for Text-Video Retrieval</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
