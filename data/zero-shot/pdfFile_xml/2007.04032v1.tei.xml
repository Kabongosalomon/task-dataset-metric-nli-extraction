<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 23-27, 2020. August 23-27, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqing</forename><surname>Bian</surname></persName>
							<email>bianshuqing@ruc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqing</forename><surname>Bian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><forename type="middle">Yu</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion</title>
					</analytic>
					<monogr>
						<title level="j" type="main">KDD</title>
						<imprint>
							<biblScope unit="volume">20</biblScope>
							<date type="published">August 23-27, 2020. August 23-27, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403143</idno>
					<note>ACM ISBN 978-1-4503-7998-4/20/08. . . $15.00 Event, USA. ACM, New York, NY, USA, 9 pages. https: //</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Information systems ? Recommender systems</term>
					<term>? Comput- ing methodologies ? Natural language generation KEYWORDS Conversational Recommender System</term>
					<term>Knowledge Graph</term>
					<term>Mutual Information Maximization * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conversational recommender systems (CRS) aim to recommend high-quality items to users through interactive conversations. Although several efforts have been made for CRS, two major issues still remain to be solved. First, the conversation data itself lacks of sufficient contextual information for accurately understanding users' preference. Second, there is a semantic gap between natural language expression and item-level user preference.</p><p>To address these issues, we incorporate both word-oriented and entity-oriented knowledge graphs (KG) to enhance the data representations in CRSs, and adopt Mutual Information Maximization to align the word-level and entity-level semantic spaces. Based on the aligned semantic representations, we further develop a KGenhanced recommender component for making accurate recommendations, and a KG-enhanced dialog component that can generate informative keywords or entities in the response text. Extensive experiments have demonstrated the effectiveness of our approach in yielding better performance on both recommendation and conversation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, conversational recommender system (CRS) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref> has become an emerging research topic in seeking to provide high-quality recommendations through conversations with users. Different from traditional recommender systems, it emphasizes interactive clarification and explicit feedback in natural languages, and has a high impact on e-commerce.</p><p>In terms of methodology, CRS requires a seamless integration between a recommender component and a dialog component. On one hand, the dialog component clarifies user intents and replies to the previous utterance with suitable responses. On the other hand, the recommender component learns user preference and recommends high-quality items based on contextual utterances. To develop an effective CRS, several solutions have been proposed to integrate the two components, including belief tracker over semi-structured user queries <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref> and switching decoder for component selection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Although these studies have improved the performance of CRS to some extent, two major issues still remain to be solved. First, a conversation mainly consists of a few sentences, lack of sufficient contextual information for accurately understanding user preference. As shown in <ref type="table" target="#tab_1">Table 1</ref>, a user is looking for scary movies similar to "Paranormal Activity (2007)", where her/his preference is simply described by two short sentences. In order to capture the user's intent, we need to fully utilize and model the contextual information. In this example, it is important to understand the underlying semantics of the word "scary" and the movie "Paranormal Activity (2007)". Apparently, it is difficult to obtain such fact information solely based on the utterance text. Second, utterances are represented in natural languages, while actual user preference is reflected over the items or entities (e.g., actor and genre). There is a natural semantic gap between the two kinds of data signals. We need an effective <ref type="table" target="#tab_1">Table 1</ref>: An illustrative example of a user-system conversation for movie recommendation. The mentioned movies and important context words are marked in italic blue font and red font, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User</head><p>I am looking for some movies System What kinds of movie do you like? User Today I'm in a mood for something scary. Any similar movies like Paranormal Activity (2007)? System It (2017) might be good for you. It is a classic thriller movie with good plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User</head><p>Great! Thank you! semantic fusion way to understand or generate the utterances. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the system has presented both the recommended movie and the reason for recommendation. Without bridging the semantic gap, it is infeasible to generate the text for explaining the recommendation, e.g., "thriller movie with good plot". For enriching the conversation information, external knowledge graph (KG) has been utilized in CRS <ref type="bibr" target="#b3">[4]</ref>. They mainly focus on incorporating item knowledge, while the word-level enrichment (e.g., the relation between "scary" and "thriller") has been somehow neglected. Furthermore, they have not considered the semantic gap between natural language and external knowledge. Therefore, the utilization of KG data is likely to be limited. In essence, the problem originates from the fact that the dialog component and the recommender component correspond to two different semantic spaces, namely word-level and entity-level semantic spaces. Our idea is to incorporate two special KGs for enhancing data representations of both components, and fuse the two semantic spaces by associating the two KGs.</p><p>To this end, in this paper, we propose a novel conversational recommendation approach via KG based semantic fusion. Specially, we incorporate a word-oriented KG (i.e., ConceptNet <ref type="bibr" target="#b22">[23]</ref>) and an itemoriented KG (i.e., DBpedia <ref type="bibr" target="#b1">[2]</ref>). ConceptNet provides the relations between words, such as the synonyms, antonyms and co-occurrence words of a word; DBpedia provides the structured facts regarding the attributes of items. We first apply graph neural networks to learn node embeddings over the two KGs separately, and then propose to apply the Mutual Information Maximization <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref> method to bridge the semantic gap between the two KGs. The core idea is to force the representations of nodes in the two KGs to be close given the word-item co-occurrence in the conversation. In this way, we can unify the data representations in the two semantic spaces. Such a step is particularly useful to connect contextual words with items (including the mentioned entities) in conversations. Based on the aligned semantic representations, we further develop a KG-enhanced recommender component for making accurate recommendations, and a KG-enhanced dialog component that can generate informative keywords or items in the response text.</p><p>To our knowledge, it is the first time that the integration of dialog and recommender systems has been addressed by using KGenhanced semantic fusion. Our model utilizes two different KGs to enhance the semantics of words and items, respectively, and unifies their representation spaces. Extensive experiments on a public CRS dataset have demonstrated the effectiveness of our approach in both recommendation and conversation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Conversational recommendation system (CRS) contains two major modules, namely the recommender component and the dialog component. We first introduce the related work in the two aspects.</p><p>Recommender systems aim to identify a subset of items that meet the user's interest from the item pool. Traditional methods are highly based on the historical user-item interaction (e.g., click and purchase) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>. However, user-item interaction data is usually sparse. To tackle the data sparsity problem, many techniques have been developed by utilizing the side information of items, such as review and taxonomy data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref>. As a comparison, CRS mainly focuses on the recommendation setting through conversation instead of historical interaction data. Especially, knowledge graphs have been widely adopted to enhance the recommendation performance and explainability <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Conversation systems aim to generate proper responses given multi-turn contextual utterances. Existing works can be categorized into retrieval-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40]</ref> and generation-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>. The first category of approaches try to find the most reasonable response from a large repository of historical conversations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>, and the generation-based methods utilize learnable models to produce the response text. Based on the attentive seq2seq architecture <ref type="bibr" target="#b28">[29]</ref>, various extensions have been made to tackle the "safe response" problem and generate informative responses <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Early conversational or interactive recommendation systems mainly utilized predefined actions to interact with users <ref type="bibr" target="#b4">[5]</ref>. Recently, several studies started to integrate the two components for understanding users' needs and recommend the right items through natural language conversation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>. Overall, these methods emphasize the precise recommendation, while the conversation component is implemented by simple or heuristic solutions. Specially, a standard CRS dataset has been released in <ref type="bibr" target="#b17">[18]</ref>, and a hierarchical RNN model was proposed for utterance generation. Furthermore, follow-up studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref> incorporated external KG to improve the CRS, where their focus was to mainly enhance the item representations.</p><p>Based on previous studies, we design a novel conversational recommendation approach by incorporating and fusing word-level and entity-level knowledge graphs. Via KG fusion, our model is able to learn better data representations in both recommender and dialog components, which leads to better performance on item recommendation and utterance generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>Conversational recommendation systems (CRS) aim to recommend proper items to a user through a multi-turn conversation. In the conversation, a chat agent analyzes and learns the preference of the user according to contextual conversation history. Then, it either generates appropriate recommendations or starts a new round of conversation for further clarification. The process ends until the task succeeds or the user leaves. In a CRS, there are two major components to develop, namely the recommender component and the dialog component. The two components should be integrated seamlessly, and successful recommendation is considered as the final goal.</p><p>Formally, let u denotes a user from user set U, i denotes an item from item set I, and w denotes a word from vocabulary V. A conversation (or a conversation history) C consists of a list of utterances, denoted by C = {s t } n t =1 , in which each utterance s t is a conversation sentence at the t-th turn. At the t-th turn, the recommender component selects a set of candidate items I t from the entire item set I according to some strategy, while the dialog component needs to produce the next utterance s t to reply to previous utterances. Note that I t can be equal to ? when there is no need for recommendation. In such a case, the dialog component may raise a clarification question or generate a chit-chat response. Given a n-turn conversation, the goal of a CRS is to generate the response utterance to the user, including both the recommendation set I n+1 and the reply utterance s n+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPROACH</head><p>In this section, we present the KG-based Semantic Fusion approach to the CRS task, named KGSF. We first introduce how to encode both word-oriented and item-oriented KGs, and then fuse the semantics of the two KGs. Based on the fused KGs, we finally describe our solutions for both recommendation and conversation tasks. The overview illustration of the proposed model is presented in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoding External Knowledge Graphs</head><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, it is difficult to fully understand user preference and generate a suitable response. We identify the two basic semantic units in dialog and recommender systems, namely word and item, respectively. Here, we utilize two separate KGs to enhance the representations of the basic semantic units on both sides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Encoding</head><p>Word-oriented KG. We adopt the widely used Con-ceptNet <ref type="bibr" target="#b22">[23]</ref> as the word-oriented KG. It stores a semantic fact as a triple ?w 1 , r , w 2 ?, where w 1 , w 2 ? V are words and r is a word relation. Not all the words in ConceptNet are useful for our task. Hence, we only consider words that appear in our corpus, and extract their related triples from ConceptNet. We also remove words with very few triples related to the words in our corpus.</p><p>To encode the word-oriented KG, we adopt graph convolutional neural network <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> (GCN) for capturing the semantic relations between word nodes. At each update, GCN receives the information from the one-hop neighborhood in the graph and performs the aggregation operation as:</p><formula xml:id="formula_0">V (l ) = ReLU(D ? 1 2 AD ? 1 2 V (l ?1) W (l ) )<label>(1)</label></formula><p>where V (l ) ? R V ?d W are the representations of nodes and W (l ) is a learnable matrix at the l-th layer, A is the adjacency matrix of the graph and D is a diagonal degree matrix with entries</p><formula xml:id="formula_1">D[i, i] = j A[i, j].</formula><p>By stacking multiple convolutions, node information can be propagated along with the graph structure. When the algorithm ends, we can obtain a d W -dimensional representation n w for a word w. Here, we do not incorporate the relation information, because the number of relations is large and many relations are not directly useful to the recommendation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Encoding</head><p>Item-oriented KG. Another kind of semantic units to consider are items and their related entities. Following <ref type="bibr" target="#b3">[4]</ref>, we utilize DBpedia <ref type="bibr" target="#b1">[2]</ref> as item-oriented KG. Similar to ConceptNet, a triple in DBpedia is denoted by ?e 1 , r, e 2 ?, where e 1 , e 2 ? E are items or entities from the entity set E and r is entity relation from the relation set R. To extract the entity subgraph, we collect all the entities appearing in our corpus by following the approach from <ref type="bibr" target="#b3">[4]</ref>. Starting from these items and entities as seeds, we extract their one-hop triples on the DBpedia graph.</p><p>For item-oriented KG, relation semantics are important to consider. Different from ConceptNet, we utilize R-GCN <ref type="bibr" target="#b19">[20]</ref> to learn item representations on the extracted subgraph. Formally, the representation of node e at (l + 1)-th layer is calculated as:</p><formula xml:id="formula_2">n (l +1) e = ? r ?R e ? ?E r e 1 Z e,r W (l ) r n (l ) e ? + W (l ) n (l ) e<label>(2)</label></formula><p>where n (l ) e ? R d E is the node representation of e at the l-th layer, E r e denotes the set of neighboring nodes for e under the relation r , W (l ) r is a learnable relation-specific transformation matrix for the embeddings from neighboring nodes with relation r , W (l ) is a learnable matrix for transforming the representations of nodes at the l-th layer and Z e,r is a normalization factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">KG Fusion via Mutual Information Maximization</head><p>Above, we obtain the node representations for the word-oriented KG and item-oriented KG, denoted by two embedding matrices V (v w for word w) and N (n e for item e), respectively. In order to bridge the semantic gap between words and items, we propose to use Mutual Information Maximization technique <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>, called MIM. MIM has been used to mutually improve the data representations of two coupled signals (e.g., input and output). Its core idea is based on the concept of Mutual Information (MI). Given two variables X and Y , their MI is defined as:</p><formula xml:id="formula_3">MI (X , Y ) = D K L (p(X , Y )||p(X )p(Y )),<label>(3)</label></formula><p>where D K L is the Kullback-Leibler (KL) divergence between the joint distribution p(X , Y ) and the product of marginals p(X )p(Y ).</p><p>Usually, MI is difficult to compute. MIM tries to maximize it instead of seeking the precise value via the following formula:</p><formula xml:id="formula_4">MI (X , Y ) ? E P [?(x, y)] ? E N [?(x ? , y ? )],<label>(4)</label></formula><p>where E P and E P denote the expectation over positive and negative samples respectively, and ?(?) is the binary classification function that outputs a real number which can be modeled by a neural network.</p><p>In our setting, we have two kinds of semantic units (namely words and entities), and would like to align their semantic representation spaces. Given a conversation, we first collect words (non-stopwords) and entities (including items) from the utterance text. For an entity-word pair ?e, w? that co-occur in a conversation, we pull their representations close through a transformation matrix: </p><formula xml:id="formula_5">?(e, w) = ? (n ? e ? T ? v w ),<label>(5)</label></formula><formula xml:id="formula_6">? ? ? Matching 0.7 ' (#) ( (#)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head><p>User Embedding ) * <ref type="figure">Figure 1</ref>: The overview of our model with a movie recommendation scenario. Here,"SA", "KA", and "CA" denotes self-attention, KG-based attention and context-based attention, respectively.</p><p>where n e and v w are the learned node representations for entity e and word w via KGs, respectively, T ? R d E ?d W is the transformation matrix that aligns two semantic spaces, and ? (?) is the sigmoid function. To apply the MIM method, we can consider all the wordentity pairs co-occurring in a conversation as positive, while random word-entity pairs are considered as negative. By integrating Eq. 5 into Eq. 4, we can derive the objective loss over all the conversations and minimize the loss with an optimization algorithm.</p><p>However, a conversation usually contains a number of contextual words, and it is time-consuming to enumerate all the word-entity pairs. Besides, some of these words are noisy, which is likely to affect the final performance. Here, we add a super tokenw for a conversation, assuming that it is able to represent the overall semantics of the contextual words. Instead of considering all the word-entity pairs, we only model the relation between each entity and the super tokenw using the ?(?) function. We utilize self-attention mechanism for learning the representation ofw:</p><formula xml:id="formula_7">vw = V (C) ? ? ,<label>(6)</label></formula><formula xml:id="formula_8">? = softmax(b ? ? tanh(W ? V (C) )),</formula><p>where V (C) is the matrix consisting of the embeddings of all the contextual words in a conversation C, ? is an attention weight vector reflecting the importance of each word, and W ? and b are parameter matrix and vector to learn. Using such a super token, we can significantly improve efficiency and identify more important semantic information from the entire conversation.</p><p>In order to effectively align the semantic space of the two KGs, we adopt the MIM loss for pre-training the parameters of the GNN models in Section 4.1.1 and 4.1.2, which forces the two semantic spaces to be close at the beginning. During the fine-tuning stage, we treat the MIM loss as a regularization constraint for GNN to prevent overfitting.</p><p>With the representations of the fused KGs, we next describe how to make recommendations and generate utterances in Section 4.3 and 4.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">KG-enhanced Recommender Module</head><p>Given the learned word and item representations, we study how to generate a set of items for recommendation in CRS.</p><p>A key point for recommendation is to learn a good representation of user preference. Different from traditional recommender systems, following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref>, we assume no previous interaction records are available. We can only utilize the conversation data to infer user preference.</p><p>First, we collect all the words that appear in a conversation c. By using a simple lookup operation, we can obtain the word or item embeddings learned through the graph neural networks in Section 4.2. We concatenate the word embeddings into a matrix V (C) . Similarly, we can derive an item embedding matrix N (C) by combining the embeddings of items.</p><p>Next, we apply the similar self-attentive mechanism in Eq. 6 to learn a single word vector v (C) for V (C) and a single item vector n (C) for N (C) . In order to combine the two parts of information, we apply the gate mechanism to derive the preference representation p u of the user u:</p><formula xml:id="formula_9">p u = ? ? v (C) + (1 ? ?) ? n (C) ,<label>(7)</label></formula><formula xml:id="formula_10">? = ? (W gate [v (C) ; n (C) ]),</formula><p>Given the learned user preference, we can compute the probability that recommends an item i from the item set to a user u:</p><formula xml:id="formula_11">Pr r ec (i) = softmax(p ? u ? n i ),<label>(8)</label></formula><p>where n i is the learned item embedding for item i. We can utilize Eq. 8 to rank all the items and generate a recommendation set to a user. To learn the parameters, we set a cross-entropy loss as:</p><formula xml:id="formula_12">L r ec = ? N j=1 M i=1 ? (1 ? y i j ) ? log 1 ? Pr (j) r ec (i))<label>(9)</label></formula><p>+y i j ? log Pr (j)</p><formula xml:id="formula_13">r ec (i) + ? * L M I M ,</formula><p>where j is the index of a conversation, i is the index of an item, L M I M is the Mutual Information Maximization loss, and ? is a weighted parameter. Here, we loop the entire collection and compute the cross-entropy loss. We only present the case that a conversation has a ground-truth recommendation. While, it is straightforward to extend the above loss to the case with multiple ground-truth items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">KG-enhanced Response Generation Module</head><p>Here, we study how to generate a reply utterance in CRS. We adopt Transformer <ref type="bibr" target="#b26">[27]</ref> to develop the encoder-decoder framework. Our encoder follows a standard Transformer architecture. We mainly introduce the KG-enhanced decoder.</p><p>To better generate responses at decoding, we incorporate KGenhanced representations of context words and items. After the self-attention sub-layer, we conduct two KG-based attention layerss to fuse the information from the two KGs:</p><formula xml:id="formula_14">A n 0 = MHA(R n?1 , R n?1 , R n?1 ),<label>(10)</label></formula><formula xml:id="formula_15">A n 1 = MHA(A n 0 , V (C) , V (C) ),<label>(11)</label></formula><formula xml:id="formula_16">A n 2 = MHA(A n 1 , N (C) , N (C) ),<label>(12)</label></formula><formula xml:id="formula_17">A n 3 = MHA(A n 2 , X, X),<label>(13)</label></formula><formula xml:id="formula_18">R n = FFN(A n 3 ),<label>(14)</label></formula><p>where MHA(Q, K, V) defines the multi-head attention function <ref type="bibr" target="#b26">[27]</ref> that takes a query matrix Q, a key matrix K, and a value matrix V as input:</p><formula xml:id="formula_19">MHA(Q, K, V) = Concat(head 1 , . . . , head h )W O ,<label>(15)</label></formula><formula xml:id="formula_20">head i = Attention(QW Q i , KW K i , VW V i ),<label>(16)</label></formula><p>and FFN(x) defines a fully connected feed-forward network, which consists of a linear transformation with a ReLU activation layer:</p><formula xml:id="formula_21">FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 .<label>(17)</label></formula><p>Above, X is the embedding matrix output by the encoder, V <ref type="bibr">(C)</ref> and N (C) are KG-enhanced representation matrices for words and items in a conversation c, respectively. And, A n 0 , A n 1 , A n 2 and A n 3 are the representations after self-attention, cross-attention with embeddings from ConceptNet, cross-attention with embeddings from DBpedia and cross-attention with encoder output, respectively. Finally, R n is the embedding matrix from the decoder at n-th layer. Acquire items' and words' representations from G 1 and G 2 by Eq. 1 and Eq. 2, respectively. <ref type="bibr" target="#b4">5</ref> Acquire v <ref type="bibr">(C )</ref> and v (C ) by self-attention using Eq. 6. Acquire p u by gate mechanism using Eq. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7</head><p>Compute Pr r ec (i) using Eq. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head><p>Perform GD on Eq. 9 w.r.t. ? ? and ? r . 9 end 10 for i = 1 ? | D | do <ref type="bibr" target="#b10">11</ref> Acquire items' and words' representations from G 1 and G 2 by Eq. 1 and Eq. 2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12</head><p>Acquire R n by KG-enhanced Transformer using Eq. 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13</head><p>Compute Pr(y |y 1 , ? ? ? , y i ?1 ) using Eq. 18. Compared with a standard Transformer decoder, we have two additional steps in Eq. 11 and 12. The idea can be described using a transformation chain: generated words</p><formula xml:id="formula_22">Eq . 11 ?????? word-oriented KG Eq . 12 ?????? item-oriented KG Eq . 13 ?????? context words.</formula><p>Following such a chain, our decoder is able to gradually inject useful knowledge information from the two KGs in a sequential manner. The rationality for Eq. 12 lies in the fact that we have fused the two KGs as in Section 4.2.</p><p>Different from chit-chat models, the generated reply is expected to contain the recommended items, related entities and descriptive keywords. We further adopt the copy mechanism to enhance the generation of such tokens. Formally, given the predicted subsequence y 1 , ? ? ? , y i?1 , the probability of generating y i as the next token is given as:</p><formula xml:id="formula_23">Pr(y i |y 1 , ? ? ? , y i?1 ) = Pr 1 (y i |R i ) + Pr 2 (y i |R i , G 1 , G 2 ),<label>(18)</label></formula><p>where Pr 1 (?) is the generative probability implemented as a softmax function over the vocabulary by taking the decoder output R i (Eq. 14) as input, Pr 2 (?) is the copy probability implemented by following a standard copy mechanism <ref type="bibr" target="#b6">[7]</ref> over the nodes of the two KGs, and G 1 , G 2 denote the two KGs we have used. To learn the response generation module, we set the cross-entropy loss as:</p><formula xml:id="formula_24">L ?en = ? 1 N N t =1</formula><p>log Pr(s t |s 1 , ? ? ? , s t ?1 )) ,</p><p>where N is the number of turns in a conversation C. We compute this loss for each utterance s t from C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Learning</head><p>Our parameters to learn are organized by three groups, namely the KG module, recommender module and conversation module, denoted by ? ? , ? r and ? d respectively. Algorithm 1 presents the To train the joint model, we pre-train the knowledge graph module ? ? using Mutual Information Maximization loss. Next, we optimize the parameters in ? r and ? ? . At each iteration, we first acquire words' and items' representations from the KG module. Then, we perform the self-attention and gate mechanism to derive user representations. Finally, we compute a cross-entropy loss by Eq. 9 with the MIM regularization, and perform gradient descent to update parameters ? r and ? ? .</p><p>When the loss of the recommender component converges, we optimize the parameters in ? d . At each iteration, we first obtain words' and items' representations from the knowledge graph module. Then, we utilize KG-enhanced Transformer to derive the contextual representations. Finally, we compute the cross-entropy loss by Eq. 19, and perform gradient descent to update parameters in ? d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>In this section, we first set up the experiments, and then report the results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>5.1.1 Dataset. We evaluate our model on the REcommendations through DIALog (REDIAL) dataset, which is a conversational recommendation dataset released by <ref type="bibr" target="#b17">[18]</ref>. This dataset was constructed through Amazon Mechanical Turk (AMT). Following a set of comprehensive instructions, the AMT workers generated dialogs for recommendation on movies in a seeker-recommender pair. It contains 10,006 conversations consisting of 182,150 utterances related to 51,699 movies. This dataset is split into training, validation and test sets using a ratio of 8:1:1. For each conversation, we start from the first sentence one by one to generate reply utterances or recommendations by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baselines.</head><p>In CRS, we consider two major tasks for evaluation, namely recommendation and conversation.</p><p>? Popularity: It ranks the items according to historical recommendation frequencies in the corpus.</p><p>? TextCNN <ref type="bibr" target="#b11">[12]</ref>: It adopts a CNN-based model to extract textual features from contextual utterances as user embedding.</p><p>? Transformer <ref type="bibr" target="#b26">[27]</ref>: It applies a Transformer-based encoderdecoder framework to generate proper responses without information from recommender module.</p><p>? REDIAL <ref type="bibr" target="#b17">[18]</ref>: This model has been proposed in the same paper with our dataset <ref type="bibr" target="#b17">[18]</ref>. It basically consists of a dialog generation module based on HRED <ref type="bibr" target="#b21">[22]</ref>, a recommender module based on auto-encoder <ref type="bibr" target="#b7">[8]</ref> and a sentiment analysis module.</p><p>? <ref type="bibr">KBRD [4]</ref>: This model utilizes DBpedia to enhance the semantics of contextual items or entities. The dialog generation module is based on the Transformer architecture, in which KG information serves as word bias for generation.</p><p>Among these baselines, Popularity and TextCNN <ref type="bibr" target="#b11">[12]</ref> are recommendation methods, and Transformer <ref type="bibr" target="#b26">[27]</ref> is the state-of-the-art text generation method. We do not include other recommendation models, since there are no historical user-item interaction records except the text of a single conversation. Besides, REDIAL <ref type="bibr" target="#b17">[18]</ref> and KBRD <ref type="bibr" target="#b3">[4]</ref> are conversation recommendation methods. We name our proposed model as KGSF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Evaluation Metrics.</head><p>In our experiments, we adopt different metrics to evaluate the two tasks. For recommendation task, following <ref type="bibr" target="#b3">[4]</ref>, we adopt Recall@k (k = 1, 10, 50) for evaluation. Besides the standard setting, we consider a specific scenario in recommender systems, namely cold start.</p><p>In CRS, this problem can be alleviated to some extent, since we have conversation contexts. In order to simulate the cold-start scenario in CRS, we only consider the test cases without any mentioned items in context. This experiment aims to examine whether our fusion strategy is useful to learn user preference from word-based utterances. For the conversation task, the evaluation consists of automatic evaluation and human evaluation. Following <ref type="bibr" target="#b3">[4]</ref>, we use Distinct n-gram (n = 2, 3, 4) to measure the diversity at sentence level. For CRS, it is particularly important that the dialog system is able to generate informative replies related to items or entities. Hence, we introduce a new metric that calculates the ratio of items in the generated utterances. Different from traditional conversation tasks, we do not need to generate response resembling the groundtruth utterance. Instead, the final goal is to successfully make the recommendations. For this reason, we adopt human evaluation (on a random selection of 100 multi-turn dialogs from the test set) instead of using BLEU metrics. We invite three annotators to score the generated candidates in two aspects, namely Fluency and Informativeness. The range of score is 0 to 2. The final performance is calculated using the average scores of the three annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Implementation Details.</head><p>We follow the procedure in Algorithm 1 to implement our approach with Pytorch 1 . The dimensionality of embeddings (including hidden vectors) is set to 300 and 128, respectively, for conversation and recommender modules. We initialize word embeddings via word2vec 2 . In the KG module, we set the layer number to 1 for both GNN networks. We use Adam optimizer <ref type="bibr" target="#b12">[13]</ref> with the default parameter setting. In experiments, the batch size is set to 32, the learning rate is 0.001, gradient clipping restricts the gradients within [0,0.1], and the normalization constant Z v,r of R-GCN in Eq. 2 is 1. During pre-training, we directly optimize the MIM loss as Section 4.2. While, during fine-tuning, the weight ? of the MIM loss in Eq. 9 is 0.025. Our code is publicly available via the link: https://github.com/RUCAIBox/KGSF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on Recommendation Task</head><p>In this subsection, we conduct a series of experiments on the effectiveness of the proposed model for the recommendation task. <ref type="table" target="#tab_3">Table 2</ref> presents the performance of different methods in the two settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Evaluation on All Data</head><p>Setting. We first consider the all data setting. As we can see, Popularity achieves a comparable performance with TextCNN. For our CRS task, utterance text is likely to be sparse and noisy, while Popularity utilizes global statistics for recommending popular items. Second, the two CRS models Redial and KBRD perform better than Popularity and TextCNN. Compared with TextCNN, Redial and KBRD only utilize the entities or items in context to make recommendations. Furthermore, KBRD performs better than Redial, since it incorporates external KG information. It indicates that KG data is useful to enhance the data representations and improves the performance of the CRS task. Finally, our model KGSF outperforms the baselines with a large margin. KGSF has incorporated both word-oriented and entity-oriented KGs, and further fuses the two KGs for enhancing the data representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Evaluation on Cold Start</head><p>Setting. For the cold start setting, first, the heuristic method Popularity performs very well, even better than TextCNN and ReDial in most cases. A possible reason is that in real-world recommender systems, a new user is likely to adopt a popular item. When there are no items or attributes mentioned in the context, the performance of KBRD and Redial has decreased substantially. As a comparison, our model KGSF still performs stably and achieves the best performance among all the methods. The major reason is that it not only utilizes item-oriented KG, but also uses word-oriented KG. By aligning the two semantic spaces, it can capture important evidence from utterance text to infer user preference in the cold start setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Ablation</head><p>Study. In our model, we have incorporated two external KGs and adopted the Mutual Information Maximization method for KG fusion. Here, we would like to examine the contribution of each part. We incorporate two variants of our model for ablation analysis, namely KGSF w/o MIM and KGSF w/o DB, which remove the MIM loss and the DBpedia KG, respectively. Overall, we can see that both components contribute to the final performance. Besides, after removing the item-oriented KG, the performance decreases more significantly. It is because once it was removed, the corresponding fusion component with MIM is also removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.4</head><p>The Effect of MIM Technique. We adopt the MIM technique to fuse semantic representations from two KGs. As shown in previous experiments, it is useful to improve the performance of our model on the recommendation task. Here, we would like to study whether its improvement is consistent and stable with the increase of the iteration number. We gradually increase the number of iterations for our model on the training set, and report the corresponding performance on the test set. <ref type="figure" target="#fig_2">Figure 2</ref> shows how the performance of our model varies with the increase of iterations. We can see that   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on Conversation Task</head><p>In this subsection, we construct a series of experiments on the effectiveness of the proposed model on the conversation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Automatic Evaluation.</head><p>We present the results of the automatic evaluation for different methods in <ref type="table" target="#tab_4">Table 3</ref>. First, ReDial performs better than Transformer in Distinct-2/3/4, since it utilizes a pre-training RNN model to encode history utterances. While, Transformer performs better with the metric of Item Ratio. A possible reason is that Transformer architecture adopts the self-attention mechanism for capturing temporal pairwise interaction, which is more suitable to model the relations between words and items than RNN and CNN. Second, among the three baselines, KBRD generates the most diverse responses and achieves the highest item ratio, i.e., containing more mentions of items in the generated text. This model  <ref type="figure">Figure 3</ref>: Ablation study on conversation task.</p><p>utilizes KG information to promote the predictive probability of entities and items. Compared with these baselines, our KGSF model is consistently better in all evaluation metrics. KGSF has utilized the KG information in two major steps, namely the knowledgeenhanced Transformer decoder and the copy mechanism, which enhances the informativeness of the generated text. <ref type="table" target="#tab_5">Table 4</ref> presents the result of human evaluation for the conversation task. First, among the three baselines, ReDial performs best in terms of the metric of Fluency, since it utilizes a pre-training encoder on multiple language tasks <ref type="bibr" target="#b23">[24]</ref>. However, we find that it tends to generate short and repetitive responses. This is the so-called "safe response" issue in the dialog generation task. Without additional supervision signal, it is likely to overfit to the frequent utterances in the training set. Second, KBRD performs best in terms of Informativeness score among the three baselines. It utilizes KG data to promote the probability of low-frequency words. Finally, our proposed model KGSF is consistently better than all the baselines with a large margin. We carefully design a KG-enhanced Transformer decoder. Our model is able to utilize contextual information effectively, and generate fluent and informative responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Human Evaluation.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Ablation Study.</head><p>We also conduct the ablation study based on three variants of our complete model, include: (1) KGSF w/o KG-D by removing the KG-based attention layers from the Transformer decoder, (2) KGSF w/o copy by removing the copy mechanism, and (3) KGSF w/o MIM by removing the MIM loss. As shown in <ref type="figure">Figure 3</ref>, first, all the techniques are useful to improve the final performance. Besides, the KG-based attention layer seems to be more important in our task, yielding a significant decrease when removed. KG-based attention layers can effectively inject the fused KG information into S1: How can I help you today?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recommender System</head><p>S2: I would like to watch a fantasy movie. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversation System</head><p>Our Our help recommend today fantasy movie perfect have seen good another recommendation S6: Great! We share the same taste. <ref type="figure">Figure 4</ref>: A sampled conversation with six-turn utterances between our CRS agent (recommender) and a real user (seeker). We use color bars to indicate attention weights of words in the recommendation component. The first-round recommendation is unsuccessful, while the second-round recommendation is successful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversation System</head><p>the decoder by multi-head attention mechanism. For the MIM loss, besides its contribution to the recommendation task (See <ref type="table" target="#tab_3">Table 2</ref>), it also improves the quality of the generated responses, which indicates its usefulness for KG-based semantic fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Analysis</head><p>In this part, we present a qualitative example to illustrate how our model works in practice. In <ref type="figure">Fig. 4</ref>, a user requests the recommendations on fantasy movies, and our system accurately identifies the key word "fantasy" by assigning a larger attention weight. The attention weights are computed by the self-attentive mechanism in Eq. 6 based on the KGenhanced word embeddings. With the focused preference of fantasy, our recommender component returns the candidate "Pan's Labyrinth". While, interestingly, our dialog component not only includes the mentions of the recommended movie, but also generates another related movie ("Stardust") in the utterance. Receiving the response, the user rejects the recommendation since she/he has watched both movies before. Then, our recommender component updates the representation of user preference, and returns another recommendation. Although the words of "have" and "seen" received a small attention weight by the recommender component, our dialog component also boosts their weights since they are helpful to generate a more informative reply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this paper, we proposed a novel KG-based semantic fusion approach for CRS. By utilizing two external KGs, we enhanced the semantic representations of words and items, and used Mutual Information Maximization to align the semantic spaces for the two different components. Based on the aligned semantic representations, we developed a KG-enhanced recommendation component for making accurate recommendations, and a KG-enhanced dialog component that can generate informative keywords or entities in the utterance text. By constructing extensive experiments, our approach yielded better performance than several competitive baselines.</p><p>As future work, we will consider using more kinds of external information to improve the performance of CRS, e.g., user demographics <ref type="bibr" target="#b35">[36]</ref>. Besides, we will investigate how to make the utterance more persuasive and explainable for the recommendation results. Finally, another interesting topic is how to incorporate historical user-item interaction data and start the conversation with a pre-learned user profile.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>6</head><label>6</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Performance (Recall@10 and Recall@50) comparison of KGFS with and without the MIM loss on test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>The training algorithm for the KGFS model. The conversation recommendation dataset D, item-oriented KG G 1 , and word-oriented KG G 2 Output: Model parameters ? ? , ? r and ? d . Randomly initialize ? ? , ? r and ? d . Pre-train the ? ? by minimizing the MIM loss in 4.2.</figDesc><table /><note>Input:123 for t = 1 ? | D | do 4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on recommendation task. Numbers marked with * indicate that the improvement is statistically significant compared with the best baseline (t-test with p-value &lt; 0.05).</figDesc><table><row><cell>Test</cell><cell></cell><cell>All data</cell><cell></cell><cell></cell><cell>Cold start</cell><cell></cell></row><row><cell>Models</cell><cell>R@1</cell><cell>R@10</cell><cell>R@50</cell><cell>R@1</cell><cell>R@10</cell><cell>R@50</cell></row><row><cell>Popularity</cell><cell>0.012</cell><cell>0.061</cell><cell>0.179</cell><cell>0.020</cell><cell>0.097</cell><cell>0.239</cell></row><row><cell>TextCNN</cell><cell>0.013</cell><cell>0.068</cell><cell>0.191</cell><cell>0.011</cell><cell>0.081</cell><cell>0.239</cell></row><row><cell>ReDial</cell><cell>0.024</cell><cell>0.140</cell><cell>0.320</cell><cell>0.021</cell><cell>0.075</cell><cell>0.201</cell></row><row><cell>KBRD</cell><cell>0.031</cell><cell>0.150</cell><cell>0.336</cell><cell>0.026</cell><cell>0.085</cell><cell>0.242</cell></row><row><cell>KGSF</cell><cell cols="6">0.039* 0.183* 0.378* 0.039* 0.174* 0.370*</cell></row><row><cell>-MIM</cell><cell>0.037</cell><cell>0.175</cell><cell>0.356</cell><cell>0.037</cell><cell>0.158</cell><cell>0.331</cell></row><row><cell>-DB</cell><cell>0.027</cell><cell>0.121</cell><cell>0.256</cell><cell>0.030</cell><cell>0.168</cell><cell>0.346</cell></row><row><cell cols="7">training algorithm for our KGSF model. The three components</cell></row><row><cell cols="4">share parameters and affect each other.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>with the MIM technique, our model can achieve a good result with fewer iterations compared to the variant without MIM. Overall, besides the performance improvement, the MIM technique is useful to improve the stability of the training process.</figDesc><table><row><cell cols="5">Automatic evaluation results on the conversation</cell></row><row><cell cols="5">task. We abbreviate Distinct-2,3,4 as Dist-2,3,4. Numbers</cell></row><row><cell cols="5">marked with * indicate that the improvement is statistically</cell></row><row><cell cols="5">significant compared with the best baseline (t-test with p-</cell></row><row><cell>value &lt; 0.05).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell cols="4">Dist-2 Dist-3 Dist-4 Item Ratio</cell></row><row><cell cols="2">Transformer 0.148</cell><cell>0.151</cell><cell>0.137</cell><cell>0.194</cell></row><row><cell>ReDial</cell><cell>0.225</cell><cell>0.236</cell><cell>0.228</cell><cell>0.158</cell></row><row><cell>KBRD</cell><cell>0.263</cell><cell>0.368</cell><cell>0.423</cell><cell>0.296</cell></row><row><cell>KGSF</cell><cell cols="3">0.289* 0.434* 0.519*</cell><cell>0.325*</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Human evaluation results on the conversation task. Numbers marked with * indicate that the improvement is statistically significant compared with the best baseline (ttest with p-value &lt; 0.05).</figDesc><table><row><cell>Models</cell><cell></cell><cell cols="3">Fluency Informativeness</cell></row><row><cell cols="2">Transformer</cell><cell>0.92</cell><cell>1.08</cell></row><row><cell>ReDial</cell><cell></cell><cell>1.37</cell><cell>0.97</cell></row><row><cell>KBRD</cell><cell></cell><cell>1.18</cell><cell>1.18</cell></row><row><cell>KGSF</cell><cell></cell><cell>1.54*</cell><cell>1.40*</cell></row><row><cell>0.6</cell><cell>KGSF</cell><cell></cell><cell cols="2">KGSF w/o copy</cell></row><row><cell></cell><cell cols="2">KGSF w/o MIM</cell><cell cols="2">KGSF w/o KG-D</cell></row><row><cell>0.45</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.15</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>distinct-2</cell><cell cols="2">distinct-3</cell><cell>distinct-4</cell><cell>item ratio</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://pytorch.org/ 2 https://radimrehurek.com/gensim/models/word2vec.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TopicMF: Simultaneously Exploiting Ratings and Reviews for Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DBpedia -A crystallization point for the Web of Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Web Semant</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recommender systems survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s</forename><surname>Bobadilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Guti?rrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="109" to="132" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Towards Knowledge-Based Recommender Dialog System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1803" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards Conversational Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantina</forename><surname>Christakopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghua</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distributed-Representation Based Hybrid Recommender System with Short Item Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarvan</forename><surname>Hankz Hankui Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Law</surname></persName>
		</author>
		<idno>arXiv:cs.IR/1703.04854</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Taxonomy-Aware Multi-Hop Reasoning Networks for Sequential Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaole</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<idno>WSDM 2019. 573???581</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jian</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2018. 505???514</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An Information Retrieval Approach to Short Text Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1408.6988</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimation-Action-Reflection: Towards Deep Interaction Between Conversational and Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM 20</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="304" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Diversity-Promoting Objective Function for Neural Conversation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Laurent Charlin, and Chris Pal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Towards Deep Conversational Recommendations</title>
		<imprint>
			<biblScope unit="page" from="9748" to="9758" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>abs/1907.00710</idno>
		<title level="m">Deep Conversational Recommender in Travel. ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="593" to="607" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ConceptNet 5.5: An Open Multilingual Graph of General Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conversational Recommender System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention is All you Need</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Deep Graph Infomax</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A Neural Conversational Model. CoRR abs/1506</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">5869</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Explainable reasoning over knowledge graphs for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canran</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5329" to="5336" />
		</imprint>
	</monogr>
	<note>Xiangnan He, Yixin Cao, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3368" to="3373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C S</forename><surname>Lui</surname></persName>
		</author>
		<idno>arXiv:cs.LG/1906.01219</idno>
		<title level="m">Toward Building Conversational Recommender Systems: A Contextual Bandit Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards Conversational Search and Recommendation: System Ask, User Respond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">We know what you want to buy: a demographic-based system for product recommendation on microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1935" to="1944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">KB4Rec: A Data Set for Linking Knowledge Bases with Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaole</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunlin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="121" to="136" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Commonsense Knowledge Aware Conversation Generation with Graph Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised Context Rewriting for Open Domain Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1834" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-view Response Selection for Human-Computer Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
