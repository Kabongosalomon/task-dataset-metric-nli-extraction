<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MagFace: A Universal Representation for Face Recognition and Quality Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Meng</surname></persName>
							<email>qmeng@aibee.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Algorithm Research</orgName>
								<orgName type="institution">Aibee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Zhao</surname></persName>
							<email>sczhao@aibee.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Algorithm Research</orgName>
								<orgName type="institution">Aibee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhida</forename><surname>Huang</surname></persName>
							<email>zdhuang@aibee.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Algorithm Research</orgName>
								<orgName type="institution">Aibee Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
							<email>fzhou@aibee.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Algorithm Research</orgName>
								<orgName type="institution">Aibee Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MagFace: A Universal Representation for Face Recognition and Quality Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of face recognition system degrades when the variability of the acquired faces increases. Prior work alleviates this issue by either monitoring the face quality in pre-processing or predicting the data uncertainty along with the face feature. This paper proposes MagFace, a category of losses that learn a universal feature embedding whose magnitude can measure the quality of the given face. Under the new loss, it can be proven that the magnitude of the feature embedding monotonically increases if the subject is more likely to be recognized. In addition, Mag-Face introduces an adaptive mechanism to learn a wellstructured within-class feature distributions by pulling easy samples to class centers while pushing hard samples away. This prevents models from overfitting on noisy low-quality samples and improves face recognition in the wild. Extensive experiments conducted on face recognition, quality assessments as well as clustering demonstrate its superiority over state-of-the-arts. The code is available at https://github.com/IrvingMeng/MagFace.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing face in the wild is difficult mainly due to the large variability exhibited by face images acquired in unconstrained settings. This variability is associated to the image acquisition conditions (such as illumination, background, blurriness, and low resolution), factors of the face (such as pose, occlusion and expression) or biases of the deployed face recognition system <ref type="bibr" target="#b35">[36]</ref>. To cope with these challenges, most relevant face analysis system under unconstrained environment (e.g., surveillance video) consists of three stages: 1) face acquisition to select from a set of raw images or capture from video stream the most suitable face image for recognition purpose; 2) feature extraction to extract discriminative representation from each face image; 3) facial application to match the reference image towards a given gallery or cluster faces into groups of same person.  <ref type="figure">Figure 1</ref>: MagFace learns for (a) in-the-wild faces (b) a universal embedding by pulling the easier samples closer to the class center and pushing them away from the origin o. As shown in our experiments and supported by mathematical proof, the magnitude l before normalization increases along with feature's cosine distance to its class center, and therefore reveals the quality for each face. The larger the l, the more likely the sample can be recognized.</p><p>To acquire the optimal reference image in the first stage, a technique called face quality assessment <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref> is often employed on each detected face. Although the ideal quality score should be indicative of the face recognition performance, most of early work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> estimates qualities based on human-understandable factors such as luminances, distortions and pose angles, which may not directly favor the face feature learning in the second stage. Alternatively, learning-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref> train quality assessment models with artificially or human labelled quality values. Theses methods are error-prone as there lacks of a clear definition of quality and human may not know the best characteristics for the whole systems.</p><p>To achieve high end-to-end application performances in the second stage, various metric-learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref> or classification losses <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5]</ref> emerged in the past few years. These works learn to represent each face image as a deterministic point embedding in the latent space regardless of the variance inherent in faces. In reality, however, low-quality or large-pose images like <ref type="figure">Fig. 1a</ref> widely exist and their facial features are ambiguous or absent.</p><p>Given these challenges, a large shift in the embedded points is inevitable, leading to false recognition. For instance, performance reported by prior state-of-the-art <ref type="bibr" target="#b28">[29]</ref> on IJB-C is much lower than LFW. Recently, confidence-aware methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7]</ref> propose to represent each face image as a Gaussian distribution in the latent space, where the mean of the distribution estimates the most likely feature values while the variance shows the uncertainty in the feature values. Despite the performance improvement, these methods seek to separate the face feature learning from data noise modeling. Therefore, additional network blocks are introduced in the architecture to compute the uncertainty level for each image. This complicates the training procedure and adds computational burden in inference. In addition, the uncertainty measure cannot be directed used in conventional metrics for comparing face features. This paper proposes MagFace to learn a universal and quality-aware face representation. The design of MagFace follows two principles: 1) Given the face images of the same subject but in different levels of quality (e.g., <ref type="figure">Fig. 1a</ref>), it seeks to learn a within-class distribution, where the highquality ones stay close to the class center while the lowquality ones are distributed around the boundary. 2) It should pose the minimum cost for changing existing inference architecture to measure the face quality along with the computation of face feature. To achieve the above goals, we choose magnitude, the independent property to the direction of the feature vector, as the indicator for quality assessment. The core objective of MagFace is to not only enlarge inter-class distance, but also maintain a cone-like withinclass structure like <ref type="figure">Fig. 1b</ref>, where ambiguous samples are pushed away from the class centers and pulled to the origin. This is realized by adaptively down-weighting ambiguous samples during training and rewarding the learned feature vector with large magnitude in the MagFace loss. To sum up, MagFace improves previous work in two aspects: 1. For the first time, MagFace explores the complete set of two properties associated with feature vector, direction and magnitude, in the problem of face recognition while previous works often neglect the importance of the magnitude by normalizing the feature. With extensive experimental study and solid mathematical proof, we show that the magnitude can reveal the quality of faces and can be bundled with the characteristics of recognition without any quality labels involved.</p><p>2. MagFace explicitly distributes features structurally in the angular direction (as shown in <ref type="figure">Fig. 1b</ref>). By dynamically assigning angular margins based on samples' hardness for recognition, MagFace prevents model from overfitting on noisy and low-quality samples and learns a well-structured distributions that are more suitable for recognition and clustering purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Face Recognition</head><p>Recent years have witnessed the breakthrough of deep convolutional face recognition techniques. A number of successful systems, such as DeepFace <ref type="bibr" target="#b34">[35]</ref>, DeepID <ref type="bibr" target="#b32">[33]</ref>, FaceNet <ref type="bibr" target="#b26">[27]</ref> have shown impressive performance on face identification and verification. Apart from the large-scale training data and deep network architectures, the major advance comes from the evolution of training losses for CNN. Most of early works rely on metric-learning based loss, including contrastive loss <ref type="bibr" target="#b7">[8]</ref>, triplet loss <ref type="bibr" target="#b26">[27]</ref>, npair loss <ref type="bibr" target="#b29">[30]</ref>, angular loss <ref type="bibr" target="#b40">[41]</ref>, etc. Suffering from the combinatorial explosion in the number of face triplets, embedding-based method is usually inefficient in training on large-scale dataset. Therefore, the main body of research in deep face recognition has focused on devising more efficient and effective classification-based loss. Wen et al. <ref type="bibr" target="#b43">[44]</ref> develop a center loss to learn centers for each identity to enhance the intra-class compactness. L 2 -softmax <ref type="bibr" target="#b24">[25]</ref> and NormFace <ref type="bibr" target="#b38">[39]</ref> study the necessity of the normalization operation and applied L 2 normalization constraint on both features and weights. From then on, several angular marginbased losses, such as SphereFace <ref type="bibr" target="#b19">[20]</ref>, AM-softmax <ref type="bibr" target="#b37">[38]</ref>, SV-AM-Softmax <ref type="bibr" target="#b41">[42]</ref>, CosFace <ref type="bibr" target="#b39">[40]</ref>, ArcFace <ref type="bibr" target="#b8">[9]</ref>, progressively improve the performance on various benchmarks to the newer level. More recently, AdaptiveFace <ref type="bibr" target="#b18">[19]</ref>, Ada-Cos <ref type="bibr" target="#b48">[49]</ref> and FairLoss <ref type="bibr" target="#b17">[18]</ref> introduce adaptive margin strategy to automatically tune hyperparameters and generate more effective supervisions during training. Compared to our method, all these work tend to suppress the effect of magnitude in the loss by normalizing the feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Face Quality Assessment</head><p>Face image quality is an important factor to enable high-performance face recognition systems <ref type="bibr" target="#b3">[4]</ref>. Traditional methods, such as ISO/IEC 19794-5 standard <ref type="bibr" target="#b0">[1]</ref>, ICAO 9303 standard <ref type="bibr" target="#b1">[2]</ref>, Brisque <ref type="bibr" target="#b30">[31]</ref>, Niqe <ref type="bibr" target="#b22">[23]</ref> and Piqe <ref type="bibr" target="#b36">[37]</ref>, describe qualities from image-based aspects (e.g., distortion, illumination and occlusion) or subject-based measures (e.g., accessories). Learning-based approaches such as FaceQNet <ref type="bibr" target="#b14">[15]</ref> and Best-Rowden <ref type="bibr" target="#b3">[4]</ref> regress qualities by networks trained on human-assessed and similarity-based labels. However, these quality labels are error-prone as human may not know the best characteristics for the recognition system and therefore cannot consider all proper factors. Recently, several uncertainty-based methods are proposed to express face qualities by the uncertainties of features. SER-FIQ <ref type="bibr" target="#b35">[36]</ref> forwards an image to a network with dropout several times and measures face quality by the variation of extracted features. Confidence-aware face recognition methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7]</ref> propose to represent each face image as a Gaussian distribution in the latent space and learn the uncertainty in the feature values. Although these methods 3 work in an unsupervised manner like ours, they require additional computational costs or network blocks, which complicate their usage in conventional face systems.</p><formula xml:id="formula_0">O " &gt; &gt; % ' ' (a) O Feasible Region by ' ' (b) O 2 1 3 Effect of ( ) Effect of ( ) ' ' (c) 2 1 3 O &gt; % &gt; " ' ' (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Face Clustering</head><p>Face clustering exploits unlabeled data to cluster them into pseudo classes. Traditional clustering methods usually work in an unsupervised manner, such as K-means <ref type="bibr" target="#b20">[21]</ref>. DBSCAN <ref type="bibr" target="#b10">[11]</ref> and hierarchical clustering. Several supervised clustering methods based on graph convolutional network (GCN) are proposed recently. For example, L-GCN <ref type="bibr" target="#b42">[43]</ref> performs reasoning and infers the likelihood of linkage between pairs in the sub-graphs. Yang et al. <ref type="bibr" target="#b45">[46]</ref> designs two graph convolutional networks, named GCN-V and GCN-E, to estimate the confidence of vertices and the connectivity of edges, respectively. Instead of developing clustering methods, we aim at improving feature distribution structure for clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first review the definition of Arc-Face <ref type="bibr" target="#b8">[9]</ref>, one of the most popular losses used in face recognition. Based on the analysis of ArcFace, we then derive the objective and prove the key properties for MagFace. In the end, we compare softmax and ArcFace with MagFace from the perspective of feature magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ArcFace Revisited</head><p>Training loss plays an important role in face representation learning. Among the various choices (see <ref type="bibr" target="#b9">[10]</ref> for a recent survey), ArcFace <ref type="bibr" target="#b8">[9]</ref> is perhaps the most widely adopted one in both academy and industry application due to its easiness in implementation and state-of-the-art performance on a number of benchmarks. Suppose that we are given a training batch of N face samples {f i , y i } N i=1 of n identities, where f i ? R d denotes the d-dimensional embedding computed from the last fully connected layer of the neural networks and y i = 1, ? ? ? , n is its associated class label. ArcFace and other variants improve the conventional softmax loss by optimizing the feature embedding on a hypersphere manifold where the learned face representation is more discriminative. By defining the angle ? j between f i and j-th class center w j ? R d as w T j f i = w j f i cos ? j , the objective of ArcFace <ref type="bibr" target="#b8">[9]</ref> can be formulated as</p><formula xml:id="formula_1">L = ? 1 N N i=1 log e s cos (?y i +m) e s cos (?y i +m) + j =y i e s cos ? j ,<label>(1)</label></formula><p>where m &gt; 0 denotes the additive angular margin and s is the scaling parameter. Despite its superior performances on enforcing intraclass compactness and inter-class discrepancy, the angular margin penalty m used by ArcFace is quality-agnostic and the resulting structure of the within-class distribution could be arbitrary in unconstrained scenarios. For example, let us consider the scenario illustrated in <ref type="figure" target="#fig_2">Fig. 2a</ref>, where we have face images of the same class in three levels of qualities indicated by the circle sizes: the larger the radius, the more uncertain the feature representation and the more difficulty the face can be recognized. Because ArcFace employs a uniform margin m, each image in one class shares the same decision boundary, i.e., B : cos(? + m) = cos(? ) with respect to the neighbor class. The three types of samples can stay at arbitrary location in the feasible region (shading area in <ref type="figure" target="#fig_2">Fig. 2a</ref>) without any penalization by the angular margin. This leads to unstable within-class distribution, e.g., the high-quality face (type 1) stay along the boundary B while the low-quality ones (type 2 and 3) are closer to the center w. This unstableness can hurt the performances on in-thewild recognition as well as other facial application such as face clustering. Moreover, hard and noisy samples are overweighted as they are hard to stay in the feasible area and the models may overfit to them.  <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9]</ref> and 512 samples of the last iteration are used for visualization. Negative losses are used to reveal the hardness for Softmax while we use cosine value of ? (angle between a feature and its class center) for ArcFace and MagFace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">MagFace</head><p>Based on the above analysis, previous cosine-similaritybased face recognition loss lacks more fine-grained constraint beyond a fixed margin m. This leads to unstable within-class structure especially in the unconstrained case (e.g., <ref type="figure" target="#fig_2">Fig. 2a</ref>) where the variability of each subject's faces is large. To address the aforementioned problem, this section proposes MagFace, a novel framework to encode quality measure into the face representation. Unlike previous work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b6">7]</ref> that call for additional uncertainty term, we pursue a minimalism design by optimizing over the magnitude a i = f i without normalization of each feature f i . Our design has two major advantages: 1) We can keep using the cosine-based metric that has been widely adopted by most existing inference systems; 2) By simultaneously enforcing its direction and magnitude, the learned face representation is more robust to the variability of faces in the wild. To our best understanding, this is the first work to unify the feature magnitude as quality indicator in face recognition.</p><p>Before defining the loss, let us first introduce two auxiliary functions related to a i , the magnitude-aware angular margin m(a i ) and the regularizer g(a i ). The design of m(a i ) follows a natural intuition: for high-quality samples x i , they should concentrate in a small region around the cluster center w with high certainty. By assuming a positive correlation between the magnitude and quality, we thereby penalize more on x i in terms of m(a i ) if its magnitude a i is large. To have a better understanding, <ref type="figure" target="#fig_2">Fig. 2b</ref> visualizes the margins m(a i ) corresponding to different magnitude values. In contrast to ArcFace ( <ref type="figure" target="#fig_2">Fig. 2a</ref>), the feasible region defined by m(a i ) has a shrinking boundary with respect to feature magnitude towards the class center w. Consequently, this boundary pulls the low-quality samples (circle 2 and 3 in <ref type="figure" target="#fig_2">Fig. 2c</ref>) to the origin where they have lower risk to be penalized. However, the structure formed solely by m(a i ) is unstable for high-quality samples like circle 1 in <ref type="figure" target="#fig_2">Fig. 2c</ref> as they have large freedom moving inside the feasible region. We therefore introduce the regularizer g(a i ) that rewards sample with large magnitude. By designing g(a i ) as a monotonically decreasing convex function with respect to a i , each sample would be pushed towards the boundary of the feasible region and the high-quality ones (circle 1) would be dragged closer to the class center w as shown in <ref type="figure" target="#fig_2">Fig. 2d</ref>. In a nutshell, MagFace extends ArcFace (Eq. 1) with magnitude-aware margin and regularizer to enforce higher diversity for inter-class samples and similarity for intra-class samples by optimizing:</p><formula xml:id="formula_2">LMag = 1 N N i=1 Li, where<label>(2)</label></formula><p>Li = ? log e s cos (?y i +m(a i )) e s cos (?y i +m(a i )) + j =y i e s cos ? j + ?gg(ai).</p><p>The hyper-parameter ? g is used to trade-off between the classification and regularization losses. The design of MagFace not only follows intuitive motivations, but also yields result with theoretical guarantees. Assuming the magnitude a i is bounded in [l a , u a ], where m(a i ) is a strictly increasing convex function, g(a i ) is a strictly decreasing convex function and ? g is large enough, we can prove (see detailed requirements and proofs in the supplementary) that the following two properties of Mag-Face loss always hold when optimizing L i over a i :</p><formula xml:id="formula_3">Property of Convergence. For a i ? [l a , u a ]</formula><p>, L i is a strictly convex function which has a unique optimal solution a * i .</p><p>Property of Monotonicity. The optimal a * i is monotonically increasing as the cosine-distance to its class center decreases and the cos-distances to other classes increase.</p><p>The property of convergence guarantees the unique optimal solution for a i as well as the fast convergence. The property of monotonicity states that the feature magnitudes reveal the difficulties for recognition, therefore can be treated as a metric for face qualities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis on Feature Magnitude</head><p>To better understand the effect of the MagFace loss, we conduct experiments on the widely used MS1M-V2 <ref type="bibr" target="#b8">[9]</ref> dataset and investigate for the training examples at convergence the relation between the feature magnitude and their similarity with class center as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p><p>Softmax. The classical softmax-based loss underlies the objective of the pioneer work <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref> on deep face recognition. Without explicit constraint on magnitude, the value of the negative loss for each sample is almost independent to its magnitude as observed from <ref type="figure" target="#fig_3">Fig. 3a</ref>. As pointed in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref>, softmax tends to create a radial feature distribution because softmax loss acts as the soft version of max operator and scaling the feature magnitude does not affect the assignment of its class. To eliminate this effect, <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b38">39]</ref> suggest that using normalized feature would benefit the task.</p><p>ArcFace. ArcFace can be considered as a special case of MagFace when m(a i ) = m and g(a i ) = 0. As shown in <ref type="figure" target="#fig_3">Fig. 3b</ref>, high-quality samples with large similarity cos(?) to class center yield large variation in magnitude. This evidence echos our motivation on the unstable structure defined by a fixed angular margin in ArcFace for easy samples. On the other hand, for low-quality samples that are difficult to be recognized (cos(?) is small), the fixed angular margin determines the magnitude needs to be large enough in order to fit inside the feasible region ( <ref type="figure" target="#fig_2">Fig. 2a</ref>). Therefore, there is a decreasing low bound for feature magnitudes w.r.t. the quality of face as indicated by the dash line in <ref type="figure" target="#fig_3">Fig. 3b</ref>.</p><p>MagFace. In contrast to ArcFace, our MagFace optimizes the feature with adaptive margin and regularization based on its magnitude. Under this loss, it is clear to observe from <ref type="figure" target="#fig_3">Fig. 3c</ref> that there is a strong correlation between the feature magnitudes and their cosine similarities with class center. Those examples at the upper-right corner are the most high-quality ones. As the magnitude becomes smaller, the examples are more deviated from the class center. This distribution strongly supports the fact that the feature magnitude learned by MagFace is a good metric for face quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we examine the proposed MagFace on three important face tasks: face recognition, quality assessment and face clustering. Sec. C in the supplementary presents the ablation study on relationships between margin distributions and recognition performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Face Recognition</head><p>Datasets. The original MS-Celeb-1M dataset <ref type="bibr" target="#b13">[14]</ref> contains about 10 million images of 100k identities. However, it consists of a great many noisy face images. Instead, we employ MS1M-V2 [9] (5.8M images, 85k identities) as our training dataset. For evaluation, we adopt LFW <ref type="bibr" target="#b15">[16]</ref>, CFP-FP <ref type="bibr" target="#b27">[28]</ref>, AgeDB-30 <ref type="bibr" target="#b23">[24]</ref>, CALFW <ref type="bibr" target="#b50">[51]</ref>, CPLFW <ref type="bibr" target="#b49">[50]</ref>, IJB-B <ref type="bibr" target="#b44">[45]</ref> and IJB-C <ref type="bibr" target="#b21">[22]</ref> as the benchmarks. All the images are aligned to 112?112 by following the setting in ArcFace. Baselines. We re-implement state-of-the-art baselines including Softmax, SV-AM-Softmax <ref type="bibr" target="#b41">[42]</ref>, SphereFace <ref type="bibr" target="#b19">[20]</ref>, CosFace <ref type="bibr" target="#b39">[40]</ref>, ArcFace <ref type="bibr" target="#b8">[9]</ref>. ResNet100 is equipped as the backbone. We use the recommended hyperparameters for each model, e.g., s = 64, m = 0.5 for ArcFace.</p><p>Training. We train models on 8 1080Tis by stochastic gradient descent. The learning rate is initialized from 0. Results on LFW, CFP-FP, AgeDB-30, CALFW and CPLFW. We directly use the aligned images and protocols adopted by ArcFace <ref type="bibr" target="#b8">[9]</ref> and present our results in Tab. 1. We note that performances are almost saturated. Compared to CosFace which is the second best baseline, ArcFace achieves 0.03%, 0.14%, 0.54% improvement on LFW, CFP-FP and CPLFW, while drops 0.12%, 0.22% on AgeDB-30 and CALFW. MagFace obtains the overall best results and surpasses ArcFace by 0.02%, 0.06%, 0.12%, 0.19% and 0.15% on five benchmarks respectively. Results on IJB-B/IJB-C. The IJB-B dataset contains 1,845 subjects with 21.8K still images and 55K frames from 7,011 videos. As the extension of IJB-B, the IJB-C dataset covers about 3,500 identities with a total of 31,334 images and 117,542 unconstrained video frames. In the 1:1 verification, the number of positive/negative matches are 10k/8M in IJB-B and 19k/15M in IJB-C. We report the TARs at FAR=1e-6,   Our implemented ArcFace is on par with the original paper, e.g., our TARs at FAR=1e-4 differ from the authors by ?0.11% and +0.14% on IJB-B and IJB-C respectively. Compared to baselines, our MagFace remains the top at all FAR criteria except for FAR=1e-6 on IJB-B as the TAR is very sensitive to the noise when the number of FP is tiny. Compared to CosFace, MagFace gains 0.50%, 0.63%, 0.32% on IJB-B at TAR@FAR=1e-6, 1e-5, 1e-4 and 1.30%, 0.99%, 0.25% on IJB-C. Compared to Arc-Face, improvements are of 2.23%, 1.38%, 0.24% on IJB-B and 3.61%, 0.98%, 0.07% on IJB-C respectively. This result demonstrates the superiority of MagFace on more challenging benchmarks. It is worth to mention that when multiple images existed for one identity, the average embedding can be further improved by aggregating features weighted by magnitudes. For instance, MagFace+ outperforms Mag-Face by 1.41%/0.98% at FAR=1e-6, 0.48%/0.41% at FAR=1e-5 and 0.18%/0.16% at FAR=1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Face Quality Assessment</head><p>In this part, we investigate the qualitative and quantitative performance of the pre-trained MagFace model mentioned in Tab. 2 for quality assessment. Visualization of the mean face. We first sample 100k images form IJB-C database and divide them into 8 groups based on feature magnitudes. We visualize the mean faces of each group in <ref type="figure" target="#fig_5">Fig. 4</ref>. It can be seen that when magni-  tude increases, the corresponding mean face reveals more details. This is because high-quality faces are inclined to be more frontal and distinctive. This implies the magnitude of MagFace feature is a good quality indicator. Sample distribution of datasets. <ref type="figure" target="#fig_6">Fig. 5</ref> plots the sample histograms of different benchmarks with respect to Mag-Face magnitudes. We observe that LFW is the least noisy one where most samples are of large magnitudes. Due to the larger age variation, the distribution of AGEDB-30 slightly shifts left compared to LFW. For CFP-FP, there are two peaks at the magnitude around 28 and 34, corresponding to the frontal and profile faces respectively. Given the large variations in face qualities, we can conclude IJB-C is much more challenging than other benchmarks. For images (more examples can be found in the supplementary) with magnitudes a 15, there are no faces or very noisy faces to observe. When feature magnitudes increase from 20 to 40, there is a clear trend that the face changes from profile, blurred and occluded, to more frontal and distinctive. Overall, this figure convinces us that MagFace is an effective tool to rank face images according to their qualities. Baselines. We choose six baselines of three types for quantitative quality evaluation. Brisque <ref type="bibr" target="#b30">[31]</ref>, Niqe <ref type="bibr" target="#b22">[23]</ref> and Piqe <ref type="bibr" target="#b36">[37]</ref> are image-based quality metrics. FaceQNet <ref type="bibr" target="#b14">[15]</ref> and SER-FIQ <ref type="bibr" target="#b35">[36]</ref> are face-based ones. For FaceQNet, we adopt the released models by the authors. For SER-FIQ, we use the "same model" version which yields the best performance in the paper. Following the authors' setting, we set m = 100 to forward each image 100 times with drop-out active in inference. As a related work, we re-implement the recent DUL <ref type="bibr" target="#b6">[7]</ref> method that can estimate uncertainty along with the face feature.</p><p>Evaluation metric. Following previous work <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4]</ref>, we evaluate the quality assessment on LFW/CFP-FP/AgeDB via the error-versus-reject curves, where images with the lowest predicted qualities are unconsidered and error rates are calculated on the remaining images. Errorversus-reject curve indicates good quality estimation when the verification error decreases consistently while increasing the ratio of unconsidered images. To compute the feature for verification, we adopt the ArcFace* as well as our MagFace models in Tab. 2.</p><p>Results on face verification. <ref type="figure" target="#fig_7">Fig. 6</ref> shows the error-versusreject curves of different quality methods in terms of false non-match rate (FNMR) reported at false match rate (FMR) threshold of 0.001. Overall, we have two high-level observations. 1) The curves on CFP-FP and AgeDB-30 are much more smooth than the ones obtained on LFW. This is because CFP-FP and AgeDB-30 consist of faces with larger variations in pose and age. Effectively dropping lowquality faces can benefit the verification performance more on these two benchmarks. 2) No matter computing the feature from ArcFace (left column) or MagFace (right column), the curves corresponding to MagFace magnitude are consistently the lowest ones across different benchmarks. This indicates that the performance of MagFace magnitude as quality generalizes well across datasets as well as face features. We then analyze the quality performance of each type of methods. 1) The image-based quality metrics (Brisque <ref type="bibr" target="#b30">[31]</ref>, Niqe <ref type="bibr" target="#b22">[23]</ref>, Piqe <ref type="bibr" target="#b36">[37]</ref>) lead to relatively higher errors in most cases as the image quality alone is not suitable for generalized face quality estimation. Factors of the face (such as pose, occlusions, and expressions) and model biases are not covered by these algorithms and might play an important role for face quality assessment. 2) The face-based methods (FaceQNet <ref type="bibr" target="#b14">[15]</ref> and SER-FIQ <ref type="bibr" target="#b35">[36]</ref>) outperforms other baselines in most cases. In particular, SER-FIQ is more effective than FaceQNet in terms of the verification error rates. This is due to the fact that SER-FIQ is built on top of the deployed recognition model so that its prediction is more suitable for the verification task. However, SEQ-FIQ takes a quadratic computational cost w.r.t. the number of sub-networks m randomly sampled using dropout. In contrary, the neglectable overhead of computing magnitude makes the proposed MagFace more practical in many realtime scenarios. Moreover, the training of MagFace does not require explicit labeling of face quality, which is not only time consuming but also error-prone to obtain. 3) At last, the uncertainty method (DUL) performs well on CFP-FP but yields more verification errors on AgeDB-30 when the proportion of unconsidered images is increased. This may indicate that the Gaussian assumption of data variance in DUL is over-simplified such that the model cannot generalize well to different kinds of quality factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face Clustering</head><p>In this section, we conduct experiments on face clustering to further investigate the structure of feature representations learned by MagFace. Baselines. We compare the performances of MagFace and ArcFace by integrating their features with various clustering methods. For fair comparisons, we constrain hyperparameters of the two models to be consistent (e.g., s=64, mean margin 0.5) during training. Four clustering methods are used in the evaluation: K-means <ref type="bibr" target="#b20">[21]</ref>, AHC <ref type="bibr" target="#b16">[17]</ref>, DB-SCAN <ref type="bibr" target="#b10">[11]</ref> and L-GCN <ref type="bibr" target="#b42">[43]</ref>. For non-deterministic algorithms (K-means and AHC), we report the average results from 10 runs. For L-GCN, we train the model on CASIA-WebFace <ref type="bibr" target="#b46">[47]</ref> (0.5M images from 10k individuals) and follow the recommended settings in the paper <ref type="bibr" target="#b42">[43]</ref>.</p><p>Benchmarks. We adopt the IJB-B <ref type="bibr" target="#b44">[45]</ref> dataset as the benchmark as it contains a clustering protocol of seven subtasks varying in the number of ground truth identities. Following <ref type="bibr" target="#b42">[43]</ref>, we evaluate on three largest sub-tasks where the numbers of identities are 512,  <ref type="table">Table 3</ref>: F-score (%) and NMI (%) on clustering benchmarks. F-measure <ref type="bibr" target="#b2">[3]</ref> are employed as the evaluation metrics. Results. Tab. 3 summarizes the clustering results. We can observe that with stronger clustering methods from Kmeans to L-GCN, the overall clustering performance can be improved. For any combination of clustering and protocol, MagFace always achieves better performance than ArcFace in terms of both F-score and NMI metrics. This consistent superiority demonstrates the MagFace feature is more suitable for clustering. Notice that we keep the same hyperparameters for clustering. The improvement of using Mag-Face must come from its better within-class feature distribution, where the high-quality samples around the class center are more likely to be separated across different classes. We further explore the relationship between feature magnitudes and the confidences of being class centers. Following the idea mentioned in <ref type="bibr" target="#b45">[46]</ref>, the confidence of being a class center for each sample is estimated based on its neighbor structure defined by face features. The samples with dense and pure local connection have high confidence, while those with sparse connections or residing in the boundary among several clusters have low confidence. From <ref type="figure" target="#fig_8">Fig. 7</ref>, it is easy to observe that the MagFace magnitude is positively correlated with confidence of class center on the IJB-B-1845 benchmark. This result reflects that the MagFace feature exhibits the expected within-class structure, where high quality samples distribute around class center while low quality ones are far away from the center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose MagFace to learn unified features for face recognition and quality assessment. By pushing ambiguous samples away from class centers, MagFace improves the within-class feature distribution from previous margin-based work for face recognition. The adequate theoretical and experimental results convince that MagFace can simultaneously access quality for the input face image. As a general framework, MagFace can be potentially extended to benefit other classification tasks such as fine-grained object recognition, person re-identification. Moreover, the proposed principle of exploring feature magnitude paves the way to estimate quality for other objects, e.g., person body in reid or action snippet in activity classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs for MagFace</head><p>Recall the MagFace loss for a sample i is L i = ? log e s cos (?y i +m(ai)) e s cos (?y i +m(ai)) + n j=1,j =yi e s cos ?j</p><formula xml:id="formula_4">+ ? g g(a i )<label>(3)</label></formula><p>Let A(a i ) = s cos(? yi + m(a i )) and B = n j=1,j =yi e s cos ?j and rewrite the loss as</p><formula xml:id="formula_5">L i = ? log e A(a i ) e A(a i ) +B + ? g g(a i )<label>(4)</label></formula><p>We first introduce and prove Lemma 1.</p><p>Lemma 1. Assume that f i is top-k correctly classified and m(a i ) ? [0, ?/2]. If the number of identities n is much larger than k (i.e., n k), the probability of ? yi + m(a i ) ? [0, ?/2] approaches 1.</p><p>Proof. Denote the angle between feature f i and center class W j , j ? {1, ? ? ? , n} as ? j . Assuming the distribution of ? j is uniform, it's easy to prove P (? j + m(a i ) ? [0, ?/2]) = ?/2?m(ai) ? . Let p = ?/2?m(ai)</p><p>? . If f i is top-k correctly classified, the probability of ? yi + m(a i ) ? [0, ?/2] is the same as the probability of there are at least k ? to satisfy ? + m(a i ) ? [0, ?/2]. Then the probability is</p><formula xml:id="formula_6">P (? yi + m(a i ) ? [0, ?/2]) = n i=k n i p i (1 ? p) (n?i) o = 1 ? k?1 i=0 n i p i (1 ? p) (n?i)<label>(5)</label></formula><p>When n is a large integer and n k, each n i p i (1 ? p) (n?i) , i = 1, 2, ? ? ? k ? 1 converges to 0. Therefore, probability of ? yi + m(a i ) ? [0, ?/2] approaches 1.</p><p>Lemma 1 is fundamental for the following proofs. The number of identities is large in real-world applications (e.g., <ref type="bibr" target="#b2">3</ref>.8M for MS1Mv2 <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9]</ref>). Therefore, the probability of ? yi + m(a i ) ? [0, ?/2] approaches 1 in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Requirements for MagFace</head><p>In MagFace, m(a i ), g(a i ), ? g are required to have the following properties:</p><p>1. m(a i ) is an increasing convex function in [l a , u a ] and m (a i ) ? (0, K], where K is a upper bound;</p><p>2. g(a i ) is a strictly convex function with g (u a ) = 0;</p><p>3. ? g ? sK ?g (la) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Proof for Property of Convergence</head><p>We prove the property of convergence by showing the strict convexity of the function L i (Property 5) and the existence of the optimum (Property 6).</p><formula xml:id="formula_7">Property 1. For a i ? [l a , u a ], L i is a strictly convex func- tion of a i .</formula><p>Proof. The first and second deriviates of A(a i ) are</p><formula xml:id="formula_8">A (a i ) = ?s sin(? y i + m(a i ))m (a i ) A (a i ) = ?s cos(? y i + m(a i ))(m (a i )) 2 ? s sin(? y i + m(a i ))m (a i )<label>(6)</label></formula><p>According to Lemma 1, we have cos(? yi + m(a i )) ? 0 and sin(? yi + m(a i )) ? 0. Because we define m(a i ) to be convex and g(a i ) to be strictly convex for a i ? [l a , u a ], m (a i ) ? 0 and g (a i ) &gt; 0 always hold. Therefore, A (a i ) ? 0.</p><p>The first and second order derivatives of the loss L i are</p><formula xml:id="formula_9">?L i ?a i = ? B e A(ai) + B A (a i ) + ? g g (a i ) ? 2 L i (?a i ) 2 = ? B (e A(ai) + B) 2 (e A(ai) + B)A (a i ) ? Be A(ai) A (a i ) 2 + ? g g (a i ) = ? B e A(ai) + B A (a i ) + B 2 (e A(ai) + B) 2 e A(ai) A (a i ) 2 + ? g g (a i )</formula><p>As B &gt; 0, e A(ai) + B &gt; 0, it's easy to prove that first two parts of ? 2 Li (?ai) 2 are non-negative while the third part is always positive. Therefore, ? 2 Li (?ai) 2 &gt; 0 and L i is a strictly convex function with respect to a i . Property 2. A unique optimal solution a * i exists in [l a , u a ].</p><p>Proof. Because the loss function L i is a strictly convex function, we have ?Li</p><formula xml:id="formula_10">?a 1 i &gt; ?Li ?a 2 i if u a ? a 1 i &gt; a 2 i ? l a .</formula><p>Next we prove that there exist a optimal solution a * i ? [l a , u a ]. If it exists, then it is unique because of the strict convexity.</p><p>As ?Li ?ai (a i ) = Bs e A(a i ) +B sin(? yi + m(a i ))m (a i ) + ? g g (a i ) and considering the constraints m (a i ) ? (0, K], g (u a ) = 0, ? g ? sK ?g (la) , the values of derivatives of l a , u a are </p><p>As ?Li ?ai is monotonically and strictly increasing, there must exist a unique value in [l a , u a ] which have a 0 derivative. Therefore, an optimal solution exists and is unique.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Proof for Property of Monotonicity</head><p>To prove the property of monotonicity, we first show that optimal a * i increases with a smaller cosine-distance to its class center (Property 3). As B can reveal the overall cosdistances to other class centers, we further prove that decreasing B (distances to other class centers increases) can increase optimal feature magnitude (Property 4). In the end, we can conclude that a * i is monotonically increasing as the cosine-distance to its class center decreases and the cosinedistances to other classes increase.</p><p>Property 3. With fixed f i and W j , j ? {1, ? ? ? , n}, j = y i , the optimal feature magnitude a * i is monotonically increasing if the cosine-distance to its class center W yi decreases.</p><p>Proof. Assuming there are two class center W 1 yi , W 2 yi and their cosine distances to feature f i are ? 1 yi , ? 2 yi . Assuming ? 1 yi &lt; ? 2 yi (i.e., class center W 1 yi has a smaller distance with feature f i ) and the corresponding optimal feature magnitudes are a * i,1 , a * i,2 . The first derivate of L i is</p><formula xml:id="formula_12">?L i ?a i = ? B e A(ai) + B A (a i ) + ? g g (a i ) =</formula><p>Bsm (a i ) e s cos(?y i +m(ai)) + B sin(? yi + m(a i )) + ? g g (a i )</p><p>For ? yi + m(a i ) ? (0, ?/2], we have cos(? 1 yi + m(a i )) &gt; cos(? 2 yi +m(a i )) and sin(? 1 yi +m(a i )) &lt; sin(? 2 yi +m(a i )). With m (a i ) &gt; 0, it's obvious that Therefore, we have</p><formula xml:id="formula_14">?Li(? 1 y i ) ?ai &lt; ?Li(? 2 y i ) ?ai</formula><p>. Based on the property of optimal solution for strictly convex function, we</p><formula xml:id="formula_15">have 0 = ?Li(? 1 y i ) ?a * i,1 = ?Li(? 2 y i ) ?a * i,2 &gt; ?Li(? 1 y i ) ?a * i,2</formula><p>, which leads to a * i,1 &gt; a * i,2 .</p><p>Property 4. With other things fixed, the optimal feature magnitude a * i is monotonically increasing with a decreasing B (i.e., increasing inter-class distance).</p><p>Proof. Assume 0 &lt; B 1 &lt; B 2 with optimum a * i,1 , a * i,2 . Similar to the proof before, it's easy to show B1sm (ai) e s cos(?y i +m(a i )) +B1 sin(? yi + m(a i )) &lt; B2sm (ai) e s cos(?y i +m(a i )) +B2 sin(? yi + m(a i )). Therefore, we have ?Li(B1) ?ai &lt; ?Li(B2) ?ai . Based on the property of optimal solution for strictly convex function, we have 0 = ?Li(B1)</p><formula xml:id="formula_16">?a * i,1 = ?Li(B2) ?a * i,2 &gt; ?Li(B1) ?a * i,2</formula><p>, which leads to a * i,1 &gt; a * i,2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head><p>B.1. Training settings for <ref type="figure" target="#fig_3">Figure 3</ref> We adopt ResNet50 as the backbone network. Models are trained on MS1Mv2 <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9]</ref> for 20 epochs with batch size 512 and initial learning rate 0.1, dropped by 0.1 every 5 epochs. 512 samples of the last iteration are used for visualization.</p><p>B.2. Settings of m(a i ), g(a i ) and ? g</p><p>In our experiments, we define function m(a i ) as a linear function defined on [l a , u a ] with m(l a ) = l m , m(u a ) = u m and g(a i ) = 1 ai + 1 u 2 a a i . Therefore, we have</p><formula xml:id="formula_17">m(a i ) = u m ? l m u a ? l a (a i ? l a ) + l m ? g ? sK ?g (l a ) = su 2 a l 2 a (u 2 a ? l 2 a ) u m ? l m u a ? l a<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study on Margin Distributions</head><p>In this section, effects of the feature distributions during training are studied. With (? g , l a , u a ) fixed to <ref type="bibr" target="#b34">(35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">110)</ref>, we carefully select various combinations of l m , u m to align the mean margin on the training dataset to ArcFace (0.5) in our implementation. Features are distributed more separated if with a larger maximum margin and a smaller minimum margin. <ref type="table" target="#tab_6">Table 4</ref> shows the recognition results with various hyperparameters. With (l m , u m ) = (0.45, 0.65), the penalty of magnitude loss degrades the performance of the recognition. With (l m , u m ) = (0.25, 1.60), the performance is also worse than then baseline as hard samples are assigned to small margins (a.k.a., hard/noisy samples are downweighted). Parameter (0.40, 0.80) balances the feature distribution and margins for hard/noisy samples, and therefore achieves a significant improvement on benchmarks.   nitudes. All the faces are sample from the IJB-C benchmark. It can be seen that faces with magnitudes around 28 are mostly profile faces while around 35 are highquality and frontal faces. That is consistent with the profile/frontal peaks in the CFP-FP benchmark and indicates that faces with similar magnitudes show similar quality patterns across benchmarks. In real applications, we can set a proper threshold for the magnitude and should be able to filter similar low-quality faces, even under various scenarios. Besides directly served as qualities, our feature magnitudes can also be used as quality labels for faces, which avoids human labelling costs. These labels are more suitable for recognition, and therefore can be used to boost other quality models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Extended Visualization of Figure 6</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Mag-CosFace</head><p>In the main text, MagFace is modified from the Arc-Face loss. In this section, we show that MagFace based on CosFace loss (denote as Mag-CosFace) can theoretically achieve the same effects. Mag-CosFace loss for a sample i is L i = ? log e s(cos ?y i ?m(ai)) e s(cos ?y i ?m(ai)) + n j=1,j =yi e s cos ?j + ? g g(a i ) </p><p>As A (a i ) ? 0, the property can be proved following that presented before. Property 6. A unique optimal solution a * i exists in [l a , u a ].</p><p>Proof. We only need to prove </p><p>Then it's easy to have there is a unique optimu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Property of Monotonicity for Mag-CosFace</head><p>Property 7. With fixed f i and W j , j ? {1, ? ? ? , n}, j = y i , the optimal feature magnitude a * i is monotonically increasing if the cosine-distance to its class center W yi decreases.</p><p>Proof. The first derivate of L i is</p><formula xml:id="formula_20">?L i ?a i = ? B e A(ai) + B</formula><p>A (a i ) + ? g g (a i ) = Bsm (a i ) e s(cos ?y i ?m(ai)) + B + ? g g (a i )</p><p>For ? 1 yi &lt; ? 2 yi , the core here is to prove</p><formula xml:id="formula_22">?Li(? 1 y i ) ?ai &lt; ?Li(? 2 y i ) ?ai</formula><p>, which is obvious as cos ? 1 yi &gt; cos ? 2 yi . The rest of the proofs is the same as those for the original MagFace. Property 8. With other things fixed, the optimal feature magnitude a * i is monotonically increasing with a decreasing B (i.e., increasing inter-class distance).</p><p>Proof. It's easy to have ?Li(B1)</p><formula xml:id="formula_23">?ai &lt; ?Li(B2) ?ai if B 1 &lt; B 2 .</formula><p>The rest of the proofs is the same as those for the original MagFace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Authors' Contributions</head><p>Shichao Zhao and Zhida Huang contribute similarly to this work. Besides involved in discussions, Shichao Zhao mainly conducted experiments on face clustering and Zhida Huang implemented baselines as well as evaluation metrics for quality experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Geometrical interpretation of the feature space (without normalization) optimized by ArcFace and MagFace. (a) Two-class distributions optimized by ArcFace, where w and w are the class centers and their decision boundaries B and B are separated by the additive margin m. Circle 1, 2, 3 represent three types samples of class w with descending qualities. (b) MagFace introduces m(ai) which dynamically adjust boundaries based on feature magnitudes, and ends to a new feasible region. (c) Effects of g(ai) and m(ai). (d) Final feature distributions of our MagFace. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of feature magnitudes and difficulties for recognition. Models are trained on MS1M-V2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>1 and divided by 10 at 10, 18, 22 epochs, and we stop the training at the 25th epoch. The weight decay is set to 5e-4 and the momentum is 0.9. We only augment training samples by random horizontal flip. For MagFace, we fix the upper bound and lower bound of the magnitude as l a = 10, u a = 110. m(a i ) is chosen to be a linear function and g(a i ) as a hyperbola. For detailed definition of m(a i ), g(a i ) and ? g , please refer to Sec. B2 in the supplementary. In the end, our mean margin as well as other hyperparameters are all consistent with ArcFace.Test. During testing, cosine distance is used as metric on comparing 512-D features. For evaluations on IJB-B/C, one identity can have multiple images. The common way to represent for an identity is to sum up the normalized feature f norm i = fi fi of each image and then normalize the embedding for comparisons, i.e., f = i f norm i i f norm i . One benefit of MagFace is that we can assign quality-aware weight f i to each normalized feature f norm i . Therefore, we further evaluate "MagFace+" in Tab. 2 by computing the identity embedding as f + = i fi i fi .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the mean faces of 100k images sampled from the IJB-C dataset. Each mean face corresponds to a group of faces based on the magnitude level of the features learned by MagFace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Distributions of magnitudes on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Face verification performance for the predicted face quality values with two evaluation models (ArcFace and MagFace). The curves show the effectiveness of rejecting low-quality face images in terms of false non-match rate (FNMR). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of MagFace magnitudes of 500 samples from IJB-B-1845 w.r.t. their confidences of being class centers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Bs e A(ai) + B sin(? yi + m(a i ))m (u a ) &gt; 0 ?L i ?a i (l a ) = Bs e A(ai) + B sin(? yi + m(a i ))m (l a ) + ? g g (l a ) &lt; sK + ? g g (l a ) ? 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>i )) +B sin(? 2 yi + m(a i )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>We present a extended visualization of figure 6 in figure 8 which has more examples of faces with feature mag-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Extended Visualization of Figure 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>( 10 )E. 1 . 5 .</head><label>1015</label><figDesc>Let A(a i ) = s(cos ? yi ? m(a i )) and B = n j=1,j =yi e s cos ?j and rewrite the loss asL i = ? log e A(a i ) e A(a i ) +B + ? g g(a i )(11) Property of Convergence for Mag-CosFace Property For a i ? [l a , u a ], L i is a strictly convex function of a i . Proof. The first and second deriviates of A(a i ) are A (ai) = ?sm (ai) A (ai) = ?sm (ai)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Bs e A(ai) + B m (u a ) &gt; 0 ?L i ?a i (l a ) = Bs e A(ai) + B m (l a ) + ? g g (l a ) &lt; sK + ? g g (l a ) ? 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Verification accuracy (%) on difficult benchmarks. "*" indicates the result quoted from the original paper.1e-5 and 1e-4 as shown in Tab. 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ArcFace 66.70 88.83 66.82 89.48 66.93 89.88 MagFace 66.75 88.86 67.33 89.62 67.06 89.96 AHC [17] ArcFace 69.72 89.61 70.47 90.54 70.66 90.90 MagFace 70.24 89.99 70.68 90.67 70.98 91.06 DBSCAN [11] ArcFace 72.72 90.42 72.50 91.15 73.89 91.96 MagFace 73.13 90.61 72.68 91.30 74.26 92.13 L-GCN [43] ArcFace 84.92 93.72 83.50 93.78 80.35 92.30 MagFace 85.27 93.83 83.79 94.10 81.58 92.79</figDesc><table><row><cell>Method</cell><cell>Net</cell><cell cols="2">IJB-B-512</cell><cell cols="3">IJB-B-1024 IJB-B-1845</cell></row><row><cell></cell><cell></cell><cell>F</cell><cell>NMI</cell><cell>F</cell><cell>NMI</cell><cell>F</cell><cell>NMI</cell></row><row><cell>K-means [21]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1,024 and 1,845, and the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>numbers of samples are 18,171, 36,575 and 68,195, respec-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tively. Normalized mutual information (NMI) and BCubed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>91.59 95.00 96.86 MagFace 0.45 0.65 35 10 110 0.50 0.49 0.52 97.23 81.12 91.44 94.95 96.96 0.40 0.80 35 10 110 0.50 0.46 0.53 97.47 85.82 92.06 95.12 96.92 0.35 1.00 35 10 110 0.50 0.42 0.54 97.40 84.35 91.65 95.05 97.02 0.25 1.60 35 10 110 0.50 0.35 0.61 97.30 81.64 91.09 94.91 96.87</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">Hyperparameters</cell><cell></cell><cell></cell><cell>Margin</cell><cell></cell><cell>CFP-FP</cell><cell></cell><cell>IJB-C (TAR@FAR)</cell></row><row><cell></cell><cell>lm</cell><cell cols="3">um ?g la</cell><cell>ua</cell><cell cols="3">mean max min</cell><cell></cell><cell>1e-6</cell><cell>1e-5</cell><cell>1e-4</cell><cell>1e-3</cell></row><row><cell>ArcFace</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.50</cell><cell>-</cell><cell>-</cell><cell>97.32</cell><cell>83.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Verification accuracy (%) on CFP-FP and IJB-C with different distributions of margins. Backbone network: ResNet50.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Information technology -Biometric data interchange formats -Part 5: Face image data. Standard, International Organization for Standardization</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Machine Readable Travel Documents. Standard, International Civil Aviation Organization</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of extrinsic clustering evaluation metrics based on formal constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Amig?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felisa</forename><surname>Verdejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="486" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning face image quality from human assessments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lacey</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3064" to="3077" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain balancing: Face recognition on longtailed domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5671" to="5679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">VggFace2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data uncertainty learning in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ArcFace: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The elements of end-to-end deep face recognition: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13290,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Performance of biometric quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elham</forename><surname>Tabassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="531" to="543" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning meta face recognition in unseen domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6163" to="6172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MS-Celeb-1M: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FaceQnet: quality assessment for face recognition based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Hernandez-Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Haraksim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Beslay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Algorithms for clustering data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Englewood</forename><surname>Cliffs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fair loss: marginaware reinforcement learning for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10052" to="10061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive-Face: Adaptive margin and sampling for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11947" to="11956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">IARPA Janus benchmark-C: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Making a &quot;completely blind&quot; image quality analyzer. IEEE Signal processing letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AgeDB: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">L2-constrained softmax loss for discriminative face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Schlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rathgeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Henniger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Busch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01103</idno>
		<title level="m">Face image quality assessment: A literature survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic face embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Jeng-Shyang Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanqiang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic and Evolutionary Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6398" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">DeepID3: Face recognition with very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SER-FIQ: Unsupervised estimation of face image quality based on stochastic embedding robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Terhorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Niklas</forename><surname>Kolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naser</forename><surname>Damer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kirchbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Kuijper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Blind image quality evaluation using perception based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Venkatanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Praneeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandrasekhar</forename><surname>Maruthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sumohana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Channappayya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swarup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Medasani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Conference on Communications (NCC)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="926" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">NormFace: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Loddon</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CosFace: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep metric learning with angular loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2593" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Support vector guided softmax loss for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11317</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Linkage based face clustering via graph convolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">IARPA Janus benchmark-B face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to cluster faces via confidence and connectivity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Feature incay for representation regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10284</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">AdaCos: Adaptively scaling cosine logits for effectively learning deep face representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10823" to="10832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cross-pose LFW: A database for studying cross-pose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Crossage LFW: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08197</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
