<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disjoint Label Space Transfer Learning with Common Factorised Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
							<email>x.chang@qmul.ac.ukyongxin.yang@ed.ac.ukt.xiang@qmul.ac.ukt.hospedales@ed.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Disjoint Label Space Transfer Learning with Common Factorised Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, a unified approach is presented to transfer learning that addresses several source and target domain labelspace and annotation assumptions with a single model. It is particularly effective in handling a challenging case, where source and target label-spaces are disjoint, and outperforms alternatives in both unsupervised and semi-supervised settings. The key ingredient is a common representation termed Common Factorised Space. It is shared between source and target domains, and trained with an unsupervised factorisation loss and a graph-based loss. With a wide range of experiments, we demonstrate the flexibility, relevance and efficacy of our method, both in the challenging cases with disjoint label spaces, and in the more conventional cases such as unsupervised domain adaptation, where the source and target domains share the same label-sets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep learning methods are now widely used in diverse applications. However, their efficacy is largely contingent on a large amount of labelled data in the target task and domain of interest. This issue continues to motivate intense interest in cross-task and cross-domain knowledge transfer. A wide range of transfer learning settings are considered which differ in whether the label spaces of source and target domains are overlapped (i.e., aligned or disjoint), as well as the amount of supervision/labelled training samples available in the target domain (see <ref type="figure" target="#fig_0">Figure 1</ref>). The standard practice of fine-tuning <ref type="bibr" target="#b16">(Yosinski et al. 2014</ref>) treats a pre-trained source model as a good initialisation for training a target problem model. It is adopted when the label spaces of both domains are either aligned or disjoint, but always requires a significant amount of labelled data from the target, albeit less than learning from scratch. Another popular problem is the unsupervised domain adaptation (UDA), where knowledge is transferred from a labelled source domain to an unlabelled target domain <ref type="bibr" target="#b15">(Tzeng et al. 2017;</ref><ref type="bibr" target="#b6">Ganin et al. 2016;</ref><ref type="bibr" target="#b2">Cao, Long, and Wang 2018)</ref>. UDA makes the simplifying assumption that the label space of source and target domains are the same, and focuses on narrowing the distribution gap between source and target domains without any labelled samples from the target.</p><p>Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. An important but less-studied transfer learning problem setting is one where the source and target domains are with disjoint label spaces, recently highlighted by <ref type="bibr" target="#b8">(Luo et al. 2017)</ref>. In these problems, which we term Disjoint Label Space Transfer Learning (DLSTL), there are both a domain shift between source and target, as well as a new set of target classes to recognise with few (semi-supervised case) or no labelled (unsupervised case) sample per category. Thus, two main challenges exist simultaneously. On one hand, there is few or no target label to drive the adaptation. On the other hand, no clear path is provided to transfer source supervision to target domain due to the disjoint label spaces. As an example, consider object recognition in two cameras <ref type="bibr">(domains)</ref> where the object categories (label-space) are different in each camera, and one source camera has dense labels, while the target camera has data but few or no labels. The traditional fine-tuning <ref type="bibr" target="#b16">(Yosinski et al. 2014</ref>) and multi-domain training <ref type="bibr" target="#b9">(Rebuffi, Bilen, and Vedaldi 2017)</ref> can address the supervised (few label) DLSTL variant, but break down if the labels are very few, and cannot exploit unlabelled data in the target camera, i.e., semi-supervised learning. Meanwhile UDA approaches <ref type="bibr" target="#b6">(Ganin et al. 2016</ref>) based on distribution alignment are ineffective since the label-spaces are disjoint and feature distributions thus should not be indistinguishable. One approach that has the potential to handle DLSTL under both unsupervised and semi-supervised settings is based on modelling attributes, which can serve as a bridge across domains for better transferring the discriminative power <ref type="bibr" target="#b6">Gebru, Hoffman, and Li 2017;</ref>. Source and target data can be aligned within the attribute space, in order to alleviate the impacts of disjoint label space in DLSTL problems. Nevertheless, attribute can be expensive to acquire which prevents it form being widely applicable.</p><p>In this paper, a novel transfer learning model is proposed, which focuses on handling the most challenging setting, unsupervised DLSTL but is applicable to other settings including semi-supervised DLSTL and UDA. The model, termed common factorised space model (CFSM), is developed based on the simple idea that recognition should be performable in a shared latent factor space for both domains where each factor can be interpreted as latent attribute <ref type="bibr" target="#b6">(Fu et al. 2014;</ref><ref type="bibr">Rastegari, Farhadi, and Forsyth 2012)</ref>. In order to automatically discover such discriminative latent factors and align them for transferring knowledge across datasets/domains, our inductive bias is that input samples from both domains should generate low-entropy codes in this common space, i.e., near binary-codes <ref type="bibr" target="#b12">(Salakhutdinov and Hinton 2009;</ref>. This is a weaker assumption than distribution matching, but does provide a criterion that can be optimised to align the two domains in the absence of common label space and/or labelled target domain training samples. Specifically, both domains should be explainable in terms of the same set of discriminative latent factors with high certainty. As a result, discriminative information from the source domain can be more effectively transferred to the target through this common factorised space. To implement this model in a neural network architecture, a common factorised space (CFS) layer is inserted between the feature output layer (the penultimate layer) and the classification layer (the final layer). This layer is shared between both domains and thus forms a common space. An unsupervised factorisation loss is then derived and applied on such common space which serves the purpose of optimising lowentropy criterion for discriminative latent factors discovery.</p><p>Somewhat uniquely, cross-domain knowledge transfer of the proposed CFSM occurs at a relatively high layer (i.e., CFS layer). Particularly when the target domain problem is a retrieval one, it is important that this knowledge is propagated down from CFS to feature extraction for effective knowledge transfer. To assist this process we define a novel graph Laplacian-based loss -which builds a graph in the higher-level CFS, and regularises the lower-level network feature output to have matching similarity structure. i.e., that inter-sample similarity structure in the shared latent factor space should be reflected in earlier feature extraction. This top-down regularisation is opposite to the use of Laplacian regularisation in existing works <ref type="bibr" target="#b0">(Belkin, Niyogi, and Sindhwani 2006;</ref> which are bottom-up, i.e., graph from lower-level regularises the higher-level features. This unique design is due to the fact that, although both spaces (CFS and feature) are latent, the former is closer to supervisions (e.g., from the labelled source data) and more aligned thanks to the factorisation loss, and thus more discriminative and 'trustworthy'.</p><p>Contributions of the paper are as follows: 1. A unified approach to transfer learning is proposed. It can be applied to different transfer learning settings but is particularly at-tractive in handling the most challenging setting of unsupervised DLSTL. This setting is under-studied with the latest efforts focus on the easier semi-supervised DLSTL setting <ref type="bibr" target="#b8">(Luo et al. 2017)</ref> with partially labelled target data. Several topical applications in computer vision such as person re-identification (Re-ID) and sketch-based image retrieval (SBIR) can be interpreted as unsupervised DLSTL which reveals its vital research and application values. 2. We propose a deep neural network based model, called common factorised space model (CFSM), that provides the first simple yet effective method for unsupervised DLSTL; it can be easily extended to semi-supervised DLSTL as well as conventional UDA problems. 3. A novel graph Laplacianbased loss is proposed to better exploit the more aligned and discriminative supervision from higher-level to improve deep feature learning. Finally, comprehensive experiments on various transfer learning settings, from UDA to DLSTL, are conducted. CFSM achieves state-of-the-art results on both unsupervised and semi-supervised DLSTL problems and performs competitively in standard UDA. The effectiveness and flexibility of the proposed model on transfer learning problems are thus demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Transfer Learning</head><p>Transfer learning (TL) aims to transfer knowledge from one domain/task to improve performance on the another (Pan, Yang, and others 2010). The most widely used TL technique for deep networks is fine-tuning <ref type="bibr" target="#b16">(Yosinski et al. 2014;</ref>. Instead of training a target network from scratch, its weights are initialised by a pretrained model from another task such as ImageNet <ref type="bibr" target="#b3">(Deng et al. 2009</ref>) classification. While fine-tuning reduces label requirement compared to learning the target problem from scratch, it is prone to over-fitting if target labels are very few <ref type="bibr" target="#b16">(Yosinski et al. 2014)</ref>. Therefore, it is ineffective for very sparsely supervised DLSTL, and not applicable to unsupervised DLSTL. Moreover, vanilla TL does not exploit available unlabelled samples for the target problem (i.e. semisupervised TL). The most related method to ours is <ref type="bibr" target="#b8">(Luo et al. 2017</ref>) which does exploit both unlabelled and few labelled data, i.e., semi-supervised DLSTL. However like other TL methods, it does not generalise to the unsupervised DLSTL setting where no target annotations are available.</p><p>Another popular setting, unsupervised domain adaptation (UDA) focuses on transferring the source supervision to the unlabelled target domain in order to obtain a model that performs well on the latter data. The typical assumption of UDA is that both domains share the same label space. Existing methods alleviate the domain gap by either minimising the distribution discrepancy <ref type="bibr" target="#b2">(Cao, Long, and Wang 2018;</ref><ref type="bibr" target="#b14">Sun and Saenko 2016)</ref> or making the dataset representations indistinguishable by adversarial learning <ref type="bibr" target="#b15">(Tzeng et al. 2017;</ref><ref type="bibr" target="#b6">Ganin et al. 2016)</ref>. Once the domain gap is eliminated, a classifier trained on source-domain labels can be applied to the target data directly. However, distribution matching is inappropriate in the disjoint label space setting. Open set domain adaptation (OSDA) (Busto and Gall 2017) generalises UDA by allowing target domain to have some novel categories in addtion to the shared ones. It focuses on identifying shared categories and aligning those. DLSTL is a more general problem setting than OSDA, since there is no assumption of any shared categories. Related to our approach that exploits a common factorisation space to discover shared latent factors/attributes, semantic attributes have been used to improve domain adaptation performance <ref type="bibr" target="#b13">(Su et al. 2016)</ref>, for example by enabling new types of self-training ) and consistency losses (Gebru, Hoffman, and Li 2017). However these methods require the attribute definition and annotation, at least in the source domain. In contrast, no expensive attribute annotation is required in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Binary Representation Learning</head><p>The use of binary codes for hashing with deep networks goes back to <ref type="bibr" target="#b12">(Salakhutdinov and Hinton 2009)</ref>. In computer vision, hashing layers were inserted between feature-and classification-layers to provide a hashing code <ref type="bibr" target="#b8">(Lin et al. 2015;</ref>. To produce a binary representation for fast retrieval, a threshold is applied on the sigmoid activated hashing layer <ref type="bibr" target="#b8">(Lin et al. 2015)</ref>. Our method is similar in working with a near-binary penultimate layer. However there are several key differences: First, our CFS serves a very different purpose to a hash code. We focus on TL to a new domain with new label-space, and the role of our CFS is to provide a representation with which different domains can be more aligned for knowledge transfer, rather than for efficient retrieval. In contrast, existing hashing methods follow the conventional supervised learning paradigm within a single domain. Second, the proposed CFS is only near-binary due to a low-entropy loss, rather than sacrificing representation power for an exactly binary code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised Learning</head><p>Graph-based regularisation is popular for semi-supervised learning (SSL) which uses both labelled and unlabelled data to achieve better performance than learning with labelled data only <ref type="bibr" target="#b20">(Zhu 2006;</ref><ref type="bibr" target="#b0">Belkin, Niyogi, and Sindhwani 2006)</ref>. In SSL, graph based regularisation is applied to regularise model predictions to respect the feature-space manifold <ref type="bibr" target="#b18">(Yue et al. 2017;</ref><ref type="bibr" target="#b8">Nadler, Srebro, and Zhou 2009;</ref><ref type="bibr" target="#b0">Belkin, Niyogi, and Sindhwani 2006)</ref>. Moreover, exploiting the graph from lower-level to regularise higher-level features is widely adopted in other scenarios, e.g., unsupervised learning <ref type="bibr" target="#b7">(Jia et al. 2015;</ref>). Due to the source?target knowledge transfer, the more 'trustworthy' layer in our method is the penultimate CFS layer, as it is closer to the supervision, rather than the feature space layer. Therefore our regularisation is applied to encourage the feature-extractor to learn representations that respect the CFS manifold shared by both domains, i.e., the regularisation direction is opposite to that in existing models.</p><p>Entropy loss for unlabelled data is another widely used SSL regulariser <ref type="bibr" target="#b20">(Zhu 2006;</ref>. It is applied at the classification layer in problems where the unlabelled and labelled data share the same label-space -and reflects the inductive bias that a classification boundary should not cut through the dense unlabelled data regions. Its typical use is on softmax classifier outputs where it encourages a classifier to pick a single label. In contrast we use entropy-loss to solve DLSTL problems by applying it element-wise on our intermediate CFS layer in order to weakly align domains by encouraging them to share a near-binary representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Definition and notation For Disjoint Label Space Transfer Learning (DLSTL), there is a source (labelled) domain S and a target (unlabelled or partially labelled) domain T 1 . The key characteristic of DLSTL is the disjoint label space assumption, i.e., the source Y S and target Y T label spaces are potentially disjoint: Y S ? Y T = ?. Instances from source/target domains are denoted X S and X T respectively. The combined inputs {X S , X T } are denoted as X. To present our model, we stick mainly to the most challenging unsupervised DLSTL setting where target labels are totally absent. The easier cases, e.g., semi-supervised DLSTL and UDA, can then be handled with minor modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architecture</head><p>The proposed model architecture consists of three modules, a feature extractor F = ? ? M (X) that can be any deep neural network and is shared between all domains. This is followed by a fully connected layer and sigmoid activation ?, which define the Common Factorised Space (CFS) layer. This provides a representation of dimension d C ,</p><formula xml:id="formula_0">F C = ? ? C (?) = ?(W ? ? M (?) + b).</formula><p>Recall that the goal of CFS is to learn a latent factor (low-entropy) representation for both source and target domains. The sigmoid activation means that the layer's scale is F C ? (0, 1) d C , so activations near 0 or 1 can be interpreted as the corresponding latent factor being present or absent. To encourage a near-binary representation, unsupervised factorisation loss is applied. For the labelled source domain only, the pre-activated F C are then classified by softmax classifier ? ? S with cross-entropy loss. The overall architecture is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularised Model Optimisation</head><p>The parameters of the proposed CFSM are ? := {? M , ? C , ? S } including parameters of the feature extractor ? M , CFS layer ? C and source classifier ? S . The training procedure can be formulated as a maximum-a-posterior (MAP) learning given labelled source {X S , Y S } and unlabelled tar-</p><formula xml:id="formula_1">get data X T ,? = argmax ? p(?|X S , Y S , X T ),<label>(1)</label></formula><p>where p(?|X S , Y S , X T ) is the posterior of model parameter ? given data X S , Y S , X T . This can be rewritten as</p><formula xml:id="formula_2">p(?|X S , Y S , X T ) ?p(?, X S , Y S , X T ) ?p(Y S |X S , X T , ?)p(?|X S , X T ).<label>(2)</label></formula><p>So the optimisation in Eq. 1 is equivalentl?</p><formula xml:id="formula_3">? = argmax ? p(Y S |X S , ?)p(?|X).<label>(3)</label></formula><p>Sampling mini-batch Common Space</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Regularisation</head><p>Unsupervised Factorisation Loss <ref type="figure">Figure 2</ref>: The proposed model architecture is illustrated. Different colours corresponding to different data streams. Green indicates source data. Blue is used for target data. Purple means joint data from both source and target domains.</p><p>The first term p(Y S |X S , ?) in Eq. 3 represents the likelihood of source labels w.r.t. ?. Optimising this term is a conventional supervised learning task with a loss denoted sup (X S , Y S ; ?).</p><p>The second term p(?|X) in Eq. 3 is a prior depending on the input data X of both source and target datasets. From an optimisation perspective, this is the regulariser that will play the key role in solving unsupervised DLSTL problems since it requires no labels. Given the model architecture, it can be further decomposed as:</p><formula xml:id="formula_4">p(?|X) =p(? M , ? C |X) =p(? C |? M , X)p(? M |X),<label>(4)</label></formula><p>where ? S is excluded since no supervision is used. Specifically, the first term p(? C |? M , X) serves as the regulariser on the CFS layer while the second term p(? M |X) regularises the deep feature extractor ? ? M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-Entropy Regularisation: Unsupervised Adaptation</head><p>We first discuss how to define the prior p(? C |? M , X) regulariser for CFS layer. The sigmoid activated outputs F C from CFS layer ? ? C can be interpreted as multi-label predictions on latent factors. The uncertainty measure for label prediction can be defined by using its entropy</p><formula xml:id="formula_5">h(? C |? M , X) = ? N i=1 &lt; F C,i , log(F C,i ) &gt; = ? N i=1 &lt; ? ? C (x i ), log(? ? C (x i )) &gt;<label>(5)</label></formula><p>where F C,i denotes the common factor representation ? ? C (x i ) of instance x i ? X. This is applied on both source and target data, so N is the number of instances in both datasets. log(?) is applied element-wise, and &lt; ?, ? &gt; is vector inner product. According to the low-uncertainty criterion <ref type="bibr" target="#b2">(Carlucci et al. 2017)</ref>, optimising the prior term p(? C |? M , X) can be achieved by minimising this uncertainty measure. Eq. 5 is thus the regulariser corresponding to the prior p(? C |? M , X). Specifically, this loss biases the representation F C to contain more certain predictions, e.g., closer to 0 or 1 for each discovered latent factor. Therefore, we denote it as unsupervised factorisation loss. In summary, the low-entropy regulariser on CFS is built upon the assumption that the two domains share a set of latent attributes and that if a source classifier is well adapted to the target, then the presence/absence of these attributes should be certain for each instance. Therefore, it essentially generalises the low-uncertainty principle (widely used in existing unsupervised and semi-supervised learning literature) to the disjoint label space setting.</p><p>Graph Regularisation: Robust Feature Learning The second prior in Eq. 4 is p(? M |X) which acts as the regulariser for the feature extractor ? ? M . The unique property of our setup so far is that the knowledge transfer into the target domain is via the CFS layer; therefore we are interested in ensuring that the feature extractor network extracts features whose similarity structure reflects that of the latent factors in the CFS layer. Unlike conventional graph Laplacian losses that regularise higher-level features with a graph built on lower-level features <ref type="bibr" target="#b0">(Belkin, Niyogi, and Sindhwani 2006;</ref><ref type="bibr" target="#b20">Zhu 2006)</ref>, we do the reverse and regularise the feature extractor ? ? M to reflect the similarity structure in F C . This is particularly important for applications where the target problem is retrieval, because we use deep features F = ? ? M (?) as an image representation.</p><p>The proposed graph loss is expressed as</p><formula xml:id="formula_6">Tr(F T ? F C F ),<label>(6)</label></formula><p>where ? F C is the graph Laplacian <ref type="bibr" target="#b1">(Cai et al. 2011</ref>) built on the common space features F C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>We unify the proposed model architecture ? := {? M , ? C , ? D S } with source {X S , Y S } and target {X T } data for unsupervised DLSTL problems from an maximum-a-posterior (MAP) perspective. This decomposes into a standard supervised term p(Y S |X S , ?) (source data only) and data-driven priors for the CFS layer and feature extraction module. They correspond to supervised loss sup (X S , Y S ; ?), unsupervised factorisation loss (Eq. 5) and the graph loss (Eq. 6) respectively. Taking all terms into account, the final optimisation objective of Eq. 3 is</p><formula xml:id="formula_7">L(?) = sup (X S , Y S ; ?) + ? M T r(F T ? F C F ) ? ? C 1 N N i=1 &lt; F C,i , log(F C,i ) &gt; .<label>(7)</label></formula><p>where ? C and ? M are balancing hyper-parameters. In order to select ? C and ? M , the model is first run by setting all weights to 1; after the first few iterations, we check the values of each loss. We then set the two hyper-parameters to rescale the losses to a similar range so that all three terms contribute approximately equally to the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini-batch organisation</head><p>Convolutional Neural Networks (CNNs) are usually trained with SGD mini-batch optimisation, but Eq. 7 is expressed in a full-batch fashion. Converting Eq. 7 to mini-batch optimisation is straightforward. However, it is worth mentioning the mini-batch scheduling: each mini-batch contains samples from both source and target domains. The supervised loss is applied only to source samples with corresponding supervision, the entropy and graph losses are applied to both, and the graph is built per-mini-batch. In this work, the number of source and target samples are equally balanced in a mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>The proposed model is evaluated on progressively more challenging problems. First, we evaluate CFSM on unsupervised domain adaptation (UDA). Second, different DLSTL settings are considered, including semi-supervised DLSTL classification and unsupervised DLSTL retrieval. CFSM handles all these scenarios with minor modifications. The effectiveness CFSM is demonstrated by its superior performance compared to the existing work. Finally insight is provided through ablation study and visualisation analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Domain Adaptation: SVHN-MNIST</head><p>Dataset and Settings We evaluate the UDA setting from <ref type="bibr" target="#b6">(Ganin et al. 2016)</ref> where <ref type="bibr">SVHN (Netzer et al. 2011</ref>) is the labelled source dataset and <ref type="bibr">MNIST (LeCun et al. 1998</ref>) is the unlabelled target. For fair comparison we use an identical feature extractor network to <ref type="bibr" target="#b8">(Luo et al. 2017</ref>). Our CFSM is pre-trained on the source dataset with cross-entropy supervision and d C = 50, followed by joint training on source and target with our regularisers as in Eq. 7. Since the labelspace is shared in UDA, we also apply entropy loss on the softmax classification of the target . We set ? M = 0.001 and ? C = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare our method with two baselines. Source only: Supervised training on the source and directly apply to target data. Joint FT: Model is initialised with source pre-train, and fine-tuning on both domains with supervised loss for source and semi-supervised entropy loss for target. We also compare several deep UDA methods including Gradient Reversal <ref type="bibr" target="#b6">(Ganin et al. 2016)</ref>, Domain Confusion <ref type="bibr" target="#b15">(Tzeng et al. 2015)</ref>, ADDA <ref type="bibr" target="#b15">(Tzeng et al. 2017)</ref>, Label Efficient Transfer (LET) (Luo et al. 2017), Asym. tri-training <ref type="bibr" target="#b11">(Saito, Ushiku, and Harada 2017)</ref> and <ref type="bibr">Respara (Rozantsev, Salzmann, and Fua 2018)</ref>.</p><p>As shown in <ref type="table">Table 1</ref>, CFSM boosts the performance on both baselines with clear margin (25.5% and 9.3% vs. Source only and Joint FT respectively). Moreover, it is 5.5% higher than <ref type="bibr">LET (Luo et al. 2017)</ref>, the nearest competitor and only alternative that also addresses the DLSTL setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised DLSTL: Digit Recognition</head><p>Dataset and Settings We follow the semi-supervised DL-STL recognition experiment of <ref type="bibr" target="#b8">(Luo et al. 2017)</ref> where again two digit datasets, SVHN and MNIST, are used. Images of digits 0 to 4 from SVHN are fully labelled as source data while images of digits 5 to 9 from MNIST are target data. The target dataset has sparse labels (k labels per class) and unlabelled images available. Thus we also add a classifier ? ? T after the CFS layer ? ? C for the target categories.</p><p>The feature extractor architecture ? ? M is exactly the same as in <ref type="bibr" target="#b8">(Luo et al. 2017)</ref> for fair comparison. We pre-train CFSM on source data as initialisation, and then train it with both source and target data using only loss in Eq. 7. We set d C = 10, ? M = ? C = 0.01. The learning rate is 0.001 and the Adam (Kingma and Ba 2014) optimiser is used.</p><p>Results The results for several degrees of target label sparsity k = 2, 3, 4, 5 (corresponding to 10, 15, 20, 25 labelled samples, or 0.034%, 0.050%, 0.066%, 0.086% of total target training data respectively), are reported in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Results are averaged over ten random splits as in <ref type="bibr" target="#b8">(Luo et al. 2017)</ref>. Besides the FT matching nets <ref type="bibr" target="#b15">(Vinyals et al. 2016)</ref> and state-of-the-art LET results from (Luo et al. 2017), we run two baselines: Train Target: Training CFSM architecture from scratch with partially labelled target data only, and FT Target: The standard pre-train/fine-tune pipeline, i.e., pretrain on the labelled source, and fine-tune on the labelled target samples only. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the performances of baseline models are significantly lower than LET and the proposed CFSM. The Train Target baseline performs poorly as it is hard to achieve good performance with few target samples and no knowledge transfer from source. The Fine-Tune Target baseline performs poorly as the annotation here is too sparse for effective fine-tuning on the target problem. Fine-Tune matching nets follows the 5-way (k ? 1)-shot learning with sparsely labelled target data only, but no improvement is shown over the other baselines. Our proposed CFSM consistently outperforms the state-of-the-art LET alternative. For example, under the most challenging setting (k = 2), CFSM is 1.8% higher than LET on mean accuracy and 0.2% lower on standard error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised DLSTL: ReID and SBIR</head><p>ReID The person re-identification (ReID) problem is to match person detections across camera views. Annotating person image identities in every camera in a camera network for training supervised models is infeasible. This motivates the topical unsupervised Re-ID problem of adapting a Re-ID model trained on one dataset with annotation to a new dataset without annotation. Although they are evaluated with retrieval metrics, contemporary Re-ID models are trained using identity prediction (classification) losses. This means that unsupervised Re-ID fits the unsupervised DLSTL setting, as the label-spaces (person identities) are different in different Re-ID datasets, and the target dataset has no labels.</p><p>We adopt two highly contested large-scale benchmarks   for unsupervised person Re-ID: Market <ref type="bibr" target="#b19">(Zheng et al. 2015)</ref> and Duke . ImageNet pretrained Resnet50 <ref type="bibr" target="#b7">(He et al. 2016</ref>) is used as the feature extractor ? ? M . Cross-entropy loss with label smoothing and triplet loss are used for the source domain as supervised learning objectives. We set d C = 2048, ? M = 2.0, ? C = 0.01. Adam optimiser is used with learning rate 3.5e ?4 . We treat each dataset in turn as source/target and perform unsupervised transfer from one to the other. Rank 1 (R1) accuracy and mean Average Precision (mAP) results on the target datasets are used as evaluation metrics.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, We show that our method outperforms the state-of-the-art alternatives purpose-designed for unsupervised person Re-ID: <ref type="bibr">UMDL (Peng et al. 2016)</ref>, <ref type="bibr">PT-GAN (Wei et al. 2018)</ref>, PUL <ref type="bibr" target="#b5">(Fan, Zheng, and Yang 2017)</ref>, CAMEL <ref type="bibr" target="#b17">(Yu, Wu, and Zheng 2017)</ref>, TJ-AIDL , SPGAN ) and MMFA <ref type="bibr" target="#b8">(Lin et al. 2018)</ref>. Note that TJ-AIDL and MMFA exploit attribute labels to help alignment and adaptation. The proposed method automatically discovers latent factors with no additional annotation. However, CFSM improves at least 3.0% over TJ-AIDL and MMFA on the R1 accuracy of both settings.</p><p>FG-SBIR Fine-grained Sketch Based Image Retrieval (SBIR) focuses on matching a sketch with its corresponding photo <ref type="bibr" target="#b12">(Sangkloy et al. 2016)</ref>. As demonstrated in <ref type="bibr" target="#b12">(Sangkloy et al. 2016)</ref>, object category labels play an important role in retrieval performance, so existing studies make a closed world assumption, i.e., all testing categories overlap with training categories. However, if deploying SBIR in a real application such as e-commerce <ref type="bibr" target="#b17">(Yu et al. 2016)</ref>, one would like to train the SBIR system once on some source object categories, and then deploy it to provide sketch-based image retrieval of new categories without annotating new data and re-training for the target object category. Unsupervised adaptation to new categories without sketch-photo pairing labels is therefore another example of the unsupervised DL-STL problem. Comparing to Re-ID, where instances are person images in different camera views, instances in SBIR are either photos or hand-drawn sketches of objects.</p><p>There are 125 object classes in the Sketchy dataset <ref type="bibr" target="#b12">(Sangkloy et al. 2016)</ref>. We randomly split 75 classes as a labelled source domain and use the remaining 50 classes to define an unlabelled target domain with disjoint label space. ImageNet pre-trained Inception-V3 <ref type="bibr" target="#b14">(Szegedy et al. 2016</ref>) is used as the feature extractor ? ? M . Cross-entropy and triplet loss are used for source supervision. We set d C = 512, ? M = 10 ?3 , ? C = 0.1. Adam optimiser with learning rate 10 ?4 is used. As a baseline, Source Only is the direct transfer alternative that uses the same architecture but trains on the source labelled data only, and is applied directly to the target without adaptation. The retrieval performance on unseen classes (tar. cls.) are reported. Results are averaged over 10 random splits. As shown in <ref type="table">Table 4</ref>, the proposed CFSM improves the retrieval accuracy on unseen cases by 2.48%.</p><p>Source only CFSM tar. cls. 23.74 ? 0.24 26.22 ? 0.25 <ref type="table">Table 4</ref>: SBIR: Sketch-photo retrieval results (%). Averaged Rank 1 accuracy and standard error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Further Analysis</head><p>Ablation study Unsupervised person Re-ID is chosen as the main benchmark for an ablation study. Firstly because it is a challenging and realistic large-scale problem in the unsupervised DLSTL setting, and secondly because it provides a bidirectional evaluation for more comprehensive analysis.</p><p>The following ablated variants are proposed and compared with the full CFSM. Source Only: The proposed architecture is learned with source data and supervised losses only. Source+Regs: The regularisers, unsupervised factorisation and graph losses can be added with source dataset only. CFSM?Graph: Our method without the proposed graph loss. CFSM+ClassicGraph: Replacing our proposed graph loss with a conventional Laplacian graph (i.e., graphs constructed in lower-level feature space extracted by ? ? M to regularise the proposed CFS). AE: Other regularisers such as feature reconstruction as in autoencoder (AE) is used to provide the prior term p(?|X). We reconstruct the deep features F using the outputs of CFS layer as hidden representations.  In this case both source and target data are used and the reconstruction error provides the regularisation loss.</p><p>The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. Firstly, by comparing the variants that use source data only (Source Only and Source+Regs) with the joint training methods, we find they are consistently inferior. This illustrates that it is crucial to leverage target domain data for adaptation. Secondly, CFSM and its variants consistently achieve better results than AE, illustrating that our unsupervised factorisation loss and graph losses provide better regularisation for cross-domain/cross-task adaptation. The effectiveness of our graph loss is illustrated by two comparisons: (1) CFSM?Graph is worse than CFSM, showing the contribution of the graph loss; and (2) replacing our graph loss with the conventional Laplacian graph loss (CFSM+ClassicGraph) shows worse results than ours, justifying our choice of regularisation direction. Finally, we note that applying our regularisers to the source only (Source+Regs) still improves the performance slightly on target dataset vs Source Only. This shows that training with these regularisers has a small benefit to representation transferability even without adaptation. Visualisation analysis To understand the impact of unsupervised factorisation loss, <ref type="figure">Figure 3</ref> illustrates the distribution of target CFS activations in the semi-supervised DLSTL setting (SVHN?MNIST). The left plot shows the activations without any such loss, leading to a distribution of moderate predictions peaked around 0.5. In contrast, the right plot shows the activation distribution on the target dataset of CFSM. We can see that our regulariser has indeed induced the target dataset to represent images with a low-entropy near-binary code. We also compare training a source model by adding low-entropy CFS loss, and then applying it to the target data. This leads to a low-entropy representation of the source data, but the middle plot shows that when transferred to the target dataset or adaptation the representation becomes high-entropy. That is, joint training with our losses is crucial to drive the adaptation that allows target dataset to be represented with near-binary latent factor codes. Qualitative Analysis We visualise the discovered latent attributes qualitatively. For each element in F C , we rank images in both source and target domains by their activation. Person images corresponding to the highest ten values of a specific F C are recorded. <ref type="figure">Figure 4</ref> shows two example factors with images from the source (first row) and target (second row) dataset. We can see that the first example in <ref type="figure">Figure 4</ref> ering both people's bags and clothes. The second example in <ref type="figure">Figure 4(b)</ref> is a higher-level latent attribute that is selective for both females, as well as textured clothes and bagcarrying. Importantly, these factors have become selective for the same latent factors across datasets, although the target dataset has no supervision (i.e., unsupervised DLSTL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We studied a challenging transfer learning setting DLSTL, where the label space between source and target labels are disjoint, and the target dataset has few or no labels. In order to transfer the discriminative cues from the labelled source to the target, we propose a simple yet effective model which uses an unsupervised factorisation loss to discover a common set of discriminative latent factors between source and target datasets. And to improve feature learning for subsequent tasks such as retrieval, a novel graph-based loss is further proposed. Our method is both the first solution to the unsupervised DLSTL, and also uniquely provides a single framework that is effective at both unsupervised and semisupervised DLSTL as well as the standard UDA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Schematic of various transfer learning problems on two criteria: the relation between source and target label space, and the amount of target problem supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>(a) is a latent attribute for the colour 'red' cov-CFS activations distribution on target data. Left: Train on source with supervised loss. Middle: Train on source with both supervised and low-entropy CFS losses. Right: CFSM, jointly trained on source and target. Illustration of images selected by two different latent factors: (a) red and (b) female/textured/bag-carrying. In each case the top row is the source (Market) data and the bottom row is the target (Duke) data. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? 0.5 94.8 ? 0.5 95.5 ? 0.3 96.7 ? 0.2</figDesc><table><row><cell></cell><cell></cell><cell>k = 2</cell><cell>k = 3</cell><cell>k = 4</cell><cell>k = 5</cell></row><row><cell>Train Target</cell><cell></cell><cell>66.5 ? 1.7</cell><cell>77.2 ? 1.1</cell><cell>83.0 ? 0.9</cell><cell>88.3 ? 1.1</cell></row><row><cell>FT Target</cell><cell></cell><cell>69.8 ? 1.6</cell><cell>79.1 ? 1.2</cell><cell>84.5 ? 0.8</cell><cell>89.3 ? 0.9</cell></row><row><cell cols="2">FT matching nets NIPS'16</cell><cell>64.5 ? 1.9</cell><cell>75.5 ? 2.4</cell><cell>79.3 ? 1.3</cell><cell>82.7 ? 1.1</cell></row><row><cell>LET</cell><cell>NIPS'17</cell><cell>91.7 ? 0.7</cell><cell>93.6 ? 0.6</cell><cell>94.2 ? 0.6</cell><cell>95.0 ? 0.4</cell></row><row><cell>CFSM</cell><cell></cell><cell>93.5</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Semi-supervised DLSTL image categorisation results (%), with mean classification accuracy and standard error for</figDesc><table><row><cell cols="3">SVHN (0-4) ? MNIST (5-9).</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">M2D</cell><cell cols="2">D2M</cell></row><row><cell>model</cell><cell></cell><cell>R1</cell><cell>mAP</cell><cell>R1</cell><cell>mAP</cell></row><row><cell>UMDL</cell><cell>CVPR'16</cell><cell>18.5</cell><cell>7.3</cell><cell cols="2">34.5 12.4</cell></row><row><cell>PTGAN</cell><cell>CVPR'18</cell><cell>27.4</cell><cell>-</cell><cell>38.6</cell><cell>-</cell></row><row><cell>PUL</cell><cell>arXiv'17</cell><cell cols="2">30.0 16.4</cell><cell cols="2">45.5 20.5</cell></row><row><cell>CAMEL</cell><cell>ICCV'17</cell><cell>-</cell><cell>-</cell><cell cols="2">54.5 26.3</cell></row><row><cell cols="2">TJ-AIDL CVPR'18</cell><cell cols="2">44.3 23.0</cell><cell cols="2">58.2 26.5</cell></row><row><cell>SPGAN</cell><cell>CVPR'18</cell><cell cols="2">46.4 26.2</cell><cell cols="2">57.7 26.7</cell></row><row><cell>MMFA</cell><cell cols="3">BMVC'18 45.3 24.7</cell><cell cols="2">56.7 27.4</cell></row><row><cell>CFSM</cell><cell></cell><cell cols="2">49.8 27.3</cell><cell cols="2">61.2 28.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Unsupervised transfer for person Re-ID (%). M2D</cell></row><row><cell>indicates Market as source dataset and Duke as target, vice</cell></row><row><cell>versa. Target Dataset Performance is reported.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on unsupervised person Re-ID benchmarks. Target dataset performance (%) is reported.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The proposed model can be easily extended to deal with multiple source and target domains</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the EP-SRC grant EP/R026173.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyogi</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sindhwani ; Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Open set domain adaptation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with distribution matching machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><forename type="middle">;</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
	<note>Autodial: Automatic domain alignment layers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep domain adaptation for describing people based on fine-grained clothing attributes. CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Lstd: A low-shot transfer detector for object detection. arXiv. Imagenet: A large-scale hierarchical image database. CVPR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification: Clustering and finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks</title>
		<editor>JMLR. [Gebru, Hoffman, and Li</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Learning multi-modal latent attributes. TPAMI. grained recognition in the wild: A multi-task domain adaptation approach. ICCV</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Laplacian auto-encoders: an explicit learning of nonlinear data manifold. Neurocomputing</title>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>Ba</pubPlace>
		</imprint>
	</monogr>
	<note>Adam: A method for stochastic optimization. arXiv. Gradient-based learning applied to document recognition. IEEE Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person re-identification. BMVC</title>
		<imprint>
			<date type="published" when="2009" />
			<publisher>NIPSW</publisher>
		</imprint>
	</monogr>
	<note>Unsupervised cross-dataset transfer learning for person re-identification. CVPR. Rastegari, Farhadi, and Forsyth 2012] Rastegari, M.; Farhadi, A.; and Forsyth, D. 2012. Attribute discovery via predictable discriminative binary codes. ECCV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilen</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Vedaldi ; Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
	</analytic>
	<monogr>
		<title level="m">NIPS. [Rozantsev, Salzmann, and Fua</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Residual parameter transfer for deep domain adaptation. CVPR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ushiku</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic hashing. International Journal of Approximate Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Burnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The sketchy database: Learning to retrieve badly drawn bunnies. SIGGRAPH</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Sangkloy et al. 2016</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep attributes driven multi-camera person re-identification</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>Rethinking the inception architecture for computer vision</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification. CVPR</title>
	</analytic>
	<monogr>
		<title level="m">Simultaneous deep transfer across domains and tasks. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Adversarial discriminative domain adaptation. CVPR. Wei et al. 2018. Person transfer gan to bridge domain gap for person reidentification. CVPR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A graph regularized deep neural network for unsupervised image representation learning</title>
	</analytic>
	<monogr>
		<title level="m">How transferable are features in deep neural networks? NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Yosinski et al. 2014</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Sketch me that shoe. ICCV</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised learning through adaptive laplacian graph trimming. Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scalable person re-identification: A benchmark</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">;</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>AAAI</publisher>
			<pubPlace>Zhu, X.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note>Semi-supervised learning literature survey</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
