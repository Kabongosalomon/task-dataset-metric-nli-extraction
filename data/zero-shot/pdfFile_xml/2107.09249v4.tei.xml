<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Aggregation of Diverse Experts for Test-Agnostic Long-Tailed Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
							<email>yifan.zhang@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanqing</forename><surname>Hong</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab 3 ByteDance</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<email>jshfeng@gmail.com</email>
						</author>
						<title level="a" type="main">Self-Supervised Aggregation of Diverse Experts for Test-Agnostic Long-Tailed Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing long-tailed recognition methods, aiming to train class-balanced models from long-tailed data, generally assume the models would be evaluated on the uniform test class distribution. However, practical test class distributions often violate this assumption (e.g., being either long-tailed or even inversely long-tailed), which may lead existing methods to fail in real applications. In this paper, we study a more practical yet challenging task, called test-agnostic long-tailed recognition, where the training class distribution is long-tailed while the test class distribution is agnostic and not necessarily uniform. In addition to the issue of class imbalance, this task poses another challenge: the class distribution shift between the training and test data is unknown. To tackle this task, we propose a novel approach, called Self-supervised Aggregation of Diverse Experts, which consists of two strategies: (i) a new skill-diverse expert learning strategy that trains multiple experts from a single and stationary long-tailed dataset to separately handle different class distributions; (ii) a novel test-time expert aggregation strategy that leverages self-supervision to aggregate the learned multiple experts for handling unknown test class distributions. We theoretically show that our self-supervised strategy has a provable ability to simulate test-agnostic class distributions. Promising empirical results demonstrate the effectiveness of our method on both vanilla and test-agnostic long-tailed recognition. Code is available at https://github. com/Vanint/SADE-AgnosticLT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Real-world visual recognition datasets typically exhibit a long-tailed distribution, where a few classes contain numerous samples (called head classes), but the others are associated with only a few instances (called tail classes) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33]</ref>. Due to the class imbalance, the trained model is easily biased towards head classes and perform poorly on tail classes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">58]</ref>. To tackle this issue, numerous studies have explored long-tailed recognition for learning well-performing models from imbalanced data <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">56]</ref>.</p><p>Most existing long-tailed studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">48,</ref><ref type="bibr">52]</ref> assume the test class distribution is uniform, i.e., each class has an equal amount of test data. Therefore, they develop various techniques, e.g., class resampling <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr">55]</ref>, cost-sensitive learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr">41,</ref><ref type="bibr">47]</ref> or ensemble learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr">53]</ref>, to re-balance the model performance on different classes for fitting the uniform class distribution. However, this assumption does not always hold in real applications, where actual test data may follow any kind of class distribution, being either uniform, long-tailed, or even inversely long-tailed to the training data (cf. <ref type="figure">Figure 1(a)</ref>). For example, one may train a recognition model for autonomous cars based on the training data collected from city areas, where pedestrians are majority classes and stone obstacles are minority classes. However, when the model is deployed to mountain areas, the pedestrians become the minority while the stones become the majority. In this case, the test class distribution is inverse to the training one, and existing methods may perform poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Class Distributions</head><p>Trained Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Class Distributions</head><p>(a) Existing long-tailed recognition methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Class Distributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Class Distributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-expert Model</head><p>Test-time Expert Aggregation (b) The idea of our proposed method <ref type="figure">Figure 1</ref>: Illustration of test-agnostic long-tailed recognition. (a) Existing long-tailed recognition methods aim to train models that perform well on test data with the uniform class distribution. However, the resulting models may fail to handle practical test class distributions that skew arbitrarily. (b) Our method seeks to learn a multi-expert model from a single long-tailed training set, where different experts are skilled in handling different class distributions, respectively. By reasonably aggregating these experts at test time, our method is able to handle unknown test class distributions.</p><p>To address the issue of varying class distributions, as the first research attempt, LADE <ref type="bibr" target="#b16">[17]</ref> assumes the test class distribution to be known and uses the knowledge to post-adjust model predictions. However, the actual test class distribution is usually unknown a priori, making LADE not applicable in practice. Therefore, we study a more realistic yet challenging problem, namely test-agnostic long-tailed recognition, where the training class distribution is long-tailed while the test distribution is agnostic. To tackle this problem, motivated by the idea of "divide and conquer", we propose to learn multiple experts with diverse skills that excel at handling different class distributions (cf. <ref type="figure">Figure 1(b)</ref>). As long as these skill-diverse experts can be aggregated suitably at test time, the multi-expert model would manage to handle the unknown test class distribution. Following this idea, we develop a novel approach, namely Self-supervised Aggregation of Diverse Experts (SADE).</p><p>The first challenge for SADE is how to learn multiple diverse experts from a single and stationary long-tailed training dataset. To handle this challenge, we empirically evaluate existing long-tailed methods in this task, and find that the models trained by existing methods have a simulation correlation between the learned class distribution and the training loss function. That is, the models learned by various losses are skilled in handling class distributions with different skewness. For example, the model trained with the conventional softmax loss simulates the long-tailed training class distribution, while the models obtained from existing long-tailed methods are good at the uniform class distribution. Inspired by this finding, SADE presents a simple but effective skill-diverse expert learning strategy to generate experts with different distribution preferences from a single long-tailed training distribution. Here, various experts are trained with different expertise-guided objective functions to deal with different class distributions, respectively. As a result, the learned experts are more diverse than previous multi-expert long-tailed methods <ref type="bibr">[49,</ref><ref type="bibr">63]</ref>, leading to better ensemble performance, and in aggregate simulate a wide spectrum of possible class distributions.</p><p>The other challenge is how to aggregate these skill-diverse experts for handling test-agnostic class distributions based on only unlabeled test data. To tackle this challenge, we empirically investigate the property of different experts, and observe that there is a positive correlation between expertise and prediction stability, i.e., stronger experts have higher prediction consistency between different perturbed views of samples from their favorable classes. Motivated by this finding, we develop a novel self-supervised strategy, namely prediction stability maximization, to adaptively aggregate experts based on only unlabeled test data. We theoretically show that maximizing the prediction stability enables SADE to learn an aggregation weight that maximizes the mutual information between the predicted label distribution and the true class distribution. In this way, the resulting model is able to simulate unknown test class distributions.</p><p>We empirically verify the superiority of SADE on both vanilla and test-agnostic long-tailed recognition. Specifically, SADE achieves promising performance on vanilla long-tailed recognition under all benchmark datasets. For instance, SADE achieves 58.8% accuracy on ImageNet-LT with more than 2% accuracy gain over previous state-of-the-art ensemble long-tailed methods, i.e., <ref type="bibr">RIDE [49]</ref> and ACE <ref type="bibr" target="#b1">[2]</ref>. More importantly, SADE is the first long-tailed approach that is able to handle various test-agnostic class distributions without knowing the true class distribution of test data in advance. Note that SADE even outperforms LADE <ref type="bibr" target="#b16">[17]</ref> that uses knowledge of the test class distribution.</p><p>Compared to previous long-tailed methods (e.g., LADE <ref type="bibr" target="#b16">[17]</ref> and RIDE <ref type="bibr">[49]</ref>), our method offers the following advantages: (i) SADE does not assume the test class distribution to be known, and provides the first practical approach to handling test-agnostic long-tailed recognition; (ii) SADE develops a simple diversity-promoting strategy to learn skill-diverse experts from a single and stationary long-tailed dataset; (iii) SADE presents a novel self-supervised strategy to aggregate skill-diverse experts at test time, by maximizing prediction consistency between unlabeled test samples' perturbed views; (iv) the presented self-supervised strategy has a provable ability to simulate test-agnostic class distributions, which opens the opportunity for tackling unknown class distribution shifts at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Long-tailed recognition Existing long-tailed recognition methods, related to our study, can be categorized into three types: class re-balancing, logit adjustment and ensemble learning. Specifically, class re-balancing resorts to re-sampling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref> or cost-sensitive learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16</ref>, 61] to balance different classes during model training. Logit adjustment <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr">43]</ref> adjusts models' output logits via the label frequencies of training data at inference time, for obtaining a large relative margin between head and tail classes. Ensemble-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">53</ref>, 63], e.g., <ref type="bibr">RIDE [49]</ref>, are based on multiple experts, which seek to capture heterogeneous knowledge, followed by ensemble aggregation. More discussions on the difference between our method and RIDE <ref type="bibr">[49]</ref> are provided in Appendix D. <ref type="bibr" target="#b2">3</ref>. Regarding test-agnostic long-tailed recognition, LADE <ref type="bibr" target="#b16">[17]</ref> assumes the test class distribution is available and uses it to post-adjust model predictions. However, the true test class distribution is usually unknown a priori, making LADE inapplicable. In contrast, our method does not rely on the true test distribution for handling this problem, but presents a novel self-supervised strategy to aggregate skill-diverse experts at test time for test-agnostic class distributions. Moreover, some ensemble-based long-tailed methods <ref type="bibr" target="#b38">[39]</ref> aggregate experts based on a labeled uniform validation set. However, as the test class distribution could be different from the validation one, simply aggregating experts on the validation set is unable to handle test-agnostic long-tailed recognition.</p><p>Test-time training Test-time training <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">40,</ref><ref type="bibr">46</ref>] is a transductive learning paradigm for handling distribution shifts <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr">45,</ref><ref type="bibr">59]</ref> between training and test data, and has been applied with success to out-of-domain generalization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref> and dynamic scene deblurring <ref type="bibr" target="#b5">[6]</ref>. In this study, we explore this paradigm to handle test-agnostic long-tailed recognition, where the issue of class distribution shifts is the main challenge. However, most existing test-time training methods seek to handle covariate distribution shifts instead of class distribution shifts, so simply leveraging them cannot resolve test-agnostic long-tailed recognition, as shown in our experiment (cf. <ref type="table" target="#tab_9">Table 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>Long-tailed recognition aims to learn a well-performing classification model from a training dataset with long-tailed class distribution. Let D s ={x i , y i } ns i=1 denote the long-tailed training set, where y i is the class label of the sample x i . The total number of training data over C classes is n s = C k=1 n k , where n k denotes the number of samples in class k. Without loss of generality, we follow a common assumption <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref> that the classes are sorted by cardinality in decreasing order (i.e., if i 1 &lt; i 2 , then n i1 ? n i2 ), and n 1 n C . The imbalance ratio is defined as max(n k )/min(n k ) = n 1 /n C . The test data D t = {x j , y j } nt j=1 is defined in a similar way. Most existing long-tailed recognition methods assume the test class distribution is uniform (i.e., p t (y) = 1/C), and seek to train models from the long-tailed training distribution p s (y) to perform well on the uniform test distribution. However, such an assumption does not always hold in practice. The actual test class distribution in real-world applications may also be long-tailed (i.e., p t (y) = p s (y)), or even inversely long-tailed to the training data (i.e., p t (y) = inv(p s (y))). Here, inv(?) indicates that the order of the long tail on classes is flipped. As a result, the models learned by existing methods may fail when the actual test class distribution is different from the assumed one. To address this, we propose to study a more practical yet challenging long-tailed problem, i.e., Test-agnostic Long-tailed Recognition. This task aims to learn a recognition model from long-tailed training data, where the resulting model would be evaluated on multiple test sets that follow different class distributions. This task is challenging due to the integration of two challenges: (1) the severe class imbalance in the training data makes it difficult to train models; <ref type="bibr" target="#b1">(2)</ref> unknown class distribution shifts between training and test data (i.e., p t (y) = p s (y)) makes models hard to generalize.  <ref type="table" target="#tab_0">(LT) methods on ImageNet-LT with various test class  distributions, including uniform, forward and backward LT distributions with imbalance ratios of 10</ref> and 50, respectively. The results show that each method strives to simulate a specific class distribution in terms of many-shot, medium-shot and few-shot classes, which does not change when the test class distribution varies. The corresponding visualization results are reported in <ref type="figure" target="#fig_3">Figure 5</ref> in Appendix D. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>To tackle the above problem, inspired by the idea of "divide and conquer", we propose to learn multiple skill-diverse experts that excel at handling different class distributions. By reasonably fusing these experts at test time, the multi-expert model would manage to handle unknown class distribution shifts and resolve test-agnostic long-tailed recognition. Following this idea, we develop a novel Self-supervised Aggregation of Diverse Experts (SADE) approach. Specifically, SADE consists of two innovative strategies: (1) learning skill-diverse experts from a single long-tailed training dataset;</p><p>(2) test-time aggregating experts with self-supervision to handle test-agnostic class distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Skill-diverse Expert Learning</head><p>As shown in <ref type="figure">Figure 2</ref>, SADE builds a three-expert model that comprises two components: (1) an expert-shared backbone f ? ; (2) independent expert networks E 1 , E 2 and E 3 . When training the model, the key challenge is how to learn skill-diverse experts from a single and stationary long-tailed training dataset. Existing ensemble-based long-tailed methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">49]</ref> seek to train experts for the uniform test class distribution, and hence the trained experts are not differentiated sufficiently for handling various class distributions (refer to <ref type="table" target="#tab_6">Table 6</ref> for an example). To tackle this challenge, we first empirically investigate existing long-tailed methods in this task. From <ref type="table" target="#tab_0">Table 1</ref>, we find that there is a simulation correlation between the learned class distribution and the training loss function. That is, the models learned by different losses are good at dealing with class distributions with different skewness. For instance, the model trained with the softmax loss is good at the long-tailed distribution, while the models obtained from long-tailed methods are skilled in the uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax Loss</head><p>Shared . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logits Experts</head><p>Balanced Softmax Loss . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverse Softmax Loss</head><p>. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>The scheme of SADE with three experts, where different experts are trained with different expertise-guided losses.</p><p>Motivated by this finding, we develop a simple skill-diverse expert learning strategy to generate experts with different distribution preferences. To be specific, the forward expert E 1 seeks to be good at the long-tailed class distribution and performs well on many-shot classes. The uniform expert E 2 strives to be skilled in the uniform distribution. The backward expert E 3 aims at the inversely long-tailed distribution and performs well on few-shot classes. Here, the forward and backward experts are necessary since they span a wide spectrum of possible class distributions, while the uniform expert ensures retaining high accuracy on the uniform distribution. To this end, we use three different expertise-guided losses to train the three experts, respectively.</p><p>The forward expert E 1 We use the softmax cross-entropy loss to train this expert, so that it directly simulates the original long-tailed training class distribution:</p><formula xml:id="formula_0">L ce = 1 n s xi?Ds ?y i log ?(v 1 (x i )),<label>(1)</label></formula><p>where v 1 (?) is the output logits of the forward expert E 1 , and ?(?) is the softmax function.</p><p>The uniform expert E 2 We aim to train this expert to simulate the uniform class distribution. Inspired by the effectiveness of logit adjusted losses for long-tailed recognition <ref type="bibr" target="#b32">[33]</ref>, we resort to the balanced softmax loss <ref type="bibr" target="#b20">[21]</ref>. Specifically, let? k = exp(v k ) C c=1 exp(v c ) be the prediction probability. The balanced softmax adjusts the prediction probability by compensating for the long-tailed class distribution with the prior of training label frequencies:</p><formula xml:id="formula_1">? k = ? k exp(v k ) C c=1 ? c exp(v c ) = exp(v k +log ? k ) C c=1 exp(v c +log ? c ) ,</formula><p>where ? k = n k n denotes the training label frequency of class k. Then, given v 2 (?) as the output logits of the expert E 2 , the balanced softmax loss for the expert E 2 is defined as:</p><formula xml:id="formula_2">L bal = 1 n s xi?Ds ?y i log ?(v 2 (x i ) + log ?).<label>(2)</label></formula><p>Intuitively, by adjusting logits to compensate for the long-tailed distribution with the prior ?, this loss enables E 2 to output class-balanced predictions that simulate the uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The backward expert E 3</head><p>We seek to train this expert to simulate the inversely long-tailed class distribution. To this end, we propose a new inverse softmax loss, based on the same rationale of logit adjusted losses <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref>. Specifically, we adjust the prediction probability by:</p><formula xml:id="formula_3">y k = exp(v k +log ? k ?log? k ) C c=1 exp(v c +log ? c ?log? c ) ,</formula><p>where the inverse training prior? is obtained by inverting the order of training label frequencies ?. Then, the new inverse softmax loss for the expert E 3 is defined as:</p><formula xml:id="formula_4">L inv = 1 n s xi?Ds ?y i log ?(v 3 (x i )+ log ??? log?),<label>(3)</label></formula><p>where v 3 (?) denotes the output logits of E 3 and ? is a hyper-parameter. Intuitively, this loss adjusts logits to compensate for the long-tailed distribution with ?, and further applies reverse adjustment with?. This enables E 3 to simulate the inversely long-tailed distribution (cf. <ref type="table" target="#tab_6">Table 6</ref> for verification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Test-time Self-supervised Aggregation</head><p>Based on the skill-diverse learning strategy, the three experts in SADE are skilled in different class distributions. The remaining challenge is how to fuse them to deal with unknown test class distributions. A basic principle for expert aggregation is that the experts should play a bigger role in situations where they have expertise. Nevertheless, how to detect strong experts for unknown test class distribution remains unknown. Our key insight is that strong experts should be more stable in predicting the samples from their skilled classes, even though these samples are perturbed.</p><p>Empirical observation To verify this hypothesis, we estimate the prediction stability of experts by comparing the cosine similarity between their predictions for a sample's two augmented views. Here, the data views are generated by the data augmentation techniques in MoCo v2 <ref type="bibr" target="#b4">[5]</ref>. From <ref type="table" target="#tab_2">Table 2</ref>, we find that there is a positive correlation between expertise and prediction stability, i.e., stronger experts have higher prediction similarity between different views of samples from their favorable classes. Following this finding, we propose to explore the relative prediction stability to detect strong experts and weight experts for the unknown test class distribution. Consequently, we develop a novel self-supervised strategy, namely prediction stability maximization.</p><p>Prediction stability maximization This strategy learns aggregation weights for experts (with frozen parameters) by maximizing model prediction stability for unlabeled test samples. As shown in <ref type="figure">Figure 3</ref>, the method comprises three major components as follows.</p><p>Data view generation For a given sample x, we conduct two stochastic data augmentations to generate the sample's two views, i.e., x 1 and x 2 . Here, we use the same augmentation techniques as the advanced contrastive learning method, i.e., MoCo v2 <ref type="bibr" target="#b4">[5]</ref>, which has been shown effective in self-supervised learning.</p><p>Learnable aggregation weight Given the output logits of three experts (v 1 , v 2 , v 3 ) ? R 3?C , we aggregate experts with a learnable aggregation weight w = [w 1 , w 2 , w 3 ] ? R 3 and obtain the final softmax prediction by? = ?(w 1 ?v 1 + w 2 ?v 2 + w 3 ?v 3 ), where w is normalized before aggregation, i.e., w 1 + w 2 + w 3 =1. </p><formula xml:id="formula_5">! ! " ~#( ?) (?) " $ " $ " " ! " $ " " ! ! ! $ ! " $ ! " $ ! -"</formula><p>Maximize prediction stability Logits ! -! <ref type="figure">Figure 3</ref>: The scheme of test-time self-supervised aggregation. Two data augmentations sampled from the same family of augmentations (t?T and t ?T ) are applied to obtain two data views.</p><p>Objective function Given the view predictions of unlabeled test data, we maximize the prediction stability based on the cosine similarity between the view predictions:</p><formula xml:id="formula_6">max w S, where S = 1 n t x?Dt? 1 ?? 2 .<label>(4)</label></formula><p>Here,? 1 and? 2 are normalized by the softmax function. In test-time training, only the aggregation weight w is updated. Since stronger experts have higher prediction similarity for their skilled classes, maximizing the prediction stability S would learn higher weights for stronger experts regarding the unknown test class distribution. Moreover, the self-supervised aggregation strategy can be conducted in an online manner for streaming test data. The pseudo-code of SADE is provided in Appendix B.</p><p>Theoretical Analysis We then theoretically analyze the prediction stability maximization strategy to conceptually understand why it works. To this end, we first define the random variables of predictions and labels as? ?p(?) and Y ?p t (y). We have the following result: Theorem 1. The prediction stability S is positive proportional to the mutual information between the predicted label distribution and the test class distribution I(? ; Y ), and negative proportional to the prediction entropy H(? ):</p><formula xml:id="formula_7">S ? I(? ; Y ) ? H(? ).</formula><p>Please refer to Appendix A for proofs. According to Theorem 1, maximizing the prediction stability S enables SADE to learn an aggregation weight that maximizes the mutual information between the predicted label distribution p(?) and the test class distribution p t (y), as well as minimizing the prediction entropy. Since minimizing entropy helps to improve the confidence of the classifier output <ref type="bibr" target="#b11">[12]</ref>, the aggregation weight is learned to simulate the test class distribution p t (y) and increase the prediction confidence. This property intuitively explains why our method has the potential to tackle the challenging task of test-agnostic long-tailed recognition at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first evaluate the superiority of SADE on both vanilla and test-agnostic long-tailed recognition. We then verify the effectiveness of SADE in terms of its two strategies, i.e., skill-diverse expert learning and test-time self-supervised aggregation. More ablation studies are reported in appendices. Here, we begin with the experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setups</head><p>Datasets We use four benchmark datasets (i.e., ImageNet-LT <ref type="bibr" target="#b30">[31]</ref>, CIFAR100-LT <ref type="bibr" target="#b2">[3]</ref>, Places-LT <ref type="bibr" target="#b30">[31]</ref>, and iNaturalist 2018 [44]) to simulate real-world long-tailed class distributions. Their data statistics and imbalance ratios are summarized in Appendix C.1. The imbalance ratio is defined as max n j /min n j , where n j denotes the data number of class j. Note that CIFAR100-LT has three variants with different imbalance ratios. Evaluation protocols In test-agnostic long-tailed recognition, following LADE <ref type="bibr" target="#b16">[17]</ref>, the models are evaluated on multiple sets of test data that follow different class distributions, in terms of micro accuracy. Same as LADE <ref type="bibr" target="#b16">[17]</ref>, we construct three kinds of test class distributions, i.e., the uniform distribution, forward long-tailed distributions as training data, and backward long-tailed distributions.</p><p>In the backward ones, the order of the long tail on classes is flipped. More details of test data construction are provided in Appendix C.2. Besides, we also evaluate methods on vanilla long-tailed recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>, where the models are evaluated on the uniform test class distribution. Here, the accuracy on three class sub-groups is also reported, i.e., many-shot classes (more than 100 training images), medium-shot classes (20?100 images) and few-shot classes (less than 20 images).</p><p>Implementation details We use the same setup for all the baselines and our method. Specifically, following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">49]</ref>, we use ResNeXt-50 for ImageNet-LT, ResNet-32 for CIFAR100-LT, ResNet-152 for Places-LT and ResNet-50 for iNaturalist 2018 as backbones, respectively. Moreover, we adopt the cosine classifier for prediction on all datasets. If not specified, we use the SGD optimizer with the momentum of 0.9 for training 200 epochs and set the initial learning rate as 0.1 with linear decay. We set ?=2 for ImageNet-LT and CIFAR100-LT, and ?=1 for the remaining datasets. During test-time training, we train the aggregation weights for 5 epochs with the batch size 128, where we use the same optimizer and learning rate as the training phase. More implementation details and the hyper-parameter statistics are reported in Appendix C.3. This subsection compares SADE with state-of-the-art long-tailed methods on vanilla long-tailed recognition. Specifically, as shown in <ref type="table" target="#tab_3">Tables 3-4</ref>, Softmax trains the model with only cross-entropy, so it simulates the longtailed training distribution and performs well on manyshot classes. However, it performs poorly on mediumshot and few-shot classes, leading to worse overall performance. In contrast, existing long-tailed methods (e.g., Decouple, Causal) seek to simulate the uniform class distribution, so their performance is more class-balanced, leading to better overall performance. However, as these methods mainly seek balanced performance, they inevitably sacrifice the performance on many-shot classes. To address this, RIDE and ACE explore ensemble learning for long-tailed recognition and achieve better performance on tail classes without sacrificing the head-class performance. In comparison, based on the increasing expert diversity derived from skill-diverse expert learning, our method performs the best on all datasets, e.g., with more than 2% accuracy gain on ImageNet-LT compared to RIDE and ACE. These results demonstrate the superiority of SADE over the compared methods that are particularly designed for the uniform test class distribution. Note that SADE also outperforms baselines in experiments with stronger data augmentation (i.e., RandAugment <ref type="bibr" target="#b6">[7]</ref>) and other architectures, as reported in Appendix D.1. <ref type="table">Table 5</ref>: Top-1 accuracy on long-tailed datasets with various unknown test class distributions. "Prior" indicates that the test class distribution is used as prior knowledge. "Uni." denotes the uniform distribution. "IR" indicates the imbalance ratio. "BS" denotes the balanced softmax <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Superiority on Vanilla Long-tailed Recognition</head><p>Method Prior <ref type="table" target="#tab_0">Backward-LT   50  25  10  5  2  1  2  5  10  25  50  50  25  10  5  2  1  2  5  10  25</ref>   </p><formula xml:id="formula_8">(a) ImageNet-LT (b) CIFAR100-LT (IR100) Forward-LT Uni. Backward-LT Forward-LT Uni.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Superiority on Test-agnostic Long-tailed Recognition</head><p>In this subsection, we evaluate SADE on test-agnostic long-tailed recognition. The results on various test class distributions are reported in <ref type="table">Table 5</ref>. Specifically, since Softmax seeks to simulate the longtailed training distribution, it performs well on forward long-tailed test distributions. However, its performance on the uniform and backward long-tailed distributions is poor. In contrast, existing longtailed methods show more balanced performance among classes, leading to better overall accuracy. However, the resulting models by these methods suffer from a simulation bias, i.e., performing similarly among classes on various class distributions (c.f. <ref type="table" target="#tab_0">Table 1</ref>). As a result, they cannot adapt to diverse test class distributions well. To handle this task, LADE assumes the test class distribution to be known and uses this information to adjust its predictions, leading to better performance on various test class distributions. However, since obtaining the actual test class distribution is difficult in real applications, the methods requiring such knowledge may be not applicable in practice. Moreover, in some specific cases like Forward-LT-3 and Backward-LT-3 distributions of iNaturalist 2018, the number of test samples on some classes becomes zero. In such cases, the test prior cannot be used in LADE, since adjusting logits with log 0 results in biased predictions. In contrast, without relying on the knowledge of test class distributions, our SADE presents an innovative self-supervised strategy to deal with unknown class distributions, and obtains even better performance than LADE that uses the test class prior (c.f. <ref type="table">Table 5</ref>). The promising results demonstrate the effectiveness and practicality of our method on test-agnostic long-tailed recognition. Note that the performance advantages of SADE become larger as the test data get more imbalanced. Due to the page limitation, the results on more datasets are reported in Appendix D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effectiveness of Skill-diverse Expert Learning</head><p>We next examine our skill-diverse expert learning strategy. The results are reported in <ref type="table" target="#tab_6">Table 6</ref>, where RIDE [49] is a state-of-the-art ensemble-based method. RIDE trains each expert with cross-entropy independently and uses KL-Divergence to improve expert diversity. However, simply maximizing the divergence of expert predictions cannot learn visibly diverse experts (cf. <ref type="table" target="#tab_6">Table 6</ref>). In contrast, the three experts learned by our strategy have significantly diverse expertise, excelling at many-shot classes, the uniform distribution (with higher overall performance), and few-shot classes, respectively. As a result, the increasing expert diversity leads to a non-trivial gain for the ensemble performance of SADE compared to RIDE. Moreover, consistent results on more datasets are reported in Appendix D.3, while the ablation studies of the expert learning strategy are provided in Appendix E.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effectiveness of Test-time Self-supervised Aggregation</head><p>This subsection evaluates our test-time self-supervised aggregation strategy.</p><p>Effectiveness in expert aggregation. As shown in  <ref type="table" target="#tab_9">Table 9</ref> show that directly applying existing test-time training methods cannot handle well the class distribution shifts, particularly on the inversely long-tailed class distribution. In comparison, our self-supervised strategy is able to aggregate multiple experts appropriately for the unknown test class distribution (cf. <ref type="table" target="#tab_7">Table 7)</ref>, leading to promising performance gains on various test class distributions (cf. <ref type="table" target="#tab_9">Table 9</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have explored a practical yet challenging task of test-agnostic long-tailed recognition, where the test class distribution is unknown and not necessarily uniform. To tackle this task, we present a novel approach, namely Self-supervised Aggregation of Diverse Experts (SADE), which consists of two innovative strategies, i.e., skill-diverse expert learning and test-time self-supervised aggregation. We theoretically analyze our proposed method and also empirically show that SADE achieves new state-of-the-art performance on both vanilla and test-agnostic long-tailed recognition.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>We organize the supplementary materials as follows:</p><p>? Appendix A: the proofs for Theorem 1.</p><p>? Appendix B: the pseudo-code of the proposed method.</p><p>? Appendix C: more details of experimental settings.</p><p>? Appendix D: more empirical results on vanilla long-tailed recognition, test-agnostic long-tailed recognition, skill-diverse expert learning, and test-time self-supervised aggregation.</p><p>? Appendix E: more ablation studies on expert learning and the proposed inverse softmax loss.</p><p>? Appendix F: more ablation studies on test-time self-supervised aggregation.</p><p>? Appendix G: more discussion on model complexity.</p><p>? Appendix H: discussion on potential limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs for Theorem 1</head><p>Proof. We first recall several key notations and define some new notations. The random variables of model predictions and ground-truth labels are defined as? ? p(?) and Y ? p(y), respectively. The number of classes is denoted by C. Moreover, we further denote the test sample set of the class k by Z k , in which the total number of samples in this class is denoted by</p><formula xml:id="formula_9">|Z k |. Let c k = 1 |Z k | ??Z k?</formula><p>represent the hard mean of all predictions of samples from the class k, and let c = indicate equality up to a multiplicative and/or additive constant.</p><p>As shown in Eq.(4), the optimization objective of our test-time self-supervised aggregation method is to maximize S = nt j=1? 1 j ?? 2 j , where n t denotes the number of test samples. For convenience, we simplify the first data view to be the original data, so the objective function becomes nt j=1? j ?? 1 j . Maximizing such an objective is equivalent to minimizing nt j=1 ?? j ?? 1 j . Here, we assume the data augmentations are strong enough to generate representative data views that can simulate the test data from the same class. In this sense, the new data view can be regarded as an independent sample from the same class. Following this, we analyze our method by connecting ?? j ?? 1 j to ?j ?Z k ? j ?c k 2 , which is similar to the tightness term in the center loss [50]:</p><formula xml:id="formula_10">?j ,? 1 j ?Z k ?? j ?? 1 j c = 1 |Z k | ?j ,? 1 j ?Z k ?? j ?? 1 j c = 1 |Z k | ?j ,? 1 j ?Z k ? j 2 ?? j ?? 1 j = ?j ?Z k ? j 2 ? 1 |Z k | ?j ?Z k ? 1 j ?Z k? j ?? 1 j = ?j ?Z k ? j 2 ? 2 1 |Z k | ?j ?Z k ? 1 j ?Z k? j ?? 1 j + 1 |Z k | ?j ?Z k ? 1 j ?Z k? j ?? 1 j = ?j ?Z k ? j 2 ? 2? j ?c k + c k 2 = ?j ?Z k ? j ?c k 2 ,</formula><p>where we use the property of the normalized predictions, i.e., ? j 2 = ? 1 j 2 = 1, and the definition of the class hard mean c k = 1</p><formula xml:id="formula_11">|Z k | ??Z k? .</formula><p>By summing over all classes k, we obtain:</p><formula xml:id="formula_12">nt j=1 ?? j ?? 1 j c = nt j=1 ? j ?c yi 2 .</formula><p>Based on this equation, following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">57]</ref>, we can interpret nt j=1 ?? j ?? 1 j as a conditional crossentropy between? and another random variable? , whose conditional distribution given Y is a standard Gaussian centered around c Y :? |Y ?N (c y , i):</p><formula xml:id="formula_13">nt j=1 ?? j ?? 1 j c = H(? ;? |Y ) = H(? |Y )+D KL (? ||? |Y ).</formula><p>Hence, we know that nt j=1 ?? j ?? 1 j is an upper bound on the conditional entropy of predictions? given labels Y :</p><formula xml:id="formula_14">nt j=1 ?? j ?? 1 j c ? H(? |Y ),</formula><p>where the symbol c ? represents "larger than" up to a multiplicative and/or an additive constant. Moreover, when? |Y ?N (c y , i), the bound is tight. As a result, minimizing</p><formula xml:id="formula_15">nt j=1 ?? j ?? 1 j is equivalent to minimizing H(? |Y ): nt j=1 ?? j ?? 1 j ? H(? |Y ).<label>(5)</label></formula><p>Meanwhile, the mutual information between predictions? and labels Y can be represented by:</p><formula xml:id="formula_16">I(? ; Y ) = H(? ) ? H(? |Y ).<label>(6)</label></formula><p>Combining Eqs.(5-6), we have:</p><formula xml:id="formula_17">nt j=1 ?? j ?? 1 j ? ?I(? ; Y ) + H(? ).</formula><p>Since S = nt j=1? j ?? 1 j , we obtain:</p><formula xml:id="formula_18">S ? I(? ; Y ) ? H(? ),</formula><p>which concludes the proof for Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Pseudo-code</head><p>This appendix provides the pseudo-code 1 of SADE, which consists of skill-diverse expert learning and test-time self-supervised aggregation. Here, the skill-diverse expert learning strategy is summarized in Algorithm 1. For simplicity, we depict the pseudo-code based on batch size 1, but we conduct batch gradient descent in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Skill-diverse Expert Learning</head><p>Require: Epochs T ; Hyper-parameters ? for Linv Initialize: Network backbone f ? ; Experts E1, E2, E3 1: for e=1,...,T do 2: for x ? Ds do // batch sampling in practice 3:</p><p>Obtain logits v1 based on f ? and E1; 4:</p><p>Obtain logits v2 based on f ? and E2; 5:</p><p>Obtain logits v3 based on f ? and E3; 6:</p><p>Compute loss Lce with v1 for Expert E1; // Eq.(1) 7:</p><p>Compute loss L bal with v2 for Expert E2; // Eq.(2) 8:</p><p>Compute loss Linv with v3 for Expert E3; // Eq.(3) 9:</p><p>Train the model with Lce + L bal + Linv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>end for 11: end for Output: The trained model {f ? , E1, E2, E3}</p><p>After training the multiple skill-diverse experts with Algorithm 1, the final prediction of the multiexpert model for vanilla long-tailed recognition is the arithmetic mean of the prediction logits of these experts, followed by a softmax function.</p><p>When it comes to test-agnostic long-tailed recognition, we need to aggregate these skill-diverse experts to handle the unknown test class distribution based on Algorithm 2. Here, to avoid the learned weights of some weak experts becoming zero, we give a stopping condition in Algorithm 2: if the weight for one expert is less than 0.05, we stop test-time training. Retaining a small amount of weight for each expert is sufficient to ensure the effect of ensemble learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Test-time Self-supervised Aggregation</head><p>Require: Epochs T ; The trained backbone f ? ; The trained experts E1, E2, E3 Initialize: Expert aggregation weights w // uniform initialization 1: for e=1,...,T do 2: for x ? Dt do // batch sampling in practice 3:</p><p>Draw two data augmentation functions t?T , t ?T ; 4:</p><p>Generate data views x 1 =t(x), x 2 =t (x); 5:</p><p>Obtain logits v 1 1 ,v 1 2 ,v 1 3 for the view x 1 ; 6:</p><p>Obtain logits v 2 1 ,v 2 2 ,v 2 3 for the view x 2 ; 7:</p><p>Normalize expert weights w via softmax function; 8:</p><p>Conduct predictions? 1 ,? 2 based on?=wv; 9:</p><p>Compute prediction stability S; // Eq. (4) 10:</p><p>Maximize S to update w; 11: end for 12:</p><p>If wi ? 0.05 for any wi ? w, then stop training. 13: end for Output: Expert aggregation weights w Note that, in test-agnostic long-tailed recognition, each model is only trained once on long-tailed training data and then directly evaluated on multiple test sets. Our test-time self-supervised strategy adapts the trained multi-expert model using only unlabeled test data during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Experimental Settings</head><p>In this appendix, we provide more details on experimental settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Benchmark Datasets</head><p>We use four benchmark datasets (i.e., ImageNet-LT <ref type="bibr" target="#b30">[31]</ref>, CIFAR100-LT <ref type="bibr" target="#b2">[3]</ref>, Places-LT <ref type="bibr" target="#b30">[31]</ref>, and iNaturalist 2018 [44]) to simulate real-world long-tailed class distributions. These datasets suffer from severe class imbalance <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">60]</ref>.Their data statistics are summarized in <ref type="table" target="#tab_0">Table 11</ref>, where CIFAR100-LT has three variants with different imbalance ratios. The imbalance ratio is defined as max n j /min n j , where n j denotes the data number of class j. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Construction of Test-agnostic Long-tailed Datasets</head><p>Following LADE <ref type="bibr" target="#b16">[17]</ref>, we construct three kinds of test class distributions, i.e., the uniform distribution, forward long-tailed distributions and backward long-tailed distributions. In the backward ones, the long-tailed class order is flipped. Here, the forward and backward long-tailed test distributions contain multiple different imbalance ratios, i.e., ? ? {2, 5, 10, 25, 50}. Note that LADE <ref type="bibr" target="#b16">[17]</ref> only constructed multiple distribution-agnostic test datasets for ImageNet-LT; while in this study, we use the same way to construct distribution-agnostic test datasets for the remaining benchmark datasets, i.e., CIFAR100-LT, Places-LT and iNaturalist 2018, as illustrated below.</p><p>Considering the long-tailed training classes are sorted in a decreasing order, the various test datasets are constructed as follows: (1) Forward long-tailed distribution: the number of the j-th class is n j = N ? ? (j?1)/C , where N indicates the sample number per class in the original uniform test dataset and C is the number of classes. (2) Backward long-tailed distribution: the number of the j-th class is n j = N ? ? (C?j)/C . In the backward long-tailed distributions, the order of the long tail on classes is flipped, so the distribution shift between training and test data is large, especially when the imbalance ratio gets higher.</p><p>For ImageNet-LT, CIFAR100-LT and Places-LT, since there are enough test samples per class, we follow the setting in LADE <ref type="bibr" target="#b16">[17]</ref> and construct the imbalance ratio set by ? ? {2, 5, 10, 25, 50}. For iNaturalist 2018, since each class only contains three test samples, we adjust the imbalance ratio set to ? ? {2, 3}. Note that when we set ? = 3, there are some classes in iNaturalist 2018 containing no test sample. All these constructed distribution-agnostic long-tailed datasets will be publicly available along with our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 More Implementation Details of Our Method</head><p>We implement our method in PyTorch. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">49]</ref>, we use ResNeXt-50 for ImageNet-LT, ResNet-32 for CIFAR100-LT, ResNet-152 for Places-LT and ResNet-50 for iNaturalist 2018 as backbones, respectively. Moreover, we adopt the cosine classifier for prediction on all datasets.</p><p>Although we have depicted the skill-diverse multi-expert framework in Section 4.1, we give more details about it here. Without loss of generality, we take ResNet <ref type="bibr" target="#b14">[15]</ref> as an example to illustrate the multi-expert model. Since the shallow layers extract more general features and deeper layers extract more task-specific features [54], the three-expert model uses the first two stages of ResNet as the expert-shared backbone, while the later stages of ResNet and the fully-connected layer constitute independent components of each expert. To be more specific, the number of convolutional filters in each expert is reduced by 1/4, since by sharing the backbone and using fewer filters in each expert <ref type="bibr">[49,</ref><ref type="bibr">63]</ref>, the computational complexity of the model is reduced compared to the model with independent experts. The final prediction is the arithmetic mean of the prediction logits of these experts, followed by a softmax function.</p><p>In the training phase, the data augmentations are the same as previous long-tailed studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>. If not specified, we use the SGD optimizer with the momentum of 0.9 and set the initial learning rate as 0.1 with linear decay. More specifically, for ImageNet-LT, we train models for 180 epochs with batch size 64 and a learning rate of 0.025 (cosine decay). For CIFAR100-LT, the training epoch is 200 and the batch size is 128. For Places-LT, following <ref type="bibr" target="#b30">[31]</ref>, we use ImageNet pre-trained ResNet-152 as the backbone, while the batch size is set to 128 and the training epoch is 30. Besides, the learning rate is 0.01 for the classifier and 0.001 for all other layers. For iNaturalist 2018, we set the training epoch to 200, the batch size to 512 and the learning rate to 0.2. In our inverse softmax loss, we set ?=2 for ImageNet-LT and CIFAR100-LT, and ?=1 for the remaining datasets.</p><p>In the test-time training, we use the same augmentations as MoCo v2 <ref type="bibr" target="#b4">[5]</ref> to generate different data views, i.e., random resized crop, color jitter, gray scale, Gaussian blur and horizontal flip. If not specified, we train the aggregation weights for 5 epochs with the batch size 128, where we adopt the same optimizer and learning rate as the training phase.</p><p>More detailed statistics of network architectures and hyper-parameters are reported in <ref type="table" target="#tab_0">Table 12</ref>. Based on these hyper-parameters, we conduct experiments on 1 TITAN RTX 2080 GPU for CIFAR100-LT, 4 GPUs for iNaturalist18, and 2 GPUs for ImageNet-LT and Places-LT, respectively. The source code of our method is available in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Discussions on Evaluation Metric</head><p>As mentioned in Section 5.1, we follow LADE <ref type="bibr" target="#b16">[17]</ref> and use micro accuracy to evaluate model performance on test-agnostic long-tailed recognition. In this appendix, we explain why micro accuracy is a better metric than macro accuracy when the test dataset exhibits a non-uniform class distribution. For instance, in the test scenario with a backward long-tailed class distribution, the tail classes are more frequently encountered than the head classes, and thus should have larger weights in evaluation. However, simply using macro accuracy treats all the categories equally and cannot differentiate classes of different frequencies.</p><p>For example, one may train a recognition model for autonomous cars based on the training data collected from city areas, where pedestrians are majority classes and stone obstacles are minority classes. Assume the model accuracy is 60% on pedestrians and 40% on stones. If deploying the model to city areas, where pedestrians/stones are assumed to have 500/50 test data, then the macro accuracy is 50% and the micro accuracy is 500?0.6+50?0.4 500+50 ?58%. In contrast, when deploying the model to mountain areas, the pedestrians become the minority, while stones become the majority. Assuming the test data numbers are changed to 50/500 on pedestrians/stones, the micro accuracy is adjusted to 50?0.6+500?0.4 50+500 ?42%, but the macro accuracy is still 50%. In this case, macro accuracy is less informative than micro accuracy for measuring model performance. Therefore, micro accuracy is a better metric to evaluate the performance of test-agnostic long-tailed recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More Empirical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 More Results on Vanilla Long-tailed Recognition</head><p>Accuracy on class subsets In the main paper, we have provided the average performance over all classes on the uniform test class distribution. In this appendix, we further report the accuracy regarding various class subsets (c.f. <ref type="table" target="#tab_0">Table 13</ref>), making the results more complete. Results on stonger data augmentations Inspired by PaCo <ref type="bibr" target="#b7">[8]</ref>, we further evaluate SADE training with stronger data augmentation (i.e., RandAugment <ref type="bibr" target="#b6">[7]</ref>) for 400 epochs. The results in <ref type="table" target="#tab_0">Table 14</ref> further demonstrate the state-of-the-art performance of SADE. Results on more neural architectures In addition to using the common practice of backbones as previous long-tailed studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">49]</ref>, we further evaluate SADE on more neural architectures. The results in <ref type="table" target="#tab_0">Table 15</ref> demonstrate that SADE is able to train different network backbones well. Results on more datasets We also conduct experiments on CIFAR10-LT with imbalance ratios of 10 and 100. Promising results in <ref type="table" target="#tab_0">Table 16</ref> further demonstrate the effectiveness and superiority of our proposed method. In the main paper, we have provided the overall performance on four benchmark datasets with various test class distributions (cf. <ref type="table">Table 5</ref>). In this appendix, we further verify the effectiveness of our method on more dataset settings (i.e., CIFAR100-IR10 and CIFAR100-IR50), as shown in <ref type="table" target="#tab_0">Table 17</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 More Results on Skill-diverse Expert Learning</head><p>This appendix further evaluates the skill-diverse expert learning strategy on CIFAR100-LT, Places-LT and iNaturalist 2018 datasets. We report the results in <ref type="table" target="#tab_0">Table 18</ref>, from which we draw the following observations. RIDE [49] is one of the state-of-the-art ensemble-based long-tailed methods, which tries to learn diverse distribution-aware experts by maximizing the divergence among expert predictions. However, such a method cannot learn sufficiently diverse experts. As shown in <ref type="table" target="#tab_0">Table 18</ref>, the three experts in RIDE perform very similarly on various groups of classes under all benchmark datasets, and each expert has similar overall performance on each dataset. Such results demonstrate that simply maximizing the KL divergence of different experts' predictions is not sufficient to learn visibly diverse distribution-aware experts.</p><p>In contrast, our proposed method learns the skill-diverse experts by directly training each expert with their customized expertise-guided objective functions, respectively. To be specific, the forward expert E 1 seeks to learn the long-tailed training distribution, so we directly train it with the crossentropy loss. For the uniform expert E 2 , we use the balanced softmax loss to simulate the uniform test distribution. For the backward expert E 3 , we design a novel inverse softmax loss to train the expert, so that it simulates the inversely long-tailed class distribution. <ref type="table" target="#tab_0">Table 18</ref> shows that the three experts trained by our method are visibly diverse and skilled in handling different class distributions. Specifically, the forward expert is skilled in many-shot classes, the uniform expert is more balanced with higher overall performance, and the backward expert is good at few-shot classes. Because of such a novel design that enhances expert diversity, our method achieves more promising ensemble performance compared to RIDE. This appendix provides more results to examine the effectiveness of our test-time self-supervised aggregation strategy. We report results in <ref type="table" target="#tab_0">Table 19</ref>, from which we draw several observations.</p><p>First of all, our method is able to learn suitable expert aggregation weights for test-agnostic class distributions, without relying on the true test class distribution. For the forward long-tailed test distribution, where the test data number of many-shot classes is more than that of medium-shot and few-shot classes, our method learns a higher weight for the forward expert E 1 who is skilled in many-shot classes, and learns relatively low weights for the expert E 2 and expert E 3 who are good at medium-shot and few-shot classes. Meanwhile, for the uniform test class distribution where all classes have the same number of test samples, our test-time expert aggregation strategy learns relatively balanced weights for the three experts. For example, on the uniform ImageNet-LT test data, the learned weights by our strategy are 0.33, 0.33 and 0.34 for the three experts, respectively. In addition, for the backward long-tailed test distributions, our method learns a higher weight for the backward expert E 3 and a relatively low weight for the forward expert E 1 . Note that when the class imbalance ratio becomes larger, our method is able to learn more diverse expert weights adaptively for fitting the actual test class distributions.</p><p>Such results not only demonstrate the effectiveness of our proposed strategy, but also verify the theoretical analysis that our method can simulate the unknown test class distribution. To our best knowledge, such an ability is quite promising, since it is difficult to know the true test class distributions in real-world application. Therefore, our method opens the opportunity for tackling unknown class distribution shifts at test time, and can serve as a better candidate to handle real-world long-tailed learning applications.  <ref type="table" target="#tab_2">Table 20</ref>. Note that the performance gain compared to existing methods gets larger as the test dataset gets more imbalanced. For example, on CIFAR100-LT with the imbalance ratio of 50, our test-time self-supervised strategy obtains a 7.7% performance gain on the Forward-LT-50 distribution and obtains a 9.2% performance gain on the Backward-LT-50 distribution, both of which are non-trivial. Such an observation is also supported by the visualization result of <ref type="figure" target="#fig_3">Figure 5</ref>, which plots the results of existing methods on ImageNet-LT with different test class distributions regarding the three class subsets.</p><p>In addition, since the imbalance degrees of the test datasets are relatively low on iNaturalist 2018, the simulated test class distributions are thus relatively balanced. As a result, the obtained performance improvement is not that significant, compared to other datasets. However, if there are more iNaturalist test samples following highly imbalanced test class distributions in real applications, our method would obtain more promising results.   In SADE, we consider three experts, where the "forward" and "backward" experts are necessary since they span a wide spectrum of possible test class distributions, while the "uniform" expert ensures that we retain high accuracy on the uniform test class distributions. Nevertheless, our approach can be straightforwardly extended to more than three experts. For the models with more experts, we adjust the hyper-parameter ? in Eq. (3) for the new experts and keep the hyper-parameters of the original three experts unchanged, so that different experts are skilled in different types of class distributions. Following this, we further evaluate the influence of the expert number on our method based on ImageNet. To be specific, when there are four experts, we set ? = 1 for the new expert; while when there are five experts, we set ? = 0.5 and ? = 1 for the two newly-added experts, respectively. As shown in <ref type="table" target="#tab_0">Table 21</ref>, with the increasing number of experts, the ensemble performance of our method is improved on vanilla long-tailed recognition, e.g., four experts obtain a 1.2% performance gain compared to three experts on ImageNet-LT. As a result, our method with more experts obtains consistent performance improvement in test-agnostic long-tailed recognition on various test class distributions compared to three experts, as shown in <ref type="table" target="#tab_2">Table 22</ref>. Even so, only three experts are sufficient to handle varied test class distributions, and provide a good trade-off between performance and efficiency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Hyper-parameters in Inverse Softmax Loss</head><p>This appendix evaluates the influence of the hyper-parameter ? in the inverse softmax loss for the backward expert, where we fix all other hyper-parameters and only adjust the value of ?. As shown in <ref type="table" target="#tab_2">Table 23</ref>, with the increase of ?, the backward expert simulates more inversely long-tailed distribution (to the training data), and thus the ensemble performance on few-shot classes is better. Moreover, when ? ? {2, 3}, our method achieves a better trade-off between head classes and tail classes, leading to relatively better overall performance on ImageNet-LT.  <ref type="table" target="#tab_2">Table 24</ref>, when the training epoch number is larger than 5, the learned expert weights by our method are converged on ImageNet-LT, which verifies that our method is robust enough. The corresponding performance on various test class distributions is reported in <ref type="table" target="#tab_2">Table 25</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Influences of Batch Size</head><p>In previous results, we set the batch size of test-time self-supervised aggregation to 128 on all datasets. In this appendix, we further evaluate the influence of the batch size on our strategy, where we adjust the batch size from 64 to 256. As shown in <ref type="table" target="#tab_2">Table 26</ref>, with different batch sizes, the learned expert weights by our method keep nearly the same, which shows that our method is insensitive to the batch size. The corresponding performance on various test class distributions is reported in <ref type="table" target="#tab_2">Table 27</ref>, where the performance is also nearly the same when using different batch sizes.   </p><formula xml:id="formula_19">E1 (w 1 ) E2 (w 2 ) E3 (w 3 ) E1 (w 1 ) E2 (w 2 ) E3 (w 3 ) E1 (w 1 ) E2 (w 2 ) E3 (w 3 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Influences of Learning Rate</head><p>In this appendix, we evaluate the influence of the learning rate on our self-supervised strategy, where we adjust the learning rate from 0.001 to 0.5. As shown in <ref type="table" target="#tab_2">Table 28</ref>, with the increase of the learning rate, the learned expert weights by our method are sharper and fit the unknown test class distributions better. For example, when the learning rate is 0.001, the weight for expert E 1 is 0.36 on the Forward-LT-50 test distribution, while when the learning rate increases to 0.5, the weight for expert E 1 becomes 0.57 on the Forward-LT-50 test distribution. Similar phenomenons are also observed on backward long-tailed test class distributions.</p><p>By observing the corresponding model performance on various test class distributions in <ref type="table" target="#tab_2">Table 29</ref>, we find that when the learning rate is too small (e.g., 0.001), our test-time self-supervised aggregation strategy is unable to converge, given a fixed training epoch number of 5. In contrast, given the same training epoch, our method can obtain better performance by reasonably increasing the learning rate.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Results of Prediction Confidence</head><p>In our theoretical analysis (i.e., Theorem 1), we find that our test-time self-supervised aggregation strategy not only simulates the test class distribution, but also makes the model predictions more confident. In this appendix, we evaluate whether our strategy can really improve the prediction confidence of models on various unknown test class distributions of ImageNet-LT. To this end, we compare the prediction confidence of our method without and with test-time self-supervised aggregation in terms of the hard mean of the highest prediction probability on all test samples.</p><p>As shown in <ref type="table" target="#tab_3">Table 30</ref>, our test-time self-supervised aggregation strategy enables the deep model to have higher prediction confidence. For example, on the Forward-LT-50 test distribution, our strategy obtains 0.015 confidence improvement, which is non-trivial since it is an average value for a large number of samples (more than 10,000 samples). In addition, when the class imbalance ratio becomes larger, our method is able to obtain more apparent confidence improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.5 Run-time Cost of Test-time Aggregation</head><p>One may be interested in the run-time cost of our test-time self-supervised aggregation strategy, so we further report its running time on Forward-LT-50 and Forward-LT-25 test class distributions for illustration. As shown in <ref type="table" target="#tab_0">Table 31</ref>, our test-time self-supervised aggregation strategy is fast in terms of per-epoch time. The actual average additional time is only 0.009 seconds per sample at test time on V100 GPUs. The result is easy to interpret since we freeze the model parameters and only learn the aggregation weights, which is much more efficient than training the whole model. More importantly, the goal of this paper is to handle a practical yet challenging test-agnostic long-tailed recognition task. For solving this challenging problem, we believe it is acceptable to allow models to be trained more, while the promising results in previous experiments have demonstrated the effectiveness of our proposed test-time self-supervised learning strategy in handling this problem. In the future, we will further extend the proposed method for better computational efficiency, e.g., exploring dynamic network routing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G More Discussions on Model Complexity</head><p>In this appendix, we discuss the model complexity of our method in terms of the number of parameters, multiply-accumulate operations (MACs) and top-1 accuracy on test-agnostic long-tailed recognition. As shown in <ref type="table" target="#tab_3">Table 33</ref>, both SADE and RIDE belong to ensemble-based long-tailed learning methods, so they have more parameters (about 1.5x) and MACs (about 1.4x) than the original backbone model, where we do not use the efficient expert assignment trick in [49] for both methods. Because of the ensemble effectiveness of the multi-expert scheme, both methods perform much better than non-ensemble methods (e.g., Softmax and other long-tailed methods). In addition, since our method and RIDE use the same multi-expert framework, both methods have the same number of parameters and MACs. Nevertheless, by using our proposed skill-diverse expert learning and test-time selfsupervised aggregation strategies, our method performs much better than RIDE with no increase in model parameters and computational costs.</p><p>One may concern the multi-expert scheme leads to more model parameters and higher computational costs than the original backbone. However, note that the main focus of this paper is to solve the challenging test-agnostic long-tailed recognition, while promising results have shown that our method addresses this problem well. In this sense, slightly increasing the model complexity is acceptable for solving this practical yet challenging problem. Moreover, since there have already been many studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">51]</ref> showing effectiveness in improving the efficiency of the multi-expert scheme, we think the computation increment is not a severe issue and we leave it to the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Potential Limitations</head><p>One concern is that this work only focuses on long-tailed classification problems. However, we believe this is enough for a new challenging task of test-agnostic long-tailed recognition, while how to extending to object detection and instance segmentation will be explored in the future. Another potential concern is the model complexity of our method. However, as discussed in Appendix G, the computation increment is not a very severe issue, while how to further accelerate our method will be explored in future. In addition, one may also expect to evaluate the proposed method on more test class distributions. However, as shown in Section 5.3, we have demonstrated the effectiveness of our method on the uniform class distribution, the forward and backward long-tailed class distributions with various imbalance ratios, and even partial class distributions. Therefore, we believe the empirical verification is sufficient for verifying our method, and the extension to more complex test class distributions is left to the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Checklist 1 . 5 .</head><label>15</label><figDesc>For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] Please refer to Appendix H. (c) Did you discuss any potential negative societal impacts of your work? [N/A] This is a fundamental research that does not have particular negative social impacts. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] Please refer to Appendix A. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Please refer to the supplemental material. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Please refer to Section 5.1 and Appendix C. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] The common practice in long-tailed recognition does not report error bars, so we follow the previous papers and do not report them. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Please refer to Appendix C.3 for details on different datasets. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [N/A] All the used benchmark datasets are publicly available. (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]We submitted the source codes of our method as an anonymized zip file.(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] These datasets are open-source benchmark datasets. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] These datasets are open-source benchmark datasets. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Performance visualizations on various unknown test class distributions, where "F" indicates the forward long-tailed distributions as training data, "B" indicates the backward long-tailed distributions to the training data, and "U" denotes the uniform distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Top-1 accuracy of existing long-tailed (LT) methods on ImageNet-LT with various test class distributions, including uniform, forward and backward long-tailed ones with imbalance ratios 10 and 50, respectively. Here, "F-LT-N " and "B-LT-N indicate the cases where test samples follow the same long-tailed distribution as training data and inversely long-tailed to the training data, with the imbalance ratio N , respectively. The results show that existing methods perform very similarly on various test class distributions in terms of their performance on many-shot, medium-shot and few-shot classes. In contrast, our proposed method is capable of adaptingbi to various test class distributions in terms of many-shot, medium-shot and few-shot performance, thus leading to better overall performance on each test class distribution.E Ablation Studies on Skill-diverse Expert Learning E.1 Discussion on Expert Number</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy of existing long-tailed</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Test class distribution</cell><cell></cell><cell>Softmax</cell><cell></cell><cell cols="3">Balanced Softmax [21]</cell><cell cols="3">LADE w/o prior [17]</cell></row><row><cell></cell><cell cols="3">Many Medium Few</cell><cell cols="3">Many Medium Few</cell><cell cols="3">Many Medium Few</cell></row><row><cell>Forward-LT-50</cell><cell>67.5</cell><cell>41.7</cell><cell>14.0</cell><cell>63.5</cell><cell>47.8</cell><cell>37.5</cell><cell>63.5</cell><cell>46.4</cell><cell>33.1</cell></row><row><cell>Forward-LT-10</cell><cell>68.2</cell><cell>40.9</cell><cell>14.0</cell><cell>64.1</cell><cell>48.2</cell><cell>31.2</cell><cell>64.7</cell><cell>47.1</cell><cell>32.2</cell></row><row><cell>Uniform</cell><cell>68.1</cell><cell>41.5</cell><cell>14.0</cell><cell>64.1</cell><cell>48.2</cell><cell>33.4</cell><cell>64.4</cell><cell>47.7</cell><cell>34.3</cell></row><row><cell>Backward-LT-10</cell><cell>67.4</cell><cell>41.9</cell><cell>13.9</cell><cell>63.4</cell><cell>49.1</cell><cell>33.6</cell><cell>64.4</cell><cell>48.2</cell><cell>34.2</cell></row><row><cell>Backward-LT-50</cell><cell>70.9</cell><cell>41.1</cell><cell>13.8</cell><cell>66.5</cell><cell>48.4</cell><cell>33.2</cell><cell>66.3</cell><cell>47.8</cell><cell>34.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Prediction stability of experts in terms of the cosine similarity between their predictions of a sample's two views. Note that expert E 1 is good at many-shot classes and expert E 3 is skilled in few-shot classes. The experts tend to have better prediction consistency for the samples from their skilled classes. Here, the imbalance ratio of CIFAR100-LT is 100.</figDesc><table><row><cell></cell><cell cols="4">Cosine similarity between view predictions</cell></row><row><cell>Model</cell><cell cols="2">ImageNet-LT</cell><cell cols="2">CIFAR100-LT</cell></row><row><cell></cell><cell cols="2">Many Med. Few</cell><cell cols="2">Many Med. Few</cell></row><row><cell>Expert E 1</cell><cell>0.60</cell><cell>0.48 0.43</cell><cell>0.28</cell><cell>0.22 0.20</cell></row><row><cell>Expert E 2</cell><cell>0.56</cell><cell>0.50 0.45</cell><cell>0.25</cell><cell>0.21 0.19</cell></row><row><cell>Expert E 3</cell><cell>0.52</cell><cell>0.53 0.58</cell><cell>0.22</cell><cell>0.23 0.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Top-1 accuracy on CIFAR100-LT, Places-LT and iNaturalist 2018, where the test class distribution is uniform. More results on three class sub-groups are reported in Appendix D.1.</figDesc><table><row><cell cols="3">(a) CIFAR100-LT</cell><cell></cell><cell cols="2">(b) Places-LT</cell><cell cols="2">(c) iNaturalist 2018</cell></row><row><cell>Imbalance Ratio</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>Method</cell><cell>Top-1 accuracy</cell><cell>Method</cell><cell>Top-1 accuracy</cell></row><row><cell cols="4">Softmax BBN [63] Causal [42] Balanced Softmax [21] 61.0 50.9 46.1 59.1 45.6 41.4 59.8 49.3 44.7 59.4 48.8 45.0 MiSLAS [62] 62.5 51.5 46.8 LADE [17] 61.6 50.1 45.6 RIDE [49] 61.8 51.7 48.0</cell><cell>Softmax Causal [42] Balanced Softmax [21] MiSLAS [62] LADE [17] RIDE [49]</cell><cell>31.4 32.2 39.4 38.3 39.2 40.3</cell><cell>Softmax Causal [42] Balanced Softmax [21] MiSLAS [62] LADE [17] RIDE [49]</cell><cell>64.7 64.4 70.6 70.7 69.3 71.8</cell></row><row><cell>SADE (ours)</cell><cell cols="3">63.6 53.9 49.8</cell><cell>SADE (ours)</cell><cell>40.9</cell><cell>SADE (ours)</cell><cell>72.9</cell></row><row><cell cols="8">Baselines We compare SADE with state-of-the-art long-tailed methods, including two-stage</cell></row><row><cell cols="8">methods (Decouple [25], MiSLAS [62]), logit-adjusted training (Balanced Softmax [21], LADE [17]),</cell></row><row><cell cols="8">ensemble learning (BBN [63], ACE [2], RIDE [49]), classifier design (Causal [42]), and representation</cell></row><row><cell cols="8">learning (PaCo [8]). Note that LADE uses the prior of test class distribution for post-adjustment</cell></row><row><cell cols="7">(although it is unavailable in practice), while all other methods do not use this prior.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Top-1 accuracy on ImageNet-LT.</figDesc><table><row><cell>Method</cell><cell cols="2">Many Med. Few All</cell></row><row><cell>Softmax</cell><cell>68.1</cell><cell>41.5 14.0 48.0</cell></row><row><cell>Decouple-LWS [25]</cell><cell>61.8</cell><cell>47.6 30.9 50.8</cell></row><row><cell>Causal [42]</cell><cell>64.1</cell><cell>45.8 27.2 50.3</cell></row><row><cell cols="2">Balanced Softmax [21] 64.1</cell><cell>48.2 33.4 52.3</cell></row><row><cell>MiSLAS [62]</cell><cell>62.0</cell><cell>49.1 32.8 51.4</cell></row><row><cell>LADE [17]</cell><cell>64.4</cell><cell>47.7 34.3 52.3</cell></row><row><cell>PaCo [8]</cell><cell>63.2</cell><cell>51.6 39.2 54.4</cell></row><row><cell>ACE [2]</cell><cell>71.7</cell><cell>54.6 23.5 56.6</cell></row><row><cell>RIDE [49]</cell><cell>68.0</cell><cell>52.9 35.1 56.3</cell></row><row><cell>SADE (ours)</cell><cell>66.5</cell><cell>57.0 43.5 58.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>50 Softmax 66.1 63.8 60.3 56.6 52.0 48.0 43.9 38.6 34.9 30.9 27.6 63.3 62.0 56.2 52.5 46.4 41.4 36.5 30.5 25.8 21.7 17.5 BS 63.2 61.9 59.5 57.2 54.4 52.3 50.0 47.0 45.0 42.3 40.8 57.8 55.5 54.2 52.0 48.7 46.1 43.6 40.8 38.4 36.3 33.7</figDesc><table><row><cell>MiSLAS</cell><cell cols="5">61.6 60.4 58.0 56.3 53.7</cell><cell>51.4</cell><cell cols="5">49.2 46.1 44.0 41.5 39.5</cell><cell cols="2">58.8 57.2 55.2 53.0 49.6</cell><cell>46.8</cell><cell cols="2">43.6 40.1 37.7 33.9 32.1</cell></row><row><cell>LADE</cell><cell cols="5">63.4 62.1 59.9 57.4 54.6</cell><cell>52.3</cell><cell cols="5">49.9 46.8 44.9 42.7 40.7</cell><cell cols="2">56.0 55.5 52.8 51.0 48.0</cell><cell>45.6</cell><cell cols="2">43.2 40.0 38.3 35.5 34.0</cell></row><row><cell>LADE</cell><cell cols="5">65.8 63.8 60.6 57.5 54.5</cell><cell>52.3</cell><cell cols="5">50.4 48.8 48.6 49.0 49.2</cell><cell cols="2">62.6 60.2 55.6 52.7 48.2</cell><cell>45.6</cell><cell cols="2">43.8 41.1 41.5 40.7 41.6</cell></row><row><cell>RIDE</cell><cell cols="5">67.6 66.3 64.0 61.7 58.9</cell><cell>56.3</cell><cell cols="5">54.0 51.0 48.7 46.2 44.0</cell><cell cols="2">63.0 59.9 57.0 53.6 49.4</cell><cell>48.0</cell><cell cols="2">42.5 38.1 35.4 31.6 29.2</cell></row><row><cell>SADE</cell><cell cols="5">69.4 67.4 65.4 63.0 60.6</cell><cell>58.8</cell><cell cols="5">57.1 55.5 54.5 53.7 53.1</cell><cell cols="2">65.9 62.5 58.3 54.8 51.1</cell><cell>49.8</cell><cell cols="2">46.2 44.7 43.9 42.5 42.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) Places-LT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(d) iNaturalist 2018</cell></row><row><cell>Method</cell><cell>Prior</cell><cell></cell><cell>Forward-LT</cell><cell></cell><cell></cell><cell>Uni.</cell><cell></cell><cell cols="3">Backward-LT</cell><cell></cell><cell cols="2">Forward-LT</cell><cell>Uni.</cell><cell cols="2">Backward-LT</cell></row><row><cell></cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>Softmax</cell><cell cols="5">45.6 42.7 40.2 38.0 34.1</cell><cell>31.4</cell><cell cols="5">28.4 25.4 23.4 20.8 19.4</cell><cell>65.4</cell><cell>65.5</cell><cell>64.7</cell><cell>64.0</cell><cell>63.4</cell></row><row><cell>BS</cell><cell cols="5">42.7 41.7 41.3 41.0 40.0</cell><cell>39.4</cell><cell cols="5">38.5 37.8 37.1 36.2 35.6</cell><cell>70.3</cell><cell>70.5</cell><cell>70.6</cell><cell>70.6</cell><cell>70.8</cell></row><row><cell>MiSLAS</cell><cell cols="5">40.9 39.7 39.5 39.6 38.8</cell><cell>38.3</cell><cell cols="5">37.3 36.7 35.8 34.7 34.4</cell><cell>70.8</cell><cell>70.8</cell><cell>70.7</cell><cell>70.7</cell><cell>70.2</cell></row><row><cell>LADE</cell><cell cols="5">42.8 41.5 41.2 40.8 39.8</cell><cell>39.2</cell><cell cols="5">38.1 37.6 36.9 36.0 35.7</cell><cell>68.4</cell><cell>69.0</cell><cell>69.3</cell><cell>69.6</cell><cell>69.5</cell></row><row><cell>LADE</cell><cell cols="5">46.3 44.2 42.2 41.2 39.7</cell><cell>39.4</cell><cell cols="5">39.2 39.9 40.9 42.4 43.0</cell><cell></cell><cell>69.1</cell><cell>69.3</cell><cell>70.2</cell></row><row><cell>RIDE</cell><cell cols="5">43.1 41.8 41.6 42.0 41.0</cell><cell>40.3</cell><cell cols="5">39.6 38.7 38.2 37.0 36.9</cell><cell>71.5</cell><cell>71.9</cell><cell>71.8</cell><cell>71.9</cell><cell>71.8</cell></row><row><cell>SADE</cell><cell cols="5">46.4 44.9 43.3 42.6 41.3</cell><cell>40.9</cell><cell cols="5">40.6 41.1 41.4 42.0 41.6</cell><cell>72.3</cell><cell>72.5</cell><cell>72.9</cell><cell>73.5</cell><cell>73.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance of each expert on the uniform test distribution, where the imbalance ratio of CIFAR100-LT is 100. The results show that our proposed method learns multiple experts with higher skill diversity, which leads to better ensemble performance.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">RIDE [49]</cell><cell></cell><cell></cell><cell cols="2">SADE (ours)</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>ImageNet-LT</cell><cell></cell><cell>CIFAR100-LT</cell><cell></cell><cell>ImageNet-LT</cell><cell></cell><cell cols="2">CIFAR100-LT</cell></row><row><cell></cell><cell cols="2">Many Med. Few All</cell><cell cols="2">Many Med. Few All</cell><cell cols="2">Many Med. Few All</cell><cell cols="3">Many Med. Few All</cell></row><row><cell>Expert E1</cell><cell>64.3</cell><cell>49.0 31.9 52.6</cell><cell>63.5</cell><cell>44.8 20.3 44.0</cell><cell>68.8</cell><cell>43.7 17.2 49.8</cell><cell>67.6</cell><cell>36.3</cell><cell>6.8 38.4</cell></row><row><cell>Expert E2</cell><cell>64.7</cell><cell>49.4 31.2 52.8</cell><cell>63.1</cell><cell>44.7 20.2 43.8</cell><cell>65.5</cell><cell>50.5 33.3 53.9</cell><cell>61.2</cell><cell cols="2">44.7 23.5 44.2</cell></row><row><cell>Expert E3</cell><cell>64.3</cell><cell>48.9 31.8 52.5</cell><cell>63.9</cell><cell>45.1 20.5 44.3</cell><cell>43.4</cell><cell>48.6 53.9 47.3</cell><cell>14.0</cell><cell cols="2">27.6 41.2 25.8</cell></row><row><cell>Ensemble</cell><cell>68.0</cell><cell>52.9 35.1 56.3</cell><cell>67.4</cell><cell>49.5 23.7 48.0</cell><cell>67.0</cell><cell>56.7 42.6 58.8</cell><cell>61.6</cell><cell cols="2">50.5 33.9 49.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The expert weights learned by our selfsupervised strategy on ImageNet-LT with various test class distributions. Our method learns suitable weights for various unknown distributions.</figDesc><table><row><cell>Test Dist.</cell><cell cols="3">Expert E1 (w1) Expert E2 (w2) Expert E3 (w3)</cell></row><row><cell>Forward-LT-50</cell><cell>0.52</cell><cell>0.35</cell><cell>0.13</cell></row><row><cell>Forward-LT-10</cell><cell>0.46</cell><cell>0.36</cell><cell>0.18</cell></row><row><cell>Uniform</cell><cell>0.33</cell><cell>0.33</cell><cell>0.34</cell></row><row><cell>Backward-LT-10</cell><cell>0.21</cell><cell>0.29</cell><cell>0.50</cell></row><row><cell>Backward-LT-50</cell><cell>0.17</cell><cell>0.27</cell><cell>0.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The performance improvement by our test-time self-supervised strategy on ImageNet-LT with various test class distributions.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ImageNet-LT</cell><cell></cell></row><row><cell>Test Dist.</cell><cell cols="2">Ours w/o test-time strategy</cell><cell cols="3">Ours w/ test-time strategy</cell></row><row><cell></cell><cell cols="2">Many Med. Few All</cell><cell cols="2">Many Med. Few</cell><cell>All</cell></row><row><cell>Forward-LT-50</cell><cell>65.6</cell><cell>55.7 44.1 65.5</cell><cell>70.0</cell><cell cols="2">53.2 33.1 69.4 (+3.9)</cell></row><row><cell>Forward-LT-10</cell><cell>66.5</cell><cell>56.8 44.2 63.6</cell><cell>69.9</cell><cell cols="2">54.3 34.7 65.4 (+1.8)</cell></row><row><cell>Uniform</cell><cell>67.0</cell><cell>56.7 42.6 58.8</cell><cell>66.5</cell><cell cols="2">57.0 43.5 58.8 (+0.0)</cell></row><row><cell cols="2">Backward-LT-10 65.0</cell><cell>57.6 43.1 53.1</cell><cell>60.9</cell><cell cols="2">57.5 50.1 54.5 (+1.4)</cell></row><row><cell cols="2">Backward-LT-50 69.1</cell><cell>57.0 42.9 49.8</cell><cell>60.7</cell><cell cols="2">56.2 50.7 53.1 (+3.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison among different test-time training strategies for handling class distribution shifts on ImageNet-LT with various unknown test class distributions.</figDesc><table><row><cell>Backbone</cell><cell>Test-time strategy</cell><cell></cell><cell></cell><cell>Forward</cell><cell></cell><cell></cell><cell>Uniform</cell><cell></cell><cell></cell><cell>Backward</cell></row><row><cell></cell><cell></cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell></cell><cell>No test-time adaptation</cell><cell cols="5">65.5 64.4 63.6 62.0 60.0</cell><cell>58.8</cell><cell cols="4">56.8 54.7 53.1 51.5 49.8</cell></row><row><cell></cell><cell>Test-time pseudo-labeling</cell><cell cols="5">67.1 66.1 64.7 63.0 60.1</cell><cell>57.7</cell><cell cols="4">54.7 51.1 48.1 45.0 42.4</cell></row><row><cell>SADE</cell><cell>Test class distribution estimation [29]</cell><cell cols="5">69.1 66.6 63.7 60.5 56.5</cell><cell>53.3</cell><cell cols="4">49.9 45.6 42.7 39.5 36.8</cell></row><row><cell></cell><cell>Entropy minimization with Tent [46]</cell><cell cols="5">68.0 67.0 65.6 62.8 60.5</cell><cell>58.6</cell><cell cols="4">56.0 53.2 50.6 48.1 45.7</cell></row><row><cell></cell><cell cols="6">Self-supervised expert aggregation (ours) 69.4 67.4 65.4 63.0 60.6</cell><cell>58.8</cell><cell cols="4">57.1 55.5 54.5 53.7 53.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc>, our self-supervised strategy learns suitable expert weights for various unknown test class distributions. For forward long-tailed distributions, the weight of the forward expert E 1 is higher; while for backward long-tailed ones, the weight of the backward expert E 3 is relatively high. This enables our multi-expert model to boost the performance on dominant classes for unknown test distributions, leading to better ensemble performance (cf.Table 8), particularly as test data get more skewed. The results on more datasets are reported in Appendix D.4, while more ablation studies of our strategy are shown in Appendix F.Superiority over test-time training methods. We then verify the superiority of our self-supervised strategy over existing test-time training approaches on various test class distributions. Specifically, we adopt three non-trivial baselines: (i) Test-time pseudo-labeling uses the multi-expert model to iteratively generate pseudo labels for unlabeled test data and uses them to fine-tune the model; (ii) Test class distribution estimation leverages BBSE<ref type="bibr" target="#b28">[29]</ref> to estimate the test class distribution and uses it to pose-adjust model predictions; (iii) Tent [46] fine-tunes the batch normalization layers of models through entropy minimization on unlabeled test data. The results in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>The effectiveness of our self-supervised aggregation strategy in dealing with (unknown) partial test class distributions on ImageNet-LT.Effectiveness on partial class distributions.Real-world test data may follow any type of class distribution, including partial class distributions (i.e., not all of the classes appear in the test data). Motivated by this, we further evaluate SADE on three partial class distributions: only many-shot classes, only medium-shot classes, and only few-shot classes. The results in Table 10 demonstrate the effectiveness of SADE in tackling more complex test class distributions.</figDesc><table><row><cell>Method</cell><cell></cell><cell>ImageNet-LT</cell><cell></cell></row><row><cell></cell><cell cols="3">Only many Only medium Only few</cell></row><row><cell>SADE w/o test-time strategy</cell><cell>67.4</cell><cell>56.9</cell><cell>42.5</cell></row><row><cell>SADE w/ test-time strategy</cell><cell>71.0</cell><cell>57.2</cell><cell>53.6</cell></row><row><cell>Accuracy gain</cell><cell>(+3.6)</cell><cell>(+0.3)</cell><cell>(+11.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>[40] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International Conference on Machine Learning, 2020. [41] Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli Ouyang, Changqing Yin, and Junjie Yan. Equalization loss for long-tailed object recognition. In Computer Vision and Pattern Recognition, pages 11662-11671, 2020. [42] Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-tailed classification by keeping the good and removing the bad momentum causal effect. In Advances in Neural Information Processing Systems, volume 33, 2020. [43] Junjiao Tian, Yen-Cheng Liu, et al. Posterior re-calibration for imbalanced datasets. In Advances in Neural Information Processing Systems, 2020. [44] Grant Van Horn, Oisinand Mac Aodha, et al. The inaturalist species classification and detection dataset. In Computer Vision and Pattern Recognition, 2018. [45] Thomas Varsavsky, Mauricio Orbes-Arteaga, et al. Test-time unsupervised domain adaptation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 428-436, 2020. Kaipeng Zhang, et al. A discriminative feature learning approach for deep face recognition. In European Conference on Computer Vision, 2016. [51] Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient ensemble and lifelong learning. In International Conference on Learning Representations, 2020. [52] Zhenzhen Weng, Mehmet Giray Ogut, Shai Limonchik, and Serena Yeung. Unsupervised discovery of the long-tail in instance segmentation using hierarchical self-supervision. In Computer Vision and Pattern Recognition, 2021. [53] Liuyu Xiang, Guiguang Ding, and Jungong Han. Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification. In European Conference on Computer Vision, 2020. [54] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, volume 27, pages 3320-3328, 2014. [55] Yuhang Zang, Chen Huang, and Chen Change Loy. Fasa: Feature augmentation and sampling adaptation for long-tailed instance segmentation. In International Conference on Computer Vision, 2021. [56] Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, and Jian Sun. Distribution alignment: A unified framework for long-tail visual recognition. In Computer Vision and Pattern Recognition, pages 2361-2370, 2021. 59] Yifan Zhang, Ying Wei, et al. Collaborative unsupervised domain adaptation for medical image diagnosis. IEEE Transactions on Image Processing, 2020. [60] Yifan Zhang, Peilin Zhao, Jiezhang Cao, Wenye Ma, Junzhou Huang, Qingyao Wu, and Mingkui Tan. Online adaptive asymmetric active learning for budgeted imbalanced data. In ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pages 2768-2777, 2018. [61] Peilin Zhao, Yifan Zhang, Min Wu, Steven CH Hoi, Mingkui Tan, and Junzhou Huang. Adaptive cost-sensitive online classification. IEEE Transactions on Knowledge and Data Engineering, 31(2):214-228, 2018. [62] Zhisheng Zhong, Jiequan Cui, Shu Liu, and Jiaya Jia. Improving calibration for long-tailed recognition. In Computer Vision and Pattern Recognition, 2021. [63] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. In Computer Vision and Pattern Recognition, pages 9719-9728, 2020.</figDesc><table /><note>[46] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021.[47] Jiaqi Wang, Wenwei Zhang, Yuhang Zang, Yuhang Cao, Jiangmiao Pang, Tao Gong, Kai Chen, Ziwei Liu, Chen Change Loy, and Dahua Lin. Seesaw loss for long-tailed instance segmentation. In Computer Vision and Pattern Recognition, pages 9695-9704, 2021.[48] Peng Wang, Kai Han, et al. Contrastive learning based hybrid networks for long-tailed image classification. In Computer Vision and Pattern Recognition, 2021.[49] Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella X Yu. Long-tailed recogni- tion by routing diverse distribution-aware experts. In International Conference on Learning Representations, 2021.[50] Yandong Wen,[57] Yifan Zhang, Bryan Hooi, Lanqing Hong, and Jiashi Feng. Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning. In Advances in Neural Information Processing Systems, 2021.[58] Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and Jiashi Feng. Deep long-tailed learning: A survey. arXiv preprint arXiv:2110.04596, 2021.[</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># classes # training data # test data imbalance ratio</cell></row><row><cell>ImageNet-LT [31]</cell><cell>1,000</cell><cell>115,846</cell><cell>50,000</cell><cell>256</cell></row><row><cell>CIFAR100-LT [3]</cell><cell>100</cell><cell>50,000</cell><cell>10,000</cell><cell>{10,50,100}</cell></row><row><cell>Places-LT [31]</cell><cell>365</cell><cell>62,500</cell><cell>36,500</cell><cell>996</cell></row><row><cell>iNaturalist 2018 [44]</cell><cell>8,142</cell><cell>437,513</cell><cell>24,426</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Statistics of the used network architectures and hyper-parameters in our study.</figDesc><table><row><cell>Items</cell><cell cols="2">ImageNet-LT CIFAR100LT</cell><cell>Places-LT</cell><cell>iNarutalist 2018</cell></row><row><cell></cell><cell cols="2">Network Architectures</cell><cell></cell><cell></cell></row><row><cell>network backbone</cell><cell>ResNeXt-50</cell><cell>ResNet-32</cell><cell>ResNet-152</cell><cell>ResNet-50</cell></row><row><cell>classifier</cell><cell></cell><cell cols="2">cosine classifier</cell><cell></cell></row><row><cell></cell><cell cols="2">Training Phase</cell><cell></cell><cell></cell></row><row><cell>epochs</cell><cell>180</cell><cell>200</cell><cell>30</cell><cell>200</cell></row><row><cell>batch size</cell><cell>64</cell><cell>128</cell><cell>128</cell><cell>512</cell></row><row><cell>learning rate (lr)</cell><cell>0.025</cell><cell>0.1</cell><cell>0.01</cell><cell>0.2</cell></row><row><cell>lr schedule</cell><cell>cosine decay</cell><cell></cell><cell>linear decay</cell><cell></cell></row><row><cell>? in inverse softmax loss</cell><cell>2</cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>weight decay factor</cell><cell>5 ? 10 ?4</cell><cell>5 ? 10 ?4</cell><cell>4 ? 10 ?4</cell><cell>2 ? 10 ?4</cell></row><row><cell>momentum factor</cell><cell></cell><cell cols="2">0.9</cell><cell></cell></row><row><cell>optimizer</cell><cell></cell><cell cols="2">SGD optimizer with nesterov</cell><cell></cell></row><row><cell></cell><cell cols="2">Test-time Training</cell><cell></cell><cell></cell></row><row><cell>epochs</cell><cell></cell><cell>5</cell><cell></cell><cell></cell></row><row><cell>batch size</cell><cell></cell><cell cols="2">128</cell><cell></cell></row><row><cell>learning rate (lr)</cell><cell>0.025</cell><cell>0.1</cell><cell>0.01</cell><cell>0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Top-1 accuracy of long-tailed recognition methods on the uniform test distribution.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">ImageNet-LT</cell><cell cols="4">CIFAR100-LT(IR10)</cell><cell cols="2">CIFAR100-LT(IR50)</cell></row><row><cell></cell><cell cols="3">Many Med. Few All</cell><cell cols="4">Many Med. Few All</cell><cell cols="2">Many Med. Few All</cell></row><row><cell>Softmax</cell><cell>68.1</cell><cell cols="2">41.5 14.0 48.0</cell><cell>66.0</cell><cell>42.7</cell><cell>-</cell><cell>59.1</cell><cell>66.8</cell><cell>37.4 15.5 45.6</cell></row><row><cell>Causal [42]</cell><cell>64.1</cell><cell cols="2">45.8 27.2 50.3</cell><cell>63.3</cell><cell>49.9</cell><cell>-</cell><cell>59.4</cell><cell>62.9</cell><cell>44.9 26.2 48.8</cell></row><row><cell cols="2">Balanced Softmax [21] 64.1</cell><cell cols="2">48.2 33.4 52.3</cell><cell>63.4</cell><cell>55.7</cell><cell>-</cell><cell>61.0</cell><cell>62.1</cell><cell>45.6 36.7 50.9</cell></row><row><cell>MiSLAS [62]</cell><cell>62.0</cell><cell cols="2">49.1 32.8 51.4</cell><cell>64.9</cell><cell>56.6</cell><cell>-</cell><cell>62.5</cell><cell>61.8</cell><cell>48.9 33.9 51.5</cell></row><row><cell>LADE [17]</cell><cell>64.4</cell><cell cols="2">47.7 34.3 52.3</cell><cell>63.8</cell><cell>56.0</cell><cell>-</cell><cell>61.6</cell><cell>60.2</cell><cell>46.2 35.6 50.1</cell></row><row><cell>RIDE [49]</cell><cell>68.0</cell><cell cols="2">52.9 35.1 56.3</cell><cell>65.7</cell><cell>53.3</cell><cell>-</cell><cell>61.8</cell><cell>66.6</cell><cell>46.2 30.3 51.7</cell></row><row><cell>SADE (ours)</cell><cell>66.5</cell><cell cols="2">57.0 43.5 58.8</cell><cell>65.8</cell><cell>58.8</cell><cell>-</cell><cell>63.6</cell><cell>61.5</cell><cell>50.2 45.0 53.9</cell></row><row><cell>Method</cell><cell cols="3">CIFAR100-LT(IR100)</cell><cell></cell><cell cols="2">Places-LT</cell><cell></cell><cell></cell><cell>iNaturalist 2018</cell></row><row><cell></cell><cell cols="3">Many Med. Few All</cell><cell cols="4">Many Med. Few All</cell><cell cols="2">Many Med. Few All</cell></row><row><cell>Softmax</cell><cell>68.6</cell><cell>41.1</cell><cell>9.6 41.4</cell><cell>46.2</cell><cell cols="3">27.5 12.7 31.4</cell><cell>74.7</cell><cell>66.3 60.0 64.7</cell></row><row><cell>Causal [42]</cell><cell>64.1</cell><cell cols="2">46.8 19.9 45.0</cell><cell>23.8</cell><cell cols="3">35.7 39.8 32.2</cell><cell>71.0</cell><cell>66.7 59.7 64.4</cell></row><row><cell cols="2">Balanced Softmax [21] 59.5</cell><cell cols="2">45.4 30.7 46.1</cell><cell>42.6</cell><cell cols="3">39.8 32.7 39.4</cell><cell>70.9</cell><cell>70.7 70.4 70.6</cell></row><row><cell>MiSLAS [62]</cell><cell>60.4</cell><cell cols="2">49.6 26.6 46.8</cell><cell>41.6</cell><cell cols="3">39.3 27.5 37.6</cell><cell>71.7</cell><cell>71.5 69.7 70.7</cell></row><row><cell>LADE [17]</cell><cell>58.7</cell><cell cols="2">45.8 29.8 45.6</cell><cell>42.6</cell><cell cols="3">39.4 32.3 39.2</cell><cell>68.9</cell><cell>68.7 70.2 69.3</cell></row><row><cell>RIDE [49]</cell><cell>67.4</cell><cell cols="2">49.5 23.7 48.0</cell><cell>43.1</cell><cell cols="3">41.0 33.0 40.3</cell><cell>71.5</cell><cell>70.0 71.6 71.8</cell></row><row><cell>SADE (ours)</cell><cell>65.4</cell><cell cols="2">49.3 29.3 49.8</cell><cell>40.4</cell><cell cols="3">43.2 36.8 40.9</cell><cell>74.5</cell><cell>72.5 73.0 72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Accuracy of long-tailed methods with stronger augmentations, where the test class distribution is uniform. Here, * denotes training with RandAugment<ref type="bibr" target="#b6">[7]</ref> for 400 epochs. The baseline results are directly copied from the work<ref type="bibr" target="#b7">[8]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="6">ImageNet-LT CIFAR100-LT(IR10) CIFAR100-LT(IR50) CIFAR100-LT(IR100) Places-LT iNaturalist 2018</cell></row><row><cell>PaCo  *  [8]</cell><cell>58.2</cell><cell>64.2</cell><cell>56.0</cell><cell>52.0</cell><cell>41.2</cell><cell>73.2</cell></row><row><cell>SADE  *  (ours)</cell><cell>61.2</cell><cell>65.3</cell><cell>57.3</cell><cell>52.2</cell><cell>41.3</cell><cell>74.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>Accuracy of SADE with various network architectures. Here, * denotes training with RandAugment [7] for 400 epochs.</figDesc><table><row><cell></cell><cell cols="2">ImageNet-LT</cell><cell></cell><cell></cell><cell cols="2">iNaturalist 2018</cell><cell></cell></row><row><cell>Backbone</cell><cell cols="3">Methods Many Med. Few All</cell><cell>Backbone</cell><cell cols="3">Methods Many Med. Few All</cell></row><row><cell>ResNeXt-50</cell><cell>SADE SADE  *</cell><cell>66.5 67.3</cell><cell>57.0 43.5 58.8 60.4 46.4 61.2</cell><cell>ResNet-50</cell><cell>SADE SADE  *</cell><cell>74.5 75.5</cell><cell>72.5 73.0 72.9 73.7 75.1 74.5</cell></row><row><cell>ResNeXt-101</cell><cell>SADE SADE  *</cell><cell>66.8 68.1</cell><cell>57.5 43.1 59.1 60.5 45.5 61.4</cell><cell>ResNet-152</cell><cell>SADE SADE  *</cell><cell>76.2 78.3</cell><cell>64.3 65.1 74.8 77.0 76.7 77.0</cell></row><row><cell>ResNeXt-152</cell><cell>SADE SADE  *</cell><cell>67.2 68.6</cell><cell>57.4 43.5 59.3 61.2 47.0 62.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Accuracy on CIFAR10-LT, where the test class distribution is uniform. Most results are directly copied from the work [62]. More Results on Test-agnostic Long-tailed Recognition</figDesc><table><row><cell cols="6">Imbalance Ratio Softmax BBN MiSLAS RIDE SADE (ours)</cell></row><row><cell>10</cell><cell>86.4</cell><cell>88.4</cell><cell>90.0</cell><cell>89.7</cell><cell>90.8</cell></row><row><cell>100</cell><cell>70.4</cell><cell>79.9</cell><cell>82.1</cell><cell>81.6</cell><cell>83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 :</head><label>17</label><figDesc>Top-1 accuracy over all classes on various unknown test class distributions. "Prior" indicates that the test class distribution is used as prior knowledge. "Uni." denotes the uniform distribution. "IR" indicates the imbalance ratio. "BS" denotes the balanced softmax<ref type="bibr" target="#b20">[21]</ref>.Furthermore, we plot the results of all methods under these benchmark datasets with various test class distributions inFigure 4. To be specific, Softmax only performs well on highly-imbalanced forward long-tailed class distributions. Existing long-tailed baselines outperform Softmax, but they cannot handle backward test class distributions well. In contrast, our method consistently outperforms baselines on all benchmark datasets, particularly on the backward long-tailed test distributions with a relatively large imbalance ratio.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) ImageNet-LT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) CIFAR100-LT (IR10)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Prior</cell><cell></cell><cell>Forward-LT</cell><cell></cell><cell></cell><cell>Uni.</cell><cell></cell><cell cols="3">Backward-LT</cell><cell></cell><cell></cell><cell cols="2">Forward-LT</cell><cell></cell><cell></cell><cell>Uni.</cell><cell></cell><cell cols="3">Backward-LT</cell></row><row><cell></cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell>Softmax</cell><cell cols="5">66.1 63.8 60.3 56.6 52.0</cell><cell>48.0</cell><cell cols="5">43.9 38.6 34.9 30.9 27.6</cell><cell cols="5">72.0 69.6 66.4 65.0 61.2</cell><cell>59.1</cell><cell cols="5">56.3 53.5 50.5 48.7 46.5</cell></row><row><cell>BS</cell><cell cols="5">63.2 61.9 59.5 57.2 54.4</cell><cell>52.3</cell><cell cols="5">50.0 47.0 45.0 42.3 40.8</cell><cell cols="5">65.9 64.9 64.1 63.4 61.8</cell><cell>61.0</cell><cell cols="5">60.0 58.2 57.5 56.2 55.1</cell></row><row><cell>MiSLAS</cell><cell cols="5">61.6 60.4 58.0 56.3 53.7</cell><cell>51.4</cell><cell cols="5">49.2 46.1 44.0 41.5 39.5</cell><cell cols="5">67.0 66.1 65.5 64.4 63.2</cell><cell>62.5</cell><cell cols="5">61.2 60.4 59.3 58.5 57.7</cell></row><row><cell>LADE</cell><cell cols="5">63.4 62.1 59.9 57.4 54.6</cell><cell>52.3</cell><cell cols="5">49.9 46.8 44.9 42.7 40.7</cell><cell cols="5">67.5 65.8 65.8 64.4 62.7</cell><cell>61.6</cell><cell cols="5">60.5 58.8 58.3 57.4 57.7</cell></row><row><cell>LADE</cell><cell cols="5">65.8 63.8 60.6 57.5 54.5</cell><cell>52.3</cell><cell cols="5">50.4 48.8 48.6 49.0 49.2</cell><cell cols="5">71.2 69.3 67.1 64.6 62.4</cell><cell>61.6</cell><cell cols="5">60.4 61.4 61.5 62.7 64.8</cell></row><row><cell>RIDE</cell><cell cols="5">67.6 66.3 64.0 61.7 58.9</cell><cell>56.3</cell><cell cols="5">54.0 51.0 48.7 46.2 44.0</cell><cell cols="5">67.1 65.3 63.6 62.1 60.9</cell><cell>61.8</cell><cell cols="5">58.4 56.8 55.3 54.9 53.4</cell></row><row><cell>SADE</cell><cell cols="5">69.4 67.4 65.4 63.0 60.6</cell><cell>58.8</cell><cell cols="5">57.1 55.5 54.5 53.7 53.1</cell><cell cols="5">71.2 69.4 67.6 66.3 64.4</cell><cell>63.6</cell><cell cols="5">62.9 62.4 61.7 62.1 63.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(c) CIFAR100-LT (IR50)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(d) CIFAR100-LT (IR100)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Prior</cell><cell></cell><cell>Forward-LT</cell><cell></cell><cell></cell><cell>Uni.</cell><cell></cell><cell cols="3">Backward-LT</cell><cell></cell><cell></cell><cell cols="2">Forward-LT</cell><cell></cell><cell></cell><cell>Uni.</cell><cell></cell><cell cols="3">Backward-LT</cell></row><row><cell></cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell>Softmax</cell><cell cols="5">64.8 62.7 58.5 55.0 49.9</cell><cell>45.6</cell><cell cols="5">40.9 36.2 32.1 26.6 24.6</cell><cell cols="5">63.3 62.0 56.2 52.5 46.4</cell><cell>41.4</cell><cell cols="5">36.5 30.5 25.8 21.7 17.5</cell></row><row><cell>BS</cell><cell cols="5">61.6 60.2 58.4 55.9 53.7</cell><cell>50.9</cell><cell cols="5">48.5 45.7 43.9 42.5 40.6</cell><cell cols="5">57.8 55.5 54.2 52.0 48.7</cell><cell>46.1</cell><cell cols="5">43.6 40.8 38.4 36.3 33.7</cell></row><row><cell>MiSLAS</cell><cell cols="5">60.1 58.9 57.7 56.2 53.7</cell><cell>51.5</cell><cell cols="5">48.7 46.5 44.3 41.8 40.2</cell><cell cols="5">58.8 57.2 55.2 53.0 49.6</cell><cell>46.8</cell><cell cols="5">43.6 40.1 37.7 33.9 32.1</cell></row><row><cell>LADE</cell><cell cols="5">61.3 60.2 56.9 54.3 52.3</cell><cell>50.1</cell><cell cols="5">47.8 45.7 44.0 41.8 40.5</cell><cell cols="5">56.0 55.5 52.8 51.0 48.0</cell><cell>45.6</cell><cell cols="5">43.2 40.0 38.3 35.5 34.0</cell></row><row><cell>LADE</cell><cell cols="5">65.9 62.1 58.8 56.0 52.3</cell><cell>50.1</cell><cell cols="5">48.3 45.5 46.5 46.8 47.8</cell><cell cols="5">62.6 60.2 55.6 52.7 48.2</cell><cell>45.6</cell><cell cols="5">43.8 41.1 41.5 40.7 41.6</cell></row><row><cell>RIDE</cell><cell cols="5">62.2 61.0 58.8 56.4 52.9</cell><cell>51.7</cell><cell cols="5">47.1 44.0 41.4 38.7 37.1</cell><cell cols="5">63.0 59.9 57.0 53.6 49.4</cell><cell>48.0</cell><cell cols="5">42.5 38.1 35.4 31.6 29.2</cell></row><row><cell>SADE</cell><cell cols="5">67.2 64.5 61.2 58.6 55.4</cell><cell>53.9</cell><cell cols="5">51.9 50.9 51.0 51.7 52.8</cell><cell cols="5">65.9 62.5 58.3 54.8 51.1</cell><cell>49.8</cell><cell cols="5">46.2 44.7 43.9 42.5 42.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(e) Places-LT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(f) iNaturalist 2018</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Prior</cell><cell></cell><cell>Forward-LT</cell><cell></cell><cell></cell><cell>Uni.</cell><cell></cell><cell cols="3">Backward-LT</cell><cell></cell><cell></cell><cell cols="2">Forward-LT</cell><cell></cell><cell></cell><cell>Uni.</cell><cell></cell><cell cols="3">Backward-LT</cell></row><row><cell></cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell><cell></cell><cell>3</cell><cell></cell><cell>2</cell><cell></cell><cell>1</cell><cell></cell><cell>2</cell><cell></cell><cell>3</cell></row><row><cell>Softmax</cell><cell cols="5">45.6 42.7 40.2 38.0 34.1</cell><cell>31.4</cell><cell cols="5">28.4 25.4 23.4 20.8 19.4</cell><cell></cell><cell>65.4</cell><cell cols="2">65.5</cell><cell></cell><cell>64.7</cell><cell></cell><cell>64.0</cell><cell></cell><cell>63.4</cell></row><row><cell>BS</cell><cell cols="5">42.7 41.7 41.3 41.0 40.0</cell><cell>39.4</cell><cell cols="5">38.5 37.8 37.1 36.2 35.6</cell><cell></cell><cell>70.3</cell><cell cols="2">70.5</cell><cell></cell><cell>70.6</cell><cell></cell><cell>70.6</cell><cell></cell><cell>70.8</cell></row><row><cell>MiSLAS</cell><cell cols="5">40.9 39.7 39.5 39.6 38.8</cell><cell>38.3</cell><cell cols="5">37.3 36.7 35.8 34.7 34.4</cell><cell></cell><cell>70.8</cell><cell cols="2">70.8</cell><cell></cell><cell>70.7</cell><cell></cell><cell>70.7</cell><cell></cell><cell>70.2</cell></row><row><cell>LADE</cell><cell cols="5">42.8 41.5 41.2 40.8 39.8</cell><cell>39.2</cell><cell cols="5">38.1 37.6 36.9 36.0 35.7</cell><cell></cell><cell>68.4</cell><cell cols="2">69.0</cell><cell></cell><cell>69.3</cell><cell></cell><cell>69.6</cell><cell></cell><cell>69.5</cell></row><row><cell>LADE</cell><cell cols="5">46.3 44.2 42.2 41.2 39.7</cell><cell>39.4</cell><cell cols="5">39.2 39.9 40.9 42.4 43.0</cell><cell></cell><cell></cell><cell cols="2">69.1</cell><cell></cell><cell>69.3</cell><cell></cell><cell>70.2</cell><cell></cell><cell></cell></row><row><cell>RIDE</cell><cell cols="5">43.1 41.8 41.6 42.0 41.0</cell><cell>40.3</cell><cell cols="5">39.6 38.7 38.2 37.0 36.9</cell><cell></cell><cell>71.5</cell><cell cols="2">71.9</cell><cell></cell><cell>71.8</cell><cell></cell><cell>71.9</cell><cell></cell><cell>71.8</cell></row><row><cell>SADE</cell><cell cols="5">46.4 44.9 43.3 42.6 41.3</cell><cell>40.9</cell><cell cols="5">40.6 41.1 41.4 42.0 41.6</cell><cell></cell><cell>72.3</cell><cell cols="2">72.5</cell><cell></cell><cell>72.9</cell><cell></cell><cell>73.5</cell><cell></cell><cell>73.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 18 :</head><label>18</label><figDesc>Performance of each expert on the uniform test distribution. Here, the training imbalance ratio of CIFAR100-LT is 100. The results show that our proposed method learns more skill-diverse experts, leading to better performance of ensemble aggregation. More Results on Test-time Self-supervised Aggregation</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RIDE [49]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>ImageNet-LT</cell><cell></cell><cell cols="2">CIFAR100-LT</cell><cell></cell><cell>Places-LT</cell><cell></cell><cell>iNaturalist 2018</cell></row><row><cell></cell><cell cols="2">Many Med. Few All</cell><cell cols="3">Many Med. Few All</cell><cell cols="2">Many Med. Few All</cell><cell cols="2">Many Med. Few All</cell></row><row><cell>Expert E1</cell><cell>64.3</cell><cell>49.0 31.9 52.6</cell><cell>63.5</cell><cell cols="2">44.8 20.3 44.0</cell><cell>41.3</cell><cell>40.8 33.2 40.1</cell><cell>66.6</cell><cell>67.1 66.5 66.8</cell></row><row><cell>Expert E2</cell><cell>64.7</cell><cell>49.4 31.2 52.8</cell><cell>63.1</cell><cell cols="2">44.7 20.2 43.8</cell><cell>43.0</cell><cell>40.9 33.6 40.3</cell><cell>66.1</cell><cell>67.1 66.6 66.8</cell></row><row><cell>Expert E3</cell><cell>64.3</cell><cell>48.9 31.8 52.5</cell><cell>63.9</cell><cell cols="2">45.1 20.5 44.3</cell><cell>42.8</cell><cell>41.0 33.5 40.2</cell><cell>65.3</cell><cell>67.3 66.5 66.7</cell></row><row><cell>Ensemble</cell><cell>68.0</cell><cell>52.9 35.1 56.3</cell><cell>67.4</cell><cell cols="2">49.5 23.7 48.0</cell><cell>43.2</cell><cell>41.1 33.5 40.3</cell><cell>71.5</cell><cell>72.0 71.6 71.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SADE (ours)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>ImageNet-LT</cell><cell></cell><cell cols="2">CIFAR100-LT</cell><cell></cell><cell>Places-LT</cell><cell></cell><cell>iNaturalist 2018</cell></row><row><cell></cell><cell cols="2">Many Med. Few All</cell><cell cols="3">Many Med. Few All</cell><cell cols="2">Many Med. Few All</cell><cell cols="2">Many Med. Few All</cell></row><row><cell>Expert E1</cell><cell>68.8</cell><cell>43.7 17.2 49.8</cell><cell>67.6</cell><cell>36.3</cell><cell>6.8 38.4</cell><cell>47.6</cell><cell>27.1 10.3 31.2</cell><cell>76.0</cell><cell>67.1 59.3 66.0</cell></row><row><cell>Expert E2</cell><cell>65.5</cell><cell>50.5 33.3 53.9</cell><cell>61.2</cell><cell cols="2">44.7 23.5 44.2</cell><cell>42.6</cell><cell>42.3 32.3 40.5</cell><cell>69.2</cell><cell>70.7 69.8 70.2</cell></row><row><cell>Expert E3</cell><cell>43.4</cell><cell>48.6 53.9 47.3</cell><cell>14.0</cell><cell cols="2">27.6 41.2 25.8</cell><cell>22.6</cell><cell>37.2 45.6 33.6</cell><cell>55.6</cell><cell>61.5 72.1 65.1</cell></row><row><cell>Ensemble</cell><cell>67.0</cell><cell>56.7 42.6 58.8</cell><cell>61.6</cell><cell cols="2">50.5 33.9 49.4</cell><cell>40.4</cell><cell>43.2 36.8 40.9</cell><cell>74.4</cell><cell>72.5 73.1 72.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 19 :</head><label>19</label><figDesc>The learned aggregation weights by our test-time self-supervised aggregation strategy on different test class distributions of ImageNet-LT, CIFAR100-LT, Places-LT and iNaturalist 2018. The results show that our self-supervised strategy is able to learn suitable expert weights for various unknown test class distributions.Relying on the learned expert weights, our method aggregates the three experts appropriately and achieves better performance on the dominant test classes, thus obtaining promising performance gains on various test distributions, as shown in</figDesc><table><row><cell>Test Dist.</cell><cell></cell><cell>ImageNet-LT</cell><cell></cell><cell cols="3">CIFAR100-LT(IR10)</cell><cell cols="3">CIFAR100-LT(IR50)</cell></row><row><cell></cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell></row><row><cell>Forward-LT-50</cell><cell>0.52</cell><cell>0.35</cell><cell>0.13</cell><cell>0.53</cell><cell>0.38</cell><cell>0.09</cell><cell>0.55</cell><cell>0.38</cell><cell>0.07</cell></row><row><cell>Forward-LT-25</cell><cell>0.50</cell><cell>0.35</cell><cell>0.15</cell><cell>0.52</cell><cell>0.37</cell><cell>0.11</cell><cell>0.54</cell><cell>0.38</cell><cell>0.08</cell></row><row><cell>Forward-LT-10</cell><cell>0.46</cell><cell>0.36</cell><cell>0.18</cell><cell>0.47</cell><cell>0.36</cell><cell>0.17</cell><cell>0.52</cell><cell>0.37</cell><cell>0.11</cell></row><row><cell>Forward-LT-5</cell><cell>0.43</cell><cell>0.34</cell><cell>0.23</cell><cell>0.46</cell><cell>0.34</cell><cell>0.20</cell><cell>0.50</cell><cell>0.36</cell><cell>0.14</cell></row><row><cell>Forward-LT-2</cell><cell>0.37</cell><cell>0.35</cell><cell>0.28</cell><cell>0.39</cell><cell>0.37</cell><cell>0.24</cell><cell>0.39</cell><cell>0.38</cell><cell>0.23</cell></row><row><cell>Uniform</cell><cell>0.33</cell><cell>0.33</cell><cell>0.34</cell><cell>0.38</cell><cell>0.32</cell><cell>0.3</cell><cell>0.35</cell><cell>0.33</cell><cell>0.33</cell></row><row><cell>Backward-LT-2</cell><cell>0.29</cell><cell>0.31</cell><cell>0.40</cell><cell>0.35</cell><cell>0.33</cell><cell>0.31</cell><cell>0.30</cell><cell>0.30</cell><cell>0.40</cell></row><row><cell>Backward-LT-5</cell><cell>0.24</cell><cell>0.31</cell><cell>0.45</cell><cell>0.31</cell><cell>0.32</cell><cell>0.37</cell><cell>0.21</cell><cell>0.29</cell><cell>0.50</cell></row><row><cell>Backward-LT-10</cell><cell>0.21</cell><cell>0.29</cell><cell>0.50</cell><cell>0.26</cell><cell>0.32</cell><cell>0.42</cell><cell>0.20</cell><cell>0.29</cell><cell>0.51</cell></row><row><cell>Backward-LT-25</cell><cell>0.18</cell><cell>0.29</cell><cell>0.53</cell><cell>0.24</cell><cell>0.30</cell><cell>0.46</cell><cell>0.18</cell><cell>0.27</cell><cell>0.55</cell></row><row><cell>Backward-LT-50</cell><cell>0.17</cell><cell>0.27</cell><cell>0.56</cell><cell>0.23</cell><cell>0.28</cell><cell>0.49</cell><cell>0.14</cell><cell>0.24</cell><cell>0.62</cell></row><row><cell>Test Dist.</cell><cell cols="3">CIFAR100-LT(IR100)</cell><cell></cell><cell>Places-LT</cell><cell></cell><cell></cell><cell>iNaturalist 2018</cell><cell></cell></row><row><cell></cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell></row><row><cell>Forward-LT-50</cell><cell>0.56</cell><cell>0.38</cell><cell>0.06</cell><cell>0.50</cell><cell>0.20</cell><cell>0.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Forward-LT-25</cell><cell>0.55</cell><cell>0.38</cell><cell>0.07</cell><cell>0.50</cell><cell>0.20</cell><cell>0.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Forward-LT-10</cell><cell>0.52</cell><cell>0.39</cell><cell>0.09</cell><cell>0.50</cell><cell>0.20</cell><cell>0.20</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Forward-LT-5</cell><cell>0.51</cell><cell>0.37</cell><cell>0.12</cell><cell>0.46</cell><cell>0.32</cell><cell>0.22</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Forward-LT-2</cell><cell>0.49</cell><cell>0.35</cell><cell>0.16</cell><cell>0.40</cell><cell>0.34</cell><cell>0.26</cell><cell>0.41</cell><cell>0.34</cell><cell>0.25</cell></row><row><cell>Uniform</cell><cell>0.40</cell><cell>0.35</cell><cell>0.24</cell><cell>0.25</cell><cell>0.34</cell><cell>0.41</cell><cell>0.33</cell><cell>0.33</cell><cell>0.34</cell></row><row><cell>Backward-LT-2</cell><cell>0.33</cell><cell>0.31</cell><cell>0.36</cell><cell>0.18</cell><cell>0.30</cell><cell>0.52</cell><cell>0.28</cell><cell>0.32</cell><cell>0.40</cell></row><row><cell>Backward-LT-5</cell><cell>0.28</cell><cell>0.30</cell><cell>0.42</cell><cell>0.17</cell><cell>0.28</cell><cell>0.55</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Backward-LT-10</cell><cell>0.23</cell><cell>0.28</cell><cell>0.49</cell><cell>0.17</cell><cell>0.27</cell><cell>0.56</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Backward-LT-25</cell><cell>0.21</cell><cell>0.26</cell><cell>0.53</cell><cell>0.17</cell><cell>0.27</cell><cell>0.56</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Backward-LT-50</cell><cell>0.16</cell><cell>0.28</cell><cell>0.56</cell><cell>0.17</cell><cell>0.27</cell><cell>0.56</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 20 :</head><label>20</label><figDesc>The performance improvement via test-time self-supervised aggregation on various test class distributions of ImageNet-LT, CIFAR100-LT, Places-LT and iNaturalist 2018.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">ImageNet-LT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR100-LT(IR10)</cell><cell></cell></row><row><cell>Test Dist.</cell><cell cols="3">Ours w/o test-time aggregation</cell><cell cols="3">Ours w/ test-time aggregation</cell><cell cols="4">Ours w/o test-time aggregation</cell><cell cols="4">Ours w/ test-time aggregation</cell></row><row><cell></cell><cell cols="2">Many Med. Few</cell><cell>All</cell><cell cols="2">Many Med. Few</cell><cell>All</cell><cell cols="3">Many Med. Few</cell><cell>All</cell><cell cols="3">Many Med. Few</cell><cell>All</cell></row><row><cell>Forward-LT-50</cell><cell>65.6</cell><cell>55.7 44.1</cell><cell>65.5</cell><cell>70.0</cell><cell cols="2">53.2 33.1 69.4 (+3.9)</cell><cell>66.3</cell><cell>58.3</cell><cell>-</cell><cell>66.3</cell><cell>69.0</cell><cell>50.8</cell><cell>-</cell><cell>71.2 (+4.9)</cell></row><row><cell>Forward-LT-25</cell><cell>65.3</cell><cell>56.9 43.5</cell><cell>64.4</cell><cell>69.5</cell><cell cols="2">53.2 32.2 67.4 (+3.0)</cell><cell>63.1</cell><cell>60.8</cell><cell>-</cell><cell>64.5</cell><cell>67.6</cell><cell>52.2</cell><cell>-</cell><cell>69.4 (+4.9)</cell></row><row><cell>Forward-LT-10</cell><cell>66.5</cell><cell>56.8 44.2</cell><cell>63.6</cell><cell>69.9</cell><cell cols="2">54.3 34.7 65.4 (+1.8)</cell><cell>64.1</cell><cell>58.8</cell><cell>-</cell><cell>64.1</cell><cell>67.2</cell><cell>54.2</cell><cell>-</cell><cell>67.6 (+3.5)</cell></row><row><cell>Forward-LT-5</cell><cell>65.9</cell><cell>56.5 43.3</cell><cell>62.0</cell><cell>68.9</cell><cell cols="2">54.8 35.8 63.0 (+1.0)</cell><cell>62.7</cell><cell>57.1</cell><cell>-</cell><cell>62.7</cell><cell>66.9</cell><cell>54/3</cell><cell>-</cell><cell>66.3 (+3.6)</cell></row><row><cell>Forward-LT-2</cell><cell>66.2</cell><cell>56.5 42.1</cell><cell>60.0</cell><cell>68.2</cell><cell cols="2">56.0 40.1 60.6 (+0.6)</cell><cell>62.8</cell><cell>56.3</cell><cell>-</cell><cell>61.6</cell><cell>66.1</cell><cell>56.6</cell><cell>-</cell><cell>64.4 (+2.8)</cell></row><row><cell>Uniform</cell><cell>67.0</cell><cell>56.7 42.6</cell><cell>58.8</cell><cell>66.5</cell><cell cols="2">57.0 43.5 58.8 (+0.0)</cell><cell>65.5</cell><cell>59.9</cell><cell>-</cell><cell>63.6</cell><cell>65.8</cell><cell>58.8</cell><cell>-</cell><cell>63.6 (+0.0)</cell></row><row><cell>Backward-LT-2</cell><cell>66.3</cell><cell>56.7 43.1</cell><cell>56.8</cell><cell>65.3</cell><cell cols="2">57.1 45.0 57.1 (+0.3)</cell><cell>62.7</cell><cell>56.9</cell><cell>-</cell><cell>60.2</cell><cell>65.6</cell><cell>59.5</cell><cell>-</cell><cell>62.9 (+2.7)</cell></row><row><cell>Backward-LT-5</cell><cell>66.6</cell><cell>56.9 43.0</cell><cell>54.7</cell><cell>63.4</cell><cell cols="2">56.4 47.5 55.5 (+0.8)</cell><cell>62.8</cell><cell>57.5</cell><cell>-</cell><cell>59.7</cell><cell>65.1</cell><cell>60.4</cell><cell>-</cell><cell>62.4 (+2.7)</cell></row><row><cell cols="2">Backward-LT-10 65.0</cell><cell>57.6 43.1</cell><cell>53.1</cell><cell>60.9</cell><cell cols="2">57.5 50.1 54.5 (+1.4)</cell><cell>63.5</cell><cell>58.2</cell><cell>-</cell><cell>59.8</cell><cell>62.5</cell><cell>61.4</cell><cell>-</cell><cell>61.7 (+1.9)</cell></row><row><cell cols="2">Backward-LT-25 64.2</cell><cell>56.9 43.4</cell><cell>51.1</cell><cell>60.5</cell><cell cols="2">57.1 50.0 53.7 (+2.6)</cell><cell>63.4</cell><cell>57.7</cell><cell>-</cell><cell>58.7</cell><cell>61.9</cell><cell>62.0</cell><cell>-</cell><cell>62.1 (+3.4)</cell></row><row><cell cols="2">Backward-LT-50 69.1</cell><cell>57.0 42.9</cell><cell>49.8</cell><cell>60.7</cell><cell cols="2">56.2 50.7 53.1 (+3.3)</cell><cell>62.0</cell><cell>57.8</cell><cell>-</cell><cell>58.6</cell><cell>62.6</cell><cell>62.6</cell><cell>-</cell><cell>63.0 (+3.8)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR100-LT(IR50)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR100-LT(IR100)</cell><cell></cell></row><row><cell>Test Dist.</cell><cell cols="3">Ours w/o test-time aggregation</cell><cell cols="3">Ours w/ test-time aggregation</cell><cell cols="4">Ours w/o test-time aggregation</cell><cell cols="4">Ours w/ test-time aggregation</cell></row><row><cell></cell><cell cols="2">Many Med. Few</cell><cell>All</cell><cell cols="2">Many Med. Few</cell><cell>All</cell><cell cols="3">Many Med. Few</cell><cell>All</cell><cell cols="3">Many Med. Few</cell><cell>All</cell></row><row><cell>Forward-LT-50</cell><cell>59.7</cell><cell>53.3 26.9</cell><cell>59.5</cell><cell>68.0</cell><cell cols="2">44.1 19.4 67.2 (+7.7)</cell><cell>60.7</cell><cell cols="2">50.3 32.4</cell><cell>58.4</cell><cell>69.9</cell><cell cols="3">48.8 14.2 65.9 (+7.5)</cell></row><row><cell>Forward-LT-25</cell><cell>59.1</cell><cell>51.8 32.6</cell><cell>58.6</cell><cell>67.3</cell><cell cols="2">46.2 19.5 64.5 (+6.9)</cell><cell>60.6</cell><cell cols="2">49.6 29.4</cell><cell>57.0</cell><cell>68.9</cell><cell cols="3">46.5 15.1 62.5 (+5.5)</cell></row><row><cell>Forward-LT-10</cell><cell>59.7</cell><cell>47.2 36.1</cell><cell>56.4</cell><cell>67.2</cell><cell cols="2">45.7 24.7 61.2 (+4.8)</cell><cell>60.1</cell><cell cols="2">48.6 28.4</cell><cell>54.4</cell><cell>68.3</cell><cell cols="3">46.9 16.7 58.3 (+3.9)</cell></row><row><cell>Forward-LT-5</cell><cell>59.7</cell><cell>46.9 36.9</cell><cell>54.8</cell><cell>67.0</cell><cell cols="2">45.7 29.9 58.6 (+3.4)</cell><cell>60.3</cell><cell cols="2">50.3 29.5</cell><cell>53.1</cell><cell>68.3</cell><cell cols="3">45.3 19.4 54.8 (+1.7)</cell></row><row><cell>Forward-LT-2</cell><cell>59.2</cell><cell>48.4 41.9</cell><cell>53.2</cell><cell>63.8</cell><cell cols="2">48.5 39.3 55.4 (+2.2)</cell><cell>60.6</cell><cell cols="2">48.8 31.3</cell><cell>50.1</cell><cell>68.2</cell><cell cols="3">47.6 22.5 51.1 (+1.0)</cell></row><row><cell>Uniform</cell><cell>61.0</cell><cell>50.2 45.7</cell><cell>53.8</cell><cell>61.5</cell><cell cols="2">50.2 45.0 53.9 (+0.1)</cell><cell>61.6</cell><cell cols="2">50.5 33.9</cell><cell>49.4</cell><cell>65.4</cell><cell cols="3">49.3 29.3 49.8 (+0.4)</cell></row><row><cell>Backward-LT-2</cell><cell>59.0</cell><cell>48.2 42.8</cell><cell>50.1</cell><cell>57.5</cell><cell cols="2">49.7 49.4 51.9 (+1.8)</cell><cell>61.2</cell><cell cols="2">49.1 30.8</cell><cell>45.2</cell><cell>63.1</cell><cell cols="3">49.4 31.7 46.2 (+1.0)</cell></row><row><cell>Backward-LT-5</cell><cell>60.1</cell><cell>48.6 41.8</cell><cell>48.2</cell><cell>50.0</cell><cell cols="2">49.3 54.2 50.9 (+2.7)</cell><cell>62.0</cell><cell cols="2">48.9 32.0</cell><cell>42.6</cell><cell>56.2</cell><cell cols="3">49.1 38.2 44.7 (+2.1)</cell></row><row><cell cols="2">Backward-LT-10 58.6</cell><cell>46.9 42.6</cell><cell>46.1</cell><cell>49.3</cell><cell cols="2">49.1 54.6 51.0 (+4.9)</cell><cell>60.6</cell><cell cols="2">48.2 31.7</cell><cell>39.7</cell><cell>52.1</cell><cell cols="3">47.9 40.6 43.9 (+4.2)</cell></row><row><cell cols="2">Backward-LT-25 55.1</cell><cell>48.9 41.2</cell><cell>44.4</cell><cell>44.5</cell><cell cols="2">46.6 57.0 51.7 (+7.3)</cell><cell>58.2</cell><cell cols="2">47.9 32.2</cell><cell>36.7</cell><cell>48.7</cell><cell cols="3">44.2 41.8 42.5 (+5.8)</cell></row><row><cell cols="2">Backward-LT-50 57.0</cell><cell>48.8 41.6</cell><cell>43.6</cell><cell>45.8</cell><cell cols="2">46.6 58.4 52.8 (+9.2)</cell><cell>66.9</cell><cell cols="2">48.6 30.4</cell><cell>35.0</cell><cell>49.0</cell><cell cols="3">42.7 42.5 42.4 (+7.4)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Places-LT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">iNaturalist 2018</cell><cell></cell><cell></cell></row><row><cell>Test Dist.</cell><cell cols="3">Ours w/o test-time aggregation</cell><cell cols="3">Ours w/ test-time aggregation</cell><cell cols="4">Ours w/o test-time aggregation</cell><cell cols="4">Ours w/ test-time aggregation</cell></row><row><cell></cell><cell cols="2">Many Med. Few</cell><cell>All</cell><cell cols="2">Many Med. Few</cell><cell>All</cell><cell cols="3">Many Med. Few</cell><cell>All</cell><cell cols="3">Many Med. Few</cell><cell>All</cell></row><row><cell>Forward-LT-50</cell><cell>43.5</cell><cell>42.5 65.9</cell><cell>43.7</cell><cell>46.8</cell><cell cols="2">39.3 30.5 46.4 (+2.7)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Forward-LT-25</cell><cell>42.8</cell><cell>42.1 29.3</cell><cell>42.7</cell><cell>46.3</cell><cell cols="2">38.9 23.6 44.9 (+2.3)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Forward-LT-10</cell><cell>42.3</cell><cell>41.9 34.9</cell><cell>42.3</cell><cell>45.4</cell><cell cols="2">39.0 27.0 43.3 (+1.0)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Forward-LT-5</cell><cell>43.0</cell><cell>44.0 33.1</cell><cell>42.4</cell><cell>45.6</cell><cell cols="2">40.6 27.3 42.6 (+0.2)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Forward-LT-2</cell><cell>43.4</cell><cell>42.4 32.6</cell><cell>41.3</cell><cell>44.9</cell><cell cols="2">41.2 29.5 41.3 (+0.0)</cell><cell>73.9</cell><cell cols="2">72.4 72.0</cell><cell>72.4</cell><cell>75.5</cell><cell cols="3">72.5 70.7 72.5 (+0.1)</cell></row><row><cell>Uniform</cell><cell>43.1</cell><cell>42.4 33.2</cell><cell>40.9</cell><cell>40.4</cell><cell cols="2">43.2 36.8 40.9 (+0.0)</cell><cell>74.4</cell><cell cols="2">72.5 73.1</cell><cell>72.9</cell><cell>74.5</cell><cell cols="3">72.5 73.0 72.9 (+0.0)</cell></row><row><cell>Backward-LT-2</cell><cell>42.8</cell><cell>41.9 33.2</cell><cell>39.9</cell><cell>37.1</cell><cell cols="2">42.9 40.0 40.6 (+0.7)</cell><cell>76.1</cell><cell cols="2">72.8 72.6</cell><cell>73.1</cell><cell>74.9</cell><cell cols="3">72.6 73.7 73.5 (+0.4)</cell></row><row><cell>Backward-LT-5</cell><cell>43.1</cell><cell>42.0 33.6</cell><cell>39.1</cell><cell>36.4</cell><cell cols="2">42.7 41.1 41.1 (+2.0)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Backward-LT-10 43.5</cell><cell>42.9 33.7</cell><cell>38.9</cell><cell>35.2</cell><cell cols="2">43.2 41.3 41.4 (+2.5)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Backward-LT-25 44.6</cell><cell>42.4 33.6</cell><cell>37.8</cell><cell>38.0</cell><cell cols="2">43.5 41.1 42.0 (+4.2)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Backward-LT-50 42.2</cell><cell>43.4 33.3</cell><cell>37.2</cell><cell>37.3</cell><cell cols="2">43.5 40.5 41.6 (+4.7)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 21 :</head><label>21</label><figDesc>Performance of our method with different numbers of experts on ImageNet-LT with the uniform test distribution.</figDesc><table><row><cell>Model</cell><cell></cell><cell>4 experts</cell><cell></cell><cell></cell><cell></cell><cell>5 experts</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Many-shot Medium-shot Few-shot All classes</cell><cell cols="4">Many-shot Medium-shot Few-shot All classes</cell></row><row><cell>Expert E 1</cell><cell>69.4</cell><cell>44.5</cell><cell>16.5</cell><cell>50.3</cell><cell>69.8</cell><cell>44.9</cell><cell>17.0</cell><cell>50.7</cell></row><row><cell>Expert E 2</cell><cell>66.2</cell><cell>51.5</cell><cell>32.9</cell><cell>54.6</cell><cell>68.8</cell><cell>48.4</cell><cell>23.9</cell><cell>52.9</cell></row><row><cell>Expert E 3</cell><cell>55.7</cell><cell>52.7</cell><cell>46.8</cell><cell>53.4</cell><cell>66.1</cell><cell>51.4</cell><cell>22.0</cell><cell>54.5</cell></row><row><cell>Expert E 4</cell><cell>44.1</cell><cell>49.7</cell><cell>55.9</cell><cell>48.4</cell><cell>56.8</cell><cell>52.7</cell><cell>47.7</cell><cell>53.6</cell></row><row><cell>Expert E 5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.1</cell><cell>59.0</cell><cell>54.8</cell><cell>47.5</cell></row><row><cell>Ensemble</cell><cell>66.6</cell><cell>58.4</cell><cell>46.7</cell><cell>60.0</cell><cell>68.8</cell><cell>58.5</cell><cell>43.2</cell><cell>60.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 22 :</head><label>22</label><figDesc>Performance of our method with different numbers of experts on various test class distributions of ImageNet-LT.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ImageNet-LT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Method Experts</cell><cell></cell><cell>Forward</cell><cell></cell><cell></cell><cell>Uniform</cell><cell></cell><cell></cell><cell>Backward</cell><cell></cell></row><row><cell></cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell></cell><cell cols="5">3 experts 69.4 67.4 65.4 63.0 60.6</cell><cell>58.8</cell><cell cols="5">57.1 55.5 54.5 53.7 53.1</cell></row><row><cell>SADE</cell><cell cols="5">4 experts 70.1 68.1 66.3 64.2 61.6</cell><cell>60.0</cell><cell cols="5">58.7 57.6 56.7 56.1 55.6</cell></row><row><cell></cell><cell cols="5">5 experts 70.7 68.9 66.8 64.5 62.1</cell><cell>60.4</cell><cell cols="5">58.7 57.2 56.3 55.6 54.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 23 :</head><label>23</label><figDesc>Influence of the hyper-parameter ? in the inverse softmax loss on ImageNet-LT with the uniform test distribution.F Ablation Studies on Test-time Self-supervised Aggregation F.1 Influences of Training EpochAs illustrated in Section 5.1, we set the training epoch of our test-time self-supervised aggregation strategy to 5 on all datasets. Here, we further evaluate the influence of the epoch number, where we adjust the epoch number from 1 to 100. As shown in</figDesc><table><row><cell>Model</cell><cell></cell><cell>? = 0.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Many-shot classes Medium-shot classes Few-shot classes All long-tailed classes</cell></row><row><cell>Forward Expert E 1</cell><cell>69.1</cell><cell>43.6</cell><cell>17.2</cell><cell>49.8</cell></row><row><cell>Uniform Expert E 2</cell><cell>66.4</cell><cell>50.9</cell><cell>33.4</cell><cell>54.5</cell></row><row><cell>Backward Expert E 3</cell><cell>61.9</cell><cell>51.9</cell><cell>40.3</cell><cell>54.2</cell></row><row><cell>Ensemble</cell><cell>71.0</cell><cell>54.6</cell><cell>33.4</cell><cell>58.0</cell></row><row><cell>Model</cell><cell></cell><cell>? = 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Many-shot classes Medium-shot classes Few-shot classes All long-tailed classes</cell></row><row><cell>Forward Expert E 1</cell><cell>69.7</cell><cell>44.0</cell><cell>16.8</cell><cell>50.2</cell></row><row><cell>Uniform Expert E 2</cell><cell>65.5</cell><cell>51.1</cell><cell>32.4</cell><cell>54.4</cell></row><row><cell>Backward Expert E 3</cell><cell>56.5</cell><cell>52.3</cell><cell>47.1</cell><cell>53.2</cell></row><row><cell>Ensemble</cell><cell>77.2</cell><cell>55.7</cell><cell>36.2</cell><cell>58.6</cell></row><row><cell>Model</cell><cell></cell><cell>? = 2</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Many-shot classes Medium-shot classes Few-shot classes All long-tailed classes</cell></row><row><cell>Forward Expert E 1</cell><cell>68.8</cell><cell>43.7</cell><cell>17.2</cell><cell>49.8</cell></row><row><cell>Uniform Expert E 2</cell><cell>65.5</cell><cell>50.5</cell><cell>33.3</cell><cell>53.9</cell></row><row><cell>Backward Expert E 3</cell><cell>43.4</cell><cell>48.6</cell><cell>53.9</cell><cell>47.3</cell></row><row><cell>Ensemble</cell><cell>67.0</cell><cell>56.7</cell><cell>42.6</cell><cell>58.8</cell></row><row><cell>Model</cell><cell></cell><cell>? = 3</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Many-shot classes Medium-shot classes Few-shot classes All long-tailed classes</cell></row><row><cell>Forward Expert E 1</cell><cell>69.6</cell><cell>43.8</cell><cell>17.4</cell><cell>50.2</cell></row><row><cell>Uniform Expert E 2</cell><cell>66.2</cell><cell>50.7</cell><cell>33.1</cell><cell>54.2</cell></row><row><cell>Backward Expert E 3</cell><cell>43.4</cell><cell>48.6</cell><cell>53.9</cell><cell>48.0</cell></row><row><cell>Ensemble</cell><cell>67.8</cell><cell>56.8</cell><cell>42.4</cell><cell>59.1</cell></row><row><cell>Model</cell><cell></cell><cell>? = 4</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Many-shot classes Medium-shot classes Few-shot classes All long-tailed classes</cell></row><row><cell>Forward Expert E 1</cell><cell>69.1</cell><cell>44.1</cell><cell>16.3</cell><cell>49.9</cell></row><row><cell>Uniform Expert E 2</cell><cell>65.7</cell><cell>50.8</cell><cell>32.6</cell><cell>54.1</cell></row><row><cell>Backward Expert E 3</cell><cell>21.9</cell><cell>38.1</cell><cell>58.9</cell><cell>34.7</cell></row><row><cell>Ensemble</cell><cell>60.2</cell><cell>57.5</cell><cell>50.4</cell><cell>57.6</cell></row><row><cell>Model</cell><cell></cell><cell>? = 5</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Many-shot classes Medium-shot classes Few-shot classes All long-tailed classes</cell></row><row><cell>Forward Expert E 1</cell><cell>69.7</cell><cell>43.7</cell><cell>16.5</cell><cell>50.0</cell></row><row><cell>Uniform Expert E 2</cell><cell>65.9</cell><cell>50.9</cell><cell>33.0</cell><cell>54.2</cell></row><row><cell>Backward Expert E 3</cell><cell>16.0</cell><cell>33.9</cell><cell>60.6</cell><cell>30.6</cell></row><row><cell>Ensemble</cell><cell>56.3</cell><cell>57.5</cell><cell>54.0</cell><cell>56.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 24 :</head><label>24</label><figDesc>The influence of the epoch number on the learned expert weights by test-time self-supervised aggregation on ImageNet-LT.</figDesc><table><row><cell>Test Dist.</cell><cell></cell><cell>Epoch 1</cell><cell></cell><cell></cell><cell>Epoch 5</cell><cell></cell><cell></cell><cell>Epoch 10</cell><cell></cell></row><row><cell></cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell></row><row><cell>Forward-LT-50</cell><cell>0.44</cell><cell>0.33</cell><cell>0.23</cell><cell>0.52</cell><cell>0.35</cell><cell>0.13</cell><cell>0.52</cell><cell>0.37</cell><cell>0.11</cell></row><row><cell>Forward-LT-25</cell><cell>0.43</cell><cell>0.34</cell><cell>0.23</cell><cell>0.50</cell><cell>0.35</cell><cell>0.15</cell><cell>0.50</cell><cell>0.37</cell><cell>0.13</cell></row><row><cell>Forward-LT-10</cell><cell>0.43</cell><cell>0.34</cell><cell>0.23</cell><cell>0.46</cell><cell>0.36</cell><cell>0.18</cell><cell>0.46</cell><cell>0.36</cell><cell>0.18</cell></row><row><cell>Forward-LT-5</cell><cell>0.41</cell><cell>0.34</cell><cell>0.25</cell><cell>0.43</cell><cell>0.34</cell><cell>0.23</cell><cell>0.43</cell><cell>0.35</cell><cell>0.22</cell></row><row><cell>Forward-LT-2</cell><cell>0.37</cell><cell>0.33</cell><cell>0.30</cell><cell>0.37</cell><cell>0.35</cell><cell>0.28</cell><cell>0.38</cell><cell>0.33</cell><cell>0.29</cell></row><row><cell>Uniform</cell><cell>0.34</cell><cell>0.31</cell><cell>0.35</cell><cell>0.33</cell><cell>0.33</cell><cell>0.34</cell><cell>0.33</cell><cell>0.32</cell><cell>0.35</cell></row><row><cell>Backward-LT-2</cell><cell>0.30</cell><cell>0.32</cell><cell>0.38</cell><cell>0.29</cell><cell>0.31</cell><cell>0.40</cell><cell>0.29</cell><cell>0.32</cell><cell>0.39</cell></row><row><cell>Backward-LT-5</cell><cell>0.27</cell><cell>0.29</cell><cell>0.44</cell><cell>0.24</cell><cell>0.31</cell><cell>0.45</cell><cell>0.23</cell><cell>0.31</cell><cell>0.46</cell></row><row><cell>Backward-LT-10</cell><cell>0.24</cell><cell>0.29</cell><cell>0.47</cell><cell>0.21</cell><cell>0.29</cell><cell>0.50</cell><cell>0.21</cell><cell>0.30</cell><cell>0.49</cell></row><row><cell>Backward-LT-25</cell><cell>0.23</cell><cell>0.29</cell><cell>0.48</cell><cell>0.18</cell><cell>0.29</cell><cell>0.53</cell><cell>0.17</cell><cell>0.3</cell><cell>0.53</cell></row><row><cell>Backward-LT-50</cell><cell>0.24</cell><cell>0.29</cell><cell>0.47</cell><cell>0.17</cell><cell>0.27</cell><cell>0.56</cell><cell>0.15</cell><cell>0.28</cell><cell>0.57</cell></row><row><cell>Test Dist.</cell><cell></cell><cell>Epoch 20</cell><cell></cell><cell></cell><cell>Epoch 50</cell><cell></cell><cell></cell><cell>Epoch 100</cell><cell></cell></row><row><cell></cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell></row><row><cell>Forward-LT-50</cell><cell>0.53</cell><cell>0.38</cell><cell>0.09</cell><cell>0.53</cell><cell>0.38</cell><cell>0.09</cell><cell>0.53</cell><cell>0.38</cell><cell>0.09</cell></row><row><cell>Forward-LT-25</cell><cell>0.51</cell><cell>0.37</cell><cell>0.12</cell><cell>0.52</cell><cell>0.37</cell><cell>0.11</cell><cell>0.50</cell><cell>0.38</cell><cell>0.12</cell></row><row><cell>Forward-LT-10</cell><cell>0.44</cell><cell>0.36</cell><cell>0.20</cell><cell>0.45</cell><cell>0.37</cell><cell>0.18</cell><cell>0.46</cell><cell>0.36</cell><cell>0.18</cell></row><row><cell>Forward-LT-5</cell><cell>0.42</cell><cell>0.35</cell><cell>0.23</cell><cell>0.42</cell><cell>0.35</cell><cell>0.23</cell><cell>0.42</cell><cell>0.35</cell><cell>0.23</cell></row><row><cell>Forward-LT-2</cell><cell>0.38</cell><cell>0.33</cell><cell>0.29</cell><cell>0.39</cell><cell>0.33</cell><cell>0.28</cell><cell>0.38</cell><cell>0.32</cell><cell>0.30</cell></row><row><cell>Uniform</cell><cell>0.33</cell><cell>0.33</cell><cell>0.34</cell><cell>0.34</cell><cell>0.32</cell><cell>0.34</cell><cell>0.32</cell><cell>0.33</cell><cell>0.35</cell></row><row><cell>Backward-LT-2</cell><cell>0.29</cell><cell>0.31</cell><cell>0.40</cell><cell>0.30</cell><cell>0.32</cell><cell>0.38</cell><cell>0.29</cell><cell>0.30</cell><cell>0.41</cell></row><row><cell>Backward-LT-5</cell><cell>0.24</cell><cell>0.31</cell><cell>0.45</cell><cell>0.23</cell><cell>0.29</cell><cell>0.48</cell><cell>0.25</cell><cell>0.30</cell><cell>0.45</cell></row><row><cell>Backward-LT-10</cell><cell>0.20</cell><cell>0.30</cell><cell>0.50</cell><cell>0.21</cell><cell>0.31</cell><cell>0.48</cell><cell>0.21</cell><cell>0.30</cell><cell>0.49</cell></row><row><cell>Backward-LT-25</cell><cell>0.16</cell><cell>0.30</cell><cell>0.54</cell><cell>0.17</cell><cell>0.29</cell><cell>0.54</cell><cell>0.17</cell><cell>0.30</cell><cell>0.53</cell></row><row><cell>Backward-LT-50</cell><cell>0.15</cell><cell>0.29</cell><cell>0.56</cell><cell>0.14</cell><cell>0.29</cell><cell>0.57</cell><cell>0.14</cell><cell>0.29</cell><cell>0.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 25 :</head><label>25</label><figDesc>The influence of the epoch number on the performance of test-time self-supervised aggregation on ImageNet-LT.</figDesc><table><row><cell>Test Dist.</cell><cell>Epoch 1</cell><cell>Epoch 5</cell><cell>Epoch 10</cell></row><row><cell></cell><cell>Many Med. Few All</cell><cell>Many Med. Few All</cell><cell>Many Med. Few All</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 26 :</head><label>26</label><figDesc>The influence of the batch size on the learned expert weights by test-time self-supervised aggregation on ImageNet-LT.</figDesc><table><row><cell>Test Dist.</cell><cell>Batch size 64</cell><cell>Batch size 128</cell><cell>Batch size 256</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 27 :</head><label>27</label><figDesc>The influence of the batch size on the performance of test-time self-supervised aggregation on ImageNet-LT.</figDesc><table><row><cell>Test Dist.</cell><cell>Batch size 64</cell><cell>Batch size 128</cell><cell>Batch size 256</cell></row><row><cell></cell><cell>Many Med. Few All</cell><cell>Many Med. Few All</cell><cell>Many Med. Few All</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 28 :</head><label>28</label><figDesc>The influence of the learning rate on the learned expert weights by test-time self-supervised aggregation on ImageNet-LT, where the number of the training epoch is 5.</figDesc><table><row><cell>Test Dist.</cell><cell cols="3">Learning rate 0.001</cell><cell cols="3">Learning rate 0.01</cell><cell cols="3">Learning rate 0.025</cell></row><row><cell></cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell></row><row><cell>Forward-LT-50</cell><cell>0.36</cell><cell>0.34</cell><cell>0.30</cell><cell>0.49</cell><cell>0.33</cell><cell>0.18</cell><cell>0.52</cell><cell>0.35</cell><cell>0.13</cell></row><row><cell>Forward-LT-25</cell><cell>0.36</cell><cell>0.34</cell><cell>0.30</cell><cell>0.48</cell><cell>0.34</cell><cell>0.18</cell><cell>0.50</cell><cell>0.35</cell><cell>0.15</cell></row><row><cell>Forward-LT-10</cell><cell>0.36</cell><cell>0.34</cell><cell>0.30</cell><cell>0.45</cell><cell>0.34</cell><cell>0.21</cell><cell>0.46</cell><cell>0.36</cell><cell>0.18</cell></row><row><cell>Forward-LT-5</cell><cell>0.36</cell><cell>0.33</cell><cell>0.31</cell><cell>0.43</cell><cell>0.34</cell><cell>0.23</cell><cell>0.43</cell><cell>0.34</cell><cell>0.23</cell></row><row><cell>Uniform</cell><cell>0.33</cell><cell>0.33</cell><cell>0.34</cell><cell>0.34</cell><cell>0.33</cell><cell>0.33</cell><cell>0.33</cell><cell>0.33</cell><cell>0.34</cell></row><row><cell>Backward-LT-5</cell><cell>0.31</cell><cell>0.32</cell><cell>0.37</cell><cell>0.25</cell><cell>0.31</cell><cell>0.44</cell><cell>0.24</cell><cell>0.31</cell><cell>0.45</cell></row><row><cell>Backward-LT-10</cell><cell>0.31</cell><cell>0.32</cell><cell>0.37</cell><cell>0.22</cell><cell>0.29</cell><cell>0.49</cell><cell>0.21</cell><cell>0.29</cell><cell>0.50</cell></row><row><cell>Backward-LT-25</cell><cell>0.31</cell><cell>0.32</cell><cell>0.37</cell><cell>0.21</cell><cell>0.28</cell><cell>0.51</cell><cell>0.18</cell><cell>0.29</cell><cell>0.53</cell></row><row><cell>Backward-LT-50</cell><cell>0.31</cell><cell>0.32.</cell><cell>0.37</cell><cell>0.20</cell><cell>0.28</cell><cell>0.52</cell><cell>0.17</cell><cell>0.27</cell><cell>0.56</cell></row><row><cell>Test Dist.</cell><cell cols="3">Learning rate 0.05</cell><cell></cell><cell>Learning rate 0.1</cell><cell></cell><cell></cell><cell>Learning rate 0.5</cell><cell></cell></row><row><cell></cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell><cell cols="3">E1 (w1) E2 (w2) E3 (w3)</cell></row><row><cell>Forward-LT-50</cell><cell>0.53</cell><cell>0.36</cell><cell>0.11</cell><cell>0.53</cell><cell>0.37</cell><cell>0.10</cell><cell>0.57</cell><cell>0.34</cell><cell>0.09</cell></row><row><cell>Forward-LT-25</cell><cell>0.51</cell><cell>0.36</cell><cell>0.13</cell><cell>0.52</cell><cell>0.36</cell><cell>0.12</cell><cell>0.57</cell><cell>0.34</cell><cell>0.09</cell></row><row><cell>Forward-LT-10</cell><cell>0.45</cell><cell>0.37</cell><cell>0.18</cell><cell>0.47</cell><cell>0.36</cell><cell>0.18</cell><cell>0.44</cell><cell>0.36</cell><cell>0.20</cell></row><row><cell>Forward-LT-5</cell><cell>0.42</cell><cell>0.35</cell><cell>0.23</cell><cell>0.47</cell><cell>0.36</cell><cell>0.18</cell><cell>0.39</cell><cell>0.36</cell><cell>0.25</cell></row><row><cell>Uniform</cell><cell>0.33</cell><cell>0.33</cell><cell>0.34</cell><cell>0.31</cell><cell>0.31</cell><cell>0.38</cell><cell>0.33</cell><cell>0.34</cell><cell>0.33</cell></row><row><cell>Backward-LT-5</cell><cell>0.24</cell><cell>0.31</cell><cell>0.45</cell><cell>0.24</cell><cell>0.29</cell><cell>0.47</cell><cell>0.21</cell><cell>0.28</cell><cell>0.51</cell></row><row><cell>Backward-LT-10</cell><cell>0.21</cell><cell>0.30</cell><cell>0.49</cell><cell>0.21</cell><cell>0.31</cell><cell>0.48</cell><cell>0.22</cell><cell>0.32</cell><cell>0.46</cell></row><row><cell>Backward-LT-25</cell><cell>0.16</cell><cell>0.28</cell><cell>0.56</cell><cell>0.17</cell><cell>0.31</cell><cell>0.52</cell><cell>0.15</cell><cell>0.30</cell><cell>0.55</cell></row><row><cell>Backward-LT-50</cell><cell>0.15</cell><cell>0.28</cell><cell>0.57</cell><cell>0.14</cell><cell>0.28</cell><cell>0.58</cell><cell>0.12</cell><cell>0.27</cell><cell>0.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 29 :</head><label>29</label><figDesc>The influence of learning rates on test-time self-supervised aggregation on ImageNet-LT, under training epoch 5.</figDesc><table><row><cell>Test Dist.</cell><cell>Learning rate 0.001</cell><cell>Learning rate 0.01</cell><cell>Learning rate 0.025</cell></row><row><cell></cell><cell>Many Med. Few All</cell><cell>Many Med. Few All</cell><cell>Many Med. Few All</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 30 :</head><label>30</label><figDesc>Comparison of prediction confidence between our method without and with test-time self-supervised aggregation on ImageNet-LT, in terms of the hard mean of the highest prediction probability on each sample. The higher the highest prediction, the better the model.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Prediction confidence on ImageNet-LT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>Forward-LT</cell><cell></cell><cell></cell><cell>Uniform</cell><cell></cell><cell></cell><cell>Backward-LT</cell><cell></cell></row><row><cell></cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell cols="6">Ours w/o test-time aggregation 0.694 0.687 0.678 0.665 0.651</cell><cell>0.639</cell><cell cols="5">0.627 0.608 0.596 0.583 0.574</cell></row><row><cell>Ours w test-time aggregation</cell><cell cols="5">0.711 0.704 0.689 0.674 0.654</cell><cell>0.639</cell><cell cols="5">0.625 0.609 0.599 0.589 0.583</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 31 :</head><label>31</label><figDesc>Run-time cost of our test-time self-supervised aggregation strategy on ImageNet-LT, compared to the run-time cost of model training. Here, we show two test class distributions for illustration, which have different numbers of test samples.</figDesc><table><row><cell>Dataset</cell><cell>Model training</cell><cell cols="2">Test-time weight learning</cell></row><row><cell></cell><cell></cell><cell cols="2">Forward-LT-50 Forward-LT-25</cell></row><row><cell>Per-epoch time</cell><cell>713 s</cell><cell>110 s</cell><cell>130 s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table 33 :</head><label>33</label><figDesc>Model complexity and performance of different methods in terms of the parameter number, Multiply-Accumulate Operations (MACs) and top-1 accuracy on test-agnostic long-tailed recognition.Here, we do not use the efficient expert assignment trick in [49] for RIDE and our method.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ImageNet-LT (ResNeXt-50)</cell><cell></cell></row><row><cell>Method</cell><cell>Params (M)</cell><cell>MACs (G)</cell><cell></cell><cell cols="2">Forward-LT</cell><cell></cell><cell></cell><cell>Uniform</cell><cell></cell><cell cols="2">Backward-LT</cell></row><row><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell>Softmax</cell><cell cols="2">25.03 (1.0x) 4.26 (1.0x)</cell><cell cols="5">66.1 63.8 60.3 56.6 52.0</cell><cell>48.0</cell><cell cols="3">43.9 38.6 34.9 30.9 27.6</cell></row><row><cell>RIDE [49]</cell><cell cols="2">38.28 (1.5x) 6.08 (1.4x)</cell><cell cols="5">67.6 66.3 64.0 61.7 58.9</cell><cell>56.3</cell><cell cols="3">54.0 51.0 48.7 46.2 44.0</cell></row><row><cell cols="3">SADE (ours) 38.28 (1.5x) 6.08 (1.4x)</cell><cell cols="5">69.4 67.4 65.4 63.0 60.6</cell><cell>58.8</cell><cell cols="3">57.1 55.5 54.5 53.7 53.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CIFAR100-LT-IR100 (ResNet-32)</cell><cell></cell></row><row><cell>Method</cell><cell>Params (M)</cell><cell>MACs (G)</cell><cell></cell><cell cols="2">Forward-LT</cell><cell></cell><cell></cell><cell>Uniform</cell><cell></cell><cell cols="2">Backward-LT</cell></row><row><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell>Softmax</cell><cell>0.46 (1.0x)</cell><cell>0.07 (1.0x)</cell><cell cols="5">63.3 62.0 56.2 52.5 46.4</cell><cell>41.4</cell><cell cols="3">36.5 30.5 25.8 21.7 17.5</cell></row><row><cell>RIDE [49]</cell><cell>0.77 (1.5x)</cell><cell>0.10 (1.4x)</cell><cell cols="5">63.0 59.9 57.0 53.6 49.4</cell><cell>48.0</cell><cell cols="3">42.5 38.1 35.4 31.6 29.2</cell></row><row><cell cols="2">SADE (ours) 0.77 (1.5x)</cell><cell>0.10 (1.4x)</cell><cell cols="5">65.9 62.5 58.3 54.8 51.1</cell><cell>49.8</cell><cell cols="3">46.2 44.7 43.9 42.5 42.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Places-LT (ResNet-152)</cell><cell></cell></row><row><cell>Method</cell><cell>Params (M)</cell><cell>MACs (G)</cell><cell></cell><cell cols="2">Forward-LT</cell><cell></cell><cell></cell><cell>Uniform</cell><cell></cell><cell cols="2">Backward-LT</cell></row><row><cell></cell><cell></cell><cell></cell><cell>50</cell><cell>25</cell><cell>10</cell><cell>5</cell><cell>2</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell></row><row><cell>Softmax</cell><cell cols="2">60.19 (1.0x) 11.56 (1.0x)</cell><cell cols="5">45.6 42.7 40.2 38.0 34.1</cell><cell>31.4</cell><cell cols="3">28.4 25.4 23.4 20.8 19.4</cell></row><row><cell>RIDE [49]</cell><cell cols="2">88.07 (1.5x) 13.18 (1.1x)</cell><cell cols="5">43.1 41.8 41.6 42.0 41.0</cell><cell>40.3</cell><cell cols="3">39.6 38.7 38.2 37.0 36.9</cell></row><row><cell cols="3">SADE (ours) 88.07 (1.5x) 13.18 (1.1x)</cell><cell cols="5">46.4 44.9 43.3 42.6 41.3</cell><cell>40.9</cell><cell cols="3">40.6 41.1 41.4 42.0 41.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">iNaturalist 2018 (ResNet-50)</cell><cell></cell></row><row><cell>Method</cell><cell>Params (M)</cell><cell>MACs (G)</cell><cell></cell><cell cols="2">Forward-LT</cell><cell></cell><cell></cell><cell>Uniform</cell><cell></cell><cell cols="2">Backward-LT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell>2</cell><cell></cell><cell>1</cell><cell></cell><cell>2</cell><cell>3</cell></row><row><cell>Softmax</cell><cell cols="2">25.56 (1.0x) 4.14 (1.0x)</cell><cell></cell><cell>65.4</cell><cell cols="2">65.5</cell><cell></cell><cell>64.7</cell><cell></cell><cell>64.0</cell><cell>63.4</cell></row><row><cell>RIDE [49]</cell><cell cols="2">39.07 (1.5x) 5.80 (1.4x)</cell><cell></cell><cell>71.5</cell><cell cols="2">71.9</cell><cell></cell><cell>71.8</cell><cell></cell><cell>71.9</cell><cell>71.8</cell></row><row><cell cols="3">SADE (ours) 39.07 (1.5x) 5.80 (1.4x)</cell><cell></cell><cell>72.3</cell><cell cols="2">72.5</cell><cell></cell><cell>72.9</cell><cell></cell><cell>73.5</cell><cell>73.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code is provided in the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by NUS ODPRT Grant R252-000-A81-133 and NUS Advanced Research and Technology Innovation Centre (ARTIC) Project Reference (ECT-RP2). We also gratefully appreciate the support of MindSpore, CANN (Compute Architecture for Neural Networks) and Ascend AI Processor used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Rony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ace: Ally complementary experts for solving long-tailed recognition in one-shot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Test-time fast adaptation for dynamic scene deblurring via meta-auxiliary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parametric contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pml: Progressive margin loss for long-tailed age classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10503" to="10512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring classification equilibrium in long-tailed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long-tailed multi-label visual recognition by collaborative training on uniform and re-balanced samplings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training independent subnetworks for robust prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marton</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Relieving long-tailed instance segmentation via pairwise class balance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhen</forename><surname>Yin-Yin He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02784</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Disentangling label distribution for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghee</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buru</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Test-time classifier adjustment module for model-agnostic domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Iwasawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7610" to="7619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Balanced meta-softmax for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Jiawei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Justin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="54" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Targeted data-driven regularization for out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadegh</forename><surname>Mohammad Mahdi Kamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Z</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="882" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring balanced feature spaces for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning loss for test-time augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Prototype-guided continual adaptation for class-incremental unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting and correcting for label shift with black box predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3122" to="3130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ttt++: When does self-supervised test-time training fail or thrive?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parth</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Bastien Van Delft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Bellot-Gurlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Largescale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long-tail learning via logit adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient test-time model adaptation without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaofo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalization on unseen domains via inference-time label-preserving target projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumanth</forename><surname>Varambally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Prathosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12924" to="12933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Influence-balanced loss for imbalanced visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seulki</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghan</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimal transport for long-tailed recognition with learnable cost matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Source-free domain adaptation via avatar prototype generation and adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Long-tailed recognition using class-balanced experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno>2020. Forward-LT-50</idno>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="86" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
		<idno>LT-50</idno>
	</analytic>
	<monogr>
		<title level="j">Many Med. Few All Many Med. Few All Many Med. Few All Forward</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Test-time Self-supervised Aggregation on Streaming Test Data In the previous experiments, we conduct the test-time self-supervised aggregation strategy in an offline manner. However, as mentioned in Section 4.2, our test-time strategy can also be conducted in an online manner and does not require access to all the test data in advance. To verify this, we further conduct our test-time strategy on steaming test data of ImageNet-LT. As shown in Table 32, our test-time strategy performs well on the streaming test data. Even when the test data come in one by one, our test-time self-supervised strategy still outperforms the state-of-the-art baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>i.e., offline Tent [46]) by a large margin</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Results of our test-time self-supervised aggregation strategy on streaming test data of ImageNet-LT, where all test-time strategies are used on the same skill-diverse multi-expert model</title>
	</analytic>
	<monogr>
		<title level="m">Backbone Test-time strategy Forward-LT Backward-LT</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
