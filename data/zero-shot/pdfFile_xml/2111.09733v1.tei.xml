<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perceiving and Modeling Density is All You Need for Image Dehazing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Ocean Information Engineering</orgName>
								<orgName type="institution">Jimei University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JOYY AI GROUP</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Design Group Co</orgName>
								<orgName type="institution">Ltd. Nanjing</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Fujian Provincial Key Laboratory of Photonics Technology</orgName>
								<orgName type="institution">Fujian Normal University</orgName>
								<address>
									<settlement>Fuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Ocean Information Engineering</orgName>
								<orgName type="institution">Jimei University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pen</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Ocean Information Engineering</orgName>
								<orgName type="institution">Jimei University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">JOYY AI GROUP</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Perceiving and Modeling Density is All You Need for Image Dehazing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the real world, the degradation of images taken under haze can be quite complex, where the spatial distribution of haze is varied from image to image. Recent methods adopt deep neural networks to recover clean scenes from hazy images directly. However, due to the paradox caused by the variation of real captured haze and the fixed degradation parameters of the current networks, the generalization ability of recent dehazing methods on real-world hazy images is not ideal. To address the problem of modeling real-world haze degradation, we propose to solve this problem by perceiving and modeling density for uneven haze distribution.</p><p>We propose a novel Separable Hybrid Attention (SHA) module to encode haze density by capturing features in the orthogonal directions to achieve this goal. Moreover, a density map is proposed to model the uneven distribution of the haze explicitly. The density map generates positional encoding in a semi-supervised way-such a haze density perceiving and modeling capture the unevenly distributed degeneration at the feature level effectively. Through a suitable combination of SHA and density map, we design a novel dehazing network architecture, which achieves a good complexity-performance trade-off.</p><p>The extensive experiments on two large-scale datasets demonstrate that our method surpasses all state-of-the-art approaches by a large margin both quantitatively and qualitatively, boosting the best published PSNR metric from 28.53 dB to 33.49 dB on the Haze4k test dataset and from 37.17 dB to 38.41 dB on the SOTS indoor test dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single image dehazing aims to generate a haze-free image from a hazy image. It is a classical image processing problem, which has been an important research topic in the vision communities within the last decade <ref type="bibr">[22, 27,</ref>   Numerous real-world vision tasks (e.g., object detection and auto drive) require high-quality clean images, and the fog and haze usually lead to degraded images. Therefore, it is of great interest to develop an effective algorithm to recover haze-free images.</p><p>Haze is a common atmospheric phenomenon in our daily life. Images with haze and fog lose details and color fidelity. Mathematically, the image degradation caused by haze could be formulated by the following model:</p><formula xml:id="formula_0">I(x) = J(x)t(x) + A(1 ? t(x))<label>(1)</label></formula><p>where I(x) is the hazy image, J(x) is the clear image, t(x) is the transmission map, A is the global atmospheric light. Methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13]</ref> based on priors first estimate the above two unknown parameters t(x) and A, then inversely solve the above infinitive formula. Unfortunately, these handcrafted priors do not always hold in diverse real-world hazy scenes, resulting in inaccurate estimated t(x).</p><p>Therefore, in recent years, more and more researchers have begun to pay attention to data-driven learning algorithms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref>. Different from traditional methods, deep learning methods achieve superior performance via training on large-scale datasets. However, deep learning based algorithms still has the following challenges: (1) Number of parameters: huge. The previous methods <ref type="bibr">Figure 2</ref>. The Overview of network architecture. The shallow layers is used to make the density map joined with the density estimation module, which consists of a stack of MHAC blocks and a Tail Module. The Deep layers emphasize on detailed reconstruction by Adaptive Features Fusion (AFF) module and Multi-branch Hybrid Attention Block (MHA). We give details of the structure and configurations in Section 3. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28]</ref> boost their performance by increasing the capacity of models which result in a large number of parameters. Lack of consideration of the characteristics of haze also leads to the application failure when encountering practical scenes. (2) Training Strategy: Complex. Complicated loss functions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20]</ref> and fancy training strategies are design to better optimize the reconstruction process. However, these designs lead to high training costs and poor convergence. (3) Speed: slow. Some recent methods <ref type="bibr" target="#b21">[22]</ref> extract the feature maps of hazy images under full resolution in every model stage in order to achieve effective performance, thus slowing the speed in the practical test.</p><p>To address the above problems, we formulate the problem in terms of a perceiving and modeling density for uneven haze distribution. From the Eq.1, we can notice that the haze degradation model is highly associated with absolute position of image pixels. The key to solve the dehazing problem lies in correctly encoding the haze intensity with its absolute position. Therefore, we propose a network to model the density of haze distribution in an end-to-end manner, which encodes the co-relationship between haze intensity and its absolute position. And our method has a powerful ability to restore image details and color fidelity on real domain and synthetic domain images, as seen in <ref type="figure" target="#fig_1">Fig.  1</ref>.</p><p>We propose the method from three different levels: primary block of the network, architecture of the network and map of haze density information to refine features:</p><p>? Primary Block: We propose an efficient attention mechanism to perceive the uneven distribution of degradation of features among channel and spatial dimensions: Separable Hybrid Attention (SHA), which effectively sam-ples the input features through a combination of different pooling operations, and strengthens the interaction of different dimensional information by channel scaling and channel shuffle. Our SHA based on horizontal and vertical encoding can obtain sufficient spatial clue from input features.</p><p>? Density Map: Density Map is a coefficient matrix that encodes the co-relationship between haze intensity and absolute position. The density map explicitly models the intensity of the haze degradation model at corresponding spatial locations. The density map is obtained in an end-to-end matter, semantic information of the scene is also introduced implicitly, which makes the density map more consistent with the actual distribution.</p><p>? Network Architecture: We design a novel network architecture that restores hazy images with the coarse-tofine strategy. The architecture of our network mainly consists of three parts: shallow layers, deep layers and density map. We build the Shallow Layers and Deep Layers of our method based on SHA. The shallow layers will generate the coarse haze-free image, which we call the pseudo-haze-free image. For modeling the uneven degradation of hazy image to refine features explicitly, we utilize the pseudo-haze-free image and the input hazy sample to generate the Density Map.</p><p>Our main contributions are summarized as follows:</p><p>? We propose SHA as a universal attention mechanism perceives efficiently the degenerated density, which can further improve the performance of various image restoration networks.</p><p>? We propose density map as a novel method to build relationship between different parts in the network, which enhances the coupling of our model. In addition, it can be applied in other low-level vision tasks in the future.</p><p>? We propose a novel dehazing method based on perceiving and modeling the haze density by highly efficient SHA and density map. Our method achieves the best performance compared with the state-of-the approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single Image Dehazing</head><p>Single image dehazing is mainly divided into two categories: a prior-based defogging method <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref> and a data-driven method based on deep learning. With the introduction of large hazy datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>, image dehazing based on the deep neural network has developed rapidly. MSBDN <ref type="bibr" target="#b6">[7]</ref> uses the classic Encoder-Decoder architecture, but repeated up-sampling and down-sampling operations result in texture information loss. The number of parameters of MSBDN <ref type="bibr" target="#b6">[7]</ref> is large, and the model is complex. FFA-Net <ref type="bibr" target="#b21">[22]</ref> proposes an FA block based on channel attention and pixel attention, obtains the final haze-free image by fusing features of different levels. With the help of two different-scale attention mechanisms, FFA-Net has obtained impressive PSNR and SSIM, but the entire model is convolutional operation at the resolution of the original image, resulting in a large amount of calculation and slow speed. It is unfriendly to restore large-size hazy images on devices with low memory. AECR-Net <ref type="bibr" target="#b26">[27]</ref> reuses the FA block and proposes to use a novel loss function based on the contrast learning to make full use of hazy samples, and use a deformable convolution block to improve the expression ability of the model, pushing the index on SOTS <ref type="bibr" target="#b15">[16]</ref> to a new high, but memory consumption of the loss function is so high. Compared with AECR-Net <ref type="bibr" target="#b26">[27]</ref>, our model only needs to utilize a simple Charboonier <ref type="bibr" target="#b4">[5]</ref> loss function to achieve higher PSNR and SSIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attention Mechanism</head><p>Attention mechanisms have been proven essential in various computer vision tasks, such as image classification <ref type="bibr" target="#b11">[12]</ref>, segmentation <ref type="bibr" target="#b8">[9]</ref>, dehazing <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b10">11]</ref> and deblurring <ref type="bibr" target="#b28">[29]</ref>. One of the classical attention mechanisms is SENet <ref type="bibr" target="#b11">[12]</ref>, which is widely used as a comparative baseline of the plugin of the backbone network. CBAM <ref type="bibr" target="#b24">[25]</ref> introduces spatial information encoding via convolutions with large-size kernels, which sequentially infers attention maps along the channel and spatial dimension. Later attention mechanism extends the idea of CBAM <ref type="bibr" target="#b24">[25]</ref> by adopting different dimension attention mechanisms to design the advanced attention module. This shows that the key of the performance of attention mechanism is to sample the original feature map fully. There is no effective information exchange between the different dimension attention encoding in before works, thus limiting the lifting of networks. In response to the above problems, we utilize two types of pooling operations to sample the original feature map and cleverly inserted the channel shuffle block in our attention module. Experiments demonstrate that the performance of our attention mechanism has been dramatically improved due to sufficient feature sampling and efficient feature exchange among different dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We propose a novel architecture as shown in <ref type="figure">Fig.2</ref>, which consists of three parts: shallow layers, deep layers and density map. Shallow layers are responsible for reconstruct high-level contextual content, and deep layers are responsible for rebuilding pixel-level detailed features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Separable Hybrid Attention Module</head><p>Attention mechanisms based on pooling operations have been applied in many visual tasks and have improved indicators, but previous attention mechanisms such as CBAM <ref type="bibr" target="#b24">[25]</ref>, and DANet <ref type="bibr" target="#b8">[9]</ref> often calculate the attention weights subsequently in the spatial and channel dimensions. But there is no useful information exchanging between the attention of different dimensions. Non-local <ref type="bibr" target="#b23">[24]</ref> and Tri-linear Attention <ref type="bibr" target="#b29">[30]</ref> have interactive feature-capability but the calculation is very large. So we propose a novel attention mechanism called Separable Hybrid Attention (SHA), which can effectively generate the fine-grained attention weights to guidance the degenerated density perception. Specifically, the combination of average pooling and maximum pooling not only smoothens the noise in features but also effectively enhances high-frequency information of features. So we utilize the average pooling (AvgPool) and maximum pooling (MaxPool) calculate in the feature F in along the horizontal and vertical directions to obtain the encoding features v. The formulas are as follows: </p><formula xml:id="formula_1">v h avg = AvgPool h (F in ), v v avg = AvgPool v (F in ) (2) v h max = MaxPool h (F in ), v v max = MaxPool v (F in )<label>(3)</label></formula><formula xml:id="formula_2">v h = v h avg + v h max , v v = v v avg + v v max<label>(4)</label></formula><p>We concatenate the encoded features and utilize the channel shuffle to interaction channel information, aiming to exchange the encoding information between different channels. Then utilizing the 1x1 convolution to reduce the dimension of features, which pass through the nonlinear activation function, so that the features of different channels are fully interactive. The formulas are as follows:</p><formula xml:id="formula_3">[y h c/r , y v c/r ] = ?(Conv(cat([v h c , v v c ])))<label>(5)</label></formula><p>Wherein Eq.5, the ? is ReLU6 activation function, c is the dimensions number and r is the channel scaling factor, r is usually 4. We use the shared 3x3 convolution to restore the number of channel dimensions for encoded features of different directions, making the isotropic features get similar attention weight values. The final weights can be determined by the larger receptive field of the input features.</p><formula xml:id="formula_4">y h c = Conv(y h c/r ), y v c = Conv(y v c/r )<label>(6)</label></formula><p>Our experiments demonstrate that the 3x3 convolution will help generate a more accurate attention weight matrix. After restoring the channel of the feature, multiply the y h c and y w c to obtain the attention weight matrix with the same size as the input feature. Finally, a Sigmoid function works to get the attention map W c?h?w :</p><formula xml:id="formula_5">W c?h?w = Sigmoid(y h c ? y v c )<label>(7)</label></formula><p>We multiply the attention weight matrix W c?h?w with the input feature F in to get the output feature F out :</p><formula xml:id="formula_6">F out = W c?h?w ? F in<label>(8)</label></formula><p>The entire SHA module calculates the attention weight matrix and the process of applying attention weight to the input features, the overview as shown in <ref type="figure" target="#fig_2">Fig.3</ref>. Compared with the previous attention mechanism for image dehazing, our method is less computationally and more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Shallow Layers</head><p>The shallow layers is stacked by several Multi-branch Hybrid Attention with COT Block and the Encoding-Decoding context module. We use the shallow layers to generate the pseudo-haze-free image, which has high-level semantic information.</p><p>Multi-branch Hybrid Attention. We design the Multibranch Hybrid Attention (MHA) Block, mainly consisting of the SHA module with parallel convolution. The multibranch design will improve the expressive ability of the network by introducing multi-scale receptive-filed. The multibranch block comprises parallel 3x3 convolution, 1x1 convolution, and a residual connection, as shown in <ref type="figure" target="#fig_5">Fig.7 (a)</ref>. The degradation is often uneven in space in degraded images, such as hazy images and rainy images, and the spatial structure and color of some areas in the picture are not affected by the degradation of the scene, so we set the local residual learning to let the feature pass the current block directly without any process, which also avoids the disappearance of the gradient.</p><p>Adaptive Features Fusion Module. We hope that the network can adjust the proportion of feature fusion adaptively according to the importance of different information. Different from the MixUP <ref type="bibr" target="#b26">[27]</ref> module in AECR-Net, we utilize the Adaptive Features Fusion Module to combine two different blocks. The formula is as follows:</p><formula xml:id="formula_7">F out = AFF(block1, block2) = ?(?) * block1 + ?(1 ? ?) * block2<label>(9)</label></formula><p>Wherein, block denotes the block module, which has the same output size, ? is the Sigmoid activation function, and  ? is a learnable factor.</p><p>Multi-branch Hybrid Attention with CoT. Long-range dependence is essential for feature representation, so we introduce an improved CoT <ref type="bibr" target="#b16">[17]</ref> block combined with the MHA block in the shallow layers to mine the long-distance dependence of sample features that further expand the receptive field. Specifically, we design a parallel mix block that uses the MHA block and the improved CoT block to capture local features and global dependencies simultaneously. To fusion the attention result, an Adaptive Features Fusion module is followed back as shown in <ref type="figure" target="#fig_5">Fig.7 (b)</ref>, we call this MHAC block. The formulas are as follows:</p><formula xml:id="formula_8">F out = AFF(MHAB(F in ), CoT(F in )) = ?(?) * (MHAB(F in ) + ?(1 ? ?) * CoT(F in )<label>(10)</label></formula><p>The F in denotes the input features, F out is the adaptive mixing results from MHA and CoT block. Considering that the BN layer will destroy the internal features of the sample, we use the IN layer to replace the BN layer in the improved CoT Block and use the ELU as the activation function. The basic block of shallow layers.</p><p>Tail Module. As shown in <ref type="figure" target="#fig_5">Fig.7(c)</ref>, we design the Tail module, which fusions the extracted features and restores the hazy image. The tanh activation function is often used on degradation reconstructions. So we use that as the activation of the output after a stack of 3x3 convolution.</p><p>Architecture of Shallow Layers. As shown in <ref type="figure">Fig.2</ref>, we use the shallow layers to reconstruct the degraded image context content. In order to effectively reduce the amount of calculation and expand the receptive field of the convolution of MHAC, we utilize 2 convolutions with a stride of 2 to reduce the resolution of the feature map to 1/4 of the original input firstly; each convolution follows a SHA module. Then use a stack of 8 MHAC blocks with 256 channels, which have 2 skip connections to introduce shallow features before up-sampling, and utilize the Tail module for obtaining the residual of the restored image of the shallow layers:</p><formula xml:id="formula_9">S(x) = Shallowlayers(x) + x<label>(11)</label></formula><p>Wherein the S(x) denotes the pseudo-haze-free image, the x denotes the hazy input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Density Map</head><p>To alleviate the problem of uneven haze distribution in the spatial position, we adopt the density map to perceive the implicit spatial correlation between pseudo-haze-free images generated by shallow layers and hazy images.</p><p>Like all classical deep learning methods, our approach adopts convolutions to encode input to high-level feature representations that map the position information to the activation area onto high-level feature maps. Unlike the classical deep learning methods that only utilize information from the hazy image, the proposed density map is based on fully exploring the correlation between the pseudo-haze-free image and the hazy image.</p><p>As is depicted in <ref type="figure" target="#fig_4">Fig. 5</ref>, it is obvious that shallow convolution network has the ability to reconstruct a pseudo hazefree image. But it's weak to model complex spatial distribution that is disturbed by contextual information variation, which leads to an inaccurate estimation of the degradation model.</p><p>Since hand-crafted prior may not model spatial corelationship between image pairs effectively, we adopt a simple convolution network to estimate a density map that shares the same spatial dimensions as input. As the illustration in <ref type="figure" target="#fig_3">Fig. 4</ref>, Density Estimation Module splices the pseudo-haze-free image and the hazy sample of the shallow layers in the channel dimension. We expand the features to 64 channels using the 3x3 convolution after utilizing the Reflected Padding to avoid the detail loss of the feature edge. And then utilize the SHA module to explore perceiving the uneven degeneration of input features fully, finally using a convolution operation to compress the shape of the feature. The sigmoid function is used to get the density map M ? R 1?H?W .After getting the density map M , we multiply M by the input feature F in ? R C?H?W to get the final output F out ? R C?H?W :</p><formula xml:id="formula_10">F out = F in ? M<label>(12)</label></formula><p>As is depicted in <ref type="figure" target="#fig_4">Fig. 5 and Fig. 6</ref>, the visualization of density map clearly demonstrate that our density map effectively model the uneven haze distribution spatially. The diff <ref type="figure">Figure 6</ref>. Visual comparisons on real-world hazy images, the diff map is the difference between the pseudo-haze-free and hazy images. It's worth noting that our density map effectively models the density of the degradation spatially compared with the diff map. map is the numerical difference between the pseudo-hazefree and haze input images, which can be considered a degenerated layer of the hazy veil. The intensity of the density map changes with the depth of scenes and hazy distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Deep layers</head><p>A lot of degraded image restoration methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b6">7]</ref> often use the fully Encoder-Decoder structure to learn a large receptive fields. However, repeated down-sampling and up-sampling operations can easily cause the loss of texture details, which will greatly affect the visual perception of the image and the natural effect, so we utilize the deep layers to repair the pixel-level texture details with the supervised signal from density map. As shown in <ref type="figure">Fig. 2</ref>, we utilize 10 MHA blocks with 16 channels to extract features at the resolution of the original input. In order to avoid unnecessary calculations caused by repeated extraction of features, we use the AFF module to introduce and mix features refined by our density map from the shallow layers adaptively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>We chose the PSNR and SSIM as experimental metrics to measure the performance of our SHA-Net.We train on two large synthetic datasets, RESIDE <ref type="bibr" target="#b15">[16]</ref> and Haze4k <ref type="bibr" target="#b19">[20]</ref>, and testing on SOTS <ref type="bibr" target="#b15">[16]</ref> and Haze4k <ref type="bibr" target="#b19">[20]</ref> testing sets, respectively. The indoor training set of RESIDE <ref type="bibr" target="#b15">[16]</ref> contains 1,399 clean image and 13,990 hazy images generated by corresponding clean images. The indoor testing set of SOTS contains 500 indoor images. The training set of Haze4k <ref type="bibr" target="#b19">[20]</ref> contains 3,000 hazy images with ground truth images and the testing set of Haze4k <ref type="bibr" target="#b19">[20]</ref> contains 1,000 hazy images with ground truth images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Settings</head><p>We augment the training dataset with randomly rotated by 90,180,270 degrees and horizontal flip. The training image patches with the size 256 ? 256 are extracted as input I in of our network. The network is trained for 7.5 ? 10 5 , 1.5 ? 10 6 steps on Haze4k <ref type="bibr" target="#b19">[20]</ref> and RESIDE <ref type="bibr" target="#b15">[16]</ref> respectively.We use Adam optimizer with initial learning rate of 2 ? 10 ?4 , and adopt the CyclicLR to adjust the learning rate, where on the triangular mode, the value of gamma is 1.0,base momentum is 0.8, max momentum is 0.9, base learning rate is initial learning rate and max learning rate is 3 ? 10 ?4 . PyTorch <ref type="bibr" target="#b20">[21]</ref> was used to implement our models with 4 RTX 3080 GPU with total batchsize of 40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Loss Function</head><p>We only use Charbonnier loss <ref type="bibr" target="#b4">[5]</ref> as our optimization objective:</p><formula xml:id="formula_11">L(?) = L char (S(x), J gt (x))+L char (D(x), J gt (x))) (13)</formula><p>Where ? denotes the parameters of our network, the S(x) denotes the pseudo-haze-free image, D(x) denotes the output of deep layers, which is the final output image, J gt stands for ground truth, and L char is the Charbonnier loss <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_12">L char = 1 N N i=1 X i ? Y i 2 + 2<label>(14)</label></formula><p>with constant emiprically set to 1e ? 3 for all experiments. To demonstrate the effectiveness of the proposed method, we conduct ablation study to analyze different elements, including SHA, CoT, AFF and different configurations of our network. We first construct our base Network as the baseline of shallow layers of dehazing network, which lack SHA, CoT, and AFF. Subsequently, we add the different modules into the baseline as: <ref type="formula" target="#formula_0">(1)</ref>  We employ the Charbonnier loss <ref type="bibr" target="#b4">[5]</ref> as training loss function for ablation study, and utilize Haze4k <ref type="bibr" target="#b19">[20]</ref> dataset for both training and testing. The performance of above models are summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Separable Hybrid Attention. Separable Hybrid</head><p>Attention module significantly improves the performance from Base to Base+SHA with an increase of 1.99 dB PSNR. Therefore, SHA is a significant component due to the highperformance gain. We also evaluate the performance of the FA <ref type="bibr" target="#b21">[22]</ref> module with the base model as show in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>We compared the Flops and Parameters of SHA module with other universal and dehazing attention modules, such as the Feature Attention module <ref type="bibr" target="#b21">[22]</ref> used in dehazing networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>, and the common CBAM <ref type="bibr" target="#b24">[25]</ref>,as shown in <ref type="table" target="#tab_2">Table 2</ref>.Compared with other attention module used in image dehazing networks, the Flops of our SHA module is lower.</p><p>The result demonstrates that our attention mechanism has better performance because of the excellent feature coding mechanism.  <ref type="table" target="#tab_3">Table 3</ref>, we demonstrate that the combination of AvgPool and MaxPool is nontrivial for the performance of attention mechanism we proposed. The Channel Shuffle operation boost the encoding information exchanging among different channels, which is essential for an efficient and effective attention mechanism. We also verify the suitable kernel size of the last convolution of SHA, as shown in <ref type="table" target="#tab_3">Table 3</ref>; we choose the 3x3 convolution as the final processing operation for parameterperformance trade-off. Effect of Deep Layers and Density Map. We verify the effect of deep layers of our method. As described in section 3.4, the deep layers effectively relieve the texture detail loss by repeated sampling. The deep layers significantly improve the model performance with an increase of 3.05 dB PSNR, as shown in <ref type="table" target="#tab_4">Table 4</ref>. The result demonstrates that the feature extracted processing with the origi- nal image resolution is essential for pixel-level texture detail restoration. And we also verify the effect of density map we proposed, as shown in <ref type="table" target="#tab_4">Table 4</ref> and <ref type="figure">Fig.6</ref>. It's worth noting that our density map effectively predicts the hazy density of hazy input, which models the uneven distribution of hazy degradation and improves the performance with an increase of 3.16 dB PSNR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Compare with SOTA Methods</head><p>Visual Comparison. To validate the superiority of our method, as shown in <ref type="figure" target="#fig_7">Fig.8</ref>, firstly we compare the visual results of our method with previous SOTA methods on synthetic hazy images from Haze4k <ref type="bibr" target="#b19">[20]</ref> dataset. It can be seen that the other methods are not able to remove the haze in all the cases, while the proposed method produced results close to the real clean scenes visually. Our method is superior in the recovery performance of image details and color fidelity. Please refer to the supplementary materials for more visual comparisons on the synthetic hazy images and real-world hazy images.</p><p>Quantitative Comparison. We compare quantitatively the dehazing results of our SHA-Net with SOTA single image dehazing methods on Haze4k <ref type="bibr" target="#b19">[20]</ref> and SOTS <ref type="bibr" target="#b15">[16]</ref> datasets. As shown in <ref type="table" target="#tab_5">Table 5</ref>, the SHA-Net outperforms all SOTA methods, achieving 33.49dB PSNR and 0.98 SSIM on Haze4k <ref type="bibr" target="#b19">[20]</ref>. It increases the PSNR by 4.93dB compared to the second best method. On SOTS <ref type="bibr" target="#b15">[16]</ref> indoor test set, the SHA-Net also outperforms all SOTA methods, achieving 38.41dB PSNR and 0.99 SSIM. It increases the PSNR by 1.24dB, compared to the second best method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a powerful image dehazing method to recover haze-free images directly. Specifically, the Separable Hybrid Attention is design to better perceive haze density and a density map is to further refine extracted features. Although our method is simple, it is superior to all the previous state-of-art methods with a very large margin on two large-scale hazy datasets. Our method has a powerful advantage in the restoration of image detail and color fidelity. We hope to further promote our method to other low-level vision tasks such as deraining, super-resolution, denosing and desnowing.</p><p>Limitations: The proposed method recovers high-fidelity haze-free images. As is shown in <ref type="figure" target="#fig_7">Fig.8</ref>, haze free images are usually in low-light mode, which is not visually pleasant in real-life scenes. Following the main idea of this work, future research can be made in various aspects to generate high-quality images with pleasant visual perceptions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>2, 1]. * Equal contribution. ? Corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Examples of our image dehazing. Hazy images are in the top row, of which (a) and (b) are from synthetic hazy dataset haze4k [20], (c) and (d) are Real-world hazy images from web. The corresponding dehazed results are in the bottom row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An illustration of the Separable Hybird Attention Module (SHA). Our SHA focuses on the directional embedding, which consists of unilateral directional pooling and convolution. The @k denotes the convolutuion kernel size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>An illustration of the pipeline that generates a density map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>An illustration of the histogram of the diff map and density map. The diff map is the numerical difference between the pseudo-haze-free image and haze images. The value of the density map is mapped from [0,1] to [0,255]. The diff map and density map are visualized by ColorJet for observation and comparison. Note that the histograms of the diff map and density map have similar intensity distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>An illustration of the basic modules of the network. (a) The Multi-branch Hybrid Attention Block (MHAB). (b) The MHAC block of shallow layers. (c) The Tail Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>base+SHA: Add the Separable Hybrid Attention module into baseline. (2) base+FA: Add the Feature Attention [22] module into baseline. (3) base+SHA+CoT: Add both SHA and CoT operation into baseline. (4) base+SHA+CoT+AFF: The complete combination of our shallow layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Visual comparisons on dehazed results of various methods on synthetic hazy images and our density map, the images from haze4k<ref type="bibr" target="#b19">[20]</ref> test dataset. The images are best viewed in the full-screen mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We get the direction encode features v h and v v from Eq.2 add Eq.3 correspondingly, so we can make the distribution of features like a normal distribution, which can better reinforce the important information of input features:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons on Haze4k testset for different configurations of Shallow Layers.</figDesc><table><row><cell>Model</cell></row><row><cell>Base</cell></row><row><cell>+FA</cell></row><row><cell>+SHA</cell></row><row><cell>+CoT</cell></row><row><cell>+AFF</cell></row><row><cell>PSNR 24.40 25.30 26.39 26.84 27.28</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The Flops and Params comparison of universal attention mechanism and dehazing network. The SWRCA is the attention module of KDDN<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table><row><cell>Attention</cell><cell cols="2">Flops(M) Params</cell></row><row><cell>SE[12]</cell><cell>4.195</cell><cell>512</cell></row><row><cell>ECA[23]</cell><cell>4.195</cell><cell>3</cell></row><row><cell>CBA[25]</cell><cell>10.619</cell><cell>1.122K</cell></row><row><cell>FA[22]</cell><cell>38.864</cell><cell>1.625K</cell></row><row><cell cols="3">SWRCA[11] 2424.311 41.088K</cell></row><row><cell>SHA(Ours)</cell><cell>15.29</cell><cell>5.192K</cell></row><row><cell cols="3">Study on Separable Hybrid Attention. We verify the ef-</cell></row><row><cell cols="3">fect of main operations of Separable Hybrid Attention we</cell></row><row><cell>proposed. As shown in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparisons on Haze4k testset for different configurations of Separable Hybrid Attention.</figDesc><table><row><cell>AvgPool</cell><cell></cell><cell></cell></row><row><cell>MaxPool</cell><cell></cell><cell></cell></row><row><cell>Channel Shuffle</cell><cell></cell><cell></cell></row><row><cell>PSNR</cell><cell cols="2">28.28 31.16 33.49</cell></row><row><cell>Conv@k1</cell><cell></cell><cell></cell></row><row><cell>Conv@k3</cell><cell></cell><cell></cell></row><row><cell>PSNR</cell><cell>32.21 33.49</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparisons on Haze4k testset for different configurations of our methods.</figDesc><table><row><cell>Shallow Layers</cell><cell></cell></row><row><cell>Deep Layers</cell><cell></cell></row><row><cell>density map</cell><cell></cell></row><row><cell>PSNR</cell><cell>27.28 30.33 33.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Quantitative comparisons of our models with the state-ofthe-art dehazing methods on Haze4k<ref type="bibr" target="#b19">[20]</ref> and SOTS<ref type="bibr" target="#b15">[16]</ref> datasets (PSNR(dB)/SSIM). Best results are underlined.</figDesc><table><row><cell>Method</cell><cell cols="4">Haze4k [20] PSNR? SSIM? PSNR? SSIM? SOTS [16]</cell></row><row><cell>DCP [10]</cell><cell>14.01</cell><cell>0.76</cell><cell>15.09</cell><cell>0.76</cell></row><row><cell>NLD[3]</cell><cell>15.27</cell><cell>0.67</cell><cell>17.27</cell><cell>0.75</cell></row><row><cell>DehazeNet [4]</cell><cell>19.12</cell><cell>0.84</cell><cell>20.64</cell><cell>0.80</cell></row><row><cell>AOD-Net [15]</cell><cell>17.15</cell><cell>0.83</cell><cell>19.82</cell><cell>0.82</cell></row><row><cell>GDN [19]</cell><cell>23.29</cell><cell>0.93</cell><cell>32.16</cell><cell>0.98</cell></row><row><cell>MSBDN [7]</cell><cell>22.99</cell><cell>0.85</cell><cell>33.79</cell><cell>0.98</cell></row><row><cell>FFA-Net [22]</cell><cell>26.96</cell><cell>0.95</cell><cell>36.39</cell><cell>0.98</cell></row><row><cell>AECR-Net [27]</cell><cell>-</cell><cell>-</cell><cell>37.17</cell><cell>0.99</cell></row><row><cell>DMT-Net [20]</cell><cell>28.53</cell><cell>0.96</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>33.49</cell><cell>0.98</cell><cell>38.41</cell><cell>0.99</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2020 challenge on nonhomogeneous dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Codruta O Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florin-Alexandru</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Vasluianu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="490" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ntire 2021 nonhomogeneous dehazing challenge report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Codruta O Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florin-Alexandru</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Vasluianu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="627" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dehazenet: An end-to-end system for single image haze removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunmei</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5187" to="5198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two deterministic half-quadratic regularization algorithms for computed imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Blanc-Feraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st International Conference on Image Processing</title>
		<meeting>1st International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast deep multi-patch hierarchical network for nonhomogeneous image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipta</forename><surname>Sourya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dutta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="482" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-scale boosted dehazing network with dense feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2157" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on graphics (TOG)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distilling image dehazing with heterogeneous task imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image dehazing using adaptive bi-channel priors on superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image haze removal algorithm using color attenuation prior and multiscale fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krati</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Verma</surname></persName>
		</author>
		<idno>2016. 3</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aod-net: All-in-one dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiulian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4770" to="4778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Benchmarking singleimage dehazing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="492" to="505" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.12292</idno>
		<title level="m">Contextual transformer networks for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Trident dehazing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="430" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Griddehazenet: Attention-based multi-scale network for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongrui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7314" to="7323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">From synthetic to real: Image dehazing collaborating with unlabeled real data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunda</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02934</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ffa-net: Feature fusion attention network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Xu Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11908" to="11915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eca-net: efficient channel attention for deep convolutional neural networks, 2020 ieee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Banggu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge transfer dehazing network for nonhomogeneous dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contrastive learning for compact single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10551" to="10560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A two-branch neural network for nonhomogeneous dehazing via ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-stage progressive image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
