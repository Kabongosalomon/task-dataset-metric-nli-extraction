<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hard Sample Aware Noise Robust Learning for Histopathology Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkai</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Mulan</roleName><forename type="first">Ying</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
						</author>
						<title level="a" type="main">Hard Sample Aware Noise Robust Learning for Histopathology Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Image classification</term>
					<term>noisy labels</term>
					<term>hard sample aware</term>
					<term>self-training</term>
					<term>label correction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning-based histopathology image classification is a key technique to help physicians in improving the accuracy and promptness of cancer diagnosis. However, the noisy labels are often inevitable in the complex manual annotation process, and thus mislead the training of the classification model. In this work, we introduce a novel hard sample aware noise robust learning method for histopathology image classification. To distinguish the informative hard samples from the harmful noisy ones, we build an easy/hard/noisy (EHN) detection model by using the sample training history. Then we integrate the EHN into a self-training architecture to lower the noise rate through gradually label correction. With the obtained almost clean dataset, we further propose a noise suppressing and hard enhancing (NSHE) scheme to train the noise robust model. Compared with the previous works, our method can save more clean samples and can be directly applied to the real-world noisy dataset scenario without using a clean subset. Experimental results demonstrate that the proposed scheme outperforms the current stateof-the-art methods in both the synthetic and real-world noisy datasets. The source code and data are available at https://github.com/bupt-ai-cz/HSA-NRL/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C ANCER is a serious threat to people's life and health.</p><p>The studies <ref type="bibr" target="#b0">[1]</ref> confirmed that the early screening of cancer is crucial for enhancing the survival rate. The pathological examination is the golden standard of early cancer detection, which can determine the tissue source, nature, and scope of the tumor relying on the visual observation of pathologists. However, there are still many challenges to overcome. During the actual diagnosis process, pathologists analyze the overall tissue along with nuclei organization, density, and variability, which requires tedious workloads. The diagnosis accuracy can be negatively affected by many factors, such as pathologist fatigue and distraction, and the complexity of the tissue structure <ref type="bibr" target="#b1">[2]</ref>.</p><p>The first two authors contributed equally to this work. This work was supported in part by the National Natural Science The deep learning (DL) techniques, such as convolutional neural networks (CNNs) have been widely used in the fields of histopathology image analysis <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. These efforts are designed to help physicians in improving the accuracy and promptness of cancer diagnosis. One typical task in the field of histopathology image analysis is image classification. Several works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> are developed to build a deep learning model to classify the histopathology images. However, all these works assume that the utilized dataset was clean for model training.</p><p>Actually, it is very expensive and difficult to collect a large dataset with clean labels <ref type="bibr" target="#b6">[7]</ref>. In the real medical diagnosis scenarios, noisy labels are often inevitable in manual annotation due to the following reasons: 1) expert domain knowledge is required to perform labeling; 2) manual annotation suffers from large intra-and inter-observer variability even among experts; 3) it is time-consuming and tedious to annotate a large number of patches. Therefore, designing robust algorithms with noisy labels is of great significance <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the literature, a lot of approaches were proposed, which can generally be classified into three categories: estimating the noise transition matrix <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, designing noise-robust loss functions <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>, and sample correcting/selecting <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b19">[20]</ref>. The schemes based on transition matrix estimating try to capture the transition probability between the noisy label and true label <ref type="bibr" target="#b17">[18]</ref>. Different transition matrix estimation methods were proposed in work <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, such as using additional softmax layer <ref type="bibr" target="#b8">[9]</ref>, utilizing trusted samples in a data-efficient manner <ref type="bibr" target="#b9">[10]</ref>, and two-step estimating scheme <ref type="bibr" target="#b10">[11]</ref>. However, these transition matrix estimations fail in real-world datasets where the utilized prior assumption is no longer valid <ref type="bibr" target="#b17">[18]</ref>. Being free of transition matrix estimation, the second category targets designing loss functions that have more noise-tolerant power. Work <ref type="bibr" target="#b11">[12]</ref> adopted mean absolute error (MAE) function which demonstrates more noise robust ability than crossentropy loss. In work <ref type="bibr" target="#b12">[13]</ref>, the authors combined Generalized Cross Entropy (GCE) loss and MAE to address the slow convergence speed of work <ref type="bibr" target="#b11">[12]</ref>. Recently, the authors in work <ref type="bibr" target="#b13">[14]</ref> proposed determinant-based mutual information loss which can be applied to any existing classification neural networks regardless of the noise pattern. However, this kind of method suffers generalization performance loss due to the low quality of the validation sets <ref type="bibr" target="#b20">[21]</ref>.</p><p>The third category targets correcting noisy labels or selecting the possibly clean samples. Bootstrap in work <ref type="bibr" target="#b14">[15]</ref> adopted the predicted correcting labels together with the raw labels to lower the interference from the noisy samples. In work <ref type="bibr" target="#b15">[16]</ref>, the authors utilized some clean annotations to reduce the noise in the large dataset. However, it is difficult to obtain the required certain amount of clean data in some cases. In work <ref type="bibr" target="#b16">[17]</ref>, a joint optimization framework was proposed to gradually estimate the true labels. The self-learning framework was applied to train the label correction network without extra supervision <ref type="bibr" target="#b17">[18]</ref>. Some works just selected the clean data by dropping the noisy data directly to avoid the estimation of true labels. Co-teaching has appeared in the literature recently <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, which trains two deep neural networks simultaneously and makes them teach each other by selecting some data of possibly clean labels. Compared with the first two types of methods, the third category is more general and can be integrated into many image classification tasks.</p><p>Although many studies are proposed to suppress the noisy labels for the general image classification problem, there are few works on the classification of noised histopathology images. Work <ref type="bibr" target="#b21">[22]</ref> proposed online uncertainty sample mining and individual re-weighting methods to train their network. In work <ref type="bibr" target="#b22">[23]</ref>, a double-softmax classification module was adopted to prevent overfitting the noisy labels and a teacherstudent module was used to strengthen the effect of clean labels. Unfortunately, almost all these approaches fail in distinguishing informative hard samples from harmful mislabeled ones. Although the work in <ref type="bibr" target="#b21">[22]</ref> realized the significance of hard samples, their algorithm still didn't separate hard samples from noisy ones, and thus many important hard samples were mistakenly discarded <ref type="bibr" target="#b23">[24]</ref>.</p><p>On the one hand, the hard samples can make training more effective and efficient <ref type="bibr" target="#b24">[25]</ref>. On the other hand, the deep neural networks can easily overfit to some label noise, and thus cause performance degradation <ref type="bibr" target="#b25">[26]</ref>. How to involve hard samples for training while reducing noise interference at the same time is of great significance. Many works found that the deep models first memorize the easy training data with clean labels and then memorize the hard or noisy data <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. This phenomenon can be used to distinguish the easy clean data from the noise, however how to distinguish the hard clean data from the noise is still not clear.</p><p>In this work, we strive to reconcile this gap by proposing a hard sample aware noise robust learning algorithm. Our analysis reveals that the prediction history for each sample can be used as guiding information for distinguishing the hard and noisy samples. A deep model for hard and noisy sample detection is thus designed and integrated into our noisy label correction architecture. In the architecture, self-training is applied to conduct the label correction automatically. Based on the corrected data, the noise suppressing and hard enhancing (NSHE) scheme is designed to further enhance the hard sample and weaken the possible noisy sample. Our key contributions are summarized as follows.</p><p>? We proposed a two-phase hard sample aware noise robust learning algorithm for histopathology image classification. Our method can save more clean samples by detecting the hard sample and noise in label correction phase. The hard samples can be further enhanced in our NSHE phase. ? We built an EHN (easy/hard/noisy) detection scheme and integrated it into our self-learning label correction flow. We found that hard and noisy samples can be recognized using sample prediction history. Different from the previous works, our scheme can save more hard samples and discard more noisy samples. ? In the NSHE phase, we smoothly trained our model to further suppress the interference from noisy samples and enhance the hard samples based on our proposed colearning architecture. ? Our proposed method can be directly applied to the real-world dataset without using clean annotations. The experimental results verify that the proposed algorithm can achieve superior performance in our collected clinical pathology data of one top hospital in Beijing, China.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Noisy Label Correction</head><p>Noisy label correction aims at improving the quality of the raw data by replacing the noisy labels with their true labels. Generally, the true labels are predicted by an extra model that is trained on a subset of clean data, such as work <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b15">[16]</ref>. However, for the real-world dataset, the required clean dataset is not available and thus these methods will fail in this case.</p><p>To get rid of the dependence on clean samples, several noisy label correction methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> were proposed based on self-learning by pseudo-labels. In fact, pseudo-labeling is a type of self-training which is often used in semi-supervised learning scenario <ref type="bibr" target="#b29">[30]</ref> with many unlabeled data <ref type="bibr" target="#b16">[17]</ref>. In the semi-supervised learning scenario, the pseudo-labels are initially assigned to unlabeled data by predictions of a model that is trained on a labeled subset. This process is repeated and pseudo-labels are thus updated gradually. However, when processing the noisy datasets with labels, the challenge comes from the uncertainty about what is correct and what is incorrect in the data. In work <ref type="bibr" target="#b16">[17]</ref>, the authors replaced all the labels with pseudo-labels to improve the quality of the original noisy dataset. The authors in work <ref type="bibr" target="#b17">[18]</ref> proposed a self-learning with multi-prototypes (SMP) scheme to train a robust model on the real-world noisy data.</p><p>In the above self-learning based methods, all the pseudo labels are involved in the correction model training. However, misleading samples are inevitable, which will thus ruin the performance of the obtained correction model. To address this problem, we proposed an EHN detection scheme based on the prediction history of each sample to recognize the possible noisy samples. Then, we will remove these noisy samples in the correction model training and thus improve the label correction quality. Note that the output of EHN is also used in guiding the noisy sample discarding module (post-processing), which can save more hard samples for our NSHE scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning to Teach</head><p>Learning to teach refers to the schemes that consist of two networks, the teacher and student networks <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The teacher tries to choose more informative samples to guide the training of student networks. However, these methods cannot process the dataset with noisy labels.</p><p>To make the above learning to teach algorithms be capable of processing noisy data, the authors in work <ref type="bibr" target="#b32">[33]</ref> proposed a novel MentorNet to supervise the training of the student network by focusing on the probably correct samples. However, the designed MentorNet suffers the disadvantage of accumulating error introduced by the sample-selection bias <ref type="bibr" target="#b18">[19]</ref>. Another method called Decoupling proposed by work <ref type="bibr" target="#b33">[34]</ref> trains two models simultaneously and updates the models by sampling with different predictions. However, in the selected subset with disagreement labels, there are still some noisy ones, which will decrease the performance of the trained model <ref type="bibr" target="#b18">[19]</ref>. To solve these problems, based on Co-training of work <ref type="bibr" target="#b34">[35]</ref>, the authors in <ref type="bibr" target="#b18">[19]</ref> proposed a learning scheme called Co-teaching, which can train the model successfully even in the extremely noised dataset. Co-teaching also includes two models, and each model adopts the samples with small losses to train its peer network. Through the prediction information exchange, the error flows can be reduced accordingly. The authors in <ref type="bibr" target="#b19">[20]</ref> tried to improve the performance by proposing an Iterative Noisy Cross-Validation (INCV) method with the seriously noised dataset.</p><p>The above Co-teaching based methods try to conduct noiserobust learning by selecting clean samples with a small loss as much as possible. However, both hard and noisy samples have a large loss, and this will inevitably ignore the informative hard samples. We attempt to solve the problem in two aspects. First, we pre-discard most of the noisy samples with our hard-sample aware post-processing module. Second, we enhance most of the hard samples while suppressing the few existing noisy ones at the same time with our NSHE scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>A. Architecture   The top architecture contains two main phases: 1) label correction, 2) NSHE scheme, as depicted by <ref type="figure" target="#fig_2">Fig. 1</ref>. The label correction takes the noisy data as the input and generates almost clean data. The NSHE scheme takes the corrected almost-clean dataset as the input and produces the final robust classification model.</p><p>For the label correction, we designed a hard sample aware self-learning to achieve high-quality pseudo-labels and further cleaned our dataset by post-processing to drop out the possibly noisy samples for NSHE. The target of the label correction phase is to restore as many clean samples as possible. After performing label correction, the NSHE scheme aims at further reducing the impact of noisy and emphasizing the hard samples at the same time. Specifically, we smoothly trained our model based on the proposed co-learning architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Label Correction</head><p>The proposed label correction architecture mainly consists of the classification/correction model, easy/hard/noisy (EHN) detection scheme, and post-processing component.</p><p>Firstly, the classification model is trained based on noisy data. By using this model, the prediction behavior for all the training samples can be obtained. Note that the prediction behavior means the prediction history for one sample through all the k (such as 30) training epochs. Then, the EHN detection scheme is applied to divide the dataset into three parts: easy, hard, and noisy. With the obtained easy and hard samples, the correction model is trained and used to correct the noisy data. Through repeating the above flow, the dataset quality is thus improved gradually. Finally, the dataset is further filtered by getting through the hard-aware post-processing component. In the following section, we will focus on the details of the EHN detection scheme, correction model, and post-processing component.</p><p>EHN Detection Scheme. Previous work <ref type="bibr" target="#b26">[27]</ref> showed that CNNs tend to memorize simple samples first, and then the networks can gradually learn all the remaining samples, even including the noisy samples, due to the high representation capacity. However, overfit to the noise leads to poor generalization performance. To avoid the memorization of noisy data, work <ref type="bibr" target="#b32">[33]</ref> selected the samples with small loss to train the model, where such samples are treated as clean ones.</p><p>Sample with small loss means the prediction probability of the model output is closer to the supervising label. However, the normalized probability is much easier to analyze than the loss value. Different from work <ref type="bibr" target="#b32">[33]</ref>, we apply the mean prediction probability value of the sample training history in our EHN detection scheme. <ref type="figure" target="#fig_3">Fig. 2</ref> shows the mean prediction probability histogram of clean and noisy samples. The figure shows most of the clean samples have higher mean prediction probabilities than the noisy ones. Therefore, we can set a threshold (such as the red dotted line in <ref type="figure" target="#fig_3">Fig. 2</ref>) to preliminarily extract some clean samples, and we call these clean samples bigger than the threshold as easy samples. However, there's still a part of clean samples that's behind the threshold, and we can't distinguish them from noisy ones. We define this part of clean data as hard samples in our work. So far as we know, there are no existing schemes that can distinguish the hard from the noisy samples.</p><p>We constructed our EHN detection scheme based on the prediction history of the training samples, as depicted by <ref type="figure" target="#fig_4">Fig.  3</ref>. For the training set D with N samples, we gradually obtain the corresponding N prediction probability maps through the training of a CNN classification model for k epochs. Our EHN detection scheme first selected easy samples D e by using the mean prediction probabilities according to the threshold T . For convenience, the threshold T is implemented as selected easy sample ratio ? e in this paper. The higher ? e corresponds to the lower threshold T , and vice versa. Then we added noise to the D e as D a by switching the labels of some samples in D e , and recorded whether the samples are noise or not as R. The noise ratio of the adding noise is the same as the original dataset, which can be estimated by the noise cross-validation algorithm of <ref type="bibr" target="#b19">[20]</ref>. After that, we trained the same classification model by using D a and recorded training history again. Then we discarded the "easy samples" of D a according to mean prediction probabilities and utilized the rest samples as training data for the Multi-Layer Perceptron (MLP) classifier. So far, we will obtain an MLP classifier that takes prediction probability maps of training history as input and output whether it is a hard sample or a noisy one. Finally, we put the samples in D \ D e into the MLP classifier and get the hard sample set D h and the noisy set D n . Algorithm 1 shows the details of the EHN detection scheme.</p><p>Label Correction Model. Our correction model is trained by using D e ? D h from the EHN detection scheme. After training, the model has some ability to correct noisy labels. Therefore, the labels of samples in D h ? D n are replaced by the pseudo labels generated from the correction model, where the pseudo labels are the class with the highest probability of model output. The reason we also put the hard samples into the correction model is that we cannot trust the result of the MLP classifier in our EHN detection scheme completely.</p><p>These steps above are called our self-learning flow; it takes the original dataset into the classification model and gets through the EHN detection scheme by using Algorithm 1. Then, it trains the correction model and updates some sample labels with the pseudo ones. Finally, it iterates over the above steps to further purify our dataset.</p><p>Post-processing Component. Our post-processing component is to drop out the noisy samples which still can not be corrected after the processing of EHN and label correction. In D n , we drop out the samples whose labels were not changed </p><formula xml:id="formula_0">Algorithm 1 EHN detection scheme Input: D = [d 1 , d 2 , ..., d N ], d i is input image, label Y = [y 1 , y 2 , ..., y N ], y i is label for d i ,</formula><formula xml:id="formula_1">y i = label of sample d i in D h ? D n , in Y 2: g i = pseudo label of sample d i in D h ? D n , in G 3: Initialize N = ? as noisy sample set 4: for all d i ? D h ? D n do 5:</formula><p>if (y i = g i and d i ? D n ) or (y i = g i and d i ? D h ) then 6:</p><formula xml:id="formula_2">N = N ? {d i } 7:</formula><p>else 8:</p><formula xml:id="formula_3">y i = g i 9:</formula><p>end if 10: end for</p><formula xml:id="formula_4">11: D o = (D e ? D h ? D n ) \ N 12: return D o</formula><p>by the correction model. In D h , we drop out the samples whose labels were changed by the correction model. Algorithm 2 shows the details of the post-processing component. After post-processing, we then obtain the almost-clean dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Noise Suppressing and Hard Enhancing (NSHE)</head><p>Here we developed our robust NSHE algorithm by using the almost-clean dataset. The overview of the NHSE phase is shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. It is found in the experiment that different samples may have completely opposite optimization directions for model parameter values, which leads to frequent dithering of model parameters during the training process, resulting in a poor effect. This phenomenon is even more serious in noisy dataset, and the noisy samples will mislead the model training. Inspired by MoCo <ref type="bibr" target="#b35">[36]</ref>, we initialized two models M 1 , M 2 with the same backbone and parameters. Formally, denoting  the parameters of M 1 as ? 1 and those of M 2 as ? 2 , we update ? 2 by:</p><formula xml:id="formula_5">? 2 ? m? 2 + (1 ? m)? 1<label>(1)</label></formula><p>Here m ? [0, 1) is a momentum coefficient. Only the parameters ? 1 are updated by back-propagation. The momentum update in (1) makes ? 2 evolve more smoothly than ? 1 . Since the almost-clean dataset still has some noisy samples, we ranked the samples according to the prediction probabilities of the labeled class at each epoch, and set a very small ratio to make the samples with small prediction probabilities unable to participate in back-propagation. To avoid confirmation bias, we proposed the co-learning architecture based on Co-teaching <ref type="bibr" target="#b18">[19]</ref>. The probabilities are computed by M 2 , namely the sample selection information was given by M 2 . To further emphasize the significance of hard samples, we used focal loss <ref type="bibr" target="#b36">[37]</ref> to strengthen hard samples. The loss function is defined as follows:</p><formula xml:id="formula_6">FL (p t ) = ? (1 ? p t ) ? log (p t )<label>(2)</label></formula><p>where p t is the predicted probability of the correct class, ? is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We extensively validate our method on five datasets. 1) DigestPath2019: It has 250 malignant images with pixellevel annotation and 410 benign images. We cropped all images into small patches, using a patch size of 256 ? 256 and a stride of 64 pixels. With the segmentation annotations, we defined patches with the lesion area accounts for more than 95% of the whole patch as malignant samples. All benign samples were cropped from benign images. Finally, we got 29334 malignant samples, 28419 benign samples, and randomly partition them into 24611 malignant, 23824 benign, and 4723 malignant, 4595 benign samples for training and testing, respectively. The training and testing patches were from different original images. The sample patches are shown in <ref type="figure" target="#fig_7">Fig. 5 (a)</ref>. 2) Camelyon16: It has 110 tumor WSIs (whole slide images) and 110 normal WSIs, and we preprocessed it in the same way as the DigestPath2019 dataset. Finally, we  classes, which crawled from websites. The training set contains many real-world noisy labels. Since the dataset is quite large, for quick experiments, we follow the previous work <ref type="bibr" target="#b19">[20]</ref> and only use the first 50 classes of the Google image subset. Finally, it contains 65944 samples for training and 2500 samples for testing. Among them, 1) to 3) are medical scenario datasets; 4) and 5) are natural computer vision datasets. We randomly added different ratios (10%, 20%, 30%, and 40%) of noise to the DigestPath2019 and the Camelyon16. Due to these two datasets only have two classes, the noise type is simply changing labels into another class. For CIFAR-10 dataset, as it originally does not contain label noise, following previous work <ref type="bibr" target="#b19">[20]</ref>, we experiment with two types of label noise: symmetric and asymmetric. Symmetric noise is generated by randomly replacing the labels for a percentage of the training data with all other classes, and asymmetric noise is only generated by replacing the labels with adjacent class. Following work <ref type="bibr" target="#b19">[20]</ref>, we tested noise ratios 20%, 50%, and 80% for symmetric noise, and noise ratio 40% for asymmetric noise. The Chaoyang and Webvision datasets are constructed in the real scenario, and the noise refers to the actual labeled samples that are wrong, rather than the artificial addition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation and Parameter Settings</head><p>For medical scenario datasets (DigestPath2019, Came-lyon16, and Chaoyang). We used the Resnet-34 as the backbone and trained it using Adam with a momentum of 0.9, and a batch size of 96. During the label correction phase, the network was trained for 30 epochs. We set the initial learning rate as 0.001, and linearly reduced it after 15 epochs. For the NSHE phase, the networks were trained for 40 epochs. We set the initial learning rate as 0.001, and linearly reduced it after 15 epochs.</p><p>For natural computer vision datasets (CIFAR-10, Webvision), we followed the same settings in work <ref type="bibr" target="#b19">[20]</ref>. For CIFAR-10, we used the Resnet-32 as the backbone and trained it using SGD with a momentum of 0.9, a learning rate of 0.02, and a batch size of 128. The networks were trained for 300 epochs both in the label correction phase and the NSHE phase. For Webvision, we used the Inception-Resnet v2 <ref type="bibr" target="#b39">[40]</ref> as the backbone and trained it using SGD with a momentum of 0.9, a learning rate of 0.01, and a batch size of 32. The networks were trained for 80 epochs both in the label correction phase and the NSHE phase.</p><p>For all the datasets, ? e was set to 0.1 for 80% noise ratio, and 1 ? 1.5 * ? for other noise ratios, where ? is the dataset noise ratio. The parameter ? in focal loss we set to 2, and the discarding ratio ? was set to 0.1 * ?. For real-world datasets Chaoyang and Webvision, ? was estimated by the noise crossvalidation algorithm of <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Criteria</head><p>We used Accuracy (ACC), Precision, Recall, F1 Score (F1), AUC, and ROC curve as evaluation criteria. Their definitions are as follows:</p><formula xml:id="formula_7">ACC = T P + T N T P + T N + F P + F N<label>(3)</label></formula><p>P recision = T P T P + F P (4)</p><formula xml:id="formula_8">Recall = T P T P + F N<label>(5)</label></formula><formula xml:id="formula_9">F 1 = 2 * P recision * Recall P recision + Recall<label>(6)</label></formula><p>where TP, TN, FP, and FN represent true positives, true negatives, false positives, and false negatives, respectively.  ROC curve is the receiver operating characteristic curve. Its abscissa is false positive rate and ordinate is the true positive rate. AUC is the area under the ROC curve.</p><p>For multi-classification tasks, we compute the Precision, Recall, F1 Score, ROC curve, and AUC for each class and average them by using macro-average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Objective Comparison</head><p>We compare our methods with the following methods using the same network architecture.     Among them, work 1) to 5) are the state-of-the-art methods for general noisy data processing in recent years; work 6) and work 7) are the state-of-the-art methods proposed for medical data scenarios. We choose these schemes to contrast with to fully prove the superiority of our method. The testings of 2) to 6) are based on the open-source codes from the authors.</p><p>We re-implemented and tested 1) and 7) based on the settings from the original papers.</p><p>For experiments on medical scenarios datasets, <ref type="table" target="#tab_3">Table I and  Table II</ref> shows the test ACC, AUC, F1 Score, Precision, Recall on DigestPath2019 and Camelyon16 with different levels of  label noise ranging from 10% to 40%. Our method almost outperforms the state-of-the-art methods across all noise ratios. <ref type="table" target="#tab_3">Table III</ref> shows these metrics on the Chaoyang dataset. Our method outperforms all other methods by a large margin in every criterion. For experiments on natural computer vision datasets, <ref type="table" target="#tab_3">Table  IV</ref> shows the test ACC, AUC, F1 Score, Precision, Recall on CIFAR-10 with different levels and different types of label noise ranging from 20% to 80%. Our method outperforms the state-of-the-art methods across all noise ratios. <ref type="table" target="#tab_7">Table V</ref> shows these metrics on Webvision dataset. Our method consistently outperforms all other methods. Besides, we show the mean ROC curves of all five datasets in <ref type="figure" target="#fig_11">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We study the effect of removing different components to provide insights into what makes our method successful.   7 and <ref type="figure">Fig. 8</ref> show the ablation study results in different noise ratios. The result details are shown in <ref type="table" target="#tab_3">Table VI</ref> to VII, and we discuss them below.</p><p>To study the effects of the NSHE scheme, we removed the NSHE scheme (w/o NSHE), namely train the single model by using the dataset from label correction. The results show the hard samples play quite a significant role in training final models. By removing the NSHE scheme, the test accuracy decreased by an average of about 1.5%.</p><p>To study the effects of the EHN detection scheme, we removed both the EHN detection scheme and NSHE scheme (w/o (EHN + NSHE)). In this situation, following work <ref type="bibr" target="#b16">[17]</ref>, we directly used the classification model as the correction model, and the dataset is processed only by the correction model. The results show the EHN detection scheme is very effective to save more hard samples and filtered more noisy ones. Without the EHN detection scheme, the test accuracy further decreased by an average of about 2.8%.</p><p>To study the effects of the correction model, we further removed the whole label correction phase (w/o whole), namely we train the model by using original data. To converge the model under the same epoch, we adjusted the learning rate, which would smooth the test accuracy in the last epoch but the highest accuracy will be affected. By removing the correction model, the test accuracy further decreased by an average of about 1.2%.</p><p>Among the NSHE scheme, EHN detection scheme, and correction model, the EHN detection scheme introduces the maximum performance gain, followed by the NSHE scheme. All components have a certain gain at any noise ratio.</p><p>To analyze the effect of self-training rounds, we recorded the training times and test accuracy in different self-training rounds. <ref type="figure">Fig. 9</ref> shows the results on DigestPath2019 dataset <ref type="figure">Fig. 9</ref>. Our method with different rounds (just first stage) results in terms of test accuracy on DigestPath2019 dataset with 20% noise ratio. <ref type="figure" target="#fig_2">Fig. 10</ref>.</p><p>Test accuracy of a simple Resnet-34 and our algorithm (backbone is Resnet-34) on DigestPath2019 dataset with different noise ratios.</p><p>with 20% noise ratio. Training more rounds consumes more computing resources but brings little gain. Therefore, we choose to train only one round in our experiment. We also studied how much higher noise ratio could our scheme tolerate to reach a similar performance against a simple model. We first directly trained a simple Resnet-34 under 5 different noise ratios (0%, 10%, 20%, 30%, 40%, respectively) on DigestPath2019 dataset. Then we also carried out experiments our algorithm (backbone is Resnet-34) at 10%, 15%, 20%, 25%, 30%, 35%, 40%, 45%, 49% noise ratios (note that we select 49% noise ratio because it is close to the theoretical limit of 50% for this type of noise on binary classification task). The results are shown in <ref type="figure" target="#fig_2">Fig. 10</ref>.</p><p>According to <ref type="figure" target="#fig_2">Fig. 10</ref>, to reach the same performance, for 0%, 10%, 20%, 30%, and 40% noise ratios, our scheme can tolerate about up to 22%, 34%, 40%, 44%, and 49% noise ratio, respectively. We also find that in the noise ratios of 0% to 20% on DigestPath2019 dataset, the results of our method are even better than the trained simple Resnet-34 on a completely clean dataset. We believe that this phenomenon is due to our enhancement of the information of hard samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Analysis of EHN Detection Scheme</head><p>Effectiveness and Convergence Analysis. To analyze the effectiveness and convergence of our EHN detection scheme, we trained on DigestPath2019 dataset with different noise ratios and plotted the curves of test ACC vs. Iterations ("test" set here means D \ D e ) of M m . The results are shown in  Confusion matrix and accuracy (ACC) of EHN detection scheme in DigestPath2019 dataset with different noise ratios. <ref type="figure" target="#fig_2">Fig. 11</ref>. According to <ref type="figure" target="#fig_2">Fig. 11</ref>, M m has converged in the later training stage at each noise ratio, so the convergence of M m is relatively stable. Also from <ref type="figure" target="#fig_2">Fig. 11</ref>, M m can achieve 98%, 97%, 96%, and 90% classification accuracy in 10% to 40% noise ratios, respectively. Thus, M m can effectively distinguish the hard and noisy samples. We also recorded the confusion matrix of EHN detection scheme as <ref type="figure" target="#fig_2">Fig. 12</ref> (clean samples are divided into easy and hard in this test, as depicted by <ref type="figure" target="#fig_3">Fig.  2 (b)</ref>).</p><p>Parameter Analysis. To analyze how sensitive the EHN detection scheme is to the training epochs k and easy sample ratio ? e , we trained on different k and ? e in DigestPath2019 dataset with 30% noise ratio, recorded the confusion matrix, and calculated the ACC of EHN detection scheme. Specifically, we first adjusted the value of k with fixed ? e = 0.55, and thus obtained the sensitivity of EHN to k. Then we adjusted the value of ? e with fixed k = 30, and thus obtained the sensitivity of EHN to ? e . There are a total of 9 different parameter configurations. The results are shown in <ref type="figure" target="#fig_2">Fig. 13</ref>. And we also plotted the ACC vs. Parameters graph as <ref type="figure" target="#fig_2">Fig. 14.</ref> These results show that as k increases from 0, the performance continues to increase. When k reaches about 30, the performance achieves maximum accuracy. However, as k increases further, the performance begins to decrease slightly. But overall, when k exceeds a certain threshold, the performance of EHN is relatively stable. And for the parameter ? e , changing the value <ref type="figure" target="#fig_2">Fig. 13</ref>. Confusion matrix and accuracy (ACC) of EHN detection scheme on different "k" and "?e" in DigestPath2019 dataset with 30% noise ratio.   within a reasonable range has little impact on the performance. By the way, our utilized parameters are effective according to <ref type="figure" target="#fig_2">Fig. 14.</ref> Why D a works? To study why M m trained on the artificial created D a can recognize hard and noisy samples in original dataset D, we plotted both the mean prediction probability histogram of the clean and noisy samples in DigestPath2019 L e a r n i n g F o r g e t t i n g <ref type="figure" target="#fig_2">Fig. 16</ref>. The diagram of "Learning" and "Forgetting" event.</p><p>dataset (40% noise ratio) D and the corresponding D a . The results are shown in <ref type="figure" target="#fig_2">Fig. 15</ref>. According to <ref type="figure" target="#fig_2">Fig. 15 (b)</ref>, there are also some clean samples which can not be distinguished from the noisy ones by mean prediction probability. Although the samples in D e are all easy ones to dataset D, part of the samples became hard ones to dataset D a . In <ref type="figure" target="#fig_2">Fig. 15</ref>, it should be noted that the mean prediction probabilities of samples trained on D a have similar distribution with the original dataset D, and this is why our EHN detection scheme works by using the artificial created D a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Hard and Noisy Sample Analysis</head><p>Behavior Analysis. To analyze the training process behavior of the hard sample and the noisy sample, inspired by <ref type="bibr" target="#b44">[45]</ref>, we calculated the frequency of the "Learning" event and  "Forgetting" event of them through the whole training epochs. The "Learning" event in the t epoch is defined as an event that the prediction probability of the labeled class is less than 0.5 in t ? 1 epoch, while greater than 0.5 in t epoch. The "Forgetting" event in the t epoch is defined as an event that the prediction probability of the labeled class is greater than 0.5 in t ? 1 epoch, while less than 0.5 in t epoch. <ref type="figure" target="#fig_2">Fig. 16</ref> shows the diagram of "Learning" and "Forgetting" event. The statistical results are shown in <ref type="figure" target="#fig_2">Fig. 17 (a)</ref>. This results show that the hard sample and the noisy sample have great behavioral differences with the increase of the epoch in training. In the early stage of training, the hard samples tend to have more learning events, while the noisy samples tend to have more forgetting events.</p><p>In the late stage of training, the hard samples tend to have more forgetting events while the noisy samples tend to have more learning events. On the whole, the frequency of learning and forgetting events of the hard samples is higher than that of the noisy samples during the whole training epochs. We also calculated the frequency histogram of the gradient absolute value. The gradient absolute value is the absolute value of the gradient between adjacent epochs, that is, the absolute value of the difference between the prediction probabilities of adjacent epochs. As shown in <ref type="figure" target="#fig_2">Fig. 17 (b)</ref>, the gradient absolute value of the hard samples tends to be higher than the noisy samples. We believe that the reason for this phenomenon is that the hard samples' hard fitting attribute makes the prediction probability of the model jump frequently. Some noisy samples, however, are conspicuously at the center of other classes; they are "super hard" for the model to fit. Their effect on the optimization of model parameters would be suppressed by the clean samples around. So in the latter part of the training process, their output probabilities would  not change much. As shown in <ref type="figure" target="#fig_2">Fig. 18</ref>, the noisy samples are scattered throughout the dataset. When these samples fall in the intersection area of two categories, their training behaviors are more similar to the hard ones; when these samples fall in the center area of other classes (yellow circles in <ref type="figure" target="#fig_2">Fig. 18</ref>), they have distinct training behavior differences with the hard samples. To further prove the conjecture above, we selected samples in the Camelyon16 dataset with 20% noise ratio and used the Resnet-34 model (pre-trained in ImageNet) to extract the features and visualized them by t-SNE <ref type="bibr" target="#b45">[46]</ref> in <ref type="figure" target="#fig_2">Fig. 19</ref>. We selected the noisy samples that fall in the center of other classes in <ref type="figure" target="#fig_2">Fig. 19</ref> and plotted their training history to compare with the hard samples in <ref type="figure" target="#fig_3">Fig. 20</ref>. It can be seen that these noisy samples are indeed difficult to predict in the latter part of the training, which confirms our previous analysis. Benefit Analysis. To show the gain of our hard-aware label correction phase more intuitively, we count the remaining noise of the processed dataset and compare it with the baseline in <ref type="table" target="#tab_3">Table VIII</ref>. The baseline is set to update the labels by classification model and drop out the samples by using the mean prediction probability of the training history. The results show that with the same number of remaining samples, our strategy eliminates more noisy samples by protecting hard samples as much as possible, and thus generates a higher quality data set for the second phase of training. Besides, for the real medical scenario dataset "Chaoyang", our hard-aware label correction phase only drops out 191 samples (total 4021 samples), and this shows that our method reduces noise with little damage to the original dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Deep learning-based histopathology image classification can improve the diagnosing accuracy of cancer. It is difficult to collect a large clean dataset for training such a classification model. The existing noisy label correction methods fail to distinguish the hard samples from the noisy samples and thus ruin the model performance. In this study, we proposed a hard sample aware noise robust learning for histopathology image classification to save more clean samples, and thus boosted the model performance. We found that the training prediction history can be used to distinguish the hard samples and noisy samples. By integrating our EHN detection scheme into the noise removing, more hard clean samples can be saved. Besides, in our NSHE scheme under co-learning architecture, we adopted different parameter updating speed for the two models. This can further suppress the interference of noisy samples. Our results provide compelling performance for the noisy dataset, and the proposed method can be directly applied to the real-word noisy scenario. In the future, we will conduct hard sample aware semantic segmentation, such as malignant tissue segmentation for histopathology images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Foundation of China under Grant 81972248, and in part by the Natural Science Foundation of Beijing Municipality under Grant 7202056. C. Zhu, W.K. Chen, and T. Peng are with the School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing 100876, China (Corresponding author: Chuang Zhu (czhu@bupt.edu.cn)). Y. W. and M.L. Jin are with Beijing Chaoyang Hospital, Capital Medical University Beijing, China.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>The architecture of our algorithm. The architecture contains label correction phase and the NSHE phase. The label correction phase first generates "Almost Clean Dataset" and then puts it into our NSHE scheme. In the label correction phase, our self-learning contains (1) training history generation, (2) easy, hard, and noisy sample detection, (3) correction model training, (4) label correction, and (5) label updating. The post-processing is conducted after the self-learning flow. In the NSHE phase, two models are trained simultaneously with the obtained "Almost Clean Dataset" to produce the final robust classification model. First, M 2 selects training data for M 1 . Then, M 1 and M 2 are updated by back-propagation and momentum manner, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Mean prediction probability histogram of the clean and noisy samples in 40% noise ratio dataset. (b) The division of easy and hard samples in clean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of EHN detection scheme. First, the easy samples are selected by the mean prediction probability of training history. Second, adding noise to De as Da and train a classification model on Da to get the training history. Third, Mm is trained by the training history. At last, the hard and noisy samples in D are distinguished by Mm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>The NSHE phase takes the obtained "Almost Clean Dataset" as input and output the final noise-robust classification model. For the training process, it initializes two models M 1 and M 2 with the same parameters. At each epoch, M 2 selects training data for M 1 by ranking the prediction values of samples. M 1 is updated by back-propagation, and M 2 is updated by M 1 and the previous M 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 3 5 :? 1 = 11 :? 2 =</head><label>351112</label><figDesc>NSHE scheme Input: almost-clean data set D o , label Y , noise discarding ratio ? , momentum ratio m Output: classification model M 1: Initialize ? 1 of M 1 , ? 2 of M 2 , let ? 1 = ? 2 2: for epoch = 1, 2, ...., M axEpoch do 3: get prediction probabilities P 2 = M 2 (D o , Y ), sort D o ascending by P 2 4: obtain discarded sample set N = D o [0 : len(D o ) * ? ] for iter = 1, 2, ...., M axIter do P 1 = M 1 (B, Y ) SGD(L, ? 1 ) m? 2 + (1 ? m)? 1 12: end for 13: end for 14: return M 2 a hyper-parameter. Algorithm 3 shows the training flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Selected samples. (a) DigestPath2019. (b) Camelyon16. (c) Chaoyang.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 )</head><label>1</label><figDesc>SELF [41] (Duc Tam Nguyen et al. 2020) first obtains the self-ensemble predictions of all training samples and then progressively removes samples whose ensemble predictions do not agree with their annotated labels [42]. 2) DM [43] (Junnan Li et al. 2020) lets its two-component and one-dimensional Gaussian mixture model be fitted to the training loss to obtain the confidence of an</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>classify true-labeled samples while removing large-loss samples at each training round. 4) Joint [17] (Tanaka et al. 2018) jointly optimizes the sample labels and the network parameters. 5) Co-tea. [19] (Han et al. 2018) maintains two networks. Each network selects samples of small training loss from the mini-batches and feeds them to the other network. 6) NF-Net [23] (Zhantao Cao et al. 2020) adopts a doublesoftmax classification module to prevent deep models from overfitting the noisy labels and a teacher-student module to strengthen the effect of clean labels. 7) OUSM [22] (Cheng Xue et al. 2019) proposes online uncertainty sample mining and individual re-weighting methods to train their network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 6 .</head><label>6</label><figDesc>(a) DigestPath2019 dataset average ROC curve from 10% to 40% noise ratios (used marco-average). (b) Camelyon16 dataset average ROC curve from 10% to 40% noise ratios (used marco-average). (c) Chaoyang dataset average ROC curve from 4 classes (used marcoaverage). (d) CIFAR-10 dataset average ROC curve from 10 classes with 20% to 80% noise ratios (used marco-average). (e) Webvision dataset average ROC curve from 50 classes (used marco-average).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Ablation study results in terms of test accuracy on Digest-Path2019. Ablation study results in terms of test accuracy on Camelyon16 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Accuracy vs. iterations graph of Mm in DigestPath2019 dataset with different noise ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Confusion matrix and accuracy (ACC) of EHN detection scheme in DigestPath2019 dataset with different noise ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 .</head><label>14</label><figDesc>(a) ACC vs. k graph of EHN detection scheme. (b) ACC vs. ?e graph of EHN detection scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 15 .</head><label>15</label><figDesc>(a) Mean prediction probability histogram of the clean and noisy samples in DigestPath2019 dataset (40% noise ratio) D. (b) Mean prediction probability histogram of the clean and noisy samples in corresponding Da.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 17 .</head><label>17</label><figDesc>(a) The forgetting and learning event frequency histogram. (b) The gradient absolute value frequency histogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 18 .</head><label>18</label><figDesc>Sample distribution diagram, the red color represents the noisy sample. Yellow circles denote the center of clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 19 .</head><label>19</label><figDesc>Visualization of Camelyon16 dataset with 20% noise ratio by using t-SNE<ref type="bibr" target="#b45">[46]</ref>, where patches with red solid border are clean malignant samples, patches with blue solid border are clean benign samples, patches with red dotted border are noisy malignant samples (true labels are benign), patches with blue dotted border are noisy benign samples (true labels are malignant).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 20 .</head><label>20</label><figDesc>Training history graph of the selected hard and noisy samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Noised Dataset Label Correction Phase NSHE Phase</head><label></label><figDesc>.....</figDesc><table><row><cell>Classification</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>EHN</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Detection Scheme</cell><cell></cell><cell>Sample Ranking</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>and</cell><cell></cell></row><row><cell>Shared</cell><cell>Training Behavior</cell><cell></cell><cell>Dropping</cell><cell></cell></row><row><cell>Architecture</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Correction</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Post-</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Processing</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Label</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Correction</cell><cell></cell><cell cols="3">Almost Clean Dataset</cell></row><row><cell>Label Updating for</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Self-learning</cell><cell>Post-processing</cell><cell>Easy</cell><cell>Hard</cell><cell>Noisy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>easy sample ratio ? e Output: easy set D e , hard set D h , noisy set D n 1: Train classification model M c by using D and Y , record training history H = [h 1 , h 2 , ..., h N ], where h i is a vector with shape of 1 * k (epoch) 2: Calculate the mean value of H as H m , H m = [mean(h 1 ), mean(h 2 ), ..., mean(h N )], sort D descending by H m , select easy samples D e = D[0 : len(D) * ? e ] 3: Add noise to D e as D a , get noisy labels Y a and record whether the samples are noise or not R = [r 1 , r 2 , ..., r N ] 4: Retrain M c by D a and Y a , record training history H a 5: Sort H a descending by mean value, select training history of hard and noisy samples H a = H a [len(H a ) * ? e : len(H a )], and choose the corresponding R from R 6: Train MLP classifier M m by using H a and R 7: Put the samples in D \ D e into M m and get D h and D n 8: return D e , D h , D n Algorithm 2 Post-processing strategy Input: easy set D e , hard set D h , noisy set D n , original labels Y , pseudo labels G generated from the correction model Output: almost clean dataset D</figDesc><table /><note>o 1:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I AVERAGE</head><label>I</label><figDesc>TEST ACC, F1 SCORE, AUC, PRECISION, RECALL(%, 3 RUNS) WITH STANDARD DEVIATION ON DIGESTPATH2019 DATASET.</figDesc><table><row><cell>Noise ratio</cell><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Joint</cell><cell>91.49?0.95</cell><cell>97.62?1.25</cell><cell>91.25?0.90</cell><cell>95.41?1.77</cell><cell>87.45?0.21</cell><cell>88.78?0.28</cell><cell>95.40?0.32</cell><cell>89.00?0.27</cell><cell>88.47?0.24</cell><cell>89.53?0.32</cell></row><row><cell>Co-tea.</cell><cell>91.08?0.79</cell><cell>98.21?1.00</cell><cell>90.59?0.80</cell><cell>97.49?1.18</cell><cell>84.60?0.51</cell><cell>90.33?0.70</cell><cell>95.86?0.87</cell><cell>90.12?0.65</cell><cell>93.49?1.29</cell><cell>87.00?0.11</cell></row><row><cell>INCV</cell><cell>93.58?0.96</cell><cell>97.91?1.01</cell><cell>93.62?0.96</cell><cell>94.34?0.92</cell><cell>92.92?1.00</cell><cell>92.09?0.39</cell><cell>96.43?0.50</cell><cell>91.97?0.38</cell><cell>94.74?0.65</cell><cell>89.36?0.14</cell></row><row><cell>OUSM</cell><cell>90.27?0.98</cell><cell>96.87?1.02</cell><cell>89.81?0.94</cell><cell>95.87?1.95</cell><cell>84.48?0.16</cell><cell>88.19?1.56</cell><cell>94.19?1.48</cell><cell>88.18?1.69</cell><cell>89.30?0.85</cell><cell>87.11?2.49</cell></row><row><cell>NF-Net</cell><cell>83.08?0.43</cell><cell>90.64?0.24</cell><cell>81.95?0.42</cell><cell>89.26?0.73</cell><cell>75.75?0.18</cell><cell>83.09?2.17</cell><cell>89.09?2.45</cell><cell>82.82?2.21</cell><cell>85.40?2.28</cell><cell>80.40?2.15</cell></row><row><cell>DM</cell><cell>92.69?0.84</cell><cell>97.76?0.64</cell><cell>92.69?0.81</cell><cell>93.98?1.30</cell><cell>91.45?0.59</cell><cell>90.53?0.87</cell><cell>96.09?1.11</cell><cell>90.46?0.83</cell><cell>92.51?1.49</cell><cell>88.51?0.96</cell></row><row><cell>SELF</cell><cell>92.22?0.77</cell><cell>97.43?0.96</cell><cell>92.21?0.79</cell><cell>92.55?1.04</cell><cell>91.78?0.66</cell><cell>91.23?0.42</cell><cell>95.89?0.55</cell><cell>91.19?0.36</cell><cell>91.44?0.33</cell><cell>90.99?0.41</cell></row><row><cell>Ours</cell><cell>94.91?0.32</cell><cell>98.40?0.57</cell><cell>95.05?0.35</cell><cell>93.68?0.63</cell><cell>96.46?0.21</cell><cell>94.46?0.20</cell><cell>98.30?0.34</cell><cell>94.53?0.23</cell><cell>94.51?0.13</cell><cell>94.55?0.38</cell></row><row><cell>Noise ratio</cell><cell></cell><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40%</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Joint</cell><cell>87.17?0.57</cell><cell>94.72?0.63</cell><cell>86.41?0.66</cell><cell>93.34?0.27</cell><cell>80.44?0.98</cell><cell>84.30?0.87</cell><cell>91.39?1.02</cell><cell>84.46?0.92</cell><cell>84.77?0.60</cell><cell>84.15?1.26</cell></row><row><cell>Co-tea.</cell><cell>86.49?0.37</cell><cell>93.55?0.46</cell><cell>86.90?0.33</cell><cell>85.50?0.47</cell><cell>88.34?0.20</cell><cell>84.92?0.57</cell><cell>92.98?0.68</cell><cell>84.94?0.54</cell><cell>86.02?0.74</cell><cell>83.89?0.41</cell></row><row><cell>INCV</cell><cell>87.84?0.52</cell><cell>94.60?0.23</cell><cell>87.65?0.52</cell><cell>90.36?0.56</cell><cell>85.10?0.49</cell><cell>85.20?0.48</cell><cell>93.52?0.56</cell><cell>84.47?0.50</cell><cell>90.19?0.56</cell><cell>79.44?0.48</cell></row><row><cell>OUSM</cell><cell>86.52?1.22</cell><cell>92.49?1.25</cell><cell>86.53?1.23</cell><cell>87.64?1.18</cell><cell>85.45?1.37</cell><cell>84.04?0.38</cell><cell>93.51?0.42</cell><cell>82.57?0.41</cell><cell>92.43?0.48</cell><cell>74.60?0.36</cell></row><row><cell>NF-Net</cell><cell>83.86?0.59</cell><cell>90.62?0.34</cell><cell>83.59?0.53</cell><cell>86.24?0.93</cell><cell>81.11?0.18</cell><cell>82.37?0.84</cell><cell>88.29?1.02</cell><cell>82.38?0.81</cell><cell>83.49?0.97</cell><cell>81.30?0.70</cell></row><row><cell>DM</cell><cell>88.87?1.03</cell><cell>95.10?0.78</cell><cell>88.60?1.12</cell><cell>92.04?0.54</cell><cell>85.41?1.63</cell><cell>85.96?0.37</cell><cell>96.87?0.38</cell><cell>84.49?0.43</cell><cell>95.97?0.39</cell><cell>75.46?0.46</cell></row><row><cell>SELF</cell><cell>90.03?1.21</cell><cell>95.78?0.64</cell><cell>89.96?1.42</cell><cell>90.97?0.89</cell><cell>88.66?1.66</cell><cell>86.23?0.98</cell><cell>94.21?0.89</cell><cell>86.02?1.00</cell><cell>87.52?0.88</cell><cell>84.03?1.21</cell></row><row><cell>Ours</cell><cell>91.72?0.69</cell><cell>97.38?0.78</cell><cell>91.47?0.75</cell><cell>95.74?0.34</cell><cell>87.57?1.10</cell><cell>87.15?0.41</cell><cell>94.26?0.46</cell><cell>86.49?0.41</cell><cell>88.58?0.63</cell><cell>85.16?0.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II AVERAGE</head><label>II</label><figDesc>TEST ACC, F1 SCORE, AUC, PRECISION, RECALL(%, 3 RUNS) WITH STANDARD DEVIATION ON CAMELYON16 DATASET.</figDesc><table><row><cell>Noise ratio</cell><cell></cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20%</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Joint</cell><cell>97.51?0.56</cell><cell>98.94?0.67</cell><cell>97.53?0.55</cell><cell>96.89?0.68</cell><cell>98.18?0.42</cell><cell>96.51?0.70</cell><cell>98.75?0.68</cell><cell>96.49?0.72</cell><cell>97.13?0.15</cell><cell>95.86?1.32</cell></row><row><cell>Co-tea.</cell><cell>98.16?0.30</cell><cell>99.34?0.27</cell><cell>98.13?0.31</cell><cell>99.87?0.09</cell><cell>96.45?0.51</cell><cell>97.57?0.27</cell><cell>99.29?0.30</cell><cell>97.53?0.27</cell><cell>99.29?0.27</cell><cell>95.84?0.27</cell></row><row><cell>INCV</cell><cell>97.87?0.16</cell><cell>99.23?0.16</cell><cell>97.84?0.16</cell><cell>99.57?0.18</cell><cell>96.17?0.15</cell><cell>97.65?0.12</cell><cell>99.33?0.13</cell><cell>97.62?0.12</cell><cell>99.42?0.07</cell><cell>95.87?0.17</cell></row><row><cell>OUSM</cell><cell>98.08?0.41</cell><cell>99.37?0.50</cell><cell>98.06?0.41</cell><cell>99.55?0.55</cell><cell>96.61?0.52</cell><cell>96.57?1.11</cell><cell>98.90?0.20</cell><cell>96.52?1.13</cell><cell>98.33?1.21</cell><cell>94.78?1.33</cell></row><row><cell>NF-Net</cell><cell>90.24?0.21</cell><cell>93.53?0.19</cell><cell>89.70?0.24</cell><cell>95.16?0.05</cell><cell>84.84?0.39</cell><cell>90.72?1.02</cell><cell>92.93?0.45</cell><cell>90.30?1.03</cell><cell>94.92?1.55</cell><cell>86.12?0.68</cell></row><row><cell>DM</cell><cell>95.10?0.40</cell><cell>96.70?0.50</cell><cell>94.91?0.43</cell><cell>99.15?0.19</cell><cell>91.01?0.63</cell><cell>93.98?0.63</cell><cell>96.90?0.62</cell><cell>94.01?0.66</cell><cell>93.85?0.34</cell><cell>94.17?1.10</cell></row><row><cell>SELF</cell><cell>97.89?0.42</cell><cell>99.02?0.34</cell><cell>97.85?0.39</cell><cell>98.02?0.52</cell><cell>97.53?0.39</cell><cell>97.44?0.32</cell><cell>98.89?0.15</cell><cell>97.49?0.41</cell><cell>97.88?0.50</cell><cell>97.06?0.37</cell></row><row><cell>Ours</cell><cell>98.82?0.20</cell><cell>99.81?0.14</cell><cell>98.81?0.20</cell><cell>99.73?0.18</cell><cell>97.73?0.22</cell><cell>98.61?0.11</cell><cell>99.78?0.18</cell><cell>98.69?0.11</cell><cell>99.61?0.14</cell><cell>97.40?0.21</cell></row><row><cell>Noise ratio</cell><cell></cell><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40%</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Joint</cell><cell>96.10?0.55</cell><cell>98.79?0.64</cell><cell>96.03?0.55</cell><cell>98.09?0.75</cell><cell>94.06?0.38</cell><cell>91.97?1.72</cell><cell>95.51?1.43</cell><cell>91.56?1.82</cell><cell>96.94?3.08</cell><cell>86.86?2.79</cell></row><row><cell>Co-tea.</cell><cell>97.49?0.12</cell><cell>99.02?0.11</cell><cell>97.44?0.12</cell><cell>99.68?0.04</cell><cell>95.30?0.20</cell><cell>95.20?0.71</cell><cell>96.25?0.72</cell><cell>95.09?0.73</cell><cell>97.56?0.77</cell><cell>92.74?0.69</cell></row><row><cell>INCV</cell><cell>96.28?0.32</cell><cell>97.64?0.32</cell><cell>96.24?0.31</cell><cell>97.41?0.43</cell><cell>95.10?0.20</cell><cell>95.40?0.23</cell><cell>96.26?0.34</cell><cell>95.28?0.24</cell><cell>98.06?0.10</cell><cell>92.65?0.36</cell></row><row><cell>OUSM</cell><cell>96.43?0.23</cell><cell>98.73?0.20</cell><cell>96.32?0.24</cell><cell>99.63?0.29</cell><cell>93.22?0.32</cell><cell>91.98?3.21</cell><cell>96.35?3.07</cell><cell>91.32?3.71</cell><cell>98.64?1.29</cell><cell>85.11?5.40</cell></row><row><cell>NF-Net</cell><cell>89.61?0.60</cell><cell>95.81?0.19</cell><cell>89.28?0.59</cell><cell>92.50?0.93</cell><cell>86.29?0.35</cell><cell>85.53?1.18</cell><cell>91.72?0.75</cell><cell>85.34?1.28</cell><cell>86.76?0.86</cell><cell>93.96?1.66</cell></row><row><cell>DM</cell><cell>93.39?1.07</cell><cell>95.28?1.19</cell><cell>93.08?1.17</cell><cell>97.74?0.60</cell><cell>88.85?1.64</cell><cell>89.55?0.12</cell><cell>97.72?0.06</cell><cell>88.39?0.14</cell><cell>99.84?0.12</cell><cell>79.30?0.15</cell></row><row><cell>SELF</cell><cell>96.98?0.54</cell><cell>98.72?0.17</cell><cell>97.00?0.46</cell><cell>97.56?0.38</cell><cell>96.43?0.56</cell><cell>96.02?1.05</cell><cell>97.53?0.31</cell><cell>95.89?1.22</cell><cell>96.78?1.31</cell><cell>94.95?1.20</cell></row><row><cell>Ours</cell><cell>98.32?0.41</cell><cell>99.57?0.22</cell><cell>98.30?0.42</cell><cell>99.84?0.09</cell><cell>96.80?0.46</cell><cell>98.17?0.84</cell><cell>99.51?0.24</cell><cell>98.16?0.28</cell><cell>99.12?0.23</cell><cell>97.22?0.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III AVERAGE</head><label>III</label><figDesc>TEST ACC, F1 SCORE, AUC, PRECISION, RECALL(%, 3 RUNS) WITH STANDARD DEVIATION ON CHAOYANG DATASET.</figDesc><table><row><cell>Method</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Joint</cell><cell>75.99?0.64</cell><cell>90.43?0.84</cell><cell>67.72?2.36</cell><cell>70.97?2.42</cell><cell>67.91?2.77</cell></row><row><cell>Co-tea.</cell><cell>79.39?0.29</cell><cell>91.72?0.68</cell><cell>71.97?0.96</cell><cell>74.57?1.45</cell><cell>70.77?1.17</cell></row><row><cell>INCV</cell><cell>80.34?0.36</cell><cell>92.63?0.11</cell><cell>74.11?0.43</cell><cell>76.22?0.22</cell><cell>73.06?0.36</cell></row><row><cell>OUSM</cell><cell>80.53?1.10</cell><cell>93.69?0.42</cell><cell>73.70?0.96</cell><cell>74.81?1.76</cell><cell>73.27?0.39</cell></row><row><cell>NF-Net</cell><cell>51.23?1.18</cell><cell>69.92?1.17</cell><cell>33.21?0.39</cell><cell>37.19?0.94</cell><cell>36.27?0.62</cell></row><row><cell>DM</cell><cell>77.25?0.21</cell><cell>87.58?0.36</cell><cell>69.78?0.32</cell><cell>70.68?0.23</cell><cell>69.11?0.38</cell></row><row><cell>SELF</cell><cell>80.49?0.42</cell><cell>93.99?0.58</cell><cell>75.31?0.63</cell><cell>76.14?0.78</cell><cell>74.69?0.59</cell></row><row><cell>Ours</cell><cell>83.40?0.20</cell><cell>94.51?0.34</cell><cell>76.54?0.33</cell><cell>78.33?0.30</cell><cell>75.45?0.42</cell></row></table><note>annotated label. By setting a confidence threshold, the training data is categorized into a labeled set and an un- labeled set. Subsequently, MixMatch [44] is employed to train a DNN for the transformed data. 3) INCV [20] (Pengfei Chen et al. 2019) randomly divides noisy training data and then employs cross-validation to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV AVERAGE</head><label>IV</label><figDesc>TEST ACC, F1 SCORE, AUC, PRECISION, RECALL(%, 3 RUNS) WITH STANDARD DEVIATION ON CIFAR-10 DATASET.</figDesc><table><row><cell>Noise ratio</cell><cell></cell><cell></cell><cell>20% Sym.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50% Sym.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Joint</cell><cell>88.37?0.06</cell><cell>98.88?0.06</cell><cell>88.33?0.03</cell><cell>88.38?0.03</cell><cell>88.37?0.06</cell><cell>81.85?0.16</cell><cell>97.88?0.06</cell><cell>81.87?0.16</cell><cell>82.03?0.12</cell><cell>81.85?0.16</cell></row><row><cell>Co-tea.</cell><cell>87.80?0.32</cell><cell>99.08?0.02</cell><cell>87.85?0.31</cell><cell>87.95?0.30</cell><cell>87.80?0.32</cell><cell>83.21?0.48</cell><cell>98.29?0.04</cell><cell>83.27?0.44</cell><cell>83.48?0.41</cell><cell>83.21?0.48</cell></row><row><cell>INCV</cell><cell>89.02?0.24</cell><cell>99.10?0.05</cell><cell>89.08?0.24</cell><cell>89.89?0.22</cell><cell>89.02?0.24</cell><cell>84.98?0.32</cell><cell>98.38?0.09</cell><cell>85.10?0.33</cell><cell>85.88?0.33</cell><cell>84.98?0.32</cell></row><row><cell>OUSM</cell><cell>81.88?0.20</cell><cell>97.01?0.06</cell><cell>81.85?0.24</cell><cell>81.88?0.25</cell><cell>81.88?0.20</cell><cell>68.66?0.81</cell><cell>93.77?0.23</cell><cell>68.50?0.77</cell><cell>69.39?0.52</cell><cell>68.66?0.81</cell></row><row><cell>NF-Net</cell><cell>80.27?1.15</cell><cell>96.76?0.50</cell><cell>80.19?1.03</cell><cell>81.02?1.21</cell><cell>80.27?1.15</cell><cell>67.38?1.24</cell><cell>90.34?0.34</cell><cell>67.58?1.11</cell><cell>67.88?1.07</cell><cell>67.38?1.24</cell></row><row><cell>DM</cell><cell>91.76?0.09</cell><cell>99.18?0.06</cell><cell>91.71?0.10</cell><cell>91.76?0.12</cell><cell>91.76?0.09</cell><cell>90.46?0.27</cell><cell>98.85?0.01</cell><cell>90.43?0.29</cell><cell>90.46?0.28</cell><cell>90.46?0.27</cell></row><row><cell>SELF</cell><cell>91.01?0.12</cell><cell>99.15?0.09</cell><cell>91.29?0.22</cell><cell>91.58?0.23</cell><cell>91.01?0.12</cell><cell>90.29?0.44</cell><cell>98.54?0.10</cell><cell>90.43?0.23</cell><cell>90.51?0.27</cell><cell>90.29?0.44</cell></row><row><cell>Ours</cell><cell>92.35?0.09</cell><cell>99.23?0.07</cell><cell>92.43?0.10</cell><cell>92.50?0.17</cell><cell>92.35?0.09</cell><cell>91.33?0.09</cell><cell>99.07?0.06</cell><cell>91.36?0.09</cell><cell>91.50?0.07</cell><cell>91.33?0.09</cell></row><row><cell>Noise ratio</cell><cell></cell><cell></cell><cell>80% Sym.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40% Asym.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Joint</cell><cell>57.83?1.93</cell><cell>89.62?0.95</cell><cell>57.90?1.96</cell><cell>57.93?2.02</cell><cell>57.84?1.93</cell><cell>87.05?0.32</cell><cell>98.03?0.12</cell><cell>87.04?0.31</cell><cell>87.09?0.37</cell><cell>87.05?0.32</cell></row><row><cell>Co-tea.</cell><cell>24.37?4.21</cell><cell>61.97?2.88</cell><cell>24.92?4.52</cell><cell>25.68?4.02</cell><cell>24.37?4.21</cell><cell>82.86?1.13</cell><cell>98.15?0.28</cell><cell>82.93?1.01</cell><cell>83.24?0.79</cell><cell>82.86?1.13</cell></row><row><cell>INCV</cell><cell>53.98?2.68</cell><cell>85.76?1.23</cell><cell>53.98?2.35</cell><cell>53.99?2.24</cell><cell>53.98?2.68</cell><cell>85.88?0.67</cell><cell>98.25?0.09</cell><cell>85.9?0.54</cell><cell>85.99?0.52</cell><cell>85.88?0.67</cell></row><row><cell>OUSM</cell><cell>40.62?1.03</cell><cell>81.18?0.97</cell><cell>40.59?1.24</cell><cell>40.87?1.46</cell><cell>40.42?1.03</cell><cell>73.87?1.20</cell><cell>95.87?0.10</cell><cell>73.24?1.29</cell><cell>75.14?0.47</cell><cell>72.87?1.20</cell></row><row><cell>NF-Net</cell><cell>18.28?3.31</cell><cell>56.30?1.89</cell><cell>18.90?3.65</cell><cell>19.78?3.72</cell><cell>18.28?3.31</cell><cell>69.98?1.11</cell><cell>91.43?0.18</cell><cell>70.02?1.03</cell><cell>70.16?1.12</cell><cell>69.98?1.11</cell></row><row><cell>DM</cell><cell>58.04?1.67</cell><cell>88.62?1.01</cell><cell>58.77?1.35</cell><cell>59.71?1.22</cell><cell>58.04?1.67</cell><cell>86.50?0.19</cell><cell>97.51?0.02</cell><cell>86.24?0.19</cell><cell>86.73?0.05</cell><cell>86.50?0.19</cell></row><row><cell>SELF</cell><cell>59.75?2.13</cell><cell>86.40?1.34</cell><cell>59.24?1.99</cell><cell>60.02?1.87</cell><cell>59.75?2.13</cell><cell>87.14?0.54</cell><cell>98.12?0.11</cell><cell>87.27?0.64</cell><cell>87.35?0.48</cell><cell>87.14?0.54</cell></row><row><cell>Ours</cell><cell>61.69?1.22</cell><cell>89.91?0.77</cell><cell>61.6?1.34</cell><cell>61.72?1.45</cell><cell>61.69?1.22</cell><cell>88.26?0.12</cell><cell>98.77?0.03</cell><cell>88.24?0.10</cell><cell>88.29?0.07</cell><cell>88.26?0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V AVERAGE</head><label>V</label><figDesc>TEST ACC, F1 SCORE, AUC, PRECISION, RECALL(%, 3 RUNS) WITH STANDARD DEVIATION ON WEBVISION DATASET.</figDesc><table><row><cell>Method</cell><cell>ACC</cell><cell>AUC</cell><cell>F1 Score</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>Joint</cell><cell>60.28?0.92</cell><cell>96.70?0.07</cell><cell>60.40?1.03</cell><cell>67.51?1.22</cell><cell>60.28?0.92</cell></row><row><cell>Co-tea.</cell><cell>63.77?1.31</cell><cell>96.98?0.09</cell><cell>63.92?1.46</cell><cell>64.02?1.51</cell><cell>63.77?1.31</cell></row><row><cell>INCV</cell><cell>65.02?0.86</cell><cell>97.11?0.04</cell><cell>65.11?0.98</cell><cell>65.05?1.02</cell><cell>65.02?0.86</cell></row><row><cell>OUSM</cell><cell>70.86?0.50</cell><cell>98.16?0.05</cell><cell>70.95?0.41</cell><cell>73.05?0.50</cell><cell>70.86?0.50</cell></row><row><cell>NF-Net</cell><cell>58.03?1.27</cell><cell>95.86?0.08</cell><cell>58.77?1.34</cell><cell>59.01?1.54</cell><cell>58.03?1.27</cell></row><row><cell>DM</cell><cell>76.92?0.24</cell><cell>97.69?0.02</cell><cell>76.98?0.27</cell><cell>77.08?0.43</cell><cell>76.92?0.24</cell></row><row><cell>SELF</cell><cell>69.18?0.32</cell><cell>97.17?0.05</cell><cell>69.21?0.38</cell><cell>69.80?0.63</cell><cell>69.18?0.32</cell></row><row><cell>Ours</cell><cell>77.52?0.51</cell><cell>98.32?0.04</cell><cell>77.58?0.59</cell><cell>78.11?0.87</cell><cell>77.52?0.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI AVERAGE</head><label>VI</label><figDesc>ACC(%, 3 RUNS) WITH STANDARD DEVIATION OF DIFFERENT ABLATION STUDY ON THE DIGESTPATH2019 DATASET.</figDesc><table><row><cell>Noise ratio</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">ACC</cell><cell></cell></row><row><cell>Ours</cell><cell>94.91?0.52</cell><cell>94.46?0.20</cell><cell>91.72?0.69</cell><cell>87.15?0.41</cell></row><row><cell>w/o NSHE</cell><cell>93.46?0.12</cell><cell>91.48?0.43</cell><cell>90.86?0.24</cell><cell>86.85?0.60</cell></row><row><cell>w/o (NSHE + EHN)</cell><cell>90.54?0.45</cell><cell>88.32?0.17</cell><cell>86.25?0.48</cell><cell>85.15?1.87</cell></row><row><cell>w/o whole</cell><cell>90.01?0.49</cell><cell>87.24?0.98</cell><cell>85.11?1.31</cell><cell>82.40?1.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII AVERAGE</head><label>VII</label><figDesc>ACC(%, 3 RUNS) WITH STANDARD DEVIATION OF DIFFERENT ABLATION STUDY ON THE CAMELYON16 DATASET.</figDesc><table><row><cell>Noise ratio</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">ACC</cell><cell></cell></row><row><cell>Ours</cell><cell>98.82?0.20</cell><cell>98.61?0.11</cell><cell>98.32?0.41</cell><cell>98.17?0.24</cell></row><row><cell>w/o NSHE</cell><cell>97.15?0.50</cell><cell>97.00?0.98</cell><cell>96.83?0.89</cell><cell>96.55?2.11</cell></row><row><cell>w/o (NSHE + EHN)</cell><cell>95.99?0.73</cell><cell>94.95?1.13</cell><cell>93.78?1.54</cell><cell>93.45?2.44</cell></row><row><cell>w/o whole</cell><cell>94.60?0.66</cell><cell>93.34?0.91</cell><cell>93.11?0.78</cell><cell>92.23?1.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII THE</head><label>VIII</label><figDesc>FINAL NOISE RATIOS OF DIFFERENT LABEL CORRECTION SCHEMES ON DIGESTPATH2019 DATASET.</figDesc><table><row><cell>Noise ratio</cell><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell>20%</cell><cell></cell></row><row><cell>Strategy</cell><cell>Remain samples</cell><cell>Remain noisy samples</cell><cell>Final noise ratio</cell><cell>Remain samples</cell><cell>Remain noisy samples</cell><cell>Final noise ratio</cell></row><row><cell>Ours Drop by mean</cell><cell>46181</cell><cell>26 181</cell><cell>0.0563% 0.392%</cell><cell>45895</cell><cell>159 859</cell><cell>0.346% 1.87%</cell></row><row><cell>Noise ratio</cell><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell>40%</cell><cell></cell></row><row><cell>Strategy</cell><cell>Remain samples</cell><cell>Remain noisy samples</cell><cell>Final noise ratio</cell><cell>Remain samples</cell><cell>Remain noisy samples</cell><cell>Final noise ratio</cell></row><row><cell>Ours Drop by mean</cell><cell>42361</cell><cell>551 1537</cell><cell>1.30% 3.63%</cell><cell>39348</cell><cell>2227 6626</cell><cell>5.66% 16.8%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Early detection of cancer: past, present, and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Schiffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gibbs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Society of Clinical Oncology Educational Book</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="65" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Breast cancer detection using extreme learning machine based on feature fusion with cnn deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="105" to="146" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Clinical-grade computational pathology using weakly supervised deep learning on whole slide images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Campanella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Geneslaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miraflor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Werneck Krauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Brogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Klimstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An augmented reality microscope with real-time artificial intelligence integration for cancer diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gadepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadowaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Mermel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="page" from="1453" to="1457" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classification and mutation prediction from non-small cell lung cancer histopathology images using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Coudray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Ocampo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakellaropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Narula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Snuderl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feny?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsirigos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="page" from="1559" to="1567" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interpretable classification of alzheimer&apos;s disease pathologies with a convolutional neural network pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Decarli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beckett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Keiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Dugger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07788</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Harnessing side information for classification under label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">465</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">L dmi: An information-theoretic noise-robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03388</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05040</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Limited gradient descent: Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="168" to="296" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust learning at noisy labeled medical images: Applied to skin lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -International Symposium on Biomedical Imaging</title>
		<meeting>-International Symposium on Biomedical Imaging</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1280" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Breast tumor classification through learning from noisy labeled ultrasound images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Physics, 2020</title>
		<imprint>
			<biblScope unit="page" from="1048" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Teaching-to-learn and learningto-teach for multi-label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth Aaai Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning to teach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y.</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03643</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05055</idno>
		<title level="m">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of Systemic Autoimmune Diseases</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Self: Learning to filter noisy labels with self-ensembling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">An empirical study of example forgetting during deep neural network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05159</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laurens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
