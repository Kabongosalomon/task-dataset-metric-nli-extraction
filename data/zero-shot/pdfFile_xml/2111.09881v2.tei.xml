<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Restormer: Efficient Transformer for High-Resolution Image Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syed</forename><surname>Waqas Zamir</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inception Institute of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Arora</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inception Institute of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahbaz</forename><surname>Fahad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Restormer: Efficient Transformer for High-Resolution Image Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from largescale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image restoration is the task of reconstructing a highquality image by removing degradations (e.g., noise, blur, rain drops) from a degraded input. Due to the ill-posed nature, it is a highly challenging problem that usually requires strong image priors for effective restoration. Since convolutional neural networks (CNNs) perform well at learning generalizable priors from large-scale data, they have emerged as a preferable choice compared to conventional restoration approaches.</p><p>The basic operation in CNNs is the 'convolution' that provides local connectivity and translation equivariance. While these properties bring efficiency and generalization to CNNs, they also cause two main issues. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DBGAN</head><p>(CVPR20) <ref type="bibr" target="#b93">[94]</ref> [58] <ref type="bibr" target="#b13">[14]</ref> [13] <ref type="bibr" target="#b92">[93]</ref> [100] MPRNet (CVPR21) <ref type="bibr" target="#b92">[93]</ref> [64]</p><p>[43] <ref type="bibr" target="#b31">[32]</ref> (a) <ref type="bibr">Deblurring</ref>  DeamNet (CVPR21) <ref type="bibr" target="#b62">[63]</ref> [55] <ref type="bibr" target="#b79">[80]</ref> [93] <ref type="bibr" target="#b91">[92]</ref> (c) Gaussian Denoising (Tab. 4) (d) Real Denoising (Tab. 6) lution operator has a limited receptive field, thus preventing it from modeling long-range pixel dependencies. (b) The convolution filters have static weights at inference, and thereby cannot flexibly adapt to the input content. To deal with the above-mentioned shortcomings, a more powerful and dynamic alternative is the self-attention (SA) mechanism <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b94">95]</ref> that calculates response at a given pixel by a weighted sum of all other positions. Self-attention is a core component in Transformer models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b76">77]</ref> but with a unique implementation, i.e., multihead SA that is optimized for parallelization and effective representation learning. Transformers have shown state-ofthe-art performance on natural language tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b61">62]</ref> and on high-level vision problems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b77">78]</ref>. Although SA is highly effective in capturing long-range pixel interactions, its complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to highresolution images (a frequent case in image restoration). Recently, few efforts have been made to tailor Transformers for image restoration tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b79">80]</ref>. To reduce the computational loads, these methods either apply SA on small spatial windows of size 8?8 around each pixel <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b79">80]</ref>, or divide the input image into non-overlapping patches of size 48?48 and compute SA on each patch independently <ref type="bibr" target="#b12">[13]</ref>. However, restricting the spatial extent of SA is contradictory to the goal of capturing the true long-range pixel relationships, especially on high-resolution images.</p><p>In this paper, we propose an efficient Transformer for image restoration that is capable of modeling global connectivity and is still applicable to large images. Specifically, we introduce a multi-Dconv head 'transposed' attention (MDTA) block (Sec. 3.1) in place of vanilla multi-head SA <ref type="bibr" target="#b76">[77]</ref>, that has linear complexity. It applies SA across feature dimension rather than the spatial dimension, i.e., instead of explicitly modeling pairwise pixel interactions, MDTA computes cross-covariance across feature channels to obtain attention map from the (key and query projected) input features. An important feature of our MDTA block is the local context mixing before feature covariance computation. This is achieved via pixel-wise aggregation of crosschannel context using 1?1 convolution and channel-wise aggregation of local context using efficient depth-wise convolutions. This strategy provides two key advantages. First, it emphasizes on the spatially local context and brings in the complimentary strength of convolution operation within our pipeline. Second, it ensures that the contextualized global relationships between pixels are implicitly modeled while computing covariance-based attention maps.</p><p>A feed-forward network (FN) is the other building block of the Transformer model <ref type="bibr" target="#b76">[77]</ref>, which consists of two fully connected layers with a non-linearity in between. In this work, we reformulate the first linear transformation layer of the regular FN <ref type="bibr" target="#b76">[77]</ref> with a gating mechanism <ref type="bibr" target="#b15">[16]</ref> to improve the information flow through the network. This gating layer is designed as the element-wise product of two linear projection layers, one of which is activated with the GELU non-linearity <ref type="bibr" target="#b26">[27]</ref>. Our gated-Dconv FN (GDFN) (Sec. 3.2) is also based on local content mixing similar to the MDTA module to equally emphasize on the spatial context. The gating mechanism in GDFN controls which complementary features should flow forward and allows subsequent layers in the network hierarchy to specifically focus on more refined image attributes, thus leading to high-quality outputs.</p><p>Apart from the above architectural novelties, we show the effectiveness of our progressive learning strategy for Restormer (Sec. 3.3). In this process, the network is trained on small patches and large batches in early epochs, and on gradually large image patches and small batches in later epochs. This training strategy helps Restormer to learn context from large images, and subsequently provides quality performance improvements at test time. We conduct comprehensive experiments and demonstrate state-of-theart performance of our Restormer on 16 benchmark datasets for several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (on single-image and dual pixel data), and image denoising (on synthetic and real data); See <ref type="figure" target="#fig_2">Fig. 1</ref>. Furthermore, we provide extensive ablations to show the effectiveness of architectural designs and experimental choices. The main contributions of this work are summarized below: ? We propose Restormer, an encoder-decoder Transformer for multi-scale local-global representation learning on high-resolution images without disintegrating them into local windows, thereby exploiting distant image context. ? We propose a multi-Dconv head transposed attention (MDTA) module that is capable of aggregating local and non-local pixel interactions, and is efficient enough to process high-resolution images. ? A new gated-Dconv feed-forward network (GDFN) that performs controlled feature transformation, i.e., suppressing less informative features, and allowing only the useful information to pass further through the network hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Image Restoration. In recent years, data-driven CNN architectures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b106">107]</ref> have been shown to outperform conventional restoration approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b74">75]</ref>. Among convolutional designs, encoder-decoder based U-Net architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b98">99]</ref> have been predominantly studied for restoration due to their hierarchical multi-scale representation while remaining computationally efficient. Similarly, skip connection based approaches have been shown to be effective for restoration due to specific focus on learning residual signals <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b105">106]</ref>. Spatial and channel attention modules have also been incorporated to selectively attend to relevant information <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b92">93]</ref>. We refer the reader to NTIRE challenge reports <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b56">57]</ref> and recent literature reviews <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b72">73]</ref>, which summarize major design choices for image restoration. Vision Transformers. The Transformer model is first developed for sequence processing in natural language tasks <ref type="bibr" target="#b76">[77]</ref>. It has been adapted in numerous vision tasks such as image recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b87">88]</ref>, segmentation <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b107">108]</ref>, object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b108">109]</ref>. The Vision Transformers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b75">76]</ref> decompose an image into a sequence of patches (local windows) and learn their mutual relationships. The distinguishing feature of these models is the strong capability to learn long-range dependencies between image patch sequences and adaptability to given input content <ref type="bibr" target="#b33">[34]</ref>. Due to these characteristics, Transformer models have also been studied for the low-level vision problems such as super-resolution <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b84">85]</ref>, image colorization <ref type="bibr" target="#b36">[37]</ref>, denoising <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b79">80]</ref>, and deraining <ref type="bibr" target="#b79">[80]</ref>. However, the computational complexity of SA in Transformers can increase quadratically with the number of image patches, therby prohibiting its application to high-resolution images. Therefore, in lowlevel image processing applications, where high-resolution   outputs need to be generated, recent methods generally employ different strategies to reduce complexity. One potential remedy is to apply self-attention within local image regions <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b79">80]</ref> using the Swin Transformer design <ref type="bibr" target="#b43">[44]</ref>. However, this design choice restricts the context aggregation within local neighbourhood, defying the main motivation of using self-attention over convolutions, thus not ideally suited for image-restoration tasks. In contrast, we present a Transformer model that can learn long-range dependencies while remaining computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our main goal is to develop an efficient Transformer model that can handle high-resolution images for restoration tasks. To alleviate the computational bottleneck, we introduce key designs to the multi-head SA layer and a multiscale hierarchical module that has lesser computing requirements than a single-scale network <ref type="bibr" target="#b43">[44]</ref>. We first present the overall pipeline of our Restormer architecture (see <ref type="figure" target="#fig_4">Fig. 2</ref>). Then we describe the core components of the proposed Transformer block: (a) multi-Dconv head transposed attention (MDTA) and (b) gated-Dconv feed-forward network (GDFN). Finally, we provide details on the progressive training scheme for effectively learning image statistics. Overall Pipeline. Given a degraded image I ? R H?W ?3 , Restormer first applies a convolution to obtain low-level feature embeddings F 0 ? R H?W ?C ; where H ?W denotes the spatial dimension and C is the number of channels. Next, these shallow features F 0 pass through a 4-level symmetric encoder-decoder and transformed into deep features F d ? R H?W ?2C . Each level of encoder-decoder contains multiple Transformer blocks, where the number of blocks are gradually increased from the top to bottom levels to maintain efficiency. Starting from the high-resolution input, the encoder hierarchically reduces spatial size, while expanding channel capacity. The decoder takes low-resolution latent features F l ? R H 8 ? W 8 ?8C as input and progressively recovers the high-resolution representations. For feature downsampling and upsampling, we apply pixel-unshuffle and pixel-shuffle operations <ref type="bibr" target="#b68">[69]</ref>, respectively. To assist the recovery process, the encoder features are concatenated with the decoder features via skip connections <ref type="bibr" target="#b65">[66]</ref>. The concatenation operation is followed by a 1?1 convolution to reduce channels (by half) at all levels, except the top one. At level-1, we let Transformer blocks to aggregate the low-level image features of the encoder with the high-level features of the decoder. It is beneficial in preserving the fine structural and textural details in the restored images. Next, the deep features F d are further enriched in the refinement stage operating at high spatial resolution. These design choices yield quality improvements as we shall see in the experiment section (Sec. 4). Finally, a convolution layer is applied to the refined features to generate residual image R ? R H?W ?3 to which degraded image is added to obtain the restored image:? = I + R. Next, we present the modules of the Transformer block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-Dconv Head Transposed Attention</head><p>The major computational overhead in Transformers comes from the self-attention layer. In conventional SA <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b76">77]</ref>, the time and memory complexity of the key-query dotproduct interaction grows quadratically with the spatial resolution of input, i.e., O(W 2 H 2 ) for images of W ?H pixels. Therefore, it is infeasible to apply SA on most image restoration tasks that often involve high-resolution im-ages. To alleviate this issue, we propose MDTA, shown in <ref type="figure" target="#fig_4">Fig. 2(a)</ref>, that has linear complexity. The key ingredient is to apply SA across channels rather than the spatial dimension, i.e., to compute cross-covariance across channels to generate an attention map encoding the global context implicitly. As another essential component in MDTA, we introduce depth-wise convolutions to emphasize on the local context before computing feature covariance to produce the global attention map.</p><p>From a layer normalized tensor Y ? R? ?? ?? , our MDTA first generates query (Q), key (K) and value (V) projections, enriched with local context. It is achieved by applying 1?1 convolutions to aggregate pixel-wise cross-channel context followed by 3?3 depth-wise convolutions to encode channel-wise spatial context, yielding</p><formula xml:id="formula_0">Q=W Q d W Q p Y, K=W K d W K p Y and V=W V d W V p Y. Where W (?) p is the 1?1 point-wise convolution and W (?)</formula><p>d is the 3?3 depth-wise convolution. We use bias-free convolutional layers in the network. Next, we reshape query and key projections such that their dot-product interaction generates a transposed-attention map A of size R? ?? , instead of the huge regular attention map of size R?? ??? <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b76">77]</ref>. Overall, the MDTA process is defined as:</p><formula xml:id="formula_1">X = W p Attention Q ,K,V + X, Attention Q ,K,V =V ? Softmax K ?Q/? ,<label>(1)</label></formula><p>where X andX are the input and output feature maps; Q ? R?? ?? ;K ? R? ??? ; andV ? R?? ?? matrices are obtained after reshaping tensors from the original size R? ?? ?? . Here, ? is a learnable scaling parameter to control the magnitude of the dot product ofK andQ before applying the softmax function. Similar to the conventional multi-head SA <ref type="bibr" target="#b16">[17]</ref>, we divide the number of channels into 'heads' and learn separate attention maps in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gated-Dconv Feed-Forward Network</head><p>To transform features, the regular feed-forward network (FN) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b76">77]</ref> operates on each pixel location separately and identically. It uses two 1?1 convolutions, one to expand the feature channels (usually by factor ?=4) and second to reduce channels back to the original input dimension. A non-linearity is applied in the hidden layer. In this work, we propose two fundamental modifications in FN to improve representation learning: (1) gating mechanism, and (2) depthwise convolutions. The architecture of our GDFN is shown in <ref type="figure" target="#fig_4">Fig. 2(b)</ref>. The gating mechanism is formulated as the element-wise product of two parallel paths of linear transformation layers, one of which is activated with the GELU non-linearity <ref type="bibr" target="#b26">[27]</ref>. As in MDTA, we also include depth-wise convolutions in GDFN to encode information from spatially neighboring pixel positions, useful for learning local image structure for effective restoration. Given an input tensor X ? R? ?? ?? , GDFN is formulated as:</p><formula xml:id="formula_2">X = W 0 p Gating (X) + X, Gating(X) = ?(W 1 d W 1 p (LN(X))) W 2 d W 2 p (LN(X)),<label>(2)</label></formula><p>where denotes element-wise multiplication, ? represents the GELU non-linearity, and LN is the layer normalization <ref type="bibr" target="#b8">[9]</ref>. Overall, the GDFN controls the information flow through the respective hierarchical levels in our pipeline, thereby allowing each level to focus on the fine details complimentary to the other levels. That is, GDFN offers a distinct role compared to MDTA (focused on enriching features with contextual information). Since the proposed GDFN performs more operations as compared to the regular FN <ref type="bibr" target="#b16">[17]</ref>, we reduce the expansion ratio ? so as to have similar parameters and compute burden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Progressive Learning</head><p>CNN-based restoration models are usually trained on fixed-size image patches. However, training a Transformer model on small cropped patches may not encode the global image statistics, thereby providing suboptimal performance on full-resolution images at test time. To this end, we perform progressive learning where the network is trained on smaller image patches in the early epochs and on gradually larger patches in the later training epochs. The model trained on mixed-size patches via progressive learning shows enhanced performance at test time where images can be of different resolutions (a common case in image restoration). The progressive learning strategy behaves in a similar fashion to the curriculum learning process where the network starts with a simpler task and gradually moves to learning a more complex one (where the preservation of fine image structure/textures is required). Since training on large patches comes at the cost of longer time, we reduce the batch size as the patch size increases to maintain a similar time per optimization step as of the fixed patch training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Analysis</head><p>We evaluate the proposed Restormer on benchmark datasets and experimental settings for four image processing tasks: (a) image deraining, (b) single-image motion deblurring, (c) defocus deblurring (on single-image, and dualpixel data), and (d) image denoising (on synthetic and real data). More details on datasets, training protocols, and additional visual results are presented in the supplementary material. In tables, the best and second-best quality scores of the evaluated methods are highlighted and underlined. Implementation Details. We train separate models for different image restoration tasks. In all experiments, we use the following training parameters, unless mentioned otherwise. Our Restormer employs a 4-level encoder-decoder.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Deraining Results</head><p>We compute PSNR/SSIM scores using the Y channel in YCbCr color space in a way similar to existing methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b92">93]</ref>. <ref type="table" target="#tab_2">Table 1</ref> shows that our Restormer achieves consistent and significant performance gains over existing approaches on all five datasets. Compared to the recent best method SPAIR <ref type="bibr" target="#b60">[61]</ref>, Restormer achieves 1.05 dB improvement when averaged across all datasets. On individual datasets, the gain can be as large as 2.06 dB, e.g., Rain100L. <ref type="figure" target="#fig_5">Figure 3</ref> shows a challenging visual example. Our Restormer reproduces a raindrop-free image while effectively preserving the structural content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Single-image Motion Deblurring Results</head><p>We evaluate deblurring methods both on the synthetic datasets (GoPro <ref type="bibr" target="#b55">[56]</ref>, HIDE <ref type="bibr" target="#b66">[67]</ref>) and the real-world datasets (RealBlur-R <ref type="bibr" target="#b64">[65]</ref>, RealBlur-J <ref type="bibr" target="#b64">[65]</ref>). <ref type="table" target="#tab_3">Table 2</ref> shows that our Restormer outperforms other approaches on all four benchmark datasets. When averaged across all datasets, our method obtains a performance boost of 0.47 dB over the recent algorithm MIMO-UNet+ <ref type="bibr" target="#b13">[14]</ref> and 0.26 dB over the previous best method MPRNet <ref type="bibr" target="#b92">[93]</ref>. Compared to MPR-Net <ref type="bibr" target="#b92">[93]</ref>, Restormer has 81% fewer FLOPs (See <ref type="figure" target="#fig_2">Fig. 1</ref>    <ref type="figure">Figure 5</ref>. Dual-pixel defocus deblurring comparison on the DPDD dataset <ref type="bibr" target="#b2">[3]</ref>. Compared to the other approaches, our Restormer more effectively removes blur while preserving the fine image details.</p><p>Moreover, our method shows 0.4 dB improvement over the Transformer model IPT <ref type="bibr" target="#b12">[13]</ref>, while having 4.4? fewer parameters and runs 29? faster. Notably, our Restormer is trained only on the GoPro <ref type="bibr" target="#b55">[56]</ref> dataset, yet it demonstrates strong generalization to other datasets by setting new stateof-the-art. <ref type="figure" target="#fig_6">Fig. 4</ref> shows that the image produced by our method is more sharper and visually closer to the groundtruth than those of the other algorithms. <ref type="table" target="#tab_5">Table 3</ref> shows image fidelity scores of the conventional defocus deblurring methods (EBDB <ref type="bibr" target="#b32">[33]</ref> and JNB <ref type="bibr" target="#b67">[68]</ref>) as well as learning based approaches on the DPDD dataset <ref type="bibr" target="#b2">[3]</ref>. Our Restormer significantly outperforms the state-of-the-art schemes for the single-image and dual-pixel defocus deblurring tasks on all scene categories. Particularly on the combined scene category, Restormer yields ? 0.6 dB improvements over the previous best method IFAN <ref type="bibr" target="#b40">[41]</ref>. Compared to the Transformer model Uformer <ref type="bibr" target="#b79">[80]</ref>, our method provides a substantial gain of 1.01 dB PSNR. <ref type="figure">Figure 5</ref> illustrates that our method is more effective in removing spatially varying defocus blur than other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Defocus Deblurring Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image Denoising Results</head><p>We perform denoising experiments on synthetic benchmark datasets generated with additive white Gaussian noise (Set12 <ref type="bibr" target="#b100">[101]</ref>, BSD68 <ref type="bibr" target="#b51">[52]</ref>, Urban100 <ref type="bibr" target="#b28">[29]</ref>, Kodak24 <ref type="bibr" target="#b19">[20]</ref> and McMaster <ref type="bibr" target="#b103">[104]</ref>) as well as on real-world datasets    (SIDD <ref type="bibr" target="#b0">[1]</ref> and DND <ref type="bibr" target="#b59">[60]</ref>). Following <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b98">99]</ref>, we use bias-free Restormer for denoising.</p><p>Gaussian denoising. <ref type="table" target="#tab_6">Table 4</ref> and <ref type="table" target="#tab_7">Table 5</ref> show PSNR scores of different approaches on several benchmark datasets for grayscale and color image denoising, respectively. Consistent with existing methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b98">99]</ref>, we include noise levels 15, 25 and 50 in testing. The evaluated methods are divided into two experimental categories:</p><p>(1) learning a single model to handle various noise levels, and (2) learning a separate model for each noise level. Our Restormer achieves state-of-the-art performance under both experimental settings on different datasets and noise levels. Specifically, for the challenging noise level 50 on high-resolution Urban100 dataset <ref type="bibr" target="#b28">[29]</ref>, Restormer achieves 0.37 dB gain over the previous best CNN-based method DRUNet <ref type="bibr" target="#b98">[99]</ref>, and 0.31 dB boost over the recent <ref type="table" target="#tab_10">Table 7</ref>.</p><p>Ablation experiments for the Transformer block. PSNR is computed on a high-resolution Urban100 dataset <ref type="bibr" target="#b28">[29]</ref>. transformer-based network SwinIR <ref type="bibr" target="#b43">[44]</ref>, as shown in <ref type="table" target="#tab_6">Table 4</ref>. Similar performance gains can be observed for the Gaussian color denoising in <ref type="table" target="#tab_7">Table 5</ref>. It is worth mentioning that DRUNet <ref type="bibr" target="#b98">[99]</ref> requires the noise level map as an additional input, whereas our method only takes the noisy image. Furthermore, compared to SwinIR <ref type="bibr" target="#b43">[44]</ref>, our Restormer has 3.14? fewer FLOPs and runs 13? faster. <ref type="figure" target="#fig_7">Figure 6</ref> presents denoised results by different methods for grayscale denoising (top row) and color denoising (middle row). Our Restormer restores clean and crisp images. Real image denoising. <ref type="table" target="#tab_8">Table 6</ref> shows that our method is the only one surpassing 40 dB PSNR on both datasets. Notably, on the SIDD dataset our Restormer obtains PSNR gains of 0.3 dB and 0.25 dB over the previous best CNN method MIRNet <ref type="bibr" target="#b91">[92]</ref> and Transformer model Uformer <ref type="bibr" target="#b79">[80]</ref>, respectively. <ref type="figure" target="#fig_7">Fig. 6</ref> (bottom row) shows that our Restormer generates clean image without compromising fine texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>For ablation experiments, we train Gaussian color denoising models on image patches of size 128?128 for 100K iterations only. Testing is performed on Urban100 <ref type="bibr" target="#b28">[29]</ref>, and analysis is provided for a challenging noise level ?=50. FLOPs and inference time are computed on image size 256?256. <ref type="table" target="#tab_10">Table 7</ref>-10 show that our contributions yield quality performance improvements. Next, we describe the influence of each component individually. Improvements in multi-head attention. <ref type="table" target="#tab_10">Table 7c</ref> demonstrates that our MDTA provides favorable gain of 0.32 dB over the baseline <ref type="table" target="#tab_10">(Table 7a</ref>). Furthermore, bringing locality to MDTA via depth-wise convolution improves robustness as removing it results in PSNR drop (see <ref type="table" target="#tab_10">Table 7b</ref>).</p><p>Improvements in feed-forward network (FN). <ref type="table" target="#tab_10">Table 7d</ref> shows that the gating mechanism in FN to control information flow yields 0.12 dB gain over the conventional FN <ref type="bibr" target="#b76">[77]</ref>. As in multi-head attention, introducing local mechanism to FN also brings performance advantages (see <ref type="table" target="#tab_10">Table 7e</ref>). We further strengthen the FN by incorporating gated depth-wise convolutions. Our GDFN <ref type="table" target="#tab_10">(Table 7f</ref>) achieves PSNR gain of 0.26 dB over the standard FN <ref type="bibr" target="#b76">[77]</ref> for the noise level 50. Overall, our Transformer block contributions lead to a significant gain of 0.51 dB over the baseline. Design choices for decoder at level-1. To aggregate encoder features with the decoder at level-1, we do not employ 1?1 convolution (that reduces channels by half) after concatenation operation. It is helpful in preserving fine textural details coming from the encoder, as shown in Table <ref type="bibr" target="#b7">8</ref>. These results further demonstrate the effectiveness of adding Transformer blocks in the refinement stage. Impact of progressive learning. <ref type="table" target="#tab_11">Table 9</ref> shows that the progressive learning provides better results than the fixed patch training, while having similar training time. Deeper or wider Restormer? <ref type="table" target="#tab_2">Table 10</ref> shows that, under similar parameters/FLOPs budget, a deep-narrow model performs more accurately than its wide-shallow counterpart. However, the wider model runs faster due to parallelization. In this paper we use deep-narrow Restormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present an image restoration Transformer model, Restormer, that is computationally efficient to handle highresolution images. We introduce key designs to the core components of the Transformer block for improved feature aggregation and transformation. Specifically, our multi-Dconv head transposed attention (MDTA) module implicitly models global context by applying self-attention across channels rather than the spatial dimension, thus having linear complexity rather than quadratic. Furthermore, the proposed gated-Dconv feed-forward network (GDFN) introduces a gating mechanism to perform controlled feature transformation. To incorporate the strength of CNNs into the Transformer model, both MDTA and GDFN modules include depth-wise convolutions for encoding spatially local context. Extensive experiments on 16 benchmark datasets demonstrate that Restormer achieves the state-ofthe-art performance for numerous image restoration tasks. <ref type="table">Table 8</ref>. Influence of concat (w/o 1x1 conv) and refinement stage at decoder (level-1). We add the components to experiment Table 7(g).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSNR (?=50) FLOPs Params</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) The convo-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Our Restormer achieves the state-of-the-art performance on image restoration tasks while being computationally efficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of Restormer for high-resolution image restoration. Our Restormer consists of multiscale hierarchical design incorporating efficient Transformer blocks. The core modules of Transformer block are: (a) multi-Dconv head transposed attention (MDTA) that performs (spatially enriched) query-key feature interaction across channels rather the spatial dimension, and (b) Gated-Dconv feed-forward network (GDFN) that performs controlled feature transformation, i.e., to allow useful information to propagate further.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Image deraining example. Our Restormer generates rain-free image with structural fidelity and without artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Single image motion deblurring on GoPro<ref type="bibr" target="#b55">[56]</ref>. Restormer generates sharper and visually-faithful result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Visual results on Image denoising. Top row: Gaussian grayscale denoising. Middle row: Gaussian color denoising. Bottom row: real image denoising. The image reproduction quality of our Restormer is more faithful to the ground-truth than other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Image deraining results. When averaged across all five datasets, our Restormer advances state-of-the-art by 1.05 dB.</figDesc><table><row><cell></cell><cell cols="2">Test100 [97]</cell><cell cols="2">Rain100H [86]</cell><cell cols="2">Rain100L [86]</cell><cell cols="3">Test2800 [22]</cell><cell cols="3">Test1200 [96]</cell><cell>Average</cell></row><row><cell>Method</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell cols="2">PSNR ?</cell><cell>SSIM ?</cell><cell cols="2">PSNR ?</cell><cell>SSIM ?</cell><cell>PSNR ?</cell><cell>SSIM ?</cell></row><row><cell>DerainNet [21]</cell><cell>22.77</cell><cell>0.810</cell><cell>14.92</cell><cell>0.592</cell><cell>27.03</cell><cell>0.884</cell><cell>24.31</cell><cell></cell><cell>0.861</cell><cell>23.38</cell><cell></cell><cell>0.835</cell><cell>22.48</cell><cell>0.796</cell></row><row><cell>SEMI [81]</cell><cell>22.35</cell><cell>0.788</cell><cell>16.56</cell><cell>0.486</cell><cell>25.03</cell><cell>0.842</cell><cell>24.43</cell><cell></cell><cell>0.782</cell><cell>26.05</cell><cell></cell><cell>0.822</cell><cell>22.88</cell><cell>0.744</cell></row><row><cell>DIDMDN [96]</cell><cell>22.56</cell><cell>0.818</cell><cell>17.35</cell><cell>0.524</cell><cell>25.23</cell><cell>0.741</cell><cell>28.13</cell><cell></cell><cell>0.867</cell><cell>29.65</cell><cell></cell><cell>0.901</cell><cell>24.58</cell><cell>0.770</cell></row><row><cell>UMRL [87]</cell><cell>24.41</cell><cell>0.829</cell><cell>26.01</cell><cell>0.832</cell><cell>29.18</cell><cell>0.923</cell><cell>29.97</cell><cell></cell><cell>0.905</cell><cell>30.55</cell><cell></cell><cell>0.910</cell><cell>28.02</cell><cell>0.880</cell></row><row><cell>RESCAN [43]</cell><cell>25.00</cell><cell>0.835</cell><cell>26.36</cell><cell>0.786</cell><cell>29.80</cell><cell>0.881</cell><cell>31.29</cell><cell></cell><cell>0.904</cell><cell>30.51</cell><cell></cell><cell>0.882</cell><cell>28.59</cell><cell>0.857</cell></row><row><cell>PreNet [64]</cell><cell>24.81</cell><cell>0.851</cell><cell>26.77</cell><cell>0.858</cell><cell>32.44</cell><cell>0.950</cell><cell>31.75</cell><cell></cell><cell>0.916</cell><cell>31.36</cell><cell></cell><cell>0.911</cell><cell>29.42</cell><cell>0.897</cell></row><row><cell>MSPFN [32]</cell><cell>27.50</cell><cell>0.876</cell><cell>28.66</cell><cell>0.860</cell><cell>32.40</cell><cell>0.933</cell><cell>32.82</cell><cell></cell><cell>0.930</cell><cell>32.39</cell><cell></cell><cell>0.916</cell><cell>30.75</cell><cell>0.903</cell></row><row><cell>MPRNet [93]</cell><cell>30.27</cell><cell>0.897</cell><cell>30.41</cell><cell>0.890</cell><cell>36.40</cell><cell>0.965</cell><cell>33.64</cell><cell></cell><cell>0.938</cell><cell>32.91</cell><cell></cell><cell>0.916</cell><cell>32.73</cell><cell>0.921</cell></row><row><cell>SPAIR [61]</cell><cell>30.35</cell><cell>0.909</cell><cell>30.95</cell><cell>0.892</cell><cell>36.93</cell><cell>0.969</cell><cell>33.34</cell><cell></cell><cell>0.936</cell><cell>33.04</cell><cell></cell><cell>0.922</cell><cell>32.91</cell><cell>0.926</cell></row><row><cell>Restormer</cell><cell>32.00</cell><cell>0.923</cell><cell>31.46</cell><cell>0.904</cell><cell>38.99</cell><cell>0.978</cell><cell>34.18</cell><cell></cell><cell>0.944</cell><cell>33.19</cell><cell></cell><cell>0.926</cell><cell>33.96</cell><cell>0.935</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell></cell><cell>18.76 dB</cell><cell></cell><cell></cell><cell>20.23 dB</cell><cell></cell><cell></cell><cell>23.66 dB</cell><cell>25.52 dB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Reference</cell><cell>Rainy</cell><cell cols="4">DerainNet [21]</cell><cell cols="2">SEMI [81]</cell><cell>UMRL [87]</cell></row><row><cell></cell><cell>18.76 dB</cell><cell></cell><cell></cell><cell cols="2">26.88 dB</cell><cell>27.16 dB</cell><cell></cell><cell></cell><cell>29.86 dB</cell><cell></cell><cell cols="2">32.15 dB</cell><cell>33.97 dB</cell></row><row><cell></cell><cell cols="2">Rainy Image</cell><cell></cell><cell cols="2">RESCAN [43]</cell><cell cols="2">PreNet [64]</cell><cell cols="2">MSPFN [32]</cell><cell cols="3">MPRNet [93]</cell><cell>Restormer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>GoPro [56]</cell><cell cols="3">HIDE [67] RealBlur-R [65] RealBlur-J [65]</cell></row><row><cell>Method</cell><cell cols="3">PSNR SSIM PSNR SSIM PSNR SSIM</cell><cell>PSNR SSIM</cell></row><row><cell>Xu et al. [84]</cell><cell>21.00 0.741</cell><cell>-</cell><cell>34.46 0.937</cell><cell>27.14 0.830</cell></row><row><cell cols="3">DeblurGAN [38] 28.70 0.858 24.51 0.871</cell><cell>33.79 0.903</cell><cell>27.97 0.834</cell></row><row><cell>Nah et al. [56]</cell><cell cols="2">29.08 0.914 25.73 0.874</cell><cell>32.51 0.841</cell><cell>27.87 0.827</cell></row><row><cell>Zhang et al. [98]</cell><cell>29.19 0.931</cell><cell>-</cell><cell>35.48 0.947</cell><cell>27.80 0.847</cell></row><row><cell cols="3">DeblurGAN-v2 [39] 29.55 0.934 26.61 0.875</cell><cell>35.26 0.944</cell><cell>28.70 0.866</cell></row><row><cell>SRN [72]</cell><cell cols="2">30.26 0.934 28.36 0.915</cell><cell>35.66 0.947</cell><cell>28.56 0.867</cell></row><row><cell>Shen et al. [67]</cell><cell>-</cell><cell>28.89 0.930</cell><cell>-</cell><cell>-</cell></row><row><cell>Gao et al. [23]</cell><cell cols="2">30.90 0.935 29.11 0.913</cell><cell>-</cell><cell>-</cell></row><row><cell>DBGAN [100]</cell><cell cols="2">31.10 0.942 28.94 0.915</cell><cell>33.78 0.909</cell><cell>24.93 0.745</cell></row><row><cell>MT-RNN [58]</cell><cell cols="2">31.15 0.945 29.15 0.918</cell><cell>35.79 0.951</cell><cell>28.44 0.862</cell></row><row><cell>DMPHN [94]</cell><cell cols="2">31.20 0.940 29.09 0.924</cell><cell>35.70 0.948</cell><cell>28.42 0.860</cell></row><row><cell>Suin et al. [71]</cell><cell cols="2">31.85 0.948 29.98 0.930</cell><cell>-</cell><cell>-</cell></row><row><cell>SPAIR [61]</cell><cell cols="2">32.06 0.953 30.29 0.931</cell><cell>-</cell><cell>28.81 0.875</cell></row><row><cell cols="3">MIMO-UNet+ [14] 32.45 0.957 29.99 0.930</cell><cell>35.54 0.947</cell><cell>27.63 0.837</cell></row><row><cell>IPT [13]</cell><cell>32.52 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MPRNet [93]</cell><cell cols="2">32.66 0.959 30.96 0.939</cell><cell>35.99 0.952</cell><cell>28.70 0.873</cell></row><row><cell>Restormer</cell><cell cols="2">32.92 0.961 31.22 0.942</cell><cell>36.19 0.957</cell><cell>28.96 0.879</cell></row><row><cell cols="5">From level-1 to level-4, the number of Transformer blocks</cell></row><row><cell cols="5">are [4, 6, 6, 8], attention heads in MDTA are [1, 2, 4, 8], and</cell></row><row><cell cols="5">number of channels are [48, 96, 192, 384]. The refinement</cell></row><row><cell cols="5">stage contains 4 blocks. The channel expansion factor in</cell></row><row><cell cols="5">GDFN is ?=2.66. We train models with AdamW optimizer</cell></row><row><cell>(?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Single-image motion deblurring results. Our Restormer is trained only on the GoPro dataset [56] and directly applied to the HIDE [67] and RealBlur [65] benchmark datasets.1 =0.9, ? 2 =0.999, weight decay 1e ?4 ) and L 1 loss for 300K iterations with the initial learning rate 3e ?4 gradually reduced to 1e ?6 with the cosine annealing [51]. For pro- gressive learning, we start training with patch size 128?128 and batch size 64. The patch size and batch size pairs are updated to [(160 2 ,40), (192 2 ,32), (256 2 ,16), (320 2 ,8), (384 2 ,8)] at iterations [92K, 156K, 204K, 240K, 276K]. For data augmentation, we use horizontal and vertical flips.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Defocus deblurring comparisons on the DPDD testset [3] (containing 37 indoor and 39 outdoor scenes). S: single-image defocus deblurring. D: dual-pixel defocus deblurring. Restormer sets new state-of-the-art for both single-image and dual pixel defocus deblurring.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Indoor Scenes</cell><cell></cell><cell></cell><cell cols="2">Outdoor Scenes</cell><cell></cell><cell></cell><cell cols="2">Combined</cell><cell></cell></row><row><cell>Method</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>MAE ?</cell><cell>LPIPS ?</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>MAE ?</cell><cell>LPIPS ?</cell><cell>PSNR ?</cell><cell>SSIM ?</cell><cell>MAE ?</cell><cell>LPIPS ?</cell></row><row><cell>EBDB S [33]</cell><cell>25.77</cell><cell>0.772</cell><cell>0.040</cell><cell>0.297</cell><cell>21.25</cell><cell>0.599</cell><cell>0.058</cell><cell>0.373</cell><cell>23.45</cell><cell>0.683</cell><cell>0.049</cell><cell>0.336</cell></row><row><cell>DMENet S [40]</cell><cell>25.50</cell><cell>0.788</cell><cell>0.038</cell><cell>0.298</cell><cell>21.43</cell><cell>0.644</cell><cell>0.063</cell><cell>0.397</cell><cell>23.41</cell><cell>0.714</cell><cell>0.051</cell><cell>0.349</cell></row><row><cell>JNB S [68]</cell><cell>26.73</cell><cell>0.828</cell><cell>0.031</cell><cell>0.273</cell><cell>21.10</cell><cell>0.608</cell><cell>0.064</cell><cell>0.355</cell><cell>23.84</cell><cell>0.715</cell><cell>0.048</cell><cell>0.315</cell></row><row><cell>DPDNet S [3]</cell><cell>26.54</cell><cell>0.816</cell><cell>0.031</cell><cell>0.239</cell><cell>22.25</cell><cell>0.682</cell><cell>0.056</cell><cell>0.313</cell><cell>24.34</cell><cell>0.747</cell><cell>0.044</cell><cell>0.277</cell></row><row><cell>KPAC S [70]</cell><cell>27.97</cell><cell>0.852</cell><cell>0.026</cell><cell>0.182</cell><cell>22.62</cell><cell>0.701</cell><cell>0.053</cell><cell>0.269</cell><cell>25.22</cell><cell>0.774</cell><cell>0.040</cell><cell>0.227</cell></row><row><cell>IFAN S [41]</cell><cell>28.11</cell><cell>0.861</cell><cell>0.026</cell><cell>0.179</cell><cell>22.76</cell><cell>0.720</cell><cell>0.052</cell><cell>0.254</cell><cell>25.37</cell><cell>0.789</cell><cell>0.039</cell><cell>0.217</cell></row><row><cell>Restormer S</cell><cell>28.87</cell><cell>0.882</cell><cell>0.025</cell><cell>0.145</cell><cell>23.24</cell><cell>0.743</cell><cell>0.050</cell><cell>0.209</cell><cell>25.98</cell><cell>0.811</cell><cell>0.038</cell><cell>0.178</cell></row><row><cell>DPDNet D [3]</cell><cell>27.48</cell><cell>0.849</cell><cell>0.029</cell><cell>0.189</cell><cell>22.90</cell><cell>0.726</cell><cell>0.052</cell><cell>0.255</cell><cell>25.13</cell><cell>0.786</cell><cell>0.041</cell><cell>0.223</cell></row><row><cell>RDPD D [4]</cell><cell>28.10</cell><cell>0.843</cell><cell>0.027</cell><cell>0.210</cell><cell>22.82</cell><cell>0.704</cell><cell>0.053</cell><cell>0.298</cell><cell>25.39</cell><cell>0.772</cell><cell>0.040</cell><cell>0.255</cell></row><row><cell>Uformer D [80]</cell><cell>28.23</cell><cell>0.860</cell><cell>0.026</cell><cell>0.199</cell><cell>23.10</cell><cell>0.728</cell><cell>0.051</cell><cell>0.285</cell><cell>25.65</cell><cell>0.795</cell><cell>0.039</cell><cell>0.243</cell></row><row><cell>IFAN D [41]</cell><cell>28.66</cell><cell>0.868</cell><cell>0.025</cell><cell>0.172</cell><cell>23.46</cell><cell>0.743</cell><cell>0.049</cell><cell>0.240</cell><cell>25.99</cell><cell>0.804</cell><cell>0.037</cell><cell>0.207</cell></row><row><cell>Restormer D</cell><cell>29.48</cell><cell>0.895</cell><cell>0.023</cell><cell>0.134</cell><cell>23.97</cell><cell>0.773</cell><cell>0.047</cell><cell>0.175</cell><cell>26.66</cell><cell>0.833</cell><cell>0.035</cell><cell>0.155</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PSNR</cell><cell></cell><cell>27.19 dB</cell><cell></cell><cell>27.44 dB</cell><cell></cell><cell cols="2">28.67 dB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Reference</cell><cell></cell><cell>Blurry</cell><cell></cell><cell cols="2">DMENet [40]</cell><cell cols="2">DPDNet [3]</cell></row><row><cell></cell><cell cols="2">27.19 dB</cell><cell></cell><cell></cell><cell>29.01 dB</cell><cell></cell><cell>28.35 dB</cell><cell></cell><cell>29.12 dB</cell><cell></cell><cell cols="2">30.45 dB</cell></row><row><cell></cell><cell cols="2">Blurry Image</cell><cell></cell><cell></cell><cell>RDPD [4]</cell><cell></cell><cell>IFAN [41]</cell><cell></cell><cell cols="2">Uformer [80]</cell><cell cols="2">Restormer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Gaussian grayscale image denoising comparisons for two categories of methods. Top super row: learning a single model to handle various noise levels. Bottom super row: training a separate model for each noise level. DnCNN [101] 32.67 30.35 27.18 31.62 29.16 26.23 32.28 29.80 26.35 FFDNet [103] 32.75 30.43 27.32 31.63 29.19 26.29 32.40 29.90 26.50 IRCNN [102] 32.76 30.37 27.12 31.63 29.15 26.19 32.46 29.80 26.22 DRUNet [99] 33.25 30.94 27.90 31.91 29.48 26.59 33.44 31.11 27.96 Restormer 33.35 31.04 28.01 31.95 29.51 26.62 33.67 31.39 28.33 FOCNet [31] 33.07 30.73 27.68 31.83 29.38 26.50 33.15 30.64 27.40 MWCNN [47] 33.15 30.79 27.74 31.86 29.41 26.53 33.17 30.66 27.42 NLRN [46] 33.16 30.80 27.64 31.88 29.41 26.47 33.45 30.] 33.19 30.81 27.74 31.91 29.44 26.54 33.37 30.85 27.53 DAGL [55] 33.28 30.93 27.81 31.93 29.46 26.51 33.79 31.39 27.97 SwinIR [44] 33.36 31.01 27.91 31.97 29.50 26.58 33.70 31.30 27.98 Restormer 33.42 31.08 28.00 31.96 29.52 26.62 33.79 31.46 28.29</figDesc><table><row><cell></cell><cell></cell><cell>Set12 [101]</cell><cell>BSD68 [52]</cell><cell cols="2">Urban100 [29]</cell></row><row><cell>Method</cell><cell cols="5">?=15 ?=25 ?=50 ?=15 ?=25 ?=50 ?=15 ?=25 ?=50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>94 27.49</cell></row><row><cell>RNAN [106]</cell><cell>-</cell><cell>-27.70 -</cell><cell cols="2">-26.48 -</cell><cell>-27.65</cell></row><row><cell>DeamNet [63</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Gaussian color image denoising. Our Restormer demonstrates favorable performance among both categories of methods. On Urban dataset<ref type="bibr" target="#b28">[29]</ref> for noise level 50, Restormer yields 0.41 dB gain over CNNbased DRUNet<ref type="bibr" target="#b98">[99]</ref>, and 0.2 dB over Transformer model SwinIR<ref type="bibr" target="#b43">[44]</ref>.IRCNN  [102] 33.86 31.16 27.86 34.69 32.18 28.93 34.58 32.18 28.91 33.78 31.20 27.70 FFDNet [103] 33.87 31.21 27.96 34.63 32.13 28.98 34.66 32.35 29.18 33.83 31.40 28.05 DnCNN [101] 33.90 31.24 27.95 34.60 32.14 28.95 33.45 31.52 28.62 32.98 30.81 27.59 DSNet [59] 33.91 31.28 28.05 34.63 32.16 29.05 34.67 32.40 29.28 ---DRUNet [99] 34.30 31.69 28.51 35.31 32.89 29.86 35.40 33.14 30.08 34.81 32.60 29.61 Restormer 34.39 31.78 28.59 35.44 33.02 30.00 35.55 33.31 30.29 35.06 32.91 30.02 RPCNN [82] -31.24 28.06 -32.34 29.25 -32.33 29.33 -31.81 28.62 BRDNet [74] 34.10 31.43 28.16 34.88 32.41 29.22 35.08 32.75 29.52 34.42 31.99 28.34.42 31.78 28.56 35.34 32.89 29.79 35.61 33.20 30.22 35.13 32.90 29.82 Restormer 34.40 31.79 28.60 35.47 33.04 30.01 35.61 33.34 30.30 35.13 32.96 30.02</figDesc><table><row><cell></cell><cell cols="2">CBSD68 [52]</cell><cell cols="2">Kodak24 [20]</cell><cell cols="3">McMaster [104]</cell><cell>Urban100 [29]</cell></row><row><cell>Method</cell><cell cols="8">?=15 ?=25 ?=50 ?=15 ?=25 ?=50 ?=15 ?=25 ?=50 ?=15 ?=25 ?=50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>56</cell></row><row><cell>RNAN [106]</cell><cell>-</cell><cell cols="2">-28.27 -</cell><cell cols="2">-29.58 -</cell><cell cols="3">-29.72 -</cell><cell>-29.08</cell></row><row><cell>RDN [107]</cell><cell>-</cell><cell cols="2">-28.31 -</cell><cell cols="2">-29.66 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-29.38</cell></row><row><cell>IPT [13]</cell><cell>-</cell><cell cols="2">-28.39 -</cell><cell cols="2">-29.64 -</cell><cell cols="3">-29.98 -</cell><cell>-29.71</cell></row><row><cell>SwinIR [44]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Real image denoising on SIDD<ref type="bibr" target="#b0">[1]</ref> and DND<ref type="bibr" target="#b59">[60]</ref> datasets. * denotes methods using additional training data. Our Restormer is trained only on the SIDD images and directly tested on DND. Among competing approaches, only Restormer surpasses 40 dB PSNR.</figDesc><table><row><cell></cell><cell cols="16">Method DnCNN BM3D CBDNet* RIDNet* AINDNet* VDN SADNet* DANet+* CycleISP* MIRNet DeamNet* MPRNet DAGL Uformer Restormer</cell></row><row><cell>Dataset</cell><cell></cell><cell>[101]</cell><cell>[15]</cell><cell>[25]</cell><cell>[6]</cell><cell>[35]</cell><cell>[89]</cell><cell>[12]</cell><cell>[90]</cell><cell>[91]</cell><cell>[92]</cell><cell>[63]</cell><cell>[93]</cell><cell>[55]</cell><cell>[80]</cell><cell>(Ours)</cell></row><row><cell cols="4">SIDD PSNR ? 23.66 25.65</cell><cell>30.78</cell><cell>38.71</cell><cell>39.08</cell><cell cols="2">39.28 39.46</cell><cell>39.47</cell><cell>39.52</cell><cell>39.72</cell><cell>39.47</cell><cell cols="3">39.71 38.94 39.77</cell><cell>40.02</cell></row><row><cell>[1]</cell><cell cols="3">SSIM ? 0.583 0.685</cell><cell>0.801</cell><cell>0.951</cell><cell>0.954</cell><cell cols="2">0.956 0.957</cell><cell>0.957</cell><cell>0.957</cell><cell>0.959</cell><cell>0.957</cell><cell cols="3">0.958 0.953 0.959</cell><cell>0.960</cell></row><row><cell cols="4">DND PSNR ? 32.43 34.51</cell><cell>38.06</cell><cell>39.26</cell><cell>39.37</cell><cell cols="2">39.38 39.59</cell><cell>39.58</cell><cell>39.56</cell><cell>39.88</cell><cell>39.63</cell><cell cols="3">39.80 39.77 39.96</cell><cell>40.03</cell></row><row><cell cols="4">[60] SSIM ? 0.790 0.851</cell><cell>0.942</cell><cell>0.953</cell><cell>0.951</cell><cell cols="2">0.952 0.952</cell><cell>0.955</cell><cell>0.956</cell><cell>0.956</cell><cell>0.953</cell><cell cols="3">0.954 0.956 0.956</cell><cell>0.956</cell></row><row><cell cols="2">Noisy</cell><cell cols="2">14.92 dB</cell><cell></cell><cell>PSNR</cell><cell></cell><cell cols="2">27.61 dB</cell><cell cols="2">31.83 dB</cell><cell cols="2">30.12 dB</cell><cell cols="2">31.74 dB</cell><cell cols="2">32.83 dB</cell></row><row><cell cols="2">Image</cell><cell></cell><cell>Noisy</cell><cell></cell><cell>Reference</cell><cell cols="7">DnCNN [101] DRUNet [99] DeamNet [63]</cell><cell cols="2">SwinIR [44]</cell><cell cols="2">Restormer</cell></row><row><cell cols="2">Noisy</cell><cell cols="2">14.81 dB</cell><cell></cell><cell>PSNR</cell><cell></cell><cell cols="2">33.83 dB</cell><cell cols="2">35.09 dB</cell><cell cols="2">34.86 dB</cell><cell cols="2">35.20 dB</cell><cell cols="2">35.63 dB</cell></row><row><cell cols="2">Image</cell><cell></cell><cell>Noisy</cell><cell></cell><cell>Reference</cell><cell cols="3">FFDNet [103]</cell><cell cols="2">DRUNet [99]</cell><cell cols="2">IPT [13]</cell><cell cols="2">SwinIR [44]</cell><cell cols="2">Restormer</cell></row><row><cell cols="2">Noisy</cell><cell cols="2">18.16 dB</cell><cell></cell><cell>PSNR</cell><cell></cell><cell cols="2">31.36 dB</cell><cell cols="2">30.25 dB</cell><cell cols="2">31.17 dB</cell><cell cols="2">31.15 dB</cell><cell cols="2">31.57 dB</cell></row><row><cell cols="2">Image</cell><cell></cell><cell>Noisy</cell><cell></cell><cell>Reference</cell><cell></cell><cell cols="2">MIRNet [92]</cell><cell cols="4">DeamNet [63] MPRNet [93]</cell><cell cols="2">Uformer [80]</cell><cell cols="2">Restormer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>UNet with Resblocks [45] 83.4 24.53 34.42 32.18 29.11</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">FLOPs Params</cell><cell>PSNR</cell></row><row><cell>Network</cell><cell></cell><cell>Component</cell><cell>(B)</cell><cell>(M) ?=15 ?=25 ?=50</cell></row><row><cell cols="2">Baseline (a) Multi-head (b)</cell><cell>MTA + FN [77]</cell><cell cols="2">83.7 24.84 34.66 32.39 29.28</cell></row><row><cell cols="2">attention (c)</cell><cell>MDTA + FN [77]</cell><cell cols="2">85.3 25.02 34.72 32.48 29.43</cell></row><row><cell cols="2">Feed-forward (d)</cell><cell>MTA + GFN</cell><cell cols="2">83.5 24.79 34.70 32.43 32.40</cell></row><row><cell cols="2">network (e)</cell><cell>MTA + DFN</cell><cell cols="2">85.8 25.08 34.68 32.45 29.42</cell></row><row><cell></cell><cell>(f)</cell><cell>MTA + GDFN</cell><cell cols="2">86.2 25.12 34.77 32.56 29.54</cell></row><row><cell>Overall</cell><cell>(g)</cell><cell>MDTA + GDFN</cell><cell cols="2">87.7 25.31 34.82 32.61 29.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 (</head><label>7</label><figDesc></figDesc><table><row><cell>g)</cell><cell>29.62</cell><cell>87.7</cell><cell>25.31</cell></row><row><cell>+ Concat</cell><cell>29.66</cell><cell>110</cell><cell>25.65</cell></row><row><cell>+ Refinement</cell><cell>29.71</cell><cell>141</cell><cell>26.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Results of training Restormer on fixed patch size and progressively large patch sizes. In progressive learning<ref type="bibr" target="#b27">[28]</ref>, we reduce batch size (as patch size increases) to have similar time per optimization step as of fixed patch training.</figDesc><table><row><cell>Patch Size</cell><cell cols="2">PSNR (?=50) Train Time (h)</cell></row><row><cell>Fixed (128 2 )</cell><cell>29.71</cell><cell>22.5</cell></row><row><cell>Progressive (128 2 to 384 2 )</cell><cell>29.78</cell><cell>23.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Deeper vs wider model. We adjust # of transformer blocks to keep flops and params constant. Deep narrow model is more accurate, while wide shallow model is faster.</figDesc><table><row><cell cols="4">Dim PSNR (?=50) Train Time (h) Test Time (s)</cell></row><row><cell>48</cell><cell>29.71</cell><cell>22.5</cell><cell>0.115</cell></row><row><cell>64</cell><cell>29.63</cell><cell>15.5</cell><cell>0.080</cell></row><row><cell>80</cell><cell>29.56</cell><cell>13.5</cell><cell>0.069</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Ming-Hsuan Yang is supported by the NSF CAREER grant 1149783. Munawar Hayat is supported by the ARC DECRA Fellowship DE200101100. Special thanks to Abdullah Abuolaim and Zhendong Wang for providing the results.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NTIRE 2019 challenge on real image denoising: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Defocus deblurring using dual-pixel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Abuolaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to reduce defocus blur by realistically modeling dual-pixel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Abuolaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Delbracio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NTIRE 2021 challenge for defocus deblurring using dualpixel images: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Abuolaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Real image denoising with feature attention. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely residual laplacian super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A deep journey into super-resolution: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatialadaptive network for single image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking coarse-to-fine approach in single image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seo-Won</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Pyo</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Jea</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering. TIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Burst image restoration and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Dudhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Kodak lossless true color image suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Franzen</surname></persName>
		</author>
		<ptr target="http://r0k.us/graphics/kodak/" />
		<imprint>
			<date type="published" when="1999-10" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Online accessed 24</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clearing the skies: A deep network architecture for single-image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Removing rain from single images via a deep detail network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring with parameter selective sharing and nested skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-guided network for fast image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shi Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (GELUs)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mix &amp; match: training convnets with mixed image sizes for improved accuracy, speed and scale resiliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berry</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08986</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed selfexemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">NTIRE 2019 challenge on image enhancement: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Focnet: A fractional optimal control network for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xixi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-scale progressive fusion network for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Edge-based defocus blur estimation with adaptive scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Karaali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio Rosito</forename><surname>Jung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transfer learning from synthetic to real-noise denoising with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Woong</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Ik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep photo: Model-based photograph enhancement and viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Colorization transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DeblurGAN: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DeblurGAN-v2: Deblurring (orders-ofmagnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep defocus map estimation using domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Jaesung Rim, Sunghyun Cho, and Seungyong Lee. Iterative filter adaptive network for single image defocus deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongseok</forename><surname>Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Single image deraining: A comprehensive benchmark analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iago</forename><forename type="middle">Breno</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><forename type="middle">Hirata</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cesar-Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recurrent squeeze-and-excitation context aggregation net for single image deraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SwinIR: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-level wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dual residual networks leveraging the potential of paired operations for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nonparametric blind super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust and interpretable blind image denoising via bias-free convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreyas</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Kadkhodaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dynamic attentive graph learning for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyuan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ntire 2021 challenge on image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Un</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se Young</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dilated residual networks with symmetric skip connection for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xili</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Spatially-adaptive image restoration using distortion-guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maitreya</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu Naresh</forename><surname>An Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adaptive consistency prior based deep network for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohai</forename><surname>Chao Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuncheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Progressive image deraining networks: A better and simpler baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Dongwei Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Real-world blur dataset for learning and benchmarking deblurring algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haeyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jucheol</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Human-aware motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Just noticeable defocus blur detection and estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Single image defocus deblurring using kernel-sharing parallel atrous convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongseok</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Spatially-attentive patch-hierarchical network for adaptive motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maitreya</forename><surname>Suin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep learning on image denoising: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunwei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lunke</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Image denoising using deep cnn with batch renormalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunwei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03106</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Semi-supervised transfer learning for image rain removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Identifying recurring patterns with deep neural networks for natural image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Unnatural l0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shicheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deep joint rain detection and removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image de-raining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Variational denoising network: Toward blind noise modeling and removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongsheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Dual adversarial network: Toward real-world noise removal and noise generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongsheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">CycleISP: Real image restoration via improved data synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning enriched features for real image restoration and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Multi-stage progressive image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Density-aware single image de-raining using a multi-stream dense network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Image de-raining using a conditional generative adversarial network. TCSVT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwanath</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Dynamic scene deblurring using spatially variant recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Plug-and-play image restoration with deep denoiser prior. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Deblurring by realistic blurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">FFDNet: Toward a fast and flexible solution for CNN-based image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Color demosaicking by local directional interpolation and nonlocal adaptive thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JEI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Residual non-local attention networks for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Residual dense network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
