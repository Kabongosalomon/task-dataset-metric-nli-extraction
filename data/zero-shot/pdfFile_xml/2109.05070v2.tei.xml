<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance-Conditioned GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marl?ne</forename><surname>Careil</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Dro?d?al</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero-Soriano</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">?cole Polytechnique de Montr?al Mila</orgName>
								<orgName type="institution" key="instit2">AI Institute</orgName>
								<address>
									<region>Quebec</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research T?l?com Paris</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance-Conditioned GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative Adversarial Networks (GANs) can generate near photo realistic images in narrow domains such as human faces. Yet, modeling complex distributions of datasets such as ImageNet and COCO-Stuff remains challenging in unconditional settings. In this paper, we take inspiration from kernel density estimation techniques and introduce a non-parametric approach to modeling distributions of complex datasets. We partition the data manifold into a mixture of overlapping neighborhoods described by a datapoint and its nearest neighbors, and introduce a model, called instance-conditioned GAN (IC-GAN), which learns the distribution around each datapoint. Experimental results on ImageNet and COCO-Stuff show that IC-GAN significantly improves over unconditional models and unsupervised data partitioning baselines. Moreover, we show that IC-GAN can effortlessly transfer to datasets not seen during training by simply changing the conditioning instances, and still generate realistic images. Finally, we extend IC-GAN to the class-conditional case and show semantically controllable generation and competitive quantitative results on ImageNet; while improving over BigGAN on ImageNet-LT. Code and trained models to reproduce the reported results are available at https://github.com/facebookresearch/ic_gan.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b17">[18]</ref> have shown impressive results in unconditional image generation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>. Despite their success, GANs present optimization difficulties and can suffer from mode collapse, resulting in the generator not being able to obtain a good distribution coverage, and often producing poor quality and/or low diversity generated samples. Although many approaches attempt to mitigate this problem -e.g. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> -, complex data distributions such as the one in ImageNet <ref type="bibr" target="#b44">[45]</ref> remain a challenge for unconditional GANs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36]</ref>. Classconditional GANs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56]</ref> ease the task of learning the data distribution by conditioning on class labels, effectively partitioning the data. Although they provide higher quality samples than their unconditional counterparts, they require labelled data, which may be unavailable or costly to obtain.</p><p>Several recent approaches explore the use of unsupervised data partitioning to improve GANs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref>. While these methods are promising and yield visually appealing samples, their quality is still far from those obtained with class-conditional GANs. These methods make use of relatively coarse and non-overlapping data partitions, which oftentimes contain data points from different types of objects or scenes. This diversity of data points may result in a manifold with low density regions, which degrades the quality of the generated samples <ref type="bibr" target="#b10">[11]</ref>. Using finer partitions, however, tends to deteriorate results <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref> because the clusters may contain too few data points for the generator and discriminator to properly model their data distribution.</p><p>In this work, we introduce a new approach, called instance-conditioned GAN (IC-GAN), which extends the GAN framework to model a mixture of local data densities. More precisely, IC-GAN learns to model the distribution of the neighborhood of a data point, also referred to as instance, by providing a representation of the instance as an additional input to both the generator and discriminator, and by using the neighbors of the instance as real samples for the discriminator. By choosing a sufficiently large neighborhood around the conditioning instance, we avoid the pitfall of excessively partitioning the data into small clusters. Given the overlapping nature of these clusters, increasing the number of partitions does not come at the expense of having less samples in each of them. Moreover, unlike when conditioning on discrete cluster indices, conditioning on instance representations naturally leads the generator to produce similar samples for similar instances. Interestingly, once trained, our IC-GAN can be used to effortlessly transfer to other datasets not seen during training by simply swapping-out the conditioning instances at inference time.</p><p>IC-GAN bears similarities with kernel density estimation (KDE), a non-parametric density estimator in the form of a mixture of parametrized kernels modeling the density around each training data point -see e.g. <ref type="bibr" target="#b3">[4]</ref>. Similar to KDE, IC-GAN can be seen as a mixture density estimator, where each component is obtained by conditioning on a training instance. Unlike KDE, however, we do not model the data likelihood explicitly, but take an adversarial approach in which we model the local density implicitly with a neural network that takes as input the conditioning instance as well as a noise vector. Therefore, the kernel in IC-GAN is no longer independent on the data point on which we condition, and instead of a kernel bandwidth parameter, we control the smoothness by choosing the neighborhood size of an instance from which we sample the real samples to be fed to the discriminator.</p><p>We validate our approach on two image generation tasks: <ref type="bibr" target="#b0">(1)</ref> unlabeled image generation where there is no class information available, and (2) class-conditional image generation. For the unlabeled scenario, we report results on the ImageNet and COCO-Stuff datasets. We show that IC-GAN outperforms previous approaches in unlabeled image generation on both datasets. Additionally, we perform a series of transfer experiments and demonstrate that an IC-GAN trained on ImageNet achieves better generation quality and diversity when testing on COCO-Stuff than the same model trained on COCO-Stuff. In the class-conditional setting, we show that IC-GAN can generate images with controllable semantics -by adapting both class and instance-, while achieving competitive sample quality and diversity on the ImageNet dataset. Finally, we test IC-GAN in ImageNet-LT, a long-tail class distribution ablated version of ImageNet, highlighting the benefits of nonparametric density estimation in datasets with unbalanced classes. <ref type="figure" target="#fig_0">Figure 1</ref> shows IC-GAN unlabeled ImageNet generations (a), IC-GAN class-conditional ImageNet generations (b), and IC-GAN transfer generations both in the unlabeled (c) and controllable class-conditional (d) setting.</p><p>2 Instance-conditioned GAN</p><p>The key idea of IC-GAN is to model the distribution of a complex dataset by leveraging fine-grained overlapping clusters in the data manifold, where each cluster is described by a datapoint x i -referred to as instance -and its nearest neighbors set A i in a feature space. Our objective is to model the underlying data distribution p(x) as a mixture of conditional distributions p(x|h i ) around each of M instance feature vectors h i in the dataset, such that p(x) ? 1 M i p(x|h i ). More precisely, given an unlabeled dataset D = {x i } M i=1 with M data samples x i and an embedding function f parametrized by ?, we start by extracting instance features h i = f ? (x i ) ?x i ? D, where f ? (?) is learned in an unsupervised or self-supervised manner. We then define the set A i of k nearest neighbors for each data sample using the cosine similarity -as is common in nearest neighbor classifiers, e.g. <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> -over the features h i . <ref type="figure">Figure 2a</ref> depicts a sample x i and its nearest neighbors.</p><p>We are interested in implicitly modelling the conditional distributions p(x|h i ) with a generator G ? G (z, h i ), implemented by a deep neural network with parameters ? G . The generator transforms samples from a unit Gaussian prior z ? N (0, I) into samples x from the conditional distribution p(x|h i ), where h i is the feature vector of an instance x i sampled from the training data. In IC-GAN, we adopt an adversarial approach to train the generator G ? G . Therefore, our generator is jointly trained with a discriminator D ? D (x, h i ) that discerns between real neighbors and generated neighbors of h i , as shown in <ref type="figure">Figure 2b</ref>. Note that for each h i , real neighbors are sampled uniformly from A i .</p><p>Both G and D engage in a two player min-max game where they try to find the Nash equilibrium for the following equation:</p><formula xml:id="formula_0">min G max D E xi?p(x),xn?U (Ai) [log D(x n , f ? (x i ))] + E xi?p(x),z?p(z) [log(1 ? D(G(z, f ? (x i )), f ? (x i )))].</formula><p>(1) (a) Neighborhood Ai of instance hi (b) Schematic illustration of the IC-GAN workflow <ref type="figure">Figure 2</ref>: Overview of IC-GAN. (a) The goal of the generator is to generate realistic images similar to the neighbors of h i , defined in the embedding space using cosine similarity. Five out of seven neighbors are shown in the figure. Note that images in the same neighborhood may belong to different classes (depicted as different shapes). (b) Conditioned on instance features h i and noise z, the generator produces a synthetic sample x g . Generated samples and real samples (neighbors of h i ) are fed to the discriminator, which is conditioned on the same h i .</p><p>Note that when training IC-GAN we use all available training datapoints to condition the model. At inference time, as in non-parametric density estimation methods such as KDE, the generator of IC-GAN also requires instance features, which may come from the training distribution or a different one.</p><p>Extension to class-conditional generation. We extend IC-GAN for class-conditional generation by additionally conditioning the generator and discriminator on a class label y. More precisely, given a labeled dataset</p><formula xml:id="formula_1">D l = {(x i , y i )} M i=1 with M data sample pairs (x i , y i ) and an embedding function f ? , we extract instance features h i = f ? (x i ) ?x i ? D l , where f ? (?)</formula><p>is learned in an unsupervised, self-supervised, or supervised manner. We then define the set A i of k nearest neighbors for each data sample using the cosine similarity over the features h i , where neighbors may be from different classes. This results in neighborhoods, where the number of neighbors belonging to the same class as the instance h i is often smaller than k. During training, real neighbors x j and their respective labels y j are sampled uniformly from A i for each h i . In the class-conditional case, we model p(x|h i , y j ) with a generator G ? G (z, h i , y j ) trained jointly with a discriminator D ? D (x, h i , y j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental evaluation</head><p>We describe our experimental setup in Section 3.1, followed by results presented in the unlabeled setting in Section 3.2, dataset transfer in Section 3.3 and class-conditional generation in Section 3.4. We analyze the impact of the number of stored instances and neighborhood size in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>Datasets. We evaluate our model in the unlabeled scenario on ImageNet <ref type="bibr" target="#b44">[45]</ref> and COCO-Stuff <ref type="bibr" target="#b5">[6]</ref>. The ImageNet dataset contains 1.2M and 50k images for training and evaluation, respectively. COCO-Stuff is a very diverse and complex dataset which contains multi-object images and has been widely used for complex scene generation. We use the train and evaluation splits of <ref type="bibr" target="#b7">[8]</ref>, and the (un)seen subsets of the evaluation images with only class combinations that have (not) been seen during training. These splits contain 76k, 2k, 675 and 1.3k images, respectively. For the class-conditional image generation, we use ImageNet as well as ImageNet-LT <ref type="bibr" target="#b33">[34]</ref>. The latter is a long-tail variant of ImageNet that contains a subset of 115k samples, where the 1,000 classes have between 5 and 1,280 samples each. Moreover, we use some samples of four additional datasets to highlight the transfer abilities of IC-GAN: Cityscapes <ref type="bibr" target="#b9">[10]</ref>, MetFaces <ref type="bibr" target="#b27">[28]</ref>, PACS <ref type="bibr" target="#b30">[31]</ref> and Sketches <ref type="bibr" target="#b14">[15]</ref>.</p><p>Evaluation protocol. We report Fr?chet Inception Distance (FID) <ref type="bibr" target="#b21">[22]</ref>, Inception Score (IS) <ref type="bibr" target="#b46">[47]</ref>, and LPIPS <ref type="bibr" target="#b56">[57]</ref>. LPIPS computes the distance between the AlexNet activations of two images generated with two different latent vectors and same conditioning. On ImageNet, we follow <ref type="bibr" target="#b4">[5]</ref>, and compute FID over 50k generated images and the 50k real validation samples are used as reference. On COCO-Stuff and ImageNet-LT, we compute the FID for each of the splits using all images in the split as reference, and sample the same number images. Additionally, in ImageNet-LT we stratify the FID by grouping classes based on the number of train samples: more than 100 (many-shot FID), between 20 and 100 (med-shot FID), and less than 20 (few-shot FID). For the reference set, we split the validation images along these three groups of classes, and generate a matching number of samples per group. In order to compute all above-mentioned metrics, IC-GAN requires instance features for sampling. Unless stated otherwise, we store 1,000 training set instances by applying k-means clustering to the training set and selecting the features of the data point that is the closest to each one of the centroids. All quantitative metrics for IC-GAN are reported over five random seeds for the input noise when sampling from the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network architectures and hyperparameters.</head><p>As feature extractor f ? , we use a ResNet50 <ref type="bibr" target="#b20">[21]</ref> trained in a self-supervised way with SwAV <ref type="bibr" target="#b6">[7]</ref> for the unlabeled scenario; for the class-conditional IC-GAN, we use a ResNet50 trained for the classification task on either ImageNet or ImageNet-LT <ref type="bibr" target="#b25">[26]</ref>. For ImageNet experiments, we use BigGAN <ref type="bibr" target="#b4">[5]</ref> as a baseline architecture, given its superior image quality and ubiquitous use in conditional image generation. For IC-GAN, we replace the class embedding layers in the generator by a fully connected layer that takes the instance features as input and reduces its dimensionality from 2,048 to 512; the same approach is followed to adapt the discriminator. For COCO-Stuff, we additionally include the state-of-the-art unconditional StyleGAN2 architecture <ref type="bibr" target="#b28">[29]</ref>, as it has shown good generation quality and diversity in the lower data regime <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. We follow its class-conditional version <ref type="bibr" target="#b27">[28]</ref> to extend it to IC-GAN by replacing the input class embedding by the instance features. Unless stated otherwise, we set the size of the neighborhoods to k = 50 for ImageNet and k = 5 for both COCO-Stuff and ImageNet-LT. See the supplementary material for details on the architecture and optimization hyperparameters.  <ref type="bibr" target="#b41">[42]</ref> at 64 ? 64 resolution, we trained an unconditional BigGAN model and report the non-official FID and IS scores -computed with Pytorch rather than TensorFlow -indicated with *. ? : increased parameters to match IC-GAN capacity. DA: 50% horizontal flips in (d) real and fake samples, and (i) conditioning instances. ch?: Channel multiplier that affects network width as in BigGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unlabeled setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Res. ?FID ?IS ImageNet. We start by comparing IC-GAN against previous work in <ref type="table" target="#tab_0">Table 1</ref>. Note that unconditional BigGAN baseline is trained by setting all labels in the training set to zero, following <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42]</ref>. IC-GAN surpasses all previous approaches at both 64 ? 64 and 128 ? 128 resolutions in both FID and IS scores. At 256 ? 256 resolution, IC-GAN outperforms the concurrent unconditional diffusion-based model of <ref type="bibr" target="#b11">[12]</ref>; the only other result we are aware of in this setting. Additional results in terms of precision and recall can be found in <ref type="table">Table 8</ref> in the supplementary material.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1a</ref>, IC-GAN generates high quality images preserving most of the appearance of the conditioning instance. Note that generated images are not mere training memorizations; as shown in the supplementary material, generated images differ substantially from the nearest training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COCO-Stuff.</head><p>We proceed with the evaluation of IC-GAN on COCO-Stuff in <ref type="table" target="#tab_2">Table 2</ref>. We also compare to state-of-the-art complex scene generation pipelines which rely on labeled bounding box annotations as conditioning -LostGANv2 <ref type="bibr" target="#b48">[49]</ref> and OC-GAN <ref type="bibr" target="#b49">[50]</ref>. Both of (a) xi these approaches use tailored architectures for complex scene generation, which have at least twice the number of parameters of IC-GAN. Our IC-GAN matches or improves upon the unconditional version of the same backbone architecture in terms of FID in all cases, except for training FID with the StyleGAN2 backbone at 256?256 resolution. Overall, the StyleGAN2 backbone is superior to BigGAN on this dataset, and StyleGAN2-based IC-GAN achieves the state-of-the-art FID scores, even when compared to the bounding-box conditioned LostGANv2 and OC-GAN. IC-GAN exhibits notably higher LPIPS than LostGANv2 and OC-GAN, which could be explained by the fact that the latter only leverage one real sample per input conditioning during training; whereas IC-GAN uses multiple real neighboring samples per each instance, naturally favouring diversity in the generated images. As shown in figures 3b and 3c, IC-GAN generates high quality diverse images given the input instance. A qualitative comparison between LostGANv2, OC-GAN and IC-GAN can be found in Section E of the supplementary material.  <ref type="table" target="#tab_2">Table 2</ref>), which may be in part due to the larger k = 50 used for ImageNet training, compared to k = 5 when training on COCO-Stuff. Qualitative results of COCO-Stuff generations from the ImageNet pre-trained IC-GAN can be found in <ref type="figure" target="#fig_0">Figure 1c</ref> (top row) and <ref type="figure" target="#fig_1">Figure 3d</ref>. These generations suggest that IC-GAN is able to effectively leverage the large scale training on ImageNet to improve the quality and diversity of the COCO-Stuff scene generation, which contains significantly less data to train.</p><formula xml:id="formula_2">(b) IC-GAN (StyleGAN2) (c) IC-GAN (BigGAN) (d) IC-GAN (BigGAN, transf.)</formula><p>We further explore how the ImageNet trained IC-GAN transfers to conditioning on other datasets using Cityscapes, MetFaces, and PACS in <ref type="figure" target="#fig_0">Figure 1c</ref>. Generated images still preserve the semantics and style of the images for all datasets, although degrading their quality when compared to samples in <ref type="figure" target="#fig_0">Figure 1a</ref>, as the instances in these datasets -in particular MetFaces and PACS-are very different from the ImageNet ones. See Section F in the supplementary material for more discussion, additional evaluations, and more qualitative examples of dataset transfer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Class-conditional setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet.</head><p>In <ref type="table" target="#tab_3">Table 3</ref>, we show that the class-conditioned IC-GAN outperforms BigGAN in terms of both FID and IS across all resolutions except the FID at 128?128 resolution. It is worth mentioning that, unlike BigGAN, IC-GAN can control the semantics of the generated images by either fixing the instance features and swapping the class conditioning, or by fixing the class conditioning and swapping the instance features; see <ref type="figure" target="#fig_0">Figure 1b</ref>. As shown in the figure, generated images preserve semantics of both the class label and the instance, generating different dog breeds on similar backgrounds, or generating camels in the snow, an unseen scenario in ImageNet to the best of our knowledge. Moreover, in <ref type="figure" target="#fig_0">Figure 1d</ref>, we show the transfer capabilities of our class-conditional IC-GAN trained on ImageNet and conditioned on instances from other datasets, generating camels in the grass, zebras in the city, and husky dogs with the style of MetFaces and PACS instances. These controllable conditionings enable the generation of images that are not present or very rare in the ImageNet dataset, e.g. camels surrounded by snow or zebras in the city. Additional qualitative transfer results which either fix the class label and swap the instance features, or vice-versa, can be found in Section F of the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet-LT.</head><p>Due to the class imbalance in ImageNet-LT, selecting a subset of instances with either k-means or uniform sampling can easily result in ignoring rare classes, and penalizing their generation. Therefore, for this dataset we use all available 115k training instances to sample from the model and compute the metrics. In <ref type="table" target="#tab_4">Table 4</ref> we compare to BigGAN, showing that IC-GAN is better in terms of FID and IS for modeling this long-tailed distribution. Note that the improvement is noticeable for each of the three groups of classes with different number of samples, see many/med/few column. In Section G of the supplementary material we present experiments when using class-balancing to train BigGAN, showing that it does not improve quality nor diversity of generated samples. We hypothesize that oversampling some classes may result in overfitting for the discriminator, leading to low quality image generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Selection of stored instances and neighborhood size</head><p>In this section, we empirically justify the k-means procedure to select the instances to sample from the model, consider the effect of the number of instances used to sample from the model, as well as the effect of the size k of the neighborhoods A i used during training. The impact of different choices for the instance embedding function f ? (x) is evaluated in the supplementary material.</p><p>Selecting instances to sample from the model. In <ref type="figure" target="#fig_2">Figure 4</ref> (left), we compare two instance selection methods in terms of FID: uniform sampling (Random) and k-means (Clustered), where we select the closest instance to each cluster centroid, using k = 50 neighbors during training (solid and dotted green lines). Random selection is consistently outperformed by k-means; selecting only 1,000 instances with k-means results in better FID than randomly selecting 5,000 instances. Moreover, storing more than 1,000 instances selected with k-means does not result in noticeable improvements in FID. Additionally, we computed FID metrics for the 1,000 ground truth images that are closest to the k-means cluster centers, obtaining 41.8 ? 0.2 FID, which is considerably higher than the 10.4 ? 0.1 FID we obtain with IC-GAN (k = 50) when using the same 1,000 cluster centers. This supports the idea that IC-GAN is generating data points that go beyond the stored instances, better recovering the data distribution.</p><p>We consider precision (P) and recall (R) <ref type="bibr" target="#b29">[30]</ref> (using an InceptionV3 <ref type="bibr" target="#b50">[51]</ref> as feature extractor and sampling 10,000 generated and real images) to disentangle the factors driving the improvement in FID, namely image quality and diversity (coverage) -see <ref type="figure" target="#fig_2">Figure 4</ref> (right). We see that augmenting the number of stored instances results in slightly worse precision (image quality) but notably better recall (coverage). Intuitively, this suggests that by increasing the number of stored instances, we can better recover the data density at the expense of slightly degraded image quality in lower density regions of the manifold -see e.g. <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure" target="#fig_2">Figure 4</ref>: Impact on the number of stored instances used to evaluate IC-GAN and the size of the neighborhood k. Experiments performed on the 64?64 unlabeled ImageNet dataset.</p><p>Neighborhood size. In <ref type="figure" target="#fig_2">Figure 4</ref> (both panels) we analyze the interplay between the neighborhood size and the number of instances used to recover the data distribution. For small numbers of stored instances, we observe that larger the neighborhoods lead to better (lower) FID scores (left-hand side of left panel). For recall, we also observe improvements for large neighborhoods when storing few instances (left-hand side of right panel), suggesting that larger neighborhoods are more effective in recovering the data distribution from few instances. This trend is reverted for large numbers of stored instances, where smaller values of k are more effective. This supports the idea that the neighborhood size acts as a bandwidth parameter -similar to KDE -, that controls the smoothness of the implicitly learnt conditional distributions around instances. For example, k = 500 leads to smoother conditional distributions than k = 5, and as a result requires fewer stored instances to recover the data distribution. Moreover, as expected, we notice that the value of k does not significantly affect precision (right panel).</p><p>Overall, k = 50 offers a good compromise, exhibiting top performance across all metrics when using at least 500 stored instances. We visualize the smoothness effect by means of a qualitative comparison across samples from different neighborhood sizes in Section K of the supplementary material. Using (very) small neighborhoods (e.g. of k = 5), results in lower diversity in the generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Data partitioning for GANs. Previous works have attempted to improve the image generation quality and diversity of GANs by partitioning the data manifold through clustering techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46]</ref>, or by leveraging mixture models in their design <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>In particular, <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b45">46]</ref> apply k-means on representations from a pre-trained feature extractor to cluster the data, and then use cluster indices to condition the generator network. Then, <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref> introduce an alternating two-stage approach where the first stage applies k-means to the discriminator feature space and the second stage trains a GAN conditioned on the cluster indices. Similarly, <ref type="bibr" target="#b41">[42]</ref> proposes to train a clustering network, which outputs pseudolabels, in cooperation with the generator. Further, <ref type="bibr" target="#b1">[2]</ref> trains a feature extractor with self-supervised pre-training tasks, and creates a k-nearest neighbor graph in the learned representation space to cluster connected points into the same sub-manifold. In this case, a different generator is then trained for each identified sub-manifold. By contrast, IC-GAN uses fine-grained overlapping data neighborhoods in tandem with conditioning on rich feature embeddings (instances) to learn a localized distribution around each data point.</p><p>Mitigating mode collapse in GANs. Works which attempt to mitigate mode collapse may also bear some similarities to ours. In <ref type="bibr" target="#b31">[32]</ref>, the discriminator takes into consideration multiple random samples from the same class to output a decision. In <ref type="bibr" target="#b34">[35]</ref>, a mixed batch of generated and real samples is fed to the discriminator with the goal of predicting the ratio of real samples in the batch. Other works use a mixture of generators <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref> and encourage each generator to focus on generating samples from a different mode. Similarly, in <ref type="bibr" target="#b13">[14]</ref>, the discriminator is pushed to form clusters in its representation space, where each cluster is represented by a Gaussian kernel. In turn, the generator tends to learn to generate samples covering all clusters, hence mitigating mode collapse. By contrast, we focus on discriminating between real and generated neighbors of an instance conditioning, by using a single generator network trained following the GAN formulation.</p><p>Conditioning on feature vectors. Very recent work <ref type="bibr" target="#b36">[37]</ref> uses image self-supervised feature representations to condition a generative model whose objective is to produce a good input reconstruction; this requires storing the features of all training samples. In contrast, our objective is to learn a localized distribution (as captured by nearest neighboring images) around each conditioning instance, and we only need to save a very small subset of the dataset features to approximately recover the training distribution.</p><p>Kernel density estimation and adversarial training. Connections between adversarial training and nonparametric density estimation have been made in prior work <ref type="bibr" target="#b0">[1]</ref>. However, to the best of our knowledge, no prior work models the dataset density in a nonparametric fashion with a localized distribution around each data point with a single conditional generation network.</p><p>Complex scene generation. Existing methods for complex scene generation, where natural looking scenes contain multiple objects, most often aim at controllability and rely on detailed conditionings such as a scene graphs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>, bounding box layouts <ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b57">58]</ref>, semantic segmentation masks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b54">55]</ref> or more recently, freehand sketches <ref type="bibr" target="#b15">[16]</ref>. All these methods leverage intricate pipelines to generate complex scenes and require labeled datasets. By contrast, our approach relies on instance conditionings which control the global semantics of the generation process, and does not require any dataset labels. It is worth noting that complex scene generation is often characterized by unbalanced, strongly long tailed datasets. Long-tail class distributions negatively affect classconditional GANs, as they struggle to generate visually appealing samples for classes in the tail <ref type="bibr" target="#b7">[8]</ref>. However, to the best of our knowledge, no other previous work tackles this problem for GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Contributions. We presented instance-conditioned GAN (IC-GAN), which models dataset distributions in a non-parametric way by conditioning both generator and discriminator on instance features. We validated our approach on the unlabeled setting, showing consistent improvements over baselines on ImageNet and COCO-Stuff. Moreover, we showed through transfer experiments, where we condition the ImageNet-trained model on instances of other datasets, the ability of IC-GAN to produce compelling samples from different data distributions. Finally, we validated IC-GAN in the class-conditional setting, obtaining competitive results on ImageNet and surpassing the Big-GAN baseline on the challenging ImageNet-LT; and showed compelling controllable generations by swapping the class-conditioning given a fixed instance or the instance given a fixed conditioning.</p><p>Limitations. IC-GAN showed excellent image quality for labeled (class-conditional) and unlabeled image generation. However, as any machine learning tool, it has some limitations. First, as kernel density estimator approaches, IC-GAN requires storing training instances to use the model. Experimentally, we noticed that for complex datasets, such as ImageNet, using 1,000 instances is enough to approximately cover the dataset distribution. Second, the instance feature vectors used to condition the model are obtained with a pre-trained feature extractor (self-supervised in the unlabeled case) and depend on it. We speculate that this limitation might be mitigated if the feature extractor and the generator are trained jointly, and leave it as future work. Third, although, we highlighted excellent transfer potential of our approach to unseen datasets, we observed that, in the case of transfer to datasets that are very different from ImageNet, the quality of generated images degrades.</p><p>Broader impacts. IC-GAN brings with it several benefits such as excellent image quality in labeled (class-conditional) and unlabeled image generation tasks, and the transfer potential to unseen datasets, enabling the use of our model on a variety of datasets without the need of fine-tuning or re-training. Moreover, in the case of class-conditional image generation, IC-GAN enables controllable generation of content by adapting either the style -by changing the instance -or the semantics -by altering the class -. Thus, we expect that our model can positively affect the workflow for creative content generators. That being said, with improving image quality in generative modeling, there is some potential for misuse. A common example are deepfakes, where a generative model is used to manipulate images or videos well enough that humans cannot distinguish real from fake, with the intent to misinform. We believe, however, that open research on generative image models also contributes to better understand such synthetic content, and to detect it where it is undesirable. Recently, the community has also started to undertake explicit efforts towards detecting manipulated content by organizing challenges such as the Deepfake Detection Challenge <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance-Conditioned GAN: Supplementary Material</head><p>We provide additional material to support the main paper. We credit the used assets by citing their web links and licenses in Section A, and continue by describing the experimental setup and used hyperparameters in Section B. We compute Precision and Recall metrics on ImageNet in Section C, and we further compare BigGAN and StyleGAN2 backbones for IC-GAN on ImageNet in Section D. We provide additional qualitative results for both IC-GAN on ImageNet in Section E and IC-GAN off-the-shelf transfer results on other datasets in Section F. Moreover, we provide results when training BigGAN with class balancing on ImageNet-LT in Section G. Finally, we show further impact studies such as the choice of feature extractor (Section H), the number of conditionings used during training (Section I), matching storage requirements for unconditional counterparts of BigGAN and StyleGAN2 and IC-GAN (Section J) and the qualitative impact of neighborhood size k for ImageNet, as well as quantitative results for ImageNet-LT and COCO-Stuff (Section K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Assets and licensing information</head><p>In <ref type="table" target="#tab_5">Tables 5 and 6</ref>, we provide the links to the used datasets, repositories and their licenses. We use Faiss <ref type="bibr" target="#b23">[24]</ref> for a fast computation of nearest neighbors and k-means algorithm leveraging GPUs, DiffAugment <ref type="bibr" target="#b58">[59]</ref> for additional data augmentation when training BigGAN, and the pre-trained SwAV <ref type="bibr" target="#b6">[7]</ref> and ResNet50 models on ImageNet-LT <ref type="bibr" target="#b25">[26]</ref> to extract instance features.   <ref type="bibr" target="#b9">[10]</ref> https://www.cityscapes-dataset.com/license MetFaces <ref type="bibr" target="#b27">[28]</ref> Creative Commons BY-NC 2.0 PACS <ref type="bibr" target="#b30">[31]</ref> Unknown Sketches <ref type="bibr" target="#b14">[15]</ref> Creative Commons Attribution 4.0 International BigGAN <ref type="bibr" target="#b4">[5]</ref> MIT StyleGAN2 <ref type="bibr" target="#b27">[28]</ref> NVIDIA Source Code Faiss <ref type="bibr" target="#b23">[24]</ref> MIT DiffAugment <ref type="bibr" target="#b58">[59]</ref> BSD 2-Clause "Simplified" PRDC <ref type="bibr" target="#b40">[41]</ref> MIT swAV <ref type="bibr" target="#b6">[7]</ref> Attribution-NonCommercial 4.0 International Pre-trained ResNet50 <ref type="bibr" target="#b25">[26]</ref> BSD</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental setup and hyperparameters</head><p>We divide the experimental section into architecture modifications in Subsection B.1 and training and hyperparameter details in Subsection B.2.</p><p>B.1 Architecture modifications for IC-GAN.</p><p>In our IC-GAN experiments, we leveraged BigGAN and StyleGAN2 backbones, and extended their architectures to handle the introduced instance conditionings.</p><p>When using BigGAN as a base architecture, IC-GAN replaces the class embedding layers in both generator and discriminator by fully connected layers. The fully connected layer in the generator has an input size of 2,048 (corresponding to the feature extractor f ? 's dimensionality) and an output size o dim that can be adjusted. For all our experiments, we used o dim = 512 -selected out of {256, 512, 1,024, 2,048}. The fully connected layer in the discriminator has a variable output size n dim to match the dimensionality of the intermediate unconditional discriminator feature vectorfollowing the practice in BigGAN <ref type="bibr" target="#b4">[5]</ref>. For the class-conditional IC-GAN, we use both the class embedding layers as well as the fully connected layers associated with the instance conditioning.</p><p>In particular, we concatenate class embeddings (of dimensionality c dim = 128) and instance embeddings (with dimensionality o dim = 512). To avoid the rapid growth of parameters when using both class and instance embeddings, we use n dim /2 as the output dimensionality for each of the embeddings in the discriminator, so that the resulting concatenation has a dimensionality of n dim .</p><p>When using StyleGAN2 as a base architecture, we modify the class-conditional architecture of <ref type="bibr" target="#b27">[28]</ref>.</p><p>In particular, we replace the class embeddings layers with a fully connected layer of output dimensionality 512 in the generator. The fully connected layer substituting the class embedding in the discriminator is of variable size. In this case, the instance features are concatenated with the noise vector at the input of the StyleGAN2's mapping network, creating a style vector for the generator. However, when it comes to the discriminator, the mapping network is only fed with the extracted instance features to obtain a modulating vector that is multiplied by the internal discriminator representation at each block.</p><p>All instance feature vectors h i are normalized with 2 norm before computing the neighborhoods and when used as conditioning for the GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Training details and hyperparameters</head><p>All models were trained while monitoring the training FID, and training was stopped according to either one of the following criteria: (1) early stopping when FID did not improve for 50 epochs -or the equivalent number of iterations depending on the batch size -, or (2) when the training FID diverged. For BigGAN, we mainly explored the hyperparameter space around previously known and successful configurations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref>. Concretely, we focused on finding the following best hyperparameters for each dataset and resolution: the batch size (BS), model capacity controlled by channel multipliers (CH), number of discriminator updates versus generator updates (D updates ), discriminator learning rate (D lr ) and generator learning rate (G lr ), while keeping all other parameters unchanged <ref type="bibr" target="#b4">[5]</ref>. For StyleGAN, we also performed a hyperparameter search around previously known successful settings <ref type="bibr" target="#b27">[28]</ref>. More precisely, we searched for the optimal D lr and G lr and R1 regularization weight ? and used default values for the other hyperparameters.</p><p>ImageNet. When using the BigGAN backbone, in the 64?64 resolution, we followed the experimental setup of <ref type="bibr" target="#b41">[42]</ref>, where: BS = 256, CH = 64, D lr = G lr = 1e?4 and found that, although the unconditional BigGAN baseline achieves better metrics with D updates = 2, IC-GAN and Big-GAN do so with D updates = 1. Note that we explored additional configurations such as increasing BS or CH but did not observe any improvement upon the aforementioned setup. In both the 128?128 and 256?256 resolutions, BigGAN hyperparameters were borrowed from <ref type="bibr" target="#b4">[5]</ref>. Data augmentation. We use horizontal flips to augment the real data fed to the discriminator in all experiments, unless stated otherwise. For COCO-Stuff and ImageNet-LT, we found that using translations with the DiffAugment framework <ref type="bibr" target="#b58">[59]</ref> improves FID scores, as the number of training samples is significantly smaller than ImageNet (5% and 10% the size of ImageNet, respectively). However, we did not see any improvement in ImageNet dataset and therefore we do not use DiffAugment in our ImageNet experiments. For ImageNet and COCO-Stuff, we augment the conditioning instance features h i by horizontally flipping all data samples x i and obtaining a new h i from the flipped image, unless stated otherwise in the tables. This effectively doubles the number of conditionings available at training time, which have the same sample neighborhood A i as their non-flipped versions. We tried applying this augmentation technique to ImageNet-LT but found that it degraded the overall metrics, possibly due to the different feature extractor used in these experiments. We hypothesize that the benefits of this technique are dependent on the usage of horizontal flips during the training stage of the feature extractor. As seen in <ref type="table">Table 7</ref>, using data augmentation in the conditioning instance features slightly improves the results for IC-GAN both when coupled with BigGAN and StyleGAN2 backbones in COCO-Stuff.</p><p>Compute resources. We used NVIDIA V100 32GB GPUs to train our models. Given that we used different batch sizes for different experiments, we adapted the resources to each dataset configuration. In particular, ImageNet 64?64 models were trained using 1 GPU, whereas ImageNet 128?128 and 256?256 models required 32 GPUs. ImageNet-LT 64?64, 128?128 and 256?256 used 1, 2 and 8 GPUs each, respectively. Finally, COCO-Stuff 128?128 and 256?256 required 4 and 16 GPUs, respectively, when using the BigGAN backbone, but required 2 and 4 GPUs when leveraging StyleGAN2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional metrics: Precision and Recall</head><p>As additional measures of visual quality and diversity, we compute Precision (P) and Recall (R) <ref type="bibr" target="#b29">[30]</ref> in <ref type="table">Table 8</ref>. Results are provided on the ImageNet dataset, following the experimental setup proposed in <ref type="bibr" target="#b40">[41]</ref>. By inspecting the results, we conclude that IC-GAN obtains better Recall (and therefore more diversity) than all the baselines in both the unlabeled and labeled settings, when selecting 10,000 random instances from the training set. Moreover, when selecting 1,000 instances with k-means, which is the standard experimental setup we used across the paper, we obtain higher Precision (as a measure of visual quality) than the other baselines in the unlabeled setting. In the labeled setting, the Precision is also higher than the one of BigGAN for 64x64 while being lower for 128?128 and 256?256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Comparison between StyleGAN2 and BigGAN backbones on ImageNet</head><p>We present additional experiments with IC-GAN using the StyleGAN2 backbone in ImageNet in <ref type="table" target="#tab_10">Table 9</ref>, comparing them to StyleGAN2 across all resolutions. IC-GAN with a StyleGAN2 backbone obtains better FID and IS than StyleGAN2 across all resolutions, further supporting that IC-GAN does not depend on a specific backbone, as already shown in the COCO-Stuff dataset in  <ref type="table" target="#tab_2">Table 2</ref>, as this dataset is much smaller than ImageNet and contains a lot of images where people are depicted. Interestingly, we qualitatively found that people and their faces are better generated with a StyleGAN2 backbone rather than the BigGAN one when trained on COCO-Stuff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional qualitative results for IC-GAN</head><p>Unlabeled ImageNet. IC-GAN generates high quality and diverse images that generally preserve the semantics and style of the conditioning. <ref type="figure" target="#fig_3">Figure 5</ref> shows three instances -a golden retriever in the water, a humming bird on a branch, and a landscape with a castle -, followed by their six closest nearest neighbors in the feature space of SwAV <ref type="bibr" target="#b6">[7]</ref>, a ResNet50 model trained with selfsupervision. Note that, although all neighbors contain somewhat similar semantics to those of the instance, the class labels do not always match. For example, one of the nearest neighbors of a golden retriever depicts a monkey in the water. The generated images depicted in <ref type="figure" target="#fig_3">Figure 5</ref> are obtained by conditioning IC-GAN with a BigGAN backbone on the features of the aforementioned instances. These highlight that generated images preserve the semantic content of the conditioning instance (a dog in the water, a bird with a long beak on a branch, and a landscape containing a water body) and present similarities with the real samples in the neighborhood of the instance. In cases such as the conditioning instance featuring a castle, the corresponding generated samples do not contain buildings; this could be explained by the fact that most of its neighbors do not contain castles either. Moreover, the generated images are not mere memorizations of training examples, as shown by the row of images immediately below, nor are they copies of the conditioning instance.</p><p>Instance feature vector and noise interpolation. In <ref type="figure" target="#fig_4">Figure 6</ref>, we provide the resulting generated images when interpolating between the instance features of two data samples (vertical axis), shown on the left of each generated image grid, and additionally interpolating between two noise vectors in the horizontal axis. The top left quadrant shows generated images when interpolating between conditioning instance features from the class husky. The generated dog changes its fur color and camera proximity according to the instance conditioning. At the top right corner, when interpolating between two mushroom instance features, generated images change their color and patterns <ref type="table">Table 8</ref>: Results for ImageNet in terms of Precision (P) and Recall (R) <ref type="bibr" target="#b29">[30]</ref> (bounded between 0 and 100), using 10,000 real and generated images. "Instance selection", only used for IC-GAN, indicates whether 1,000 conditioning instances are selected with k-means (k-means 1,000) or 10,000 conditioning instances are sampled uniformly (random 10,000) from the training set to obtain 10,000 generated images in both cases. *: Generated images obtained with the paper's opensourced code.   <ref type="bibr" target="#b41">[42]</ref> at 64?64 resolution, we trained an unconditional BigGAN model and report the non-official FID and IS scores -computed with Pytorch rather than TensorFlow -indicated with *. ? : increased parameters to match IC-GAN capacity. DA: 50% horizontal flips in real and fake samples (d), and conditioning instances (i). ch?: Channel multiplier that affects network width.  Unlabeled COCO-Stuff. Training IC-GAN with a StyleGAN2 backbone on COCO-Stuff has resulted in quantitative results that surpass those achieved by the state-of-the-art LostGANv2 <ref type="bibr" target="#b48">[49]</ref> and OC-GAN <ref type="bibr" target="#b49">[50]</ref>, controllable and conditional complex scene generation pipelines that rely on heavily labeled data (bounding boxes and class labels), tailored intermediate steps and somewhat complex architectures. In <ref type="figure" target="#fig_5">Figure 7</ref>, we compare generated images obtained with LostGANv2 and OC-GAN with those generated by IC-GAN with a StyleGAN2 backbone. Note that the two former methods use a bounding box layout with class labels as a conditioning, while we condition on the features extracted from the real samples x i depicted in <ref type="figure" target="#fig_5">Figure 7a</ref>. We compare the generations obtained with two random seeds for all methods, and observe that IC-GAN generates higher quality images in all cases, especially for the top three instances. Moreover, the diversity in the generations using two random seeds for LostGANv2 and OC-GAN is lower than for IC-GAN. This is not surprising, as the former methods are restricted by their bounding box layout conditioning that specifies the number of objects, their classes and their expected positions in the generated images. By contrast, IC-GAN conditions on an instance feature vector, which does not require any object label, cardinality or position to be satisfied, allowing more freedom in the generations.</p><p>ImageNet. Class-conditional IC-GAN with a BigGAN backbone has shown comparable quantitative results to those of BigGAN for 256?256 resolution in Subsection 3.4. In <ref type="figure" target="#fig_7">Figure 8</ref>, we can qualitatively compare BigGAN (ch ? 64) (first rows) and IC-GAN (ch ? 64) (second and third rows), for three class labels: goldfish, limousine and red fox. By visually inspecting the generated images, we can observe that the generation quality is similar for both BigGAN and IC-GAN in these specific cases. Moreover, IC-GAN allows controllability of the semantics by changing the conditioning instance features. For instance, changing the background in which the goldfish are swimming into lighter colors in <ref type="figure" target="#fig_7">Figure 8a</ref>, generating limousines in generally dark and uniform backgrounds or, instead, in an urban environment with a road and buildings <ref type="figure" target="#fig_7">(Figure 8b</ref>), or generating red foxes with a close up view or with a full body shot as seen in <ref type="figure" target="#fig_7">Figure 8c</ref>.</p><p>Swapping classes for class-conditional IC-GAN on ImageNet. In <ref type="figure" target="#fig_7">Figure 8</ref>, we show that we can change the appearance of the generated images by leveraging different instances of the same class. In <ref type="figure" target="#fig_8">Figure 9</ref>, we take a further step and condition on instance features from other classes. More specifically, in <ref type="figure" target="#fig_8">Figure 9</ref> (top), we condition on the instance features of a snowplow in the woods surrounded by snow, and ask to generate snowplows, camels and zebras. Perhaps surprisingly, the generated images effectively get rid of the snowplow, and replace it by camel-looking and zebralooking objects, respectively, while maintaining a snowy background in the woods. Moreover, when comparing the generated images with the closest samples in ImageNet, we see that for generated camels in the snow, the closest images are either a camel standing in dirt or other animals in the snow; for the generated zebras in the snow, we find one sample of a zebra standing in the snow, while others are standing in other locations/backgrounds. In <ref type="figure" target="#fig_8">Figure 9</ref> (bottom), we condition on the features of an instance that depicts a golden retriever on a beach with overall purple tones, and ask to generate golden retrievers, camels or zebras. In most cases, generated images contain camels and zebras standing on water, while other generations contain purple or blue tones, similar to the instance used as conditioning. Note that, except one generated zebra image, the closest samples in ImageNet do not depict camels or zebras standing in the water nor on the beach.</p><p>F Additional off-the-shelf transfer results for IC-GAN Is IC-GAN able to shift the generated data distribution by conditioning on different instances? As discussed in Section 3.3, we can transfer an IC-GAN trained on unlabeled ImageNet to COCO-Stuff and obtain better metrics and qualitative results than with the same IC-GAN trained on COCO-Stuff. We hypothesize that the success of this experiment comes from the flexibility of our conditioning strategy, where the generative model exploits the generalization capabilities of the feature extractor when dealing with unseen instances to shift the distribution of generated images  from ImageNet to COCO-Stuff. To test this hypothesis we design the following experiment: we compute FID scores of generated images obtained by conditioning IC-GAN with instance features from either ImageNet or COCO-Stuff and use either COCO-Stuff or ImageNet as a reference dataset to compute FID. In <ref type="table" target="#tab_0">Table 10</ref> (first row) we show that when using COCO-Stuff for both the instance features and the reference dataset, IC-GAN scores 8.5 FID; this is a lower FID score than the 43.6 FID obtained in <ref type="table" target="#tab_0">Table 10</ref> (second row) when conditioning IC-GAN on ImageNet instance features and using COCO-Stuff as reference dataset. Moreover, when using COCO-Stuff instance features and ImageNet as reference dataset, in <ref type="table" target="#tab_0">Table 10</ref> (third row), we obtain 37.2 FID. This shows that, by changing the conditioning instance features, IC-GAN successfully exploits the generalization capabilities of the feature extractor to shift the distribution of generated images to be closer to the COCO-Stuff distribution. Additionally, note that the distance between ImageNet and COCO-Stuff datasets can be quantified with an FID score of 37.2 2 .</p><p>What is being transferred when IC-GAN is conditioned on instances other than the ones in the training dataset? From the point of view of KDE, what is being transferred is the kernel shape, not the kernel location (that is controlled by instances). The kernel shape is predicted using a generative model from each input instance and we probe the kernel via sampling from the generator.  Thus, we transfer a function that predicts kernel shape from a conditioning, and this function seems to be robust to diverse instances as shown in the paper (e.g. see <ref type="figure" target="#fig_0">Figure 1c</ref> and 1d). Moreover, by visually inspecting the generated images in our transfer experiments, we observed that when transferring an IC-GAN trained on ImageNet to COCO-Stuff, if the model is conditioned on images that contain unseen classes in ImageNet, such as "giraffe", the model will still generate an animal that would look like a giraffe without the skin patterns and characteristic antennae, because ImageNet contains other animals to draw inspiration from. This suggests that the model generates plausible images that have some similar features to those present in the instance conditioning, but adapting it to the training dataset style. Along these lines, we also observed that in some cases, shapes and other object characteristics from one dataset are transferred to another (ImageNet to COCO-Stuff). Moreover, when we conditioned on instances from Cityscapes, the generated images were rather colorful, resembling more the color palette of ImageNet images rather than the Cityscapes one.</p><p>Off-the-shelf transfer results for IC-GAN. In <ref type="figure" target="#fig_0">Figure 10</ref>, we provide additional generated samples and their closest images in the ImageNet training set, when conditioning on unseen instance features from other datasets. Generated images often differ substantially from the closest image in ImageNet.   Although generated images using a COCO-Stuff and Cityscapes instances may have somewhat similar looking images in ImageNet (for the first and second instances in <ref type="figure" target="#fig_0">Figure 10</ref>), the differences accentuate when conditioning on instance features from MetFaces, PACS or Sketch datasets, where, for instance, IC-GAN with a BigGAN backbone generates images resembling sketch strokes in the last row, even if the closest ImageNet samples depict objects that are not sketches.</p><p>Off-the-shelf transfer results for class-conditional IC-GAN. In <ref type="figure" target="#fig_0">Figure 11</ref>, we show additional results when transferring a class-conditional IC-GAN with a BigGAN backbone trained on ImageNet to other datasets, using an ImageNet class label but an unseen instance. We are able to generate camels in the grass by conditioning on an image of a cow in the grass from COCO-Stuff, we show generated images with a zebra in an urban environment by conditioning on a Cityscapes instance, and we generate cartoon-ish birdhouses by conditioning on a PACS cartoon house instance. This highlights the ability of class-conditional IC-GAN to transfer to other datasets styles while maintaining the class label semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Class balancing in ImageNet-LT</head><p>We experimented with class balancing for BigGAN in the ImageNet-LT dataset. In <ref type="table" target="#tab_0">Table 11</ref>, we compare (1) BigGAN, where both the class distribution for the generator and the data for the discriminator are long-tailed; (2) BigGAN (CB), a class-balanced version of BigGAN, where the generator samples class labels from a uniform distribution and the samples fed to the discriminator are also class-balanced; and (3) BigGAN (T = 2) where the class distribution is balanced with a softmax temperature of T = 2 providing a middle ground between the long-tail and the uniform distributions.</p><p>In the latter case, the probability to sample class c (with a frequency f c in the long-tailed training set) during training is given by p c = softmax(T ?1 ln f c ).</p><p>Interestingly, balancing the class distribution (either with uniform class distribution or with T=2) harmed performance in all cases except for the validation Inception Score. We hypothesize that oversampling rare classes, and thus the few images therein, may result in overfitting for the discriminator, leading to low quality image generations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Choice of feature extractor</head><p>We study the choice of the feature extractor used to obtain instance features in <ref type="table" target="#tab_0">Table 12</ref>. We compare results using an IC-GAN with a BigGAN backbone when coupling it with a ResNet50 feature extractor trained with either self supervision (SwAV) or with supervision for classification purposes (RN50) on ImageNet dataset. Results highlight similar IC-GAN performances for both feature extractors, suggesting that the choice of feature extractor that does not greatly impact the performance of our method when leveraging unlabeled datasets. Given that for the unlabeled scenario we assume no labels are available, we use the SwAV feature extractor. However, in the class-conditional case, we observe that the IC-GAN coupled with a RN50 feature extractor surpasses IC-GAN coupled with a SwAV feature extractor. Therefore, we choose the RN50 feature extractor for the class-conditional experiments. For ImageNet-LT, we transfer these findings and use a RN50 trained on ImageNet-LT as feature extractor, assuming we do not have access to the entire ImageNet dataset and its labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Number of conditioning instance features at train time</head><p>To demonstrate that using many fine-grained overlapping partitions results in better performance than using a few coarser partitions, we trained IC-GAN with a BigGAN backbone by conditioning on all 1.2M training instance features at training time in ImageNet and a neighborhood size of k = 50, and compared it quantitatively with an IC-GAN trained by conditioning on only 1,000 instance features  at training time. In this case, we extend the neighborhood size to k = 1,200 to better cover the training distribution 3 . Note that using k = 50 would result in using at most 50,000 training samples during training, an unfair comparison. The 1,000 instance features are selected by applying k-means to the entire ImageNet training set. We then use the same instances to generate images. Results are presented in <ref type="table" target="#tab_0">Table 13</ref> and emphasize the importance of training with all available instances, which results in significantly better FID and IS presumably due to the increased number of conditionings and their smaller neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Matching storage requirements for IC-GAN and unconditional models</head><p>We hypothesize that the good performance of IC-GAN on ImageNet and COCO-Stuff can not solely be attributed to the slight increase of parameters and the extra memory required to store the instance features used at test time, but also to the IC-GAN design, including the finegrained overlapping partitions and the instance conditionings. To test for this hypothesis, we performed experiments with the unconditional BigGAN baseline on ImageNet and COCO-Stuff, training it by setting all   <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42]</ref>, and increasing the generator capacity such that it matches the IC-GAN storage requirements. In particular, we not only endowed the unconditional BigGAN with additional parameters to compensate for the capacity mismatch, but also for the instances required by IC-GAN. Moreover, we performed analogous experiments for the unconditional StyleGAN2 in COCO-Stuff.</p><p>ImageNet. Given its instance conditioning, the IC-GAN (BigGAN backbone) generator introduces an additional 4.5M parameters when compared to the unconditional BigGAN generator. Moreover, IC-GAN requires an extra 8MB to store the 1,000 instance features used at inference time. This 8MB can be translated into roughly 2M parameters <ref type="bibr" target="#b3">4</ref> . Therefore, we compensate for this additional storage in IC-GAN by increasing the unconditional BigGAN capacity by expanding the width of both generator and discriminator hidden layers. We follow the practice in <ref type="bibr" target="#b4">[5]</ref>, where the generator and discriminator's width are changed together. The resulting unconditional BigGAN baseline generator has an additional 6.5M parameters. Results are reported in <ref type="table" target="#tab_0">Table 14</ref>, showing that adding extra capacity to the unconditional BigGAN leads to an improvement in terms of FID and IS. However, IC-GAN still exhibits significantly better FID and IS, highlighting that the improvements cannot be solely attributed to the increase in parameters nor instance feature storage requirements.</p><p>COCO-Stuff. Similarly, IC-GAN (BigGAN backbone) trained on COCO-Stuff requires 4M additional parameters on top of the extra storage required by the 1,000 stored instance features (8MB again translated into roughly 2M parameters). Therefore, we add 6M extra parameters to the unconditional BigGAN generator. In the case of IC-GAN with a StyleGAN2 backbone, the instance feature conditionings constitute 1M additional parameters. We therefore increase the capacity of the unconditional StyleGAN2 model by 3M to match the storage requirements of IC-GAN (StyleGAN2 backbone). The results are presented in <ref type="table" target="#tab_0">Table 15</ref>, where it is shown that both the unconditional BigGAN and unconditional StyleGAN2 do not take advantage of the additional capacity and achieve poorer performance than the model with lower capacity, possibly due to overfitting. When compared to IC-GAN, the results match the findings in the ImageNet dataset: IC-GAN exhibits lower FID when using BigGAN or StyleGAN2 backbones, compared to their respective unconditional models with the same storage requirements, further highlighting that IC-GAN effectively benefits from its design, finegrained overlapping partitions, and instance conditionings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Additional neighborhood size impact studies</head><p>We additionally study the impact of the neighborhood size for ImageNet-LT in <ref type="table" target="#tab_0">Table 16</ref> and in COCO-Stuff in <ref type="table" target="#tab_0">Table 17</ref>, showing that in both cases, IC-GAN with a BigGAN backbone and k = 5  achieves the best FID and IS metrics. The choice of a lower neighborhood size k = 5 than in the ImageNet case (k = 50) could suggest that the number of semantically similar neighboring samples is smaller for these two datasets. This wouldn't be completely surprising given that these two datasets are considerably smaller than ImageNet. Increasing the value of k in COCO-Stuff and ImageNet-LT would potentially gather samples with different semantics within a neighborhood, which could potentially harm the training and therefore the generated images quality.</p><p>Finally, in <ref type="figure" target="#fig_0">Figure 12</ref>, we qualitatively show generated images in ImageNet when using an IC-GAN trained with varying neighborhood sizes. The findings further support the ones presented in Subsection 3.5, showing that smaller neighborhoods result in generated images with less diversity, while bigger neighborhood sizes, for example k = 500 result in more varied but lower quality generations, supporting that k controls the smoothing effect.  <ref type="table" target="#tab_0">Table 17</ref>: Impact of the number of neighbors (k) used to train IC-GAN (BigGAN backbone) on COCO-Stuff 128?128. Reported results on COCO-Stuff evaluation set. As a feature extractor, a ResNet50 trained with self-supervision (SwAV) is used.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) IC-GAN samples (b) Class-conditional IC-GAN samples (c) IC-GAN transfer samples (d) Class-conditional IC-GAN transfer samples Samples from unlabeled (a) and class-conditional (b) IC-GAN trained on the 256?256 ImageNet dataset. For each subfigure, the first column represents instances used to condition the model and the next three columns depict model samples. For class-conditional generation in (b) we include samples conditioned on the same image but different labels. We highlight the generalization capacities of IC-GAN by applying the ImageNet-trained model to instances from other datasets in unlabeled (c) and class-conditional (d) scenarios. Panels (c) and (d) display samples conditioned on instances from the COCO-Stuff, Cityscapes, MetFaces, and PACS datasets (from top to bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison for scene generation on 256?256 COCO-Stuff.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 *</head><label>4</label><figDesc>? 0.0 15.4* ? 0.0 IC-GAN (BigGAN) + DA (d,i) 64 9.2* ? 0.0 23.5* ? 0.1 IC-GAN (StyleGAN2) + DA (d,i) 64 8.5* ? 0.0 23.5* ? 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on unlabeled ImageNet (256?256). Next to each input sample x i , used to obtain the instance features h i = f ? (x i ), the six nearest neighbors in the feature space of f ? are displayed. Below the neighbors, generated images sampled from IC-GAN with a BigGAN backbone and conditioned on h i are depicted. Immediately below the generated images, the closest image in the ImageNet training set is shown for each example (cosine distance in the feature space of f ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results on unlabeled ImageNet (256?256) using IC-GAN (BigGAN backbone) and interpolating between two instance feature vector conditionings (vertical axis) and two input noise vectors (horizontal axis).The two images depicted to the left of the generated image grids are used to extract the instance feature vectors used for the interpolation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparison for scene generation on 256?256 COCO-Stuff with other stateof-the-art scene generation methods. (a) Data samples x i from which instance features h i = f (x i ) are obtained for IC-GAN, and labeled bounding box conditionings are obtained for LostGANv2 and OC-GAN. Images generated with two random seeds with (b) LostGANv2 [49], (c) OC-GAN [50], (d) IC-GAN (StyleGAN2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Class label Goldfish(b) Class label Limousine (c) Class label Red fox</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative results in 256?256 ImageNet. For each class, generated images with BigGAN are presented in the first row, while the second and third row show generated images using classconditional IC-GAN with a BigGAN backbone, conditioned on the instance feature extracted from the data sample to their left (x i ) and their corresponding class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Generated 256?256 images with a class-conditional IC-GAN (BigGAN backbone) trained on ImageNet. Next to each data sample x i , used to obtain the instance features h i = f ? (x i ), we find generated images sampled from IC-GAN using h i and six sampled noise vectors. Below the generated images, the closest image in the ImageNet training set are shown (Cosine similarity in the feature space of f ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative off-the-shelf transfer results in 256 ? 256, using an IC-GAN trained on unlabeled ImageNet and conditioning on unseen instances from other datasets. The instances come from the following datasets (top to bottom): COCO-Stuff, Cityscapes, MetFaces, PACS (cartoons), Sketches. Next to each data sample x i , used to obtain the instance features h i = f ? (x i ), generated images conditioning on h i are displayed. Immediately below each generated image, the closest image in the ImageNet training set is displayed (Euclidean distance in the feature space of f ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Qualitative off-the-shelf transfer results in 256?256, using a class-conditional IC-GAN trained on ImageNet and conditioning on unseen instances from other datasets and a class label. The instances come from the following datasets (top to bottom): COCO-Stuff, Cityscapes, PACS (cartoons). On the left, a data sample x i is depicted, used to obtain the instance features h i = f ? (x i ). Next to the data samples, generated images conditioning on h i and a class label (under the data samples) are displayed. Just below the generated images, the closest image in the ImageNet training set are shown (Euclidean distance in the feature space of f ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>xi (b) IC-GAN trained with k = 5 (c) IC-GAN trained with k = 20 (d) IC-GAN trained with k = 50 (e) IC-GAN trained with k = 100 (f) IC-GAN trained with k = 500</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Qualitative results in 64 ? 64 unlabeled ImageNet when training IC-GAN (BigGAN backbone) with different neighborhood sizes k. (a) Data samples x i used to obtain the instance features h i = f ? (x i ). (b-f) Generated images with IC-GAN (BigGAN backbone), sampling different noise vectors, for different neighborhood sizes k used during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results for ImageNet in unlabeled setting. For fair comparison with</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results on COCO-Stuff. IC-GAN trained on ImageNet indicated as "transf". Some non-zero standard deviations are reported as 0.0 because of rounding. This raises the question of how close ImageNet and COCO-Stuff data distributions are. We compute the FID between real data train split of the two datasets at 128?128 resolution and obtain a score of 37.2. Hence, the remarkable transfer capabilities of IC-GAN are not explained by dataset similarity and may be attributed to the effectiveness of the ImageNet pre-trained feature extractor and generator. When we replace the conditioning instances from COCO-Stuff with those of ImageNet, we obtain a train FID score of 43.5, underlining the important distribution shift that can be implemented by changing the conditioning instances.Interestingly, the transferred IC-GAN also outperforms LostGANv2 and OC-GAN which condition on labeled bounding box annotations. Transferring the model from ImageNet boosts diversity w.r.t.</figDesc><table><row><cell>?FID</cell><cell>? LPIPS</cell></row></table><note>the model trained on COCO-Stuff (see LPIPS in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>Res.</cell><cell>?FID</cell><cell>?IS</cell></row><row><cell>BigGAN* [5]</cell><cell>64</cell><cell>12.3 ? 0.0</cell><cell>27.0 ? 0.2</cell></row><row><cell>BigGAN* [5] + DA (d)</cell><cell>64</cell><cell>10.2 ? 0.1</cell><cell>30.1 ? 0.1</cell></row><row><cell>IC-GAN</cell><cell>64</cell><cell>8.5 ? 0.0</cell><cell>39.7 ? 0.2</cell></row><row><cell>IC-GAN + DA(d, i)</cell><cell>64</cell><cell>6.7 ? 0.0</cell><cell>45.9 ? 0.3</cell></row><row><cell>BigGAN* [5]</cell><cell>128</cell><cell>9.4 ? 0.0</cell><cell>98.7 ? 1.1</cell></row><row><cell>BigGAN* [5] + DA(d)</cell><cell>128</cell><cell cols="2">8.0 ? 0.0 107.2 ? 0.9</cell></row><row><cell>IC-GAN</cell><cell cols="3">128 10.6 ? 0.1 100.1 ? 0.5</cell></row><row><cell>IC-GAN + DA(d, i)</cell><cell>128</cell><cell cols="2">9.5 ? 0.1 108.6 ? 0.7</cell></row><row><cell>BigGAN* [5] (ch ? 64)</cell><cell>256</cell><cell cols="2">8.0 ? 0.1 139.1 ? 0.3</cell></row><row><cell cols="2">BigGAN* [5] (ch ? 64) + DA(d) 256</cell><cell cols="2">8.3 ? 0.1 125.0 ? 1.1</cell></row><row><cell>IC-GAN (ch ? 64)</cell><cell>256</cell><cell cols="2">8.3 ? 0.1 143.7 ? 1.1</cell></row><row><cell>IC-GAN (ch ? 64) + DA(d, i)</cell><cell>256</cell><cell cols="2">7.5 ? 0.0 152.6 ? 1.1</cell></row><row><cell>BigGAN  ? [5] (ch ? 96)</cell><cell>256</cell><cell>8.1</cell><cell>144.2</cell></row><row><cell>IC-GAN (ch ? 96) + DA(d)</cell><cell>256</cell><cell cols="2">8.2 ? 0.1 173.8 ? 0.9</cell></row></table><note>Class-conditional results on ImageNet.*: Trained using open source code. DA: 50% horizontal flips in (d) real and fake samples, and (i) conditioning instances. ch?: Channel multiplier that affects network width.? : numbers from the original paper, as training diverged with the BigGAN opensourced code.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Class-conditional results on ImageNet-LT. *: Trained using open source code. ? 0.1 18.1 ? 0.2 28.1 ? 0.1 28.8 / 32.8 / 48.4 ? 0.2 16.0 ? 0.1 IC-GAN 64 23.2 ? 0.1 19.5 ? 0.1 23.4 ? 0.1 23.8 / 28.0 / 42.7 ? 0.1 17.6 ? 0.1</figDesc><table><row><cell></cell><cell cols="2">Res. ?train FID</cell><cell>?train IS</cell><cell>?val FID</cell><cell>many/med/few ?val FID</cell><cell>?val IS</cell></row><row><cell cols="5">BigGAN* [5] 27.6 BigGAN* [5] 128 64 31.4 ? 0.1 30.6 ? 0.1 35.4 ? 0.1</cell><cell>34.0 / 43.5 / 64.4 ? 0.2</cell><cell>24.9 ? 0.2</cell></row><row><cell>IC-GAN</cell><cell>128</cell><cell cols="3">23.4 ? 0.1 39.6 ? 0.2 24.9 ? 0.1</cell><cell>24.3 / 31.4 / 53.6 ? 0.3</cell><cell>32.5 ? 0.1</cell></row><row><cell cols="2">BigGAN* [5] 256</cell><cell cols="3">27.8 ? 0.0 58.2 ? 0.2 31.4 ? 0.1</cell><cell>28.1 / 40.9 / 67.6 ? 0.3</cell><cell>44.7 ? 0.2</cell></row><row><cell>IC-GAN</cell><cell>256</cell><cell cols="3">21.7 ? 0.1 66.5 ? 0.3 23.4 ? 0.1</cell><cell>20.6 / 32.4 / 60.0 ? 0.2</cell><cell>51.7 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Links to the assets used in the paper.</figDesc><table><row><cell>Name</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Assets licensing information.</figDesc><table /><note>Name License ImageNet [45] and ImageNet-LT [34] Terms of access: https://www.image-net.org/download.php COCO-Stuff [6] https://www.flickr.com/creativecommons Cityscapes</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>For IC-GAN, we explored D lr , G lr ? {4e?4, 2e?4, 1e?4, 4e?5, 2e?5, 1e?5} and D updates ? {1, 2}. For 128 ? 128, we used BS = 2,048, CH = 96 (as in<ref type="bibr" target="#b4">[5]</ref>), D lr = 1e?4, G lr = 4e?5 and D updates = 1. For 256 ? 256, we set BS = 2,048 and CH = 64 (half capacity, therefore faster training) for both BigGAN and IC-GAN, and used D lr = G lr = 1e?4 with D updates = 2 for IC-GAN. When using the StyleGAN2 architecture both as a baseline and as a backbone, we explored BS ? {32, 64, 128, 256, 512, 1,024}, D lr , G lr ? {1e?2, 7e?3, 5e?3, 2.5e?3, 1e?4, 5e?4} and We searched for D lr , G lr ? {1e?3, 4e?4, 1e?4, 4e?5, 1e?5} and D updates ? {1, 2}. For both unconditional BigGAN and IC-GAN, we chose D lr = 4e?4 and G lr = 1e?4 in 128 ? 128 and D lr = G lr = 1e?4 in 256 ? 256. For both resolutions, unconditional BigGAN uses D updates = 2 and IC-GAN, D updates = 1. When using StyleGAN2 architecture, we tried several learning rates D lr , G lr ? {1e?3, 1.5e?3, 2e?3, 2.5e?3, 3e?3} in combination with ? ? {2e?1, 1e?2, 5e?2, 1e?1, 2e?1, 5e?1, 1, 2, 10}. For the unconditional StyleGAN2 and IC-GAN trained at resolution 128?128, we chose D lr = G lr = 2.5e?3 with ? = 5e?2. At resolution 256?256, we found that D lr = G lr = 3e?3 with ? = 0.5 were optimal for IC-GAN while we obtained D lr = G lr = 2e?3 with ? = 2e?1 for the unconditional StyleGAN.ImageNet-LT. We explored BS ? {128, 256, 512, 1,024, 2,048} and CH ? {48, 64, 96} and found BS = 128 and CH = 64 to be the best configuration. We explored D lr , G lr ? {1e?3, 4e?4, 1e?4, 4e?5, 1e?5} and D updates ? {1, 2}. In 64 ? 64, we used D lr = 1e?3, G lr = 1e?5 and D updates = 1 for both BigGAN and IC-GAN setup. In 128?128 and 256?256, we used D lr = G lr = 1e?4 and D updates = 2.</figDesc><table /><note>? ? {2e?1, 1e?2, 5e?2, 1e?1, 2e?1, 5e?1, 1, 2, 10} and selected BS = 64 and D lr = G lr = 2.5e?3 and ? = 5e?2 for all resolutions.COCO-Stuff. When using BigGAN architecture, we explored BS ? {128, 256, 512, 2,048} and CH ? {32, 48, 64} and found BS = 256 and CH = 48 to be the best choice.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :Table 2 .</head><label>72</label><figDesc>Comparison between IC-GAN with and without data augmentation using the COCO-Stuff dataset. ? : 50% chance of horizontally flipping data samples x i to later obtain h i . The backbone for each IC-GAN is indicated with the number of parameters between parentheses. To compute FID in the training split, we use a subset of 1,000 training instance features (selected with k-means) as conditionings. StyleGAN2, despite being designed for unconditional generation, is outperformed by the unconditional counterpart of BigGAN, that uses a single label for the entire dataset, in ImageNet. We suspect that there might be some biases introduced in the architecture at design time, as BigGAN was proposed for ImageNet and StyleGAN2 was tested on datasets with human faces, cars, and dogs, generally with presumably lower complexity and less number of data points than ImageNet.</figDesc><table><row><cell></cell><cell>Backbone (M)</cell><cell></cell><cell></cell><cell>?FID</cell></row><row><cell></cell><cell></cell><cell>train</cell><cell>eval</cell><cell>eval seen</cell><cell>eval unseen</cell></row><row><cell>128x128</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IC-GAN</cell><cell>BigGAN (22)</cell><cell cols="3">18.0 ? 0.1 45.5 ? 0.7 85.0 ? 1.1</cell><cell>60.6 ? 0.9</cell></row><row><cell>IC-GAN  ?</cell><cell>BigGAN (22)</cell><cell cols="3">16.8 ? 0.1 44.9 ? 0.5 81.5 ? 1.3</cell><cell>60.5 ? 0.5</cell></row><row><cell>IC-GAN</cell><cell>StyleGAN2 (24)</cell><cell>8.9 ? 0.0</cell><cell cols="2">36.2 ? 0.2 74.3 ? 0.8</cell><cell>50.8 ? 0.3</cell></row><row><cell>IC-GAN  ?</cell><cell>StyleGAN2 (24)</cell><cell>8.7 ? 0.0</cell><cell cols="2">35.8 ? 0.1 74.0 ? 0.7</cell><cell>50.5 ? 0.6</cell></row><row><cell>256x256</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IC-GAN</cell><cell>BigGAN (26)</cell><cell cols="3">25.6 ? 0.1 53.2 ? 0.3 91.1 ? 3.3</cell><cell>68.3 ? 0.9</cell></row><row><cell>IC-GAN  ?</cell><cell>BigGAN (26)</cell><cell cols="3">24.6 ? 0.1 53.1 ? 0.4 88.5 ? 1.8</cell><cell>69.1 ? 0.6</cell></row><row><cell>IC-GAN</cell><cell cols="4">StyleGAN2 (24.5) 10.1 ? 0.0 41.8 ? 0.3 78.5 ? 0.9</cell><cell>57.8 ? 0.6</cell></row><row><cell cols="2">IC-GAN  ? StyleGAN2 (24.5)</cell><cell>9.6 ? 0.0</cell><cell cols="2">41.4 ? 0.2 76.7 ? 0.6</cell><cell>57.5 ? 0.5</cell></row></table><note>This intuition is further supported by StyleGAN2 improving over the BigGAN backbone in the COCO-Stuff experiments in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Results for ImageNet in unlabeled setting, comparing BigGAN and StyleGAN backbones.</figDesc><table /><note>For fair comparison with</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Moreover, in the bottom left quadrant, lorikeet instance features are interpolated with flying hummingbird instance features, and the generated images change their color and appearance accordingly. Finally, in the bottom right grid, we interpolate instance features from a tiger and instance features from a white wolf, resulting in different blends between the striped pelt of the tiger and the white fur of the wolf.</figDesc><table><row><cell>accordingly.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell>Uncond. BigGAN [36]</cell><cell>128</cell><cell>25.3</cell><cell>20.4</cell></row><row><cell>StyleGAN2 + DA (d)</cell><cell>128</cell><cell>27.8 ? 0.1</cell><cell>18.8 ? 0.1</cell></row><row><cell>IC-GAN (BigGAN) + DA (d,i)</cell><cell>128</cell><cell>11.7 ? 0.0</cell><cell>48.7 ? 0.1</cell></row><row><cell>IC-GAN (StyleGAN2) + DA (d,i)</cell><cell>128</cell><cell>15.2 ? 0.1</cell><cell>38.3 ? 0.2</cell></row><row><cell>StyleGAN2 + DA (d)</cell><cell>256</cell><cell>41.3 ? 0.1</cell><cell>19.7 ? 0.1</cell></row><row><cell cols="2">IC-GAN (BigGAN) (ch ? 64) + DA (d,i) 256</cell><cell>17.4 ? 0.1</cell><cell>53.5 ? 0.5</cell></row><row><cell>IC-GAN (BigGAN) (ch ? 96) + DA (d)</cell><cell>256</cell><cell>15.6 ? 0.1</cell><cell>59.0 ? 0.4</cell></row><row><cell>IC-GAN (StyleGAN2) + DA (d,i)</cell><cell>256</cell><cell>23.1 ? 0.1</cell><cell>42.2 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>FID scores on COCO-Stuff 128 ? 128, when using an IC-GAN trained on ImageNet and tested with instance features from either COCO-Stuff or ImageNet and using either of those datasets as reference. The metrics obtained by sampling 1,000 instance features (k-means) from either ImageNet or COCO, and generating 76,000 samples. As a reference, 76,000 real samples from COCO-Stuff or ImageNet training set are used.</figDesc><table><row><cell></cell><cell cols="3">train instance dataset eval instance dataset FID reference dataset</cell><cell>?FID</cell></row><row><cell>IC-GAN</cell><cell>ImageNet</cell><cell>COCO-Stuff</cell><cell>COCO-Stuff</cell><cell>8.5 ? 0.1</cell></row><row><cell>IC-GAN</cell><cell>ImageNet</cell><cell>ImageNet</cell><cell>COCO-Stuff</cell><cell>43.6 ? 0.1</cell></row><row><cell>IC-GAN</cell><cell>ImageNet</cell><cell>COCO-Stuff</cell><cell>ImageNet</cell><cell>37.2 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>ImageNet-LT quantitative results for different class balancing techniques. "t.": training and "v.": validation.</figDesc><table><row><cell></cell><cell>Res.</cell><cell>?t. FID</cell><cell>?t. IS</cell><cell>?v. FID</cell><cell>?v. [many/med/few] FID</cell><cell>?v. IS</cell></row><row><cell>BigGAN</cell><cell>64</cell><cell cols="3">27.6 ? 0.1 18.1 ? 0.2 28.1 ? 0.1</cell><cell>28.8/32.8/48.4 ? 0.2</cell><cell>16.0 ? 0.1</cell></row><row><cell>BigGAN (CB)</cell><cell>64</cell><cell cols="3">62.1 ? 0.1 10.7 ? 0.2 56.2 ? 0.1</cell><cell>62.2/59.7/74.7 ? 0.2</cell><cell>11.0 ? 0.0</cell></row><row><cell>BigGAN (T = 2)</cell><cell>64</cell><cell cols="3">30.6 ? 0.1 16.8 ? 0.1 29.2 ? 0.1</cell><cell>30.9/33.3/49.5 ? 0.2</cell><cell>16.4 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Feature extractor impact with SwAV (ResNet50 trained with a self-supervised approach) and RN50 (ResNet50 trained for the classification task in ImageNet). Experiments performed in 64?64 ImageNet, using 1,000 training instance features at test time, selected with k-means.</figDesc><table><row><cell>?FID</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Comparison between training IC-GAN (BigGAN backbone) using only 1,000 conditioning instance features (selected with k-means) or all training instance features during training, in IC-GAN 64 ? 64. At test time, we condition IC-GAN on 1,000 training instance features, selected with k-means. = 1,200, trained with only 1,000 conditionings) 24.8 ? 0.1 16.4 ? 0.1 labels in the training set to zero, following</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Comparing IC-GAN with the unconditional counterparts of BigGAN on 64?64 ImageNet with the same storage requirements. Storage-G counts the storage required for the generator, Storage-I the storage required for the training instance features, and Storage-All is the sum of both generator and instance features required storage. FID and IS scores are computed using Pytorch code.</figDesc><table><row><cell>Method</cell><cell cols="4">#prms. Storage-G Storage-I Storage-All</cell><cell>?FID</cell><cell>?IS</cell></row><row><cell>Unconditional BigGAN</cell><cell>32.5M</cell><cell>124MB</cell><cell>0MB</cell><cell cols="2">124MB 30.0 ? 0.1 12.1 ? 0.1</cell></row><row><cell>Unconditional BigGAN</cell><cell>39M</cell><cell>149MB</cell><cell>0MB</cell><cell cols="2">149MB 16.9 ? 0.0 14.6 ? 0.1</cell></row><row><cell>IC-GAN (BigGAN)</cell><cell>37M</cell><cell>141MB</cell><cell>8MB</cell><cell cols="2">149MB 10.4 ? 0.1 21.9 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>Comparing IC-GAN with the unconditional counterparts on 128?128 COCO-Stuff, with the same storage requirements. Storage-G counts the storage required for the generator, Storage-I the storage required for the training instance features, and Storage-All is the sum of both generator and instance features required storage.</figDesc><table><row><cell></cell><cell cols="4">#prms. Storage-G Storage-I Storage-All</cell><cell cols="2">?FID</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>train</cell><cell>eval</cell></row><row><cell>Unconditional BigGAN</cell><cell>18M</cell><cell>68MB</cell><cell>0MB</cell><cell cols="3">68MB 17.9 ? 0.1 46.9 ? 0.5</cell></row><row><cell>Unconditional BigGAN</cell><cell>24M</cell><cell>92MB</cell><cell>0MB</cell><cell cols="3">92MB 28.8 ? 0.1 58.1 ? 0.5</cell></row><row><cell>IC-GAN (BigGAN)</cell><cell>22M</cell><cell>84MB</cell><cell>8MB</cell><cell cols="3">92MB 16.8 ? 0.1 44.9 ? 0.5</cell></row><row><cell>Unconditional StyleGAN2</cell><cell>23M</cell><cell>88MB</cell><cell>0MB</cell><cell>88MB</cell><cell>8.8 ? 0.1</cell><cell>37.8 ? 0.2</cell></row><row><cell>Unconditional StyleGAN2</cell><cell>26M</cell><cell>100MB</cell><cell>0MB</cell><cell>100MB</cell><cell>9.4 ? 0.0</cell><cell>38.4 ? 0.3</cell></row><row><cell>IC-GAN (StyleGAN2)</cell><cell>24M</cell><cell>92MB</cell><cell>8MB</cell><cell>100MB</cell><cell>8.7 ? 0.0</cell><cell>35.8 ? 0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Impact of the number of neighbors (k) used to train class-conditional IC-GAN (BigGAN backbone) in ImageNet-LT 64?64. Reported results in ImageNet validation set. As a feature extractor, a ResNet50 is trained as a classifier on the same dataset is used. 50k instance features are sampled from the training set. ? 0.1 17.6 ? 0.1 k = 20 24.1 ? 0.1 16.8 ? 0.1 k = 50 24.1 ? 0.1 16.7 ? 0.1 k = 100 25.6 ? 0.1 16.3 ? 0.1 k = 500 27.1 ? 0.1 15.3 ? 0.1</figDesc><table><row><cell></cell><cell>?FID</cell><cell>?IS</cell></row><row><cell>k = 5</cell><cell>23.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We subsampled 76,000 ground-truth images from ImageNet training set and used all COCO-Stuff training ground-truth images.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that this setup resembles the class partition in ImageNet, where 1,000 classes contain approximately 1,200 images each.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We store both parameters and instance features as float32.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anton van den Hengel, and Lingqiao Liu. A generative adversarial density estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>M Ehsan Abbasnejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Armandpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00816</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Partition-guided GANs. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Specifying object attributes and relations in interactive scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Spinger-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1xsqj09Fm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">COCO-Stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero-Soriano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04027</idno>
		<title level="m">Generating unseen complex scenes: are we there yet? arXiv preprint</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Instance selection for GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The deepfake detection challenge (DFDC) preview dataset. CoRR, abs/1910.08854</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pflaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Baram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.08854" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mixture density generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Eghbal-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Alexa</surname></persName>
		</author>
		<idno>44:1-44:10</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH)</title>
		<meeting>SIGGRAPH)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SketchyCOCO: image generation from freehand scene sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengying</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-agent diverse generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viveka</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><forename type="middle">K</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Class-splitting generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guillermo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grinblat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">M</forename><surname>Uzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Granitto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07359</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MGAN: Training generative adversarial nets with multiple generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Tu Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06991</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PacGAN: The power of two samples in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Diverse image generation via self-conditioned GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mixed batches and symmetric discriminators for GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-fidelity image generation with fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Data instance prior (DISP) in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineeth</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<title level="m">Unrolled generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reliable fidelity and diversity metrics for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Muhammad Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02162</idno>
		<title level="m">Self-labeled conditional GANs. arXiv preprint</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatiallyadaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semi-parametric image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Logo synthesis and manipulation with clustered generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image synthesis from reconfigurable layout and style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Learning layout and style reconfigurable GANs for controllable image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11571</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Object-centric image generation from layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Sylvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07449</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Local class-specific and global image-level generative adversarial networks for semantic-guided scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12982</idno>
		<title level="m">Grafit: Learning fine-grained image representations with coarse labels. arXiv preprint</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1606.04080" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis and semantic manipulation with conditional GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image generation from layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for dataefficient GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
