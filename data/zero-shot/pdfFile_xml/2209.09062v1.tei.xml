<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Overview of the SV-Ident 2022 Shared Task on Survey Variable Identification in Social Science Publications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tornike</forename><surname>Tsereteli</surname></persName>
							<email>tornike.tsereteli@uni-mannheim.de</email>
							<affiliation key="aff0">
								<orgName type="department">Data and Web Science Group</orgName>
								<orgName type="institution">University of Mannheim</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavuz</forename><surname>Selim Kartal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
							<email>ponzetto@uni-mannheim.de</email>
							<affiliation key="aff0">
								<orgName type="department">Data and Web Science Group</orgName>
								<orgName type="institution">University of Mannheim</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Zielinski</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Fraunhofer ISI</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Eckert</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Web-based Information Systems and Services</orgName>
								<orgName type="institution">Stuttgart Media University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Mayr</surname></persName>
							<email>philipp.mayr@gesis.organdrea.zielinski@isi.fraunhofer.deeckert@hdm-stuttgart.de</email>
							<affiliation key="aff1">
								<orgName type="department">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Overview of the SV-Ident 2022 Shared Task on Survey Variable Identification in Social Science Publications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we provide an overview of the SV-Ident shared task as part of the 3rd Workshop on Scholarly Document Processing (SDP) at COLING 2022. In the shared task, participants were provided with a sentence and a vocabulary of variables, and asked to identify which variables, if any, are mentioned in individual sentences from scholarly documents in full text. Two teams made a total of 9 submissions to the shared task leaderboard. While none of the teams improve on the baseline systems, we still draw insights from their submissions. Furthermore, we provide a detailed evaluation. Data and baselines for our shared task are freely available at https://github. com/vadis-project/sv-ident.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social science publications often use and reference survey datasets, containing hundreds or thousands of questions, using so-called survey variables. 1 While publications may focus only on a specific subset of these variables, explicit references are usually missing: the lack of explicit links between survey variables and publications, in turn, limits access to research along the FAIR principles <ref type="bibr" target="#b4">(Wilkinson et al., 2016)</ref>. To address this issue, we propose a task where variable mentions in unstructured documents are linked to items from a catalog of survey research datasets using Natural Language Processing (NLP) methods. Automatically identifying which variable is mentioned in a given text is challenging due to the diverse linguistic realizations of variables <ref type="bibr" target="#b8">(Zielinski and Mutschke, 2018)</ref>. A short example text is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. All three QD3_1 EU CITIZENSHIP: FEEL TO BE EU CITIZEN QD2_3 ATTACHMENT TO: EUROPEAN UNION In the subsequent section, we focus on two theoretical concepts -mere exposure and hostile media perceptions -both of which have not been employed within the context of European identity.</p><p>The dependent variable (identity) was operationalized by two indicators: Citizens were asked to answer the following question: "Please tell me how attached you feel to the European Union" by using a scale from 1 (not attached at all) to 4 (very attached). Furthermore, the item "You feel you are a citizen of the EU" was used in index building.</p><p>... sentences mention and are linked to relevant variables. The first sentence mentions three concepts: 2 mere-exposure effect, hostile media perceptions, and European identity. The first two concepts are defined later in the text (we omit their links in this example), while the latter is defined in the bottom two sentences in the figure. The second and third sentences both are explicit mentions, as they include direct quotations of variable questions. Ideally, a system should link relevant variables to each of the sentences in the example. Specifically, when only provided the given context, it should link the first sentence to the variables QD2_3 and QD3_1, the second sentence to QD2_3, and the third sentence to QD3_1. A larger variant of the example is provided in <ref type="figure" target="#fig_3">Figure 3</ref> in the Appendix.</p><p>The Survey Variable Identification 3 (henceforth, SV-Ident) shared task aims at promoting the developing of systems that can identify variables within the text of scholarly publications from the social sciences in different languages (initially, we focus here on English and German). The shared task is divided into two sub-tasks: a) Variable Detection and b) Variable Disambiguation. The former deals with identifying sentences that contain variable mentions, while the latter focuses on linking the correct variables mentioned in a sentence. Variable mentions are often implicit (e.g., sentences 1 and 3 in <ref type="figure" target="#fig_0">Figure 1)</ref>, and understanding when a variable is mentioned may require contextual information as well as knowledge from external sources (e.g., a variable vocabulary). Since annotating scientific texts requires domain knowledge, training data is costly to create and thus scarce. To overcome these limitations, NLP systems, e.g., models using pre-trained language models (PLMs) and transfer learning are promising technologies to use.</p><p>In this paper, we report the results on the first edition of the SV-Ident shared task. Two teams made a total of 9 submissions to the leaderboard. One of the teams developed systems for both sub-tasks and submitted a system description paper. While none of the teams improve on the baselines, we use the submissions provided by the teams to collect a few initial findings on the difficulties and challenges of the SV-Ident task. Crucially, we find that there is a difference between the performance on two types of variable mentions: explicit and implicit. Implicitly mentioned variables (sentence 1 in <ref type="figure" target="#fig_0">Figure 1</ref>) are significantly more difficult to detect and disambiguate, as they require contextual knowledge. This opens up new research questions for future work, such as, for instance: can implicit mentions of survey variables be further categorized into finergrained classes or can co-reference resolution be used to link variable mentions across different parts of a document? In order to foster future research on this task, we release all of our code to reproduce the analysis results and the annotation guidelines for creating the dataset at https://github.com/ vadis-project/sv-ident.</p><p>The remainder of this paper is organized as follows: we provide an overview of the dataset used in ?2. In ?3, we describe the task definition and evaluation metrics. We present the submitted systems in ?4 and provide a detailed analysis of the results in ?5. We briefly discuss related work in ?6 and frame the shared task into a broader context in ?7. Finally, we summarize the shared task and propose future work in ?8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The SV-Ident 2022 shared task has been conceived in the context of the VADIS project 4 and organized as part of the third Workshop on Scholarly Document Processing (SDP) (Chandrasekaran et al., 2020), co-located at the 2022 International Conference on Computational Linguistics. In the following, we describe the data collection process and the dataset used for the shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SV-Ident Corpus</head><p>The SV-Ident Corpus contains publicly-available scientific publications from the Social Science Open Access Repository (SSOAR) 5 in full text. To collect the corpus, we first filter the 5,000 most popular research datasets using search logs from GESIS Search. <ref type="bibr">6</ref> We then retrieve the publications linked to these datasets as our candidate set. Finally, only those publications that had at least one associated research dataset with indexed variables on the GESIS Search platform are retained. This results in 285 documents from the original set of 120k publications. For this set of candidates, we then select 44 documents for annotation, which include the most popular ones as well as publications linked to variable vocabularies of different sizes.</p><p>Each document in our dataset has been annotated in PDF format using the INCEpTION software <ref type="bibr">(Klie et al., 2018)</ref> by two domain experts (graduate students trained in the social sciences). Annotators are provided with the whole document and asked to label all sentences that contained variables, including the variables the sentences mentioned. We first conduct two calibration rounds, for which annotators are given 50 two-page documents from the dataset collected by <ref type="bibr" target="#b8">Zielinski and Mutschke (2018)</ref>. Afterwards, the selected 44 documents are annotated in three annotation rounds over a period of 8 weeks (on average, each annotator spent between 1-2 hours on each document). Texts are then extracted, and all parsing errors are manually corrected.  words. Because annotators have access to all parts of the document at once, the annotation setup allows the use of document-level knowledge to infer sentence-level labels. The annotations include the variable IDs that are mentioned in a text from a set of possible candidates, confidence scores for the annotations, and, for the test set, annotators also classified each mentioned variable into an explicit or an implicit mention (examples of explicit and implicit mentions were both found in the annotation guidelines). We generally define explicit mentions as those which do not require contextual information to be labeled correctly. The opposite is true for implicit mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SV-Ident Shared Task Dataset</head><p>When annotating, the set of candidate variables is potentially made up of all variables from the research datasets linked to a publication on the GESIS platform: this set usually contains hundreds or thousands of variables, thus making the annotation task impractical and hard to scale. To help reduce the size of the set of possible survey variable labels, annotators are provided with a tool to find matches using different methods. The first method uses an ensemble of four sentencetransformer models to predict the top 20 variables that are semantically most similar to the reference sentence for each model. The annotators receive recommendations for variables for which at least two models predict them to be in the top 20 results. The second method allows annotators to search using a method of matching strings approximately rather than exactly: specifically, we use the Token Set Ratio metric, which compares the number of insertion and deletion operations for unique and com-  mon words between the strings to be compared. <ref type="bibr">7</ref> The last method simply provides annotators with the full list of variables to manually search through. All three methods have their drawbacks. The first two might fail to recommend valid variables for cases with high linguistic variation, vagueness, or infrequent words, while the last may provide annotators with a search space that is too large. While we do not control for such possible failures, future work may draw insights from the analysis of the annotations.</p><p>The dataset for the shared task is a subset of the SV-Ident corpus. More specifically, 14 out of the 44 annotated documents from the SV-Ident Corpus are additionally filtered out due to missing links to research data, incorrect annotations, or PDF parsing errors, leaving 30 documents in total. The dataset consists of 18 documents (7 English and 11 German) for the training and development sets and 12 documents (6 for each English and German) for the blind test set.</p><p>An example of a sentence and its metadata, including annotated labels from the dataset, is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Each instance in our dataset contains: a document ID (doc_id); a binary label (is_variable), where a value of 1 implies that the sentence contains a variable; the language of the sentence (lang); a list of document-level linked research datasets (re-search_data); the sentence (sentence); a unique ID (uuid); and a list of annotated variables (variable). Raw sentence counts for each of the dataset splits are provided in <ref type="table" target="#tab_2">Table 1</ref>. Since the test set contains more English sentences, during evaluation, we compute the mean of the scores for each language for the competing systems (see ?5 for more details). In total, there are 3,823 training, 425 validation, and 1,724 test sentences. English and German sentences are roughly evenly distributed at 3,035 and 2,937 instances for each language, respectively.   Because of the challenging nature of the annotation task, we join the annotated instances of variable mentions and link variables of each annotator. We provide agreement scores between annotators for English and German instances separately <ref type="table" target="#tab_4">(Table 2</ref>). We calculate the Cohen's Kappa score for agreement on Variable Detection. The scores for both English and German range between 0.46 and 0.48, which indicate that there is a moderate agreement. For Variable Disambiguation, we use Krippendorff's Alpha. Both languages have an agreement score of 0.08, which implies that the agreement is close to random. One reason for such low agreement is the large number of possible variables to choose from, given that the total vocabulary size for all the documents is very large <ref type="bibr">(27,</ref><ref type="bibr">365</ref> variables that are often similar). The annotators labeled 1,165 unique variables (around 4% of the vocabulary) a total of 11,356 times <ref type="table" target="#tab_5">(Table 3</ref>). In the future, we plan to analyze this disagreement with respect to the choice of variables further.</p><p>Looking at the document-level, variables occur with different frequency in different documents (shown in <ref type="table" target="#tab_7">Table 4</ref>). The size of the variable vocabulary (i.e., the subset of all variables, containing only the variables from the research datasets that are linked to a publication) related to a publication ranges from 64 to 5,733. The number of annotated variables is at least 13 and at most 1,204 for English, and for German 20 and 1,143, respectively. The number of uniquely annotated variables is at most 153. In the final analysis, we investigate at which ratio sentences of a document are annotated. While the annotation ratio is at least 7%, it is at  most 86% for relatively dense documents. <ref type="bibr">8</ref> In addition to document-level differences, variables may require contextual knowledge to be disambiguated. Based only on the test set, annotators agree that 242 sentences had explicit, 13 implicit, and 18 both types of mentions. At the fine-grained annotator-level, the first annotator labeled close to 37% more implicit than explicit mentions, while the second labeled nearly thirteen times as many explicit as implicit mentions. Given that we did not conduct calibration rounds on this specific concept, annotators may not have shared the same understanding, since this distinction was introduced only in the third round of annotations. Future work will focus on further analyzing and validating the annotations. We make our dataset available on GitHub as well as on HuggingFace. 9 In addition, we also release, as the trial dataset, the data that were originally created by Zielinski and Mutschke (2018) (while the annotation procedure does not follow the same guideline, the data can be used as additional training data). Notably, consecutive sentences mentioning the same variable as well as vague variable mentions were not annotated in the trial data. We manually filter the trial data, after which, 446 English and 573 German sentences remain in the training set and 87 and 111 in the test set for each language, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>The task of SV-Ident deals with identifying variable mentions in a text. For simplicity, the task is formulated as a sentence-level task, but can also be solved using document-level information (inline with the data annotation process). The shared task is decomposed into two sub-tasks: Variable Detection and Variable Disambiguation, where the former task can be used to help filter candidate sentences for the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tasks</head><p>Task 1: Variable Detection. The first task can be seen as a binary text classification task. More formally, given a set of texts T (in our case, sentences), for each t, where t ? T , systems should predict the binary label l ? [0, 1] for t, where a value of 1 implies that t mentions a variable.</p><p>Task 2: Variable Disambiguation. The second task can be viewed as an information retrieval (IR) task, where the goal is to identify all relevant documents (i.e., variables) for a given query (i.e., input sentence). More formally, given a set of queries Q that mention variables, where Q ? T , and the set of all documents D (in our case, variables), for each q, where q ? Q, systems should predict the subset of documents D that are mentioned in q, where D ? D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>To evaluate systems, we use standard text classification and information retrieval evaluation metrics. For the first task, systems are evaluated using the standard F 1 ? macro score averaged across languages and documents. F 1 ? macro is defined as follows:</p><formula xml:id="formula_0">F 1 = 1 N n n?N F 1 n = 1 N n n?N 2P n R n P n + R n ,<label>(1)</label></formula><p>where P and R are the precision and recall scores, respectively. The F 1 ? macro averages the scores for P and R across classes (i.e., scores are computed for each class separately and each is weighted equally). For the second task, systems were evaluated using the (Mean) Average Precision (MAP) score with a recall cutoff value of 10 (denoted as MAP@10). Average Precision (AP) measures the average of the precision scores at each relevant item returned (i.e., recall level) in a search result set. MAP is the mean of the AP scores when computed across more than one query. MAP considers the ranking position of each relevant document. It further assumes that a user desires to retrieve many relevant documents. MAP is defined as follows:</p><formula xml:id="formula_1">MAP@K = 1 N n n?N AP @K n = 1 N n n?N 1 K k k?K P @k,<label>(2)</label></formula><p>where P is the precision score, K the recall level, and N the number of queries. We choose MAP over accuracy, because MAP incorporates the rank of the predicted document, which accuracy ignores.</p><p>In a realistic use-case, a user may be interested in being recommended up to K relevant variables per sentence. While we did not empirically test what value of K would be most suitable for a user, we choose K to equal 10, since 95% of all sentences are labeled with up to 10 variables. In addition to F 1 ? macro and MAP@10, we provide secondary metrics, which are not used for ranking the submitted systems, but can provide additional insights into the results. These include precision (P), recall (R), different values of K for MAP, and R-precision, which is the precision at recall R, where R is the number of relevant documents for a query.</p><p>In order to account for dataset imbalance during evaluation, for each score function f (i.e., evaluation metric), we compute the average score across languages and documents. The intuition is that languages and documents are equally important, and a model should perform well on all. The average score is computed as follows:</p><formula xml:id="formula_2">average score = 1 L l l?L 1 D l d d?D l f (d),<label>(3)</label></formula><p>where L is the set of languages, D the set of documents, and D l the set of documents for a given language, for l ? L and D l ? D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Shared Task Setup</head><p>The shared task was hosted on CodaLab. 10 After registering for the shared task, participants could download the test set and were asked to submit their predictions on CodaLab as a single file for each task (submissions were allowed from July 18th through August 1st, 2022). Submissions were limited to 20 for each task. For each submission, an automated evaluation system would upload the computed scores to the public leaderboard.</p><p>Two teams participated in our challenge on Co-daLab, and one of the teams submitted a system description, which is included in the proceedings. We summarize the report here. The participant (H?velmeyer and Kartal, 2022) treated both tasks, at least partly, as a problem of semantic textual similarity <ref type="bibr" target="#b0">(Agirre et al., 2013)</ref>. For Task 1, sentences were first preprocessed by randomly undersampling in order to balance the data, removing stopwords, lemmatizing the data, and using only a subset of the fields from the vocabulary metadata based on preliminary experiments. Then, test sentences and vocabulary data were converted into dense sentence representations using Sentence-T5 <ref type="bibr">(Ni et al., 2022)</ref> for English and Sahajtomar/German-semantic 11 (henceforth, GS) for German. Similarity scores were computed for those test sentence and vocabulary item pairs. Pairs with a score greater than a predetermined threshold were classified as sentences containing variables. For Task 2, the same sentence representations were used, but for all test sentences. The variables were then ranked based on their scores, with a higher score implying a greater similarity. While other methods were also implemented, such a Logistic Regression and Multinominal Naive Bayes classifiers, the best performing systems used Sentence-T5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>This section first describes the baseline systems for each task and later provides the results of the shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We train a transformer-based model for Variable Detection and implement lexical and neural zeroshot baselines for Variable Disambiguation.</p><p>The baseline system for the first task uses a transfer learning approach by fine-tuning a pre-trained language model (PLM) on the training and validation datasets. We use a PLM that was further pre-trained on a corpus of English social science abstracts, SsciBERT <ref type="bibr" target="#b2">(Shen et al., 2022)</ref>, which outperforms <ref type="bibr">BERT (Devlin et al., 2019)</ref> and SciB-ERT (Beltagy et al., 2019) models on the SV-Ident test set. Because no multilingual or German PLM counterparts exist that have been pre-trained on scientific texts, we use the specialized monolingual SsciBERT for both English and German data.</p><p>For the second task, we implement three baseline systems in a zero-shot setting: a lexical as well as sparse and dense retrieval models. We choose BM25 as our lexical baseline, using <ref type="bibr">Elasticsearch. 12</ref> For the sparse model, we use SPARTA <ref type="bibr" target="#b6">(Zhao et al., 2021)</ref> and a multilingual sentencetransformer 13 <ref type="bibr" target="#b1">(Reimers and</ref><ref type="bibr">Gurevych, 2019, 2020)</ref> as the dense retriever. Rather than training the models on the data, we use them to first encode the query and documents (i.e., variable metadata) and later rank those which are most semantically similar to a query by computing the cosine similarity between query-document pairs. The similarity computation assumes that instances that are closer together in vector space are semantically more similar. While participant 2 conducts an ablation study on the choice of metadata to use for matching the variables, we choose to include all metadata and leave finding the the optimal combination of metadata to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Task 1 had two participants and a single baseline system, while Task 2 had one participant and three baseline systems. In the tables below, the systems are denoted as follows: participant 1 as Unk, participant 2 as S-T5/GS (or S-T5 for English and GS for German), the baseline for Task 1 as SSBert * , and the baselines for Task 2 as BM25 * , Sparse * for the SPARTA model, and Dense * for the multilingual sentence-transformer (all baselines across text and tables are always marked with a * asterisk).</p><p>Variable Detection. For this task, none of the participating systems are able to beat the average score of the baseline. Unk scores lower than chance likelihood, while, with a score of 60.17, S-T5/GS comes close to SSBert * , which has a score of 66.10 <ref type="table" target="#tab_9">(Table 5</ref>). Breaking the scores down into the average scores across documents for each language, S-T5 outperforms the baseline for English. Thus, Task 1 can also be solved in a zero-shot setting, given that the Sentence-T5 model was not finetuned on the provided data. Similar large PLMs may show further improvements.  At the document-level, systems show varying performance (see <ref type="table" target="#tab_15">Table 8</ref> in the Appendix). For the document with the ID 21357, participants' systems have low scores, while SSBert * has the highest score across all documents. Furthermore, for 7 out of the 12 documents, the baseline system has the highest score. In addition to the number of positive and negative instances, we also report the number of variables associated with a document as well as the year of the publication. When computing the Pearson correlation coefficient, we find a weak correlation between the F 1 scores and the size of the search space (i.e., vocabulary size) for Unk (r = 0.132, p = 0.68), S-T5/GS (r = 0.266, p = 0.26), and SSBert * (r = 0.152, p = 0.64). With respect to the year of the document, we find a moderate correlation for Unk (r = 0.272, p = 0.39), S-T5/GS (r = 0.274, p = 0.39), and SSBert * (r = 0.392, p = 0.21). However, these correlations may not generalize due to the small number of documents.</p><p>Given the low annotator agreement with respect to the fine-grained labels, explicit and implicit, we report scores for the cases where both annotators agree on the label (see <ref type="table">Table 9</ref> in the Appendix) as well as for each annotator independently (see Tables 10 and 11 in the Appendix). We divide the labels into explicit, implicit, and mixed classes, where sentences that contain explicit and implicit variables are labeled as mixed. In cases where both annotators agree on the label, systems perform better on explicit than on implicit or mixed mentions. The same is true for annotator 1, except for S-T5/GS. This implies that explicit mentions are easier to detect and disambiguate. This is not the case for annotator 2. A possible explanation could be the low number of implicit annotations, which may be due to a difference in understanding of the labels. Unk outperforms all systems for the cases when both annotators agree on the label. This is surprising given the low average performance of the system (unfortunately, no system description was provided).</p><p>Variable Disambiguation. For the second task, we report only a single submission together with the results for three baselines <ref type="table" target="#tab_11">(Table 6</ref>). As described in Section 5, the baselines include BM25, SPARTA (henceforth, Sparse * ), and a multilingual sentence-transformer (henceforth, Dense * ). While we provide participants all the test sentences, we only evaluate performance on the subset of instances that contain variable mentions, as Task 1 already validates Variable Detection performance (this setup ignores false positive queries submitted by the participants). Unless explicitly stated, the following discussion mainly focuses on the MAP@10 scores. While the participant's system performs close to Dense * for English, Dense * scores twice as high for German. Sparse * outperforms all systems on English data. This is likely due to the system having been trained on a large English retrieval corpus. 14 BM25 * and Sparse * perform worse on German. Lexical models, such as BM25 * , are prone to perform worse for languages that have many rare words, such as German, which allows compound nouns. Furthermore, because Sparse * is only specialized for English, it does not perform well for data in a different language. Overall, Dense * outperforms all systems by at least 0.5 points for English, except for Sparse * , and by at least around 10 points for German.</p><p>At the document-level, scores vary significantly (see <ref type="table" target="#tab_2">Table 12</ref> in the Appendix). Scores across different values of K improve as K increases. For dense documents (i.e., documents with a high ratio of variable mention sentences), scores increase significantly when going from k = 1 to k = 5, such as for the IDs 21357, 57204, and 66324. Furthermore, while some systems perform well on a document, others perform poorly. For example, the document with ID 66324 shows the lowest performance by all systems except for BM25 * , which has a score of 22.01 and is the second-highest document score for BM25 * . For 57561, BM25 * achieves only a score of 1.60, while all other systems score higher than 16. S-T5/GS outperforms all baselines only once and twice when compared  to only Dense * . Such exceptions may be caused by a larger overlap between the tokens in the document and the underlying data used to train the models. In addition, we find a moderate correlation between MAP@10 scores and the vocabulary size (and a strong correlation for Dense * ) for S-T5/GS (r = 0.395.p = 0.20), BM25 * (r = 0.465.p = 0.13), Sparse * (r = 0.427.p = 0.17), and Dense * (r = 0.623.p = 0.03). As the search space increases, performance goes down. Finally, we find that MAP@10 is highly correlated with R-Precision (r = 0.941, p = 4.99), which implies that MAP is a good metric in the absence of the ground truth number of relevant variables. Performance on the annotator-level is similar to that of Task 1: scores are highest when both annotators agree on the label (see <ref type="table" target="#tab_2">Table 13</ref> in the Appendix). For both annotators, scores for the explicit class are consistently higher than for either implicit or mixed classes (see <ref type="table" target="#tab_2">Tables 14 and 15</ref> in the Appendix). This means that for the task of Variable Detection, knowing whether a variable is mentioned explicitly or implicitly can mean a 10 to 20 point absolute difference in performance. In the case when either both annotators agree on the label or when looking only at annotator 1, Sparse * outperforms all systems. Exploring other sparse models is a promising future direction for disambiguating implicit variable mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Identifying mentions of survey variables in text was first introduced by <ref type="bibr">Mutschke (2017, 2018)</ref>   As the predecessor of our task, they created the first dataset for the problem of SV-Ident. <ref type="table" target="#tab_13">Table 7</ref> shows the statistical differences between the OM and SV-Ident datasets. Although fewer documents are annotated in SV-Ident, the number of instances in SV-Ident is almost 5 times that of OM. To have a greater diversity of survey variables, SV-Ident corpus uses 76 datasets with more than 27k variables from different research studies, such as ALLBUS, ISSP, and Eurobarometer, whereas OM only used a single dataset. Moreover, the SV-Ident corpus comes up with modified and additional annotation features: the unknown (UNK) token was used for ambiguous variable mentions; consecutive mentions of the same variable were included; confidence levels of the annotations and variable mention types were labeled; and variables were linked across languages. As a result, our corpus is much larger and more diverse.</p><p>Given that identifying variables requires semantic relations, other NLP tasks deal with a fundamentally similar perspective, such as entity linking (EL), recognizing textual entailment (RTE), semantic textual similarity (STS), plagiarism detection, and detecting previously fact-checked news. EL can be conceptualized as linking mentions to variables in a knowledge base <ref type="bibr">(Rao et al., 2013)</ref>. Since there are many similar survey variables in research datasets, disambiguating the right variable for a sentence is similar to determining the identity of an entity from a knowledge base. The RTE task is to identify whether a sentence entails a given candidate hypothesis or not <ref type="bibr">(Dzikovska et al., 2013)</ref>. A question answering adaptation of <ref type="bibr">RTE (Dagan et al., 2013)</ref> is similar to SV-Ident, as the question and each answer form a hypothesis, which then re-quires the system to determine whether a sentence entails a given candidate hypothesis. STS is yet another similar task, which aims to find the similarity level between given texts <ref type="bibr" target="#b0">(Agirre et al., 2013)</ref>. STS was organized as a shared International Workshop on Semantic Evaluation between 2012 and 2017, and STS models have been developed for various domains <ref type="bibr" target="#b3">(Wang et al., 2020;</ref><ref type="bibr" target="#b5">Yang et al., 2020;</ref><ref type="bibr">Guo et al., 2020)</ref>. In the task of Plagiarism Detection of PAN, 16 a system should extract all plagiarized passages from a given set of candidate documents with (external) or without (intrinsic) comparing them to potential source documents <ref type="bibr">(Potthast et al., 2013)</ref>. Lastly, Detecting Previously Fact-Checked Claims, a shared task by the CheckThat! Lab <ref type="bibr">(Nakov et al., 2022)</ref>, aims to match the most similar claimstext fragments from social media or political debate scripts -to a corpus of verified claims. The corpus is used to find the most similar claims, which does not require direct linking, as is done in SV-Ident Task 2, because implicit links are inferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Why SV-Ident?</head><p>Today's search engines are the core elements of information access for social scientists. While search engines have seen many improvements in terms of keyword search and text understanding, they suffer from a limited capability of retrieving information from interconnected data sources, such as academic literature and research datasets. Nonetheless, they show outstanding performance on retrieving such documents individually. Current interlinking infrastructures typically only link research datasets to publications on the citation-level. Such systems do not yet consider fine-grained linking of publications to individual survey variables from research datasets. As demonstrated in the SV-Ident shared task, survey variables may be mentioned implicitly, which makes their manual or automatic identification non-trivial. Currently, social scientists have to manually identify such variables, which is timeconsuming. In addition to these limitations, search engines do not yet support queries specific to social science topics, concepts, or relations. Yet, keyword search, which is widely used, has many known problems (e.g., vocabulary mismatch or complex queries). As a result, social scientists are unable to access interlinked publications and research data. Thus, the re-use and reproducibility of research is limited.</p><p>SV-Ident, and more generally the VADIS project, plays an important role in filling the gap in the lack of infrastructure for social scientists <ref type="bibr">(Kartal et al., 2022)</ref>. SV-Ident aims to build automatic models for identifying survey variables in social science publications. This directly enables a more finegrained interlinking of publications and research datasets. More specifically, variables can be linked on the sentence-level, which allows new features to be developed. Within the VADIS project, we aim to develop variable-based automatic summarization, which will allow scientists to quickly get an overview of a publication with respect to the variables used. Furthermore, we plan to incorporate variable recommendation algorithms into the GESIS Search platform to enable scientists to find relevant variables outside the scope of variables they are already familiar with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This overview reports on the results of the SV-Ident 2022 shared task. We introduce two subtasks relevant for SV-Ident, namely, Variable Detection and Variable Disambiguation. We report on data, which is currently the largest of its kind, that was collected, annotated, and made publicly available for this challenge. Baseline as well as participants' systems are described and evaluated. We find that nearly all systems perform better on explicit variable mentions, opening up new directions of research. Finally, we contextualize the shared task into related work and highlight its importance within a broader context. Future work will further analyze the distinction between different variable mention types. In addition, multi-task learning could solve both tasks jointly or in combination with adjacent tasks. Co-reference resolution could be used to help disambiguate implicit variable mentions. Finally, evaluating systems on more diverse metrics, such as fairness or robustness, is critical for applied research. Furthermore, the item "You feel you are a citizen of the EU" was used in index building.</p><p>The independent variable was operationalized by a combination of several items: The participants were first asked the following question: "Could you tell me to what extent you?a) watch television on a TV set or via the Internet, b) listen to the radio, c) read the written press d) use the Internet?"</p><p>Later they were questioned: "Where do you get most of your news on European political matters? Firstly? And then?"</p><p>Possible answers were respectively: Television, the Press, Radio, and the Internet.</p><p>The respondents were asked, "Do you think that the [national] television present(s) the EU too positively, objectively, or too negatively?"</p><p>The same question was repeated regarding the radio and the press. Even the unintended, casual contact with news on the EU provided by the media fostered European identity.    <ref type="table">Table 9</ref>: Fine-grained results across types of variable mentions for Task 1. Sys = system, P = precision, R = recall, # = number of (positive) sentences.  <ref type="table" target="#tab_2">Table 10</ref>: Fine-grained results across types of variable mentions for annotator 1 for Task 1. Sys = system, P = precision, R = recall, # = number of (positive) sentences.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of explicit (blue) and implicit (red) variable mentions in sentences from a social science article (source: Ejaz et al. (2017)) mapped to survey variables. Lines with arrows show contextual dependence. Linked variables: QD2_3 and QD3_1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example sentence with provided metetadata and labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Example explicit (blue) and implicit (red) sentences from a social science article (source: Ejaz et al. (2017)) mapped to survey variables. Lines with arrows show contextual dependence. Linked variables: QD2_3, QD3_1, QE3_1, QE3_2, QE3_3, QE3_4, QE3_5, QE5b_1, QE5b_2, QE5b_3, QE5b_4, QE11_1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Common errors include sentence breaks (due to incorrect splitting of abbreviations, such as et al. or i.e.), page breaks (due to improper handling of footnotes), and missing spaces between</figDesc><table><row><cell>{'doc_id': '55534',</cell></row><row><cell>'is_variable': 1,</cell></row><row><cell>'lang': 'en',</cell></row><row><cell>'research_data': ['ZA5876'],</cell></row><row><cell>'sentence': 'The respondents were asked, "Do</cell></row><row><cell>you think that the [national]-</cell></row><row><cell>television present(s) the EU to</cell></row><row><cell>opositively, objectively, or too</cell></row><row><cell>negatively?"',</cell></row><row><cell>'uuid': '39238aee-2d44-4aa9-999f-eb597a1f0da9',</cell></row><row><cell>'variable': ['exploredata-ZA5876_Varqc3b',</cell></row><row><cell>'exploredata-ZA5876_Varqe11_1',</cell></row><row><cell>'exploredata-ZA5876_Varqc3a',</cell></row><row><cell>'exploredata-ZA5876_Varqe11_3']}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Total number of sentences in the SV-Ident shared task dataset per language for each dataset split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">: Inter-annotator agreement scores. We use</cell></row><row><cell cols="2">Cohen's Kappa for Task 1 (detection), while Krippen-</cell></row><row><cell cols="2">dorff's Alpha is used for Task 2 (disambiguation).</cell></row><row><cell>Type</cell><cell>Count</cell></row><row><cell>Total # variables (vocab. size)</cell><cell>27,365</cell></row><row><cell># annotated variables (tokens)</cell><cell>11,356</cell></row><row><cell># uniquely used Variables (types)</cell><cell>1,165</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Vocabulary size, variable types, and tokens in our SV-Ident dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Maximum and minimum number of related variables, annotated variables, and the ratio of annotated sentences for each document in English and German.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results for task 1 (detection).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Results for task 2 (disambiguation).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Comparison between the OpenMinTeD and SV-Ident datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Fine-grained results across documents for Task 1. Sys = system, P = precision, R = recall, # p/n = number of positive/negative sentences, Vars = total number of variables, Lang = language of the document. T5/GS 38.16 53.99 58.73 SSBert * 53.62 58.51 70.52 T5/GS 25.96 49.92 47.72 SSBert * 36.28 50.02 50.72 T5/GS 26.18 49.88 47.50 SSBert * 36.97 50.35 58.28</figDesc><table><row><cell>Type</cell><cell>System</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>#</cell></row><row><cell></cell><cell>Unk</cell><cell cols="3">59.45 66.96 57.49</cell><cell></cell></row><row><cell cols="5">A1+2exp S-A1+2imp Unk Unk S-A1+2mix Unk S-Average S-T5/GS 30.10 51.27 51.31 48.58 49.60 47.60 48.51 49.45 47.60 52.18 55.34 50.90</cell><cell>242 13 18</cell></row><row><cell></cell><cell cols="4">SSBert * 42.29 52.96 59.84</cell><cell></cell></row><row><cell></cell><cell>Unk</cell><cell cols="3">57.66 66.01 56.25</cell><cell></cell></row><row><cell>A1+2</cell><cell cols="4">S-T5/GS 39.08 53.80 57.34</cell><cell>273</cell></row><row><cell></cell><cell cols="4">SSBert * 54.70 58.88 68.92</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>T5/GS 41.11 54.31 57.15 SSBert * 57.05 60.17 68.59 T5/GS 44.86 55.42 57.19 SSBert * 50.77 53.41 54.99 T5/GS 36.33 53.91 60.79 SSBert * 49.40 56.00 68.23 Average Unk 50.85 55.29 51.76 S-T5/GS 40.77 54.55 58.38 SSBert * 52.40 56.53 63.94 T5/GS 60.51 63.65 62.29 SSBert * 69.49 69.58 69.45</figDesc><table><row><cell>Type</cell><cell>System</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>#</cell></row><row><cell></cell><cell>Unk</cell><cell cols="3">57.51 69.06 56.39</cell><cell></cell></row><row><cell cols="2">A1exp S-A1imp Unk Unk S-A1mix Unk S-A1 S-</cell><cell cols="3">46.00 47.67 49.37 49.06 49.15 49.53 42.31 65.88 52.89</cell><cell>339 403 166 908</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>T5/GS 58.40 63.71 63.03 SSBert * 65.45 65.39 66.13</figDesc><table><row><cell cols="4">Type A2exp System M@1 M@5 M@10 M@20 R-Prec System F1 P R Unk 47.38 70.97 54.03 S-T5/GS 11.55 14.12 15.20 16.72 16.55 BM25 * 2.98 3.30 3.49 3.57 3.52 Sparse * 0.03 0.24 0.51 0.86 1.34 Dense * 20.50 27.88 30.41 32.06 31.73 S-T5/GS 9.47 14.07 15.75 16.85 13.03 BM25 * 9.09 14.64 15.50 15.92 14.02 Sparse * 8.03 10.37 10.77 11.49 10.30 Dense * 11.74 18.41 19.16 19.96 15.68 S-T5/GS 6.65 11.87 14.47 16.08 15.94 BM25 * 4.92 6.96 7.04 7.24 7.93 Sparse * 0.00 1.16 1.63 2.01 1.91 Dense * 14.12 18.17 19.88 21.09 18.06 S-T5/GS 1.28 3.85 4.13 4.95 2.56 BM25 * 0.00 0.00 0.00 0.00 0.00 Sparse * 0.00 0.00 0.26 0.26 0.00 Dense * 7.69 15.15 16.71 17.12 7.69 Type System M@1 M@5 M@10 M@20 R-Prec # 864 # Vars Lang Year 160 209 de 2003 110 457 de 1999 51 477 de 1993 39 239 de 2002 # A1+2exp S-T5/GS 14.38 21.62 22.99 24.18 19.77 242 BM25 * 13.49 16.00 16.78 17.17 16.34 Sparse * 11.56 14.57 15.57 16.23 14.82 Dense * 24.90 32.43 34.31 35.11 30.29 A1+2imp S-T5/GS 0.00 5.85 9.62 14.21 11.70 13 BM25 * 0.00 4.72 7.79 9.82 8.46 Sparse * 5.13 18.07 25.97 27.73 24.90 Dense * 1.54 6.22 11.85 15.15 9.42 A1+2mix S-T5/GS 6.07 11.10 13.74 15.55 16.13 18 BM25 * 0.00 1.73 1.94 2.72 4.48 Sparse * 4.03 11.63 15.14 18.51 18.31 Dense * 8.05 14.60 15.84 18.66 20.09 Average S-T5/GS 6.81 12.85 15.45 17.98 15.87 BM25 * 4.50 7.48 8.84 9.91 9.76 Sparse * 6.90 14.75 18.89 20.82 19.34 Dense * 11.50 17.75 20.67 22.97 19.93 S-ID 16547 19944 21279 21357 S-T5/GS 12.92 19.91 21.52 22.95 19.03 21622 A1+2 BM25 * 11.68 14.25 15.12 15.63 14.97 273 Sparse * 10.61 14.53 16.12 17.05 15.66</cell></row><row><cell>Dense * 22.27 29.57</cell><cell>31.60</cell><cell>32.70</cell><cell>28.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Fine-grained results across types of variable mentions for Task 2. Sys = system, M = MAP, R-Prec = R-Precision, # = number of (positive) sentences.</figDesc><table><row><cell>Type</cell><cell cols="4">System M@1 M@5 M@10 M@20 R-Prec</cell><cell>#</cell></row><row><cell></cell><cell>S-T5/GS 14.14 20.47</cell><cell>21.87</cell><cell>22.77</cell><cell>16.97</cell><cell></cell></row><row><cell>A1exp</cell><cell>BM25 * 13.91 15.98 Sparse * 11.77 14.84</cell><cell>16.76 15.61</cell><cell>17.22 16.25</cell><cell>15.27 13.92</cell><cell>271</cell></row><row><cell></cell><cell>Dense * 25.28 31.48</cell><cell>32.60</cell><cell>33.30</cell><cell>28.05</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In the following, we use the terms survey variable and variable interchangeably.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Concepts that have been operationalized by variables are also treated as variables throughout this work.3 https://vadis-project.github.io/ sv-ident-sdp2022/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://vadis-project.github.io/ 5 https://www.gesis.org/ssoar/home 6 https://search.gesis.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We use the RapidFuzz library (https://github. com/maxbachmann/RapidFuzz) to match relevant variables given a search query.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Variable-dense documents are usually short in our dataset. 9 https://huggingface.co/datasets/ vadis/sv-ident</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://codalab.lisn.upsaclay.fr/ competitions/6400</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://huggingface.co/Sahajtomar/ German-semantic</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">https://github.com/microsoft/ MSMARCO-Passage-Ranking</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">https://pan.webis.de/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">provide results for explicit, implicit, and mixed mention types for each annotator individually as well as for the case when both annotators agreed on the labels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement The SV-Ident 2022 shared task is organized by the VADIS project (Kartal  et al., 2022). This work is supported by DFG project VADIS, grant numbers ZA 939/5-1, PO 1900/5-1, EC 477/7-1, KR 4895/3-1. We would like to also thank Jan Hendrik Bla? and Joudie Mekky for their contributions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This section contains a figure with example sentences mapped to variables and additional detailed evaluation results for both SV-Ident tasks. More specifically, <ref type="table">Tables 8 and 12 provide results for  each document, while Tables 9-11 and Tables 13-</ref></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">*SEM 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity</title>
		<meeting>the Main Conference and the Shared Task: Semantic Textual Similarity<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Making monolingual sentence embeddings multilingual using knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.365</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4512" to="4525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04510</idno>
		<title level="m">SsciBERT: A pre-trained language model for social science texts</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MedSTS: a resource for clinical semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Rastegar-Mojarad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-018-9431-1</idno>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="72" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumontier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Ijsbrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabrielle</forename><surname>Aalbersberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myles</forename><surname>Appleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; -Willem</forename><surname>Axton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><surname>Boiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silva</forename><surname>Bonino Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">E</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jildau</forename><surname>Bourne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">J</forename><surname>Bouwman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merc?</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Crosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Dillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Dumon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">T</forename><surname>Edmunds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Evelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandra</forename><surname>Finkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alasdair</forename><forename type="middle">J G</forename><surname>Gonzalez-Beltran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Goble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaap</forename><surname>Grethe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Heringa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>C &amp;apos;t Hoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Hooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">J</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryann</forename><forename type="middle">E</forename><surname>Lusher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martone</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.18</idno>
	</analytic>
	<monogr>
		<title level="m">The fair guiding principles for scientific data management and stewardship. Scientific Data</title>
		<meeting><address><addrLine>Arie Baak, Niklas Blomberg; Albert Mons, Abel L. Packer; Marco Roos, Rene van Schaik, Susanna-Assunta Sansone, Erik Schultes, Thierry Sengstag, Ted Slater, George Strawn, Morris A. Swertz, Mark Thompson, Johan van der Lei, Erik van Mulligen; Andra Waagmeester, Peter Wittenburg, Katherine Wolstencroft</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160018</biblScope>
		</imprint>
	</monogr>
	<note>Bengt Persson</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measurement of semantic textual similarity in clinical texts: comparison of transformer-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.2196/19735</idno>
	</analytic>
	<monogr>
		<title level="j">JMIR medical informatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">19735</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SPARTA: Efficient open-domain question answering via sparse transformer matching retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyusong</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.47</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="565" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining social science publications for survey variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mutschke</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2907</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on NLP and Computational Social Science</title>
		<meeting>the Second Workshop on NLP and Computational Social Science</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards a gold standard corpus for variable detection and linking in social science publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mutschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fine-grained results across types of variable mentions for annotator 2 for Task 1. Sys = system, P = precision, R = recall, # = number of (positive) sentences</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Fine-grained results across documents for Task 2. Sys = system, M = MAP, R-Prec = R-Precision, # = number of (positive) sentences, Vars = total number of variables</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Lang = language of the document.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Fine-grained results across types of variable mentions for annotator 1 for Task 2. Sys = system, M = MAP, R-Prec = R-Precision, # = number of (positive) sentences. Type System M@1 M@5 M@10 M@20 R-Prec #</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Fine-grained results across types of variable mentions for annotator 2 for Task 2. Sys = system, M = MAP, R-Prec = R-Precision, # = number of (positive) sentences</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
