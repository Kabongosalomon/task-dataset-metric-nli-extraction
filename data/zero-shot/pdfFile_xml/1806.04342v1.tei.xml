<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Focused Hierarchical RNNs for Conditional Sequence Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018">2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithms</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Polytechnique Montreal</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konrad?o?na</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithms</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Jagiellonian University</orgName>
								<address>
									<settlement>Cracow</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithms</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">AdeptMind Scholar</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithms</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">University of Montreal</orgName>
								<address>
									<addrLine>7 Senior Cifar Member</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">McGill Univer-sity</orgName>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithms</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="institution">HEC Montreal. Corre-spondence to: Nan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithms</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Polytechnique Montreal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Focused Hierarchical RNNs for Conditional Sequence Processing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 35 th International Conference on Machine Learning</title>
						<meeting>the 35 th International Conference on Machine Learning <address><addrLine>Stockholm, Sweden</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2018">2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a weight to each token independently. We present a mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed. We formulate this using a multilayer conditional sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inputs and controls information flow into the layer above. We train it using policy gradient methods. We evaluate this method on several types of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and probe the behavior of the gates in more controlled settings. We then evaluate this approach on large scale Question Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent improvements for both tasks over prior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as compared to the baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a weight to each token independently. We present a mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed. We formulate this using a multilayer conditional sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inputs and controls information flow into the layer above. We train it using policy gradient methods. We evaluate this method on several types of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and probe the behavior of the gates in more controlled settings. We then evaluate this approach on large scale Question Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent improvements for both tasks over prior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as compared to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent Neural Networks (RNNs) with attention are wildly used for many sequence modeling tasks, such as: image captioning <ref type="bibr" target="#b39">(Yao et al., 2016;</ref><ref type="bibr" target="#b26">Lu et al., 2017)</ref>, speech recognition <ref type="bibr" target="#b6">(Chan et al., 2016;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2016)</ref>, text summarization  and Question and Answering (QA) <ref type="bibr" target="#b18">(Kadlec et al., 2016)</ref>. The attention mechanism allows the model to look over the entire sequence and pick up the most relevant information. This not only allows the model to learn a dynamic summarization of the input sequence, it allows gradients to be passed directly to the earlier time-steps in the input sequence, which also helps with the vanishing and exploding gradient problem <ref type="bibr" target="#b16">(Hochreiter, 1991;</ref><ref type="bibr" target="#b3">Bengio et al., 1994;</ref><ref type="bibr" target="#b17">Hochreiter, 1998)</ref>.</p><p>Most of these models use a simple form of encoder with attention that is identical to the first one proposed <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>, where the attention looks over the entire encoded sequence and assigns a soft weight to each token. However, for more complex tasks we conjecture that more structured encoding mechanisms may help the attention to more effectively identify and selectively process relevant information within the input.</p><p>Imagine reading a Wikipedia article and trying to identify information that is relevant to answering a question before one knows what the question is. Now, compare this to the situation where the context or question is given before reading the article. It would be much easier to read over the article, identify relevant information, group items and selectively process relevant information based on its relevance to a given context or question.</p><p>Keeping this intuition in mind, we have developed a focused RNN encoder that is modeled by a multi-layer RNN that groups input sub-sequences based on gates that are controlled or conditioned on a question or input context. We dub the core part of the minimal form of this model a focused hierarchical encoder module. Our approach represents a general framework that applies to many sequence modeling tasks where the context or question can be beneficial to focus (attend) over the input sequence. Our focused encoder module examined here is based on a two layer LSTM where the upper layer is updated when a group of relevant tokens has been read. The boundaries of the arXiv:1806.04342v1 [stat.ML] 12 Jun 2018 group are computed using a discrete gating mechanism that takes as input the lower and upper level units as well as the context or question, and it is trained using policy gradient methods. We evaluate our model on several tasks of different levels of complexity. We began with toy tasks with limited vocabulary size, where we analyze the performance, the generalization ability as well as the gating and the attention mechanisms. We then move on to challenging large scale QA tasks such as MS MARCO <ref type="bibr" target="#b29">(Nguyen et al., 2016)</ref> and SearchQA <ref type="bibr" target="#b10">(Dunn et al., 2017)</ref>. Our model outperforms the baseline for both tasks. For the SearchQA task, it significantly outperforms recently proposed methods <ref type="bibr" target="#b4">(Buck et al., 2018)</ref>.</p><p>The key contributions of our work are the following:</p><p>? We explore the use of a conditional discrete stochastic boundary gating mechanism that helps the encoder to focus on parts relevant to the context, we use a soft attention to look over the relevant states.</p><p>? We use a reinforcement learning approach to learn the boundary gating mechanism.</p><p>? The elements above form the building blocks of our proposed focused hierarchical encoder module, we examine its properties with synthetic data experiments and we show the benefits of using it for QA tasks.</p><p>Our model takes an input sequence, a question or context sequence and generates an answer. It can be applied to any sequence tasks where the context or question is beneficial to modulating the processing of an input sequence.</p><p>2 Focused Hierarchical RNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture</head><p>Our model consists of: the focused hierarchical encoder (FHE), the context encoder and the decoder. Compared to a regular RNN with attention, we replace the encoder with a context-aware focused RNN encoder. The focused hierarchical encoder is modeled by a twolayer LSTM. The lower layer operates at the input token level, while the upper layer focuses on tokens relevant to the context. We train a conditional boundary gate to decide, depending on the context or question, whether it is useful to update the upper-level LSTM with a summary of the current set of tokens or not.</p><p>Lower-level Layer As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, FHE has two layers. Let P = (x 1 , . . . , x n ) be the sequence of input tokens, h t be the LSTM hidden state and c t be the LSTM cell state at time t. To make our model as generic as possible, the lower-level layer may be also augmented with other available information. The question for large QA tasks are non-trivial and hence we augment the lower-layer inputs with the question encoding at each step.</p><formula xml:id="formula_0">h l t , c l t = LSTM(x t , h l t?1 , c l t?1 )<label>(1)</label></formula><p>Conditional Boundary Gate For each token in the passage, the boundary gate decides if information at the current time step should be stored in the upper-level representation. We hypothesize that the question is essential in deciding how to represent the passage. To capture this dependency, the boundary gate computation is conditioned on the question embedding q. The question embedding can vary in complexity depending on the difficulty of the task. In the simplest setting, the question embedding is simply a retrieved vector.</p><p>The output of the boundary gate is a scalar b t ? (0, 1) that is taken to be the parameter of a Bernoulli distributionb t ? Bernoulli(b t ) that regulates the gate's opening at time step t. In the simplest case, the boundary gate forward pass is formulated as</p><formula xml:id="formula_1">b t = ?(w b LReLU(W b z t + b b )),<label>(2)</label></formula><p>where W b , b b and w b are trainable weights, LReLU(?) is a leaky ReLU activation, and z t is the input that varies and depends on the task. In our experiments we used the following input</p><formula xml:id="formula_2">z t = [q h l t , h l t , q],<label>(3)</label></formula><p>where is the element-wise product. Hence, we essentially use three groups of features: question representation multiplied with lower-layer hidden states, question, and lower-layer representations. These three groups are concatenated together and passed through an MLP to yield boundary gate decisions (i.e., open/close).</p><p>In a more complex task that has stronger dependency on the upper-level hidden states (see the following paragraph), these can also be used to augment the boundary gate input z t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Upper-level Layer</head><p>The upper-layer LSTM states (denoted h u t ) update only when the corresponding lower-layer boundary gate is open.</p><formula xml:id="formula_3">h u t ,c u t = LSTM(h l t , h u t?1 , c u t?1 ) (4) b t ? Bernoulli(b t ) (5) c u t =b tc u t + (1 ?b t )c u t?1 (6) h u t =b th u t + (1 ?b t )h u t?1 (7)</formula><p>Final Output The final output of FHE is a sequence of lower-level states H l = {h l 1 , . . . , h l n } and a sequence of upper-level states H u = {h u 1 , . . . , h u n }, where only k of them are unique and k = tb t , the number of times the boundary gates open over the length of the document. The upper-level states H u are typically the only ones being processed by the downstream modules. Hence, all downstream operations are performed faster than if they had to process H l (the effective size of H u is smaller than the size of H l ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>We train our model to maximize the log-likelihood of the answer (A) given the context (Q) and the passage (P ), using the output probabilities given by our answer decoder:</p><formula xml:id="formula_4">R = log p(A | Q, P ).<label>(8)</label></formula><p>Policy Gradient The discrete decisions involved in sampling the boundary variableb t make it impossible to use standard gradient back-propagation to learn the parameters of the boundary gate. We instead apply REINFORCE-style estimators <ref type="bibr" target="#b36">(Williams, 1992)</ref>. Denote by ? b the model policy over the binary vector of decisions b = {b 1 , . . . ,b n }. We need to take the derivative:</p><formula xml:id="formula_5">b ?? b (b)R b = E b?? b [? log ? b (b)R b ],<label>(9)</label></formula><p>where the reward R b can be formulated differently depending on the task. In our synthetic experiments (Section 4), we let R b = ?h u ?bt . For large scale natural language QA tasks (Section 5), we use R b = log p(A | Q, P, b). The aforementioned gradient can be approximated by sampling from the policy ? b and computing the corresponding terms.</p><p>Rewards We use the final reward R b for each decision in the sequence. Following previous work <ref type="bibr" target="#b36">(Williams, 1992;</ref><ref type="bibr" target="#b0">Andrychowicz &amp; Kurach, 2016)</ref>, we add an exploration term ?H(? b ) that prevents the policy from collapsing too soon during training. The ? is a hyperparameter to be set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sparsity Constraints</head><p>We add a constraint on the sparsity of the upper-level representations. We want the model to avoid grouping each token on its own and storing information at each step on the upper level (i.e., always opening the boundary gates). As a remedy we add a small penalty G(b) the model needs to pay for storing information at the upper level. In practice, we found the following formulation to work the best:</p><formula xml:id="formula_6">?G(b) = ?ReLU T t=1 b t ? ?T<label>(10)</label></formula><p>where ? &gt; 0 and ? ? [0, 1) are hyper-parameters and T is the input sequence length. Hence, ? is the strength of penalty and ? is the proportion of the time the gates could open without being penalized. Intuitively, we let a certain number of gates until a open threshold ?T without any penalty. Each open gate above the threshold is penalized. This is the same as constraining the policy to act within a certain region. One can skip ? (by setting ? = 0) and then the penalty is just ?b t applied at each time step:</p><formula xml:id="formula_7">?G(b) = ?ReLU T t=1 b t ? ?T ?=0 = T t=1 ?b t . (11)</formula><p>Note that hyper-parameters ? and ? directly affect the sparsity of upper-level representations that can be formally defined as the average value ofb t and will be called gate openness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>As we have discussed above, our focused hierarchical encoder is modeled by a hierarchical RNN controlled by gates conditioned on an input context or question. The idea of using hierarchical RNNs to model data in which long term dependencies must be captured was first explored in El Hihi &amp; <ref type="bibr" target="#b11">Bengio (1995)</ref>.</p><p>More recently, <ref type="bibr" target="#b21">Koutnik et al. (2014)</ref> propose a stacked RNN with a different updating rate for each layer, fixed a priori. <ref type="bibr" target="#b12">Graves (2016)</ref> propose a RNN that learns the number of timesteps to ponder on an input before moving onto the next input. <ref type="bibr" target="#b32">Srivastava et al. (2015)</ref> utilizes skip-connections between layers in a feedforward network for training a deep network. <ref type="bibr" target="#b38">Yao et al. (2015)</ref> uses a soft differential depth gate to connect the lower and the upper layers and <ref type="bibr" target="#b31">Sordoni et al. (2015)</ref> explore a multi-scale architecture where the hierarchy is fixed. Both of these uses a soft-differential gate compared to what can be seen as a hard gate in the <ref type="bibr" target="#b8">Chung et al. (2016)</ref>. The Skip-RNN <ref type="bibr" target="#b5">(Campos et al., 2017)</ref> learns an updating rate by predicting how many steps to skip in the future. Our document encoder bears similarities to the Hierarchical Multi-Scale LSTMs (HMSTMs) of <ref type="bibr" target="#b8">Chung et al. (2016)</ref>. The HML-STM extends a 3-layered LSTM to have multiple gates at each time step, which decide which of the LSTM layers should be updated, and has been applied to unconditional character-level language modelling. In contrast we learn a context-conditional sequence segmentation that only encodes relevant information to the context; this information is fed to an attention mechanism to help with identifying the most relevant information.</p><p>The upper states of our network can be considered as a memory focusing on relevant information that can be attended to at each step of the answer generation process. In particular, we use a soft-attention mechanism <ref type="bibr" target="#b1">(Bahdanau et al., 2015)</ref>, which has become ubiquitous in conditional language generation systems. <ref type="bibr" target="#b37">Yang et al. (2016)</ref> and <ref type="bibr" target="#b22">Kumar et al. (2016)</ref> use a layered, hierarchical attention in that they attend to both word and sentence level representations. We use similar ideas but we learn how to attend to information within the sequence structure rather than relying on a fixed strategy. Another form of structured encoding encoding mechanism would be the <ref type="bibr" target="#b27">Miller et al. (2016)</ref>, where the attention is separate into pairs of key and value. The key corresponds to the attention distribution and the value is used to encode the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Synthetic Experiments</head><p>We first study two synthetic tasks that allow us to analyze our proposed gating, attention mechanism and its generalization ability, and then in Section 5 we study the more complex tasks of natural language question and answering.</p><p>The synthetic tasks are the picking task and the Pixel-by-Pixel MNIST QA task. For the picking task, we analyze the gating mechanism and show how the model utilizes the question (context) to dynamically group the passage tokens and how the attention mechanism utilizes this information.</p><p>We also test the generalization ability our model following the setup in <ref type="bibr" target="#b13">(Graves et al., 2014)</ref>. For the Pixel-by-Pixel MNIST QA task, we show better accuracy with our FHE module over the baseline. The tasks are chosen due to the natural of the tasks. The gating mechanism for the picking task depends solely on the question, whereas the gating mechanism for the Pixel-by-Pixel MNIST QA task is independent of the question, but solely dependent on the data.</p><p>We compare the performance of our focused hierarchical encoder module to two baseline architectures: a 1-layer LSTM (LSTM1) and a 2-layer LSTM (LSTM2) 1 .</p><p>For the picking task, FHE utilizes less memory compare to LSTM2, as the baseline LSTM2 model needs to store and attend over all states, whereas FHE only needs to attend to unique elements of H u . For example, when gate openness is below 10%, the attention module for FHE only attends to than 10% of memory compared to a LSTM2 baseline model. Hyper-parameters All models (FHE, LSTM1 and LSTM2) for a certain task has the same number of hidden units (256 for picking task and 128 for Pixel-by-Pixel MNIST QA task). In FHE module we used ? = 0 hence, we did not use exploration term mentioned in Section 2.2. Instead, we used simpler idea that is sufficient in the synthetic experiments conducted -we add a small value to b t (0.01 for picking task and 0.1 for Pixel-by-Pixel MNIST QA task) to encourage exploration. The values of ? and ? depend on the task and are provided later. Learning rates used for all models are 0.0001 with the Adam optimizer <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Picking task</head><p>Given a sequence of randomly generated digits of length n, the goal of the picking task is to determine the most frequent digit within the first k digits 2 , where k ? n. Hence, the value of k is understood as the question. We study three tasks with input sequences of n ? {100, 200, 400} digits respectively. Sample points for the task are presented in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The input digits x i are one-hot encoded vectors (size 10) and the question embedding q is a vector retrieved from the lookup table (that is learnt during training) with n entries. To obtain the final sequence representation, soft attention (as in <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>) is applied on the upper-level states H u (for FHE and LSTM2) or the lower-level states H l (for LSTM1). Finally, the representation is concatenated with the question embedding and fed to one layer feed-forward neural network to produce the final prediction (i.e. probabilities for all 10 classes).</p><p>As introduced in Section 2.2, there are two hyperparameters (? and ?) that affect the sparsity of higher-level representations in FHE. We explore two approaches for determining their values.</p><p>One approach is to fix these hyper-parameters to a small value (for example ? = 0.1 and ? = 0.25) in the beginning of training, such that the gates can almost freely open. Once the desired accuracy has been reached, we enforce constraints on our hyper-parameters. This provides a level of control over the accuracy-sparsity tradeoff -we used this approach with the requirement of achieving a desired accuracy a. We tested FHE models with a ? {80%, 90%, 95%, 98%} and call them FHE80, FHE90, etc. The relationship between accuracy and gate openness is visualized in <ref type="figure">Figure 2</ref>.</p><p>Another approach is to set ? and ? to a fixed value from the start, so the gate openness of the model is more restricted right from the start. We find that the model performs better with fixed the hyper-parameters (the results for ? = 1 and ? = 10% are presented in <ref type="table" target="#tab_1">Table 2</ref> as FHE-fixed).</p><p>The results achieved for each model and sequence length are presented in <ref type="table" target="#tab_1">Table 2 and Table 3</ref>. For each setup at least two runs were performed and the difference in result between the pair were typically neglectable (&lt; 0.5%).</p><p>The picking task is useful to validate our gating mechanism. Once trained we can inspect the positions of the opened gates. <ref type="figure" target="#fig_1">Figure 3</ref> shows that our model learns to open gates around k'th step only and attend a single gate right after the k'th step. The lower-level LSTM is used to count the occurrences of the various digits. That information is then passed to the upper-level LSTM at a single gate. The atten- <ref type="figure">Figure 2</ref>. A relationship between accuracy and gate openness for picking task and sequence length n = 100. The best performance is achieved for gate openness around 10%.    tion mechanism then uses the information around the same time step to provide the mode (i.e. solve the task).</p><p>We tested the generalization ability of FHE. The models trained on short sequences (n = 200) were evaluated on longer sequences and k ? 200. The results are in <ref type="table">Table 4</ref>. The models can not be evaluated for k larger than maximum sequence length used during training because the question embeddings are parts of the models. FHE generalizes better to longer sequences by a wide margin. We believe this is due to the boundary gates being open for only the first k-steps, and the attention mechanism not attending over possibly misleading states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pixel-by-Pixel MNIST QA task</head><p>We adapt the Pixel-by-Pixel MNIST classification task <ref type="bibr">(Le-Cun et al., 1998;</ref><ref type="bibr" target="#b24">Le et al., 2015)</ref> to the question and answering setting. The passage encoder reads in MNIST digits one pixel at a time. The question asked is whether the image is a specific digit and the answer is either True or False. The data is balanced such that approximately half of the answers are True and the other half are False.</p><p>The LSTM2 reached an accuracy of 98.4% on the validation set, and FHE 3 outperformed the baseline by having an accuracy of 99.1%. <ref type="figure" target="#fig_2">Figure 4</ref> shows a visualization of the gates for the passage encoder learned by the model. The model learns to open the boundary gate almost always around the digit. We also found that for this particular task, the gates do not depend on the question. We hypothesize that this is because it is much easier for the passage encoder to learn to open the gates when there is a white pixel. In any case, these experiments illustrate how our proposed mechanism modulates gates based on input questions and features in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Large Scale Natural Language QA Tasks</head><p>Next, we explore the more complex task of natural language question answering. We study our approach using the MS MARCO and SearchQA datasets and tasks. These tasks are well-suited for our model since they both involve searching over a long input passage for answers to a question. Our results are that for the MS MARCO task, we achieved scores higher than the baseline models. Our model on SearchQA significantly outperforms very recent <ref type="bibr">3</ref> We used ? = 0.0001 and ? = 50%.</p><p>work <ref type="bibr" target="#b4">(Buck et al., 2018)</ref>. We also run ablation studies on the model for MS MARCO task to show the importance of each component in the model.</p><p>To obtain competitive results on these difficult questionanswering tasks we embed FHE with a modified version of both the question encoder and the answer decoder. All changes with respect to what was presented earlier are detailed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Question Encoder</head><p>Following recent work <ref type="bibr" target="#b9">(Cui et al., 2016;</ref><ref type="bibr" target="#b7">Chen et al., 2017)</ref>, we use a bidirectional LSTM that first reads the question and then performs self-attention to construct a vector representation of it. At the model-level, the question-encoder module outputs the vector q, which is then used as conditioning information in a FHE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Decoder</head><p>The answer decoder follows the standard decoding procedure in RNN with attention <ref type="bibr" target="#b1">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b14">Gulcehre et al., 2016)</ref>. The only difference is that the decoder looks over the upper-level hidden states h u t learned using a FHE conditioned on the question. The upper-level states H u provide an abstracted, dynamic representation of the passage. Because they receive lower-layer input only when the boundary gate is open, the resulting hidden states can be viewed as a sectional summary of the tokens between these "open" time-steps. The upper layer thus summarizes a passage in a smaller number of states. This can be beneficial because it enables the encoder LSTM to maintain information over a longer time-horizon, reduces the number of hidden states, and makes learning the subsequent attention softmax layer easier.</p><p>Pointer Softmax In order to predict the next answer word and to avoid large-vocabulary issues, we use the pointer softmax <ref type="bibr" target="#b14">(Gulcehre et al., 2016)</ref>. This method decomposes as two softmaxes: one places a distribution over a shortlist of words and the other places a distribution over words in the document. The softmax parameters are W o ? R |V |?D h and b o ? R|V |, where |V | is the size of the shortlist vocabulary 4 . A switching network enables the model to learn the mixture proportions over the two distributions. Switching variable z j determines how to interpolate between the indices in the document and the shortlist words. It is computed via an MLP. Let o j be the distribution of word in the shortlist, and ? j be the distribution over words in the document index. Then the pointer softmax Hyper-parameters All components of the model <ref type="bibr">(FHE, question encoder, decoder)</ref> in all natural language QA experiments uses 300 hidden units. FHE hyper-parameters were fixed (? = 0.001, ? = 0.5, ? = 50%). We use the Adam optimizer <ref type="bibr" target="#b20">(Kingma &amp; Welling, 2014)</ref> with a learning rate of 0.001.</p><formula xml:id="formula_8">P j ? R |V |+D is P j = [z j o j ; (1 ? z j )? j ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SearchQA Question and Answering Task</head><p>Search QA <ref type="bibr" target="#b10">(Dunn et al., 2017)</ref> is large scale QA dataset in the form of Question-Context-Answer. The questionanswer pairs are real Jeopardy! questions crawled from J!Archive. The contexts are text snippets retrieved by Google. It contains 140, 461 question-context-answer pairs. Each pair is coupled with a set of 49.6 ? 2.10 snippets, and each snippet is 37.3?11.7 tokens long on average. Answers are on average 1.47 ? 0.58 tokens long.</p><p>We use the same metric as reported in <ref type="bibr" target="#b4">Buck et al. (2018)</ref>, which are F1 scores for multi-word answers and Exact Match (EM) for single word answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA Results</head><p>Our model outperformed the recently proposed AQA model <ref type="bibr" target="#b4">(Buck et al., 2018)</ref> by 8 points in EM and more than 6 points in terms of F1 scores. See detailed results in <ref type="table" target="#tab_4">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">MS MARCO Question and Answering Task</head><p>The Microsoft Machine Reading Comprehension Dataset (MS MARCO) <ref type="bibr" target="#b29">(Nguyen et al., 2016)</ref> is one of the largest publicly available QA datasets. Each example in the dataset consists of a query, several context passages retrieved by the Bing search engine (ten per query on average), and several human generated answers (synthesized from the given contexts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Span-based vs Generative</head><p>Most of the recent question and answering models for MS MARCO are span-based 5 <ref type="bibr" target="#b35">(Weissenborn et al., 2017;</ref><ref type="bibr" target="#b33">Tan et al., 2017;</ref><ref type="bibr" target="#b30">Shen et al., 2017;</ref><ref type="bibr" target="#b34">Wang &amp; Jiang, 2016)</ref>. Span-based models are currently state of the art according to Bleu-1 and Rouge scores on the MS MARCO leaderboard, but are clearly limited as they cannot answer questions where the answer is not contained in the passage. In comparison, generative models, such as ours synthesize a novel answer for the given question. Generative models could learn a disentangled representation, and therefore generalize better. Our approach takes the first step towards closing the gap between generative models and span-based models.</p><p>We report the model performance using Bleu-1 and Rouge-L, which are the standard evaluation for MS MARCO task. <ref type="table">Table 7</ref> reports performance evaluated using the MS MARCO dataset. Specifically, we evaluate the quality of the generated answers for different models. Our model outperforms all competing methods in terms of test set Bleu-1 and Rouge-L. <ref type="table">Table 7</ref> shows also the results of learning our model without some of its key components. The ablation studies are evaluated on the validation set only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QA Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>The largest gain came from the elementwise-product between question and context. This result is to be expected, since it is difficult for the model to encode the appropriate information without direct knowledge of the question.</p><p>The pointer softmax is another important module of the model. The MS MARCO dataset contains many rare words, with around 90% of words appealing less than 20 times in the dataset. It is difficult for the model to generate words it has only seen a few times, and therefore the pointer-softmax provides a significant gain.</p><p>Our experiments also show the importance of learned boundaries. This results is supportive of our hypothesis that learned boundaries help with better document encoding, and therefore generates better answers.</p><p>Overall, the different components in our model are all needed to achieve the final score.</p><p>Model Exploration <ref type="figure" target="#fig_3">Figure 5</ref> reports the results of our attention mechanism on an example from the MS MARCO dataset. Our attention focuses on the relevant passage (the one that contains the answer) as well as other salient phrases of the passage given the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation</head><p>We performed a human evaluation study to compare answers generated by our model to answers generated by the LSTM1 baseline model in <ref type="table">Table 7</ref>.</p><p>We randomly selected 23 test-set questions and their corresponding answers. The order of the questions are randomized for each questionnaire. We collected a total of 690 responses (30 volunteers each given 23 examples) where volunteers were shown both answers side-by-side and were asked to pick their preferred answers. 63% of the time, volunteers preferred the answers generated from our model. Volunteers are students from our lab, and were not aware of which samples came from which model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced a focusing mechanism for encoder recurrent neural networks and evaluated our approach on the popular task of natural-language question answering. Our proposed model uses a discrete stochastic gating function that conditions on a vector representation of the question to control information flow from a word-level representation to a concept-level representation of the document. We trained the gates with policy gradient techniques. Using synthetic tasks we showed that the mechanism correctly learns when to open the gates given the context (question) and the input (passage). Further, experiments on MS MARCO and SearchQA -recent large-scale QA datasets -showed that our proposed model outperforms strong baselines and in the case of SearchQA outperforms prior work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A visualization of FHE. Lower-Layer LSTM processes each step. For each token, the boundary gate decides (based on the current lower-layer LSTM state and question embedding) if information should be stored in the upper-level representation. Higher-Layer LSTM states update only when the corresponding gate is open.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Gate openness (G) conditioned on the position asked (P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>A visualization of the gating mechanism learned using the Pixel-by-Pixel MNIST dataset. Red pixels indicate a gate opening and are overlayed on top of the digit which is white on a gray background. The digits are vectorized row-wise which explains why white pixels appear left of the red pixels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>We visualize the attention heat map of the passage encoder using an example from the MS MARCO validation set. Darker background color indicates higher attention (figure better seen on-screen). The question related to this passage is "walgreens store sales average" and the ground truth answer is "Approximately $15,000 per year.". Our model learns to attend to the passage containing the answer (first line). In addition, most of the other high-intensity passages are related to the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Sample points for picking task (sequence length n = 30). The first k digits are underlined and the target mode is bolded.</figDesc><table><row><cell>INPUT</cell><cell></cell><cell>TARGET</cell></row><row><cell>SEQUENCE</cell><cell>K</cell><cell>MODE</cell></row><row><cell>random examples</cell><cell></cell><cell></cell></row><row><cell cols="2">805602017082838371701316304473 10</cell><cell>0</cell></row><row><cell cols="2">638733290890396690255937986485 23</cell><cell>3</cell></row><row><cell cols="2">164551937579373896813981125982 26</cell><cell>1</cell></row><row><cell>malicious examples</cell><cell></cell><cell></cell></row><row><cell>666333666288882888819999999990</cell><cell>6</cell><cell>6</cell></row><row><cell cols="2">666333666288882888819999999990 10</cell><cell>6</cell></row><row><cell cols="2">666333666288882888819999999990 20</cell><cell>8</cell></row><row><cell cols="2">666333666288882888819999999990 30</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Accuracy (%) for picking task for LSTM1, LSTM2 and FHE-fixed. Our model and LSTM2 are on par with performing while LSTM1 is behind for longer input sequences.</figDesc><table><row><cell>LENGTH</cell><cell cols="4">LSTM1 LSTM2 FHE-FIXED</cell></row><row><cell>100</cell><cell>99.4</cell><cell>99.7</cell><cell>99.5</cell><cell></cell></row><row><cell>200</cell><cell>97.0</cell><cell>99.2</cell><cell>99.4</cell><cell></cell></row><row><cell>400</cell><cell>92.9</cell><cell>97.5</cell><cell>96.9</cell><cell></cell></row><row><cell cols="5">Table 3. Accuracy (%) for picking task for the models providing</cell></row><row><cell cols="5">a level of control over the accuracy-sparsity trade-off at a cost of</cell></row><row><cell cols="2">slightly lower performance.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>LENGTH</cell><cell cols="4">FHE80 FHE90 FHE95 FHE98</cell></row><row><cell>100</cell><cell>93.4</cell><cell>94.2</cell><cell>96.6</cell><cell>98.7</cell></row><row><cell>200</cell><cell>92.3</cell><cell>92.4</cell><cell>93.6</cell><cell>93.6</cell></row><row><cell>400</cell><cell>87.2</cell><cell>90.5</cell><cell>90.0</cell><cell>91.0</cell></row><row><cell cols="5">Table 4. Test accuracy (%) for longer sequence length for picking</cell></row><row><cell cols="4">task on model trained on sequence length n = 200.</cell><cell></cell></row><row><cell>LENGTH</cell><cell cols="4">LSTM1 LSTM2 FHE-FIXED</cell></row><row><cell>200</cell><cell>97.1</cell><cell>99.2</cell><cell>99.4</cell><cell></cell></row><row><cell>400</cell><cell>55.9</cell><cell>61.4</cell><cell>97.6</cell><cell></cell></row><row><cell>800</cell><cell>39.6</cell><cell>39.7</cell><cell>95.6</cell><cell></cell></row><row><cell>1600</cell><cell>29.5</cell><cell>28.6</cell><cell>93.3</cell><cell></cell></row><row><cell>10000</cell><cell>18.5</cell><cell>14.8</cell><cell>66.8</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Focus (F) is the average of final attention weight set for a given step. Hence, focus sums to one and it is always lower than gate openness (because our model attends only over unique states). Result showed for sequence length n = 200. The first four plots illustrate FHE model having 99.4% accuracy and 10% gate openness, while the last four are for FHE model having 97% accuracy but 5% gate openness.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Accuracy (%) for validation set of Pixel-by-Pixel MNIST QA task. Our model slightly outperform both LSTM1 and LSTM2.</figDesc><table><row><cell cols="3">LSTM1 LSTM2 FHE-FIXED</cell></row><row><cell>97.3</cell><cell>98.4</cell><cell>99.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>SearchQA results measured in F1 and Exact Match (EM) for validation and test set. Our model and AMANDA<ref type="bibr" target="#b23">(Kundu &amp; Ng, 2018</ref>) are on par with performing while the other models are behind.Table 7. MS MARCO results using BLEU-1 and Rouge-L evaluation. Our model clearly outperforms both standard memory networks and sequence-to-sequence models. In addition in both Bleu-1 and Rouge-L, we outperform strong baselines. Available results for the first three methods are taken from their respective papers (hence the not available ones). Ablation study results show that our model benefits the most from elementwise product between questions and context. The pointer softmax also gives a significant gain for performance.</figDesc><table><row><cell>MODELS</cell><cell cols="2">VALIDATION F1 EM</cell><cell cols="2">TEST F1 EM</cell></row><row><cell>TF-IDF MAX (DUNN ET AL., 2017)</cell><cell>-</cell><cell>13.0</cell><cell>-</cell><cell>12.7</cell></row><row><cell>ASR (DUNN ET AL., 2017)</cell><cell>24.1</cell><cell>43.9</cell><cell cols="2">22.8 41.3</cell></row><row><cell>AQA (BUCK ET AL., 2018)</cell><cell>47.7</cell><cell>40.5</cell><cell cols="2">45.6 38.7</cell></row><row><cell>HUMAN (DUNN ET AL., 2017)</cell><cell>-</cell><cell>-</cell><cell>43.9</cell><cell>-</cell></row><row><cell>LSTM1 + POINTER SOFTMAX</cell><cell>52.8</cell><cell>41.9</cell><cell cols="2">48.7 39.7</cell></row><row><cell>LSTM2 + POINTER SOFTMAX</cell><cell>55.3</cell><cell>44.7</cell><cell cols="2">51.9 41.7</cell></row><row><cell>OUR MODEL</cell><cell>56.7</cell><cell>49.6</cell><cell cols="2">53.4 46.8</cell></row><row><cell>CONCURRENT WORK</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AMANDA (KUNDU &amp; NG, 2018)</cell><cell>57.7</cell><cell>48.6</cell><cell cols="2">56.6 46.8</cell></row><row><cell>GENERATIVE MODELS</cell><cell></cell><cell cols="4">VALIDATION BLEU-1 ROUGE-L BLEU-1 ROUGE-L TEST</cell></row><row><cell>SEQ-TO-SEQ (NGUYEN ET AL., 2016)</cell><cell></cell><cell>-</cell><cell cols="2">8.9</cell><cell>-</cell><cell>-</cell></row><row><cell>MEMORY NETWORK (NGUYEN ET AL., 2016)</cell><cell></cell><cell>-</cell><cell cols="2">11.9</cell><cell>-</cell><cell>-</cell></row><row><cell>ATTENTION MODEL (HIGGINS &amp; NHO, 2017)</cell><cell></cell><cell>9.3</cell><cell cols="2">12.8</cell><cell>-</cell><cell>-</cell></row><row><cell>LSTM1 + POINTER SOFTMAX</cell><cell></cell><cell>24.8</cell><cell cols="2">26.5</cell><cell>28</cell><cell>28</cell></row><row><cell>LSTM2 + POINTER SOFTMAX</cell><cell></cell><cell>24.3</cell><cell cols="2">23.3</cell><cell>27</cell><cell>28</cell></row><row><cell>OUR MODEL</cell><cell></cell><cell>27.3</cell><cell cols="2">26.7</cell><cell>30</cell><cell>30</cell></row><row><cell>ABLATION STUDY</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">OUR MODEL -DOT-PRODUCT BETWEEN QUESTION AND CONTEXT</cell><cell>18.5</cell><cell cols="2">19.3</cell><cell>-</cell><cell>-</cell></row><row><cell>OUR MODEL -POINTER SOFTMAX</cell><cell></cell><cell>20.5</cell><cell cols="2">18.7</cell><cell>-</cell><cell>-</cell></row><row><cell>OUR MODEL -LEARNED BOUNDARIES</cell><cell></cell><cell>23.5</cell><cell>24</cell><cell></cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that baseline models are equivalent to FHE with the boundary gate fully open for LSTM2 (bt = 1 for each t) or always closed for LSTM1 (bt = 0 for each t).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">If there is more than one mode, the largest value digit should be picked.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use a short-list of 100 or 10,000 most frequent words for SearchQA or MS MARCO tasks, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For MS MARCO, span-based models are trained using "gold-spans", obtained by a preprocessing step which selects the passage in the document maximizing the Bleu-1 score with the answer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Anirudh Goyal for help in shaping this idea. The authors would also like to Alex Lamb, Olexa Bilaniuk, Julian Serban and Sandeep Subramanian for their helpful feedback and discussions. We also thank IBM, Google, and the Canada Research Chairs program for funding this research and Compute Canada for providing access to computing resources. Nan Ke would like to thank Microsoft Research for helping to fund this project as a part of the research internship at Microsoft Research in Montreal. Konrad?o?na would like to acknowledge the support of Applica.ai project co-financed by the European Regional Development Fund (POIR.01.01.01-00-0144/17-00). Zhouhan would like to thank AdeptMind by helping to fund a part of this research through his Adept-Mind scholarship. The authors would also like to express debt of gratitude towards those who contributed to theano over the years (as it is no longer maintained), making it such a great tool.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning efficient algorithms with hierarchical attentive memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03218</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<idno>1045-9227</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ask the right questions: Active question reformulation with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gir?-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06834</idno>
		<title level="m">Skip rnn: Learning to skip state updates in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<title level="m">Reading wikipedia to answer open-domain questions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01704</idno>
		<title level="m">Hierarchical multiscale recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attention-over-attention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Searchqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">A new q&amp;a dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>El Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Neural Information Processing Systems</title>
		<meeting>the 8th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="493" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08148</idno>
		<title level="m">Pointing the unknown words</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">LSTM encoder-decoder architecture with attention mechanism for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen Netzen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Institut f?r Informatik, Lehrstuhl Prof. Brauer, Technische Universit?t M?nchen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Diploma thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="DOI">10.1142/S0218488598000094</idno>
		<idno>0218-4885. doi: 10.1142/ S0218488598000094</idno>
		<ptr target="http://dx.doi.org/10.1142/S0218488598000094" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Uncertain. Fuzziness Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleindienst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A clockwork rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A question-focused multi-factor attention network for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08290</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1504.00941</idno>
		<ptr target="http://arxiv.org/abs/1504.00941" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-H</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marco</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m">A human generated machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reasonet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1047" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A hierarchical recurrent encoder-decoder for generative context-aware query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grue Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="553" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04815</idno>
		<title level="m">From answer extraction to answer generation for machine reading comprehension</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Making neural qa as simple as possible but not simpler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seiffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<title level="m">Depth-gated recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Boosting image captioning with attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenReview</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
