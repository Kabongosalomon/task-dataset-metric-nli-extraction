<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Study of Training End-to-End Vision-and-Language Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
							<email>zdou@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhgan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<email>lijuanw@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
							<email>chezhu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<email>luyuan@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
							<email>nzeng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Study of Training End-to-End Vision-and-Language Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-and-language (VL) pre-training has proven to be highly effective on various VL downstream tasks. While recent work has shown that fully transformer-based VL models can be more efficient than previous region-featurebased methods, their performance on downstream tasks often degrades significantly. In this paper, we present METER, a Multimodal End-to-end TransformER framework, through which we investigate how to design and pre-train a fully transformer-based VL model in an endto-end manner. Specifically, we dissect the model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa, De-BERTa), multimodal fusion module (e.g., merged attention vs. co-attention), architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked image modeling). We conduct comprehensive experiments and provide insights on how to train a performant VL transformer. METER achieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images for pre-training, surpassing the state-of-the-art region-featurebased model by 1.04%, and outperforming the previous best fully transformer-based model by 1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy of 80.54%. Code and pre-trained models are released at https://github.com/zdou0830/METER.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision-and-language (VL) tasks, such as visual question answering (VQA) <ref type="bibr" target="#b0">[1]</ref> and image-text retrieval <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b40">40]</ref>, require an AI system to comprehend both the input image and text contents. Vision-and-language pre-training (VLP) has now become the de facto practice to tackle these tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b51">51]</ref>. Specifically, large amounts of image-caption pairs are fed into a model that consumes both images and texts to pretrain representations that contain rich * Work was done when the author interned at Microsoft.  <ref type="figure">Figure 1</ref>. An overview of the proposed METER framework. We systematically investigate how to train a performant vision-andlanguage transformer, and dissect the model designs along multiple dimensions: vision encoder, text encoder, multimodal fusion module, architectural design (encoder-only vs. encoder-decoder), and pre-training objectives. multimodal knowledge and is helpful for downstream tasks. Transformers <ref type="bibr" target="#b56">[55]</ref> are prevalent in natural language processing and have recently shown promising performance in computer vision <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">36]</ref>. While almost all the existing VLP models adopt transformers for their multimodal fusion module, most of them <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b51">51]</ref> use pre-trained object detectors (e.g., Faster RCNN <ref type="bibr" target="#b44">[44]</ref>) on the vision side to extract region features from images. This can lead to several problems: first, the object detectors are not perfect, but are usually kept frozen during VLP, which limits the capacity of the VLP models; second, it is time-consuming to extract region features <ref type="bibr" target="#b24">[25]</ref>. On the other hand, vision transformers (ViTs) have been an increasingly active research topic in computer vision and have shown great potential in vision feature extraction. Therefore, a natural question arises: can we train a fully transformer-based VLP model with ViTs as the image encoder?</p><p>Recent works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b61">60</ref>] that tried to adopt vision transformers have not shown satisfactory performance and typically underperform state-of-the-art region-feature-based VLP models (e.g., VinVL <ref type="bibr" target="#b66">[65]</ref>). To close the performance gap, we present METER, a Multimodal End-to-end TransformER framework, through which we thoroughly investigate how to design and pre-train a fully transformer- based VLP model in an end-to-end manner. Specifically, as shown in <ref type="figure">Figure 1</ref>, we dissect the model designs along multiple dimensions, including vision encoders (e.g., CLIP-ViT <ref type="bibr" target="#b41">[41]</ref>, Swin transformer <ref type="bibr" target="#b36">[36]</ref>), text encoders (e.g., RoBERTa <ref type="bibr" target="#b35">[35]</ref>, DeBERTa <ref type="bibr" target="#b15">[16]</ref>), multimodal fusion module (e.g., merged attention vs. co-attention), architectural design (e.g., encoder-only vs. encoder-decoder), and pretraining objectives (e.g., masked image modeling <ref type="bibr" target="#b1">[2]</ref>).</p><p>We perform the investigation by pre-training models under METER on four commonly used image-caption datasets: COCO <ref type="bibr" target="#b33">[33]</ref>, Conceptual Captions <ref type="bibr" target="#b47">[47]</ref>, SBU Captions <ref type="bibr" target="#b39">[39]</ref>, and Visual Genome <ref type="bibr" target="#b25">[26]</ref>. We test them on visual question answering <ref type="bibr" target="#b0">[1]</ref>, visual reasoning <ref type="bibr" target="#b50">[50]</ref>, image-text retrieval <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b40">40]</ref>, and visual entailment <ref type="bibr" target="#b60">[59]</ref> tasks. Through extensive analyses, we summarize our findings as follows:</p><p>? Vision transformer (ViT) plays a more vital role than language transformer in VLP, and the performance of transformers on pure vision or language tasks is not a good indicator for its performance on VL tasks. ? The inclusion of cross-attention benefits multimodal fusion, which results in better downstream performance than using self-attention alone. ? Under a fair comparison setup, the encoder-only VLP model performs better than the encoder-decoder model for VQA and zero-shot image-text retrieval tasks. ? Adding the masked image modeling loss in VLP will not improve downstream task performance in our settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Glossary of VLP Models</head><p>In this section, we provide an overview of representative VLP models, and divide them into three categories based on how they encode images, as summarized in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>OD-based Region Features. Most previous work use pretrained object detectors (ODs) to extract visual features. Among them, ViLBERT <ref type="bibr" target="#b38">[38]</ref> and LXMERT <ref type="bibr" target="#b51">[51]</ref> use coattention for multimodal fusion, where two transformers are applied independently to region and text features, and another transformer fuses the representations of the two modalities in a later stage. On the other hand, Visual-BERT <ref type="bibr" target="#b30">[30]</ref>, VL-BERT <ref type="bibr" target="#b49">[49]</ref>, and UNITER <ref type="bibr" target="#b5">[6]</ref> use a merged attention fusion module that feeds both region and text features together into a single transformer. OSCAR <ref type="bibr" target="#b32">[32]</ref> and VinVL <ref type="bibr" target="#b66">[65]</ref> feed additional image tags into the transformer model, and demonstrate state-of-the-art performance across VL tasks. However, extracting region features can be timeconsuming, and the pre-trained ODs are usually frozen during pre-training, which limits the capacity of VLP models.</p><p>CNN-based Grid Features. To tackle the above two issues, researchers have tried different ways to pre-train VL models in an end-to-end fashion. Among them, Pixel-BERT <ref type="bibr" target="#b19">[20]</ref> and CLIP-ViL <ref type="bibr" target="#b48">[48]</ref> propose to feed grid features from convolutional neural networks (CNNs) and text directly into a transformer. SOHO <ref type="bibr" target="#b18">[19]</ref> proposes to to first discretize grid features using a learned vision dictionary, then feed the discretized features into their cross-modal module. While using grid features directly can be efficient, inconsistent optimizers are typically used for CNN and transformer. For example, PixelBERT <ref type="bibr" target="#b19">[20]</ref> and CLIP-ViL <ref type="bibr" target="#b48">[48]</ref> use AdamW <ref type="bibr" target="#b37">[37]</ref> for transformer and SGD for CNN. Recent work on vision transformers (ViTs) has also shown that CNN can achieve slightly worse accuracy/FLOPs trade-offs than their ViT counterparts <ref type="bibr" target="#b36">[36]</ref>, motivating researchers to develop ViT-based VLP models.</p><p>ViT-based Patch Features. ViLT <ref type="bibr" target="#b24">[25]</ref> directly feeds image patch features and text token embeddings into a pretrained ViT model, and fine-tunes the model on imagecaption datasets. More recently, visual parsing <ref type="bibr" target="#b61">[60]</ref> and ALBEF <ref type="bibr" target="#b29">[29]</ref> use ViT as the image encoder and design different pre-training objectives for ViT-based VLP models. However, all these models lag behind the state-of-the-art performance on downstream tasks such as visual question answering. In this paper, we investigate how to pre-train a ViT-based model in an end-to-end manner that closes the performance gap while maintaining fast inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The METER Framework</head><p>Based on the previous work, we identify several important modules in VLP models as in <ref type="figure">Figure 1</ref>. In this section, we first illustrate our METER framework, then our default settings, which paves the way for our analyses hereinafter.</p><p>Overview. Given a text sentence l and an image v, a VLP model first extracts both text features l = l 1 , ? ? ? , l N and visual features v = v 1 , ? ? ? , v M via a text encoder and a vision encoder. The text and visual features are then fed into a multimodal fusion module to produce cross-modal representations, which are then optionally fed into a decoder before generating the final outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>Vision Encoder. In this paper, we focus on patch features, and study the use of vision transformers (ViTs) <ref type="bibr" target="#b11">[12]</ref> for vision encoder. Specifically, in ViT, an image is first segmented into patches, and then the patches are fed into a transformer model. ViT has become a popular research topic recently <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b65">64]</ref>, and has been introduced into VLP <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b61">60]</ref>. However, all these models only achieve inferior performance compared to state-of-the-art region-feature-based models (e.g., VinVL <ref type="bibr" target="#b66">[65]</ref>). Also, different pre-trained ViTs are used, lacking a systematic study of which ViTs are the best for VLP. In this work, we compare the original ViT <ref type="bibr" target="#b11">[12]</ref>, DeiT <ref type="bibr" target="#b52">[52]</ref>, Distilled-DeiT <ref type="bibr" target="#b52">[52]</ref>, CaiT <ref type="bibr" target="#b53">[53]</ref>, VOLO <ref type="bibr" target="#b65">[64]</ref>, BEiT <ref type="bibr" target="#b1">[2]</ref>, Swin Transformer <ref type="bibr" target="#b36">[36]</ref> and CLIP-ViT <ref type="bibr" target="#b41">[41]</ref>, to provide a comprehensive analysis on the role of vision transformers.</p><p>Text Encoder. Following BERT <ref type="bibr" target="#b10">[11]</ref> and RoBERTa <ref type="bibr" target="#b35">[35]</ref>, VLP models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b51">51]</ref> first segment the input sentence into a sequence of subwords <ref type="bibr" target="#b46">[46]</ref>, then insert two special tokens at the beginning and the end of the sentence to generate the input text sequence. After we obtain the text</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attn</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Attn</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feedforward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mx</head><p>Visual Feature Text Feature</p><formula xml:id="formula_0">Q V K V V V Q V K L V L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attn</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Attn</head><p>Feedforward</p><formula xml:id="formula_1">Q L K V V V Q L K L V L</formula><p>(a) Co-attention model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attn</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feedforward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mx</head><p>Visual Feature Text Feature embeddings, existing works either feed them directly to the multimodal fusion module <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">30]</ref>, or to several text-specific layers <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b51">51]</ref> before the fusion. For the former, the fusion module is typically initialized with BERT, and the role of text encoding and multimodal fusion is therefore entangled and absorbed in a single BERT model. Here, we aim to disentangle the two modules, and use a text encoder first before sending the features into the fusion module. Language model (LM) pre-training has demonstrated impressive performance across tasks and different pretrained LMs have been proposed; however, most VLP models still only use BERT for initialization <ref type="bibr" target="#b5">[6]</ref>. In this work, we study the use of BERT <ref type="bibr" target="#b10">[11]</ref>, RoBERTa <ref type="bibr" target="#b35">[35]</ref>, ELEC-TRA <ref type="bibr" target="#b7">[8]</ref>, ALBERT <ref type="bibr" target="#b27">[28]</ref>, and DeBERTa <ref type="bibr" target="#b15">[16]</ref> for text encoding. Besides, we also experiment on only using a simple word embedding look-up layer initialized with the BERT embedding layer as used in many previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b66">65]</ref>.</p><formula xml:id="formula_2">Q V K V/L V V/L Q L K V/L V V/L (b) Merged attention model.</formula><p>Multimodal Fusion. We study two types of fusion modules, namely, merged attention and co-attention <ref type="bibr" target="#b16">[17]</ref>, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. In the merged attention module, the text and visual features are simply concatenated together, then fed into a single transformer block. In the co-attention module, on the other hand, the text and visual features are fed into different transformer blocks independently, and techniques such as cross-attention are used to enable crossmodal interaction. For region-based VLP models, as shown in <ref type="bibr" target="#b3">[4]</ref>, the merged attention and co-attention models can achieve comparable performance. Yet, the merged attention module is more parameter-efficient, as the same set of parameters are used for both modalities. Since end-toend VLP models are becoming increasingly popular, in this work, we re-examine the impact of both types of fusion modules in our new context.</p><p>Encoder-Only vs. Encoder-Decoder. Many VLP models such as VisualBERT <ref type="bibr" target="#b30">[30]</ref> adopt the encoder-only architecture, where the cross-modal representations are directly fed into an output layer to generate the final outputs. Recently, VL-T5 <ref type="bibr" target="#b6">[7]</ref> and SimVLM <ref type="bibr" target="#b59">[58]</ref>, on the other hand, advocate the use of a transformer encoder-decoder architecture, where the cross-modal representations are first fed into a decoder and then to an output layer. In their models,  the decoder attends to both the encoder representations and the previously generated tokens, producing the outputs autoregressively. <ref type="figure" target="#fig_1">Figure 3</ref> shows the difference between them when performing the masked language modeling task. For the encoder-decoder model, when performing classification tasks such as VQA, we feed the text inputs into its encoder and feed a classification token into the decoder, and the decoder then generates the output class accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pre-training Objectives</head><p>Now, we introduce how we pre-train our models. Specifically, we will first briefly review masked language modeling and image-text matching, which have been used extensively in almost every VLP model. Then, we will shift our focus to how we can design and explore interesting masked image modeling tasks.</p><p>Masked Language Modeling. The masked language modeling (MLM) objective is first introduced in pure language pre-training <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">35]</ref>. In VLP, MLM with images has also proven to be useful. Specifically, given an image-caption pair, we randomly mask some of the input tokens, and the model is trained to reconstruct the original tokens given the masked tokens l mask and its corresponding visual input v.</p><p>Image-Text Matching. In image-text matching, the model is given a batch of matched or mismatched image-caption pairs, and the model needs to identify which images and captions correspond to each other. Most VLP models treat image-text matching as a binary classification problem. Specifically, a special token (e.g., <ref type="bibr">[CLS]</ref>) is inserted at the beginning of the input sentence, and it tries to learn a global cross-modal representation. We then feed the model with either a matched or mismatched image-caption pair v, l with equal probability, and a classifier is added on top of the [CLS] token to predict a binary label y, indicating if the sampled image-caption pair is a match.</p><p>Masked Image Modeling. Similar to the MLM objective, researchers have tried masked image modeling (MIM) on the vision side. For example, many previous work, such as LXMERT <ref type="bibr" target="#b51">[51]</ref> and UNITER <ref type="bibr" target="#b5">[6]</ref>, mask some of the input regions, and the model is trained to regress the original a man hitting a tennis ball with a racquet.  region features. Formally, given a sequence of visual fea-</p><formula xml:id="formula_3">tures v = v 1 , ? ? ? , v M ,</formula><p>where v i is typically a region feature, we randomly mask some of the visual features, and the model outputs the reconstructed visual features o v given the rest of the visual features and the unmasked tokens t, and regression aims to minimize the mean squared error loss. Researchers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b51">51]</ref> have also tried to first generate object label for each region using a pre-trained object detector, which can contain high-level semantic information, and the model is trained to predict the object labels for the masked regions instead of the original region features.</p><p>Notably, recent state-of-the-art models (e.g., AL-BEF <ref type="bibr" target="#b29">[29]</ref>, VinVL <ref type="bibr" target="#b66">[65]</ref>) do not apply MIM during VLP. In addition, in ViLT <ref type="bibr" target="#b24">[25]</ref>, the authors also demonstrate that masked patch regression is not helpful in their setting. These results make it questionable whether MIM is truly effective for VLP models. To further investigate this, we treat masked image modeling as a masked patch classification task, and propose two ways of implementing the idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Masked Patch Classification with In-batch Negatives.</head><p>By imitating MLM which uses a text vocabulary, we first propose to let the model reconstruct input patches by using a dynamically constructed vocabulary constructed with inbatch negatives. Concretely, at each training step, we sample a batch of image-caption pairs { v k , l k } B k=1 , where B is the batch size. We treat all the patches in {v k } B k=1 as candidate patches, and for each masked patch, we mask 15% of the input patches, and the model needs to select the original patch within this candidate set. Denoting the original patch representations and our model's output representations of</p><formula xml:id="formula_4">{v k } B k=1 as {c(v k )} B k=1 and {h(v k )} B k=1</formula><p>, respectively, we can represent the output probability at position i for the k-th instance as:</p><formula xml:id="formula_5">p(v k i |[v k,mask ; l k ]) = e h(v k i ) T c(v k i ) j,k e h(v k i ) T c(v k j )</formula><p>.</p><p>(1)</p><p>The model is trained to maximize its probability similar to noise contrastive estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>2) Masked Patch Classification with Discrete Code. Inspired by BEiT <ref type="bibr" target="#b1">[2]</ref>, we also propose to obtain discrete repre-sentations of the input patches, and the model is then trained to reconstruct the discrete tokens. Specifically, we first use the VQ-VAE <ref type="bibr" target="#b54">[54]</ref> model in DALL-E <ref type="bibr" target="#b43">[43]</ref> to tokenize each image into a sequence of discrete tokens. We resize each image so that the number of patches is equal to the number of tokens, and thus each patch corresponds to a discrete token. Then, we randomly mask 15% of the patches and feed the masked image patches to the model as before, but now the model is trained to predict the discrete tokens instead of the masked patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Our Default Settings for METER</head><p>There are many different model designs under METER, and we detail our default settings in this part.</p><p>Model Architecture. The default setting of model architecture is shown in <ref type="figure" target="#fig_0">Figure 2a</ref>. In the bottom part, there are one pre-trained visual encoder and one pre-trained text encoder. On top of each encoder, we stack M = 6 transformer encoding layers, with each layer consisting of one selfattention block, one cross-attention block, and one feedforward network block. Unless otherwise stated, the hidden size is set to 768, and the number of heads is set to 12 for the top layers. Note that there is no decoder and no parameter sharing between the vision and language branches.</p><p>Pre-training Objectives. Unless otherwise stated, we pretrain the models with masked language modeling (MLM) and image-text matching (ITM) only. For MLM, we mask 15% of the input text tokens, and the model tries to reconstruct the original tokens. For ITM, we feed the model with either matched or mismatched image-caption pairs with equal probability, and the model needs to identify whether the input is a match.</p><p>Pre-training Datasets. Following previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">29]</ref>, we pre-train models on four commonly used datasets, including COCO <ref type="bibr" target="#b33">[33]</ref>, Conceptual Captions <ref type="bibr" target="#b47">[47]</ref>, SBU Captions <ref type="bibr" target="#b39">[39]</ref>, and Visual Genome <ref type="bibr" target="#b25">[26]</ref>. The statistics of these datasets is shown in Appendix. The combined training data consists of about 4M images in total.</p><p>Downstream Tasks. For ablation and analysis, we mainly focus on VQAv2 <ref type="bibr" target="#b0">[1]</ref>, arguably the most popular dataset for VLP evaluation. We also test on Flickr30k zero-shot imagetext retrieval to remove any confounders that may be introduced during finetuning <ref type="bibr" target="#b16">[17]</ref>. For VQAv2, we follow the standard practice <ref type="bibr" target="#b5">[6]</ref> to train the models with both training and validation data, and test the models on the test-dev set. For Flickr30k, we follow the standard splits.</p><p>For comparison with state of the arts, we also evaluate models on visual reasoning (NLVR 2 <ref type="bibr" target="#b50">[50]</ref>), visual entailment (SNLI-VE <ref type="bibr" target="#b60">[59]</ref>), and image-text retrieval (COCO <ref type="bibr" target="#b33">[33]</ref> and Flickr30k <ref type="bibr" target="#b40">[40]</ref>) tasks. For retrieval tasks, we evaluate models in both zero-shot and finetuning settings.  Implementation Details. We pre-train our models using AdamW <ref type="bibr" target="#b37">[37]</ref> for 100k steps. The learning rates for the bottom and top layers are set to 1e-5 and 5e-5 respectively during pre-training. The warm-up ratio is set to 10%, and the learning rate is linearly decayed to 0 after 10% of the total training steps. We use center-crop to resize each image into the size of 224?224 or 384?384 depending on the adopted vision transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we provide comprehensive analysis of each individual module design. Specifically, (i) we study the impact of vision and language encoders in Section 4.1, (ii) we perform analysis on multimodal fusion designs in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Impact of Vision and Language Encoders</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Explorations without VLP</head><p>Since pre-training is time-consuming, we first perform an exploration study by comparing different text and visual en-  coders without VLP for efficiency. Concretely, we initialize the bottom layers with specific pre-trained vision and text encoders, and randomly initialize the top layers. Then, we directly finetune the models on three tasks, including VQAv2, SNLI-VE, and Flickr30k retrieval. The learning rates for the bottom and top layers are set to 1e-5 and 1e-4, and the number of training epochs is set to 10 for all the tasks. We choose CLIP-ViT-224/32 <ref type="bibr" target="#b41">[41]</ref> and RoBERTa <ref type="bibr" target="#b35">[35]</ref> as the default encoders. Here, N and M in ViT-N/M denote image resolution and patch size, respectively.</p><p>Impact of Text Encoders. As shown in <ref type="table" target="#tab_4">Table 2</ref>, there are no significant differences between the model performance of different text encoders. RoBERTa seems to achieve the most robust performance in this setting. Also, as can be seen from the Emb-only results, it is necessary to have a pre-trained encoder because otherwise the downstream task performance will be degraded.</p><p>Impact of Vision Encoders. As summarized in <ref type="table" target="#tab_5">Table 3</ref>, both CLIP-ViT-224/16 and Swin Transformer can achieve decent performance in this setting. Notably, Swin Transformer can achieve an VQA score of 72.38 on the test-dev set without any VLP, which is already comparable to some VLP models after pre-training.</p><p>Conclusion. If we directly finetune the models on downstream tasks without any VLP, RoBERTa and Swin Transformer or CLIP-ViT perform the best. While models such as DeBERTa and BEiT can achieve better performance than the two models on pure language or vision tasks such as MNLI <ref type="bibr" target="#b57">[56]</ref> or ImageNet classification <ref type="bibr" target="#b9">[10]</ref>, that does not necessarily indicate that they are more suitable for VL tasks.  Useful Tricks. In experiments, we found two tricks for ViT-based VLP models that can greatly boost the performance. First, it is better to use a larger learning rate for the randomly initialized parameters than parameters initialized with pre-trained models, which is also found useful in some other NLP tasks <ref type="bibr" target="#b34">[34]</ref>. As shown in <ref type="table" target="#tab_9">Table 5</ref>, using the same learning rate for all parts of the model will lead to degraded performance, possibly because the pre-trained parameters already contain certain amounts of knowledge about vision and language, and finetuning them aggressively can result in the loss of these valuable information.</p><p>Second, similar to several previous work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b65">64]</ref>, we find that increasing the image resolution during finetuning can improve the model performance by a large margin, especially when the ratio of image resolution to patch size is low. <ref type="figure">Figure 5</ref> shows that increasing the image resolution from 224 to 576 can improve the VQA score by about 3 and 1 points for the CLIP-ViT-224/32 and CLIP-ViT-224/16 model, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis of the Multimodal Fusion Module</head><p>Now, following the default setting in Section 3.3, we perform investigations on multimodal fusion. First, we design both merged attention and co-attention models and investigate their performance. For the merged attention model <ref type="figure" target="#fig_0">(Figure 2b)</ref>, the top transformer consists of M merged encod-  ing layer, with each layer consisting of one self-attention block and one feed-forward network block. To help the model distinguish between the two modalities, we add a modality embedding to the input features before feeding them to the top transformer. For the co-attention model <ref type="figure" target="#fig_0">(Figure 2a</ref>), we feed the text and visual features to two M colayer transformers separately, and each top transformer encoding layer consists of one self-attention block, one crossattention block, and one feed-forward network block. Compared with merged attention, co-attention allows separate transformation functions for the vision and language modalities. We set M merged = 12 and M co = 6 so that the numbers of parameters of the two models are roughly comparable to each other.</p><p>Results. <ref type="table" target="#tab_11">Table 6</ref> reports the downstream performance of the two models. The co-attention model performs better than the merged attention model in our setting, indicating that it is important to have different sets of parameters for the two modalities. Note that this contradicts with the findings in region-based VLP models <ref type="bibr" target="#b3">[4]</ref>, possibly because (i) findings of region-based VLP models cannot directly apply to ViT-based VLP models; (ii) most region-based VLP models only use pre-trained visual encoders, and also do not have a pre-trained text encoder included, thus the inconsistency between the two modalities will not favor symmetrical architecture like the co-attention model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Encoder-Only vs. Encoder-Decoder</head><p>We then compare the encoder-only and encoder-decoder architecture. For the encoder-only model, we use the same co-attention model as in Section 4.2. For the encoderdecoder model, we set the number of layers to 3 for both the encoder and decoder, and each decoding layer has two separate cross-attention blocks that attend to the vision and text representations, respectively. According to <ref type="bibr" target="#b6">[7]</ref>, we adopt T5-style <ref type="bibr" target="#b42">[42]</ref> language modeling objective as it works well for their model. Specifically, we mask 15% of input text tokens and replace contiguous text span with sentinel tokens, and the decoder is trained to reconstruct the masked tokens. For image-text matching, we feed the decoder with a special class token and it generates a binary output. <ref type="table" target="#tab_11">Table 6</ref>, the encoder-only model can outperform the encoder-decoder model on our two discriminative tasks, which is consistent with the findings in <ref type="bibr" target="#b6">[7]</ref>.  However, it should be noted that the encoder-decoder architecture is more flexible, as it can perform tasks such as image captioning which may not be that straightforward for an encoder-only model to be applied to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablations on Pre-training Objectives</head><p>In all the previous experiments, we pre-train our models with different objectives, following the default setting in Section 3.3. Now, we alter the pre-training objectives.</p><p>Results. As summarized in <ref type="table" target="#tab_13">Table 7</ref>, both masked language modeling and image-text matching can bring performance improvements on downstream tasks. However, both of our masked image modeling objectives can lead to degraded performance on both VQAv2 and Flickr30k retrieval tasks. This further indicates that conclusions in region-based VLP models may not necessarily hold in vision transformerbased models. We hypothesize that the performance drop is due to the conflicts between different objectives, and some techniques in multi-task optimization <ref type="bibr" target="#b58">[57,</ref><ref type="bibr" target="#b63">62]</ref> may be borrowed to resolve the conflicts, which we list as one of the future directions. Another possible reason is that image patches can be noisy, thus the supervisions on reconstructing these noisy patches can be uninformative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with Prior Arts</head><p>In this section, we evaluate our best-performing models (i.e., RoBERTa-base+Swin Transformer and RoBERT-base+CLIP-ViT-224/16 with co-attention fusion module, and with image resolutions set to 384 and 288, respectively), and compare them with previous work. We evaluate the models on visual question answering (VQAv2), visual reasoning (NLVR 2 ), visual entailment (SNLI-VE), Flickr30k retrieval tasks in zero-shot and finetuning settings, and COCO retrieval tasks in the finetuning setting.</p><p>Main Results. As in <ref type="table">Table 8</ref> and 9, compared with models pre-trained with fewer than 10M images, our CLIP-based model (METER-CLIP-ViT BASE ) can achieve either the best or the second best scores on all the downstream tasks. Notably, our model can achieve a VQA score of 77.64% on the VQAv2 test-std set using only 4M images for pre-training,  <ref type="table">Table 8</ref>. Comparisons with models pre-trained with &lt;10M images on visual question answering, visual reasoning, visual entailment, and zero-shot image retrieval (IR) and text retrieval (TR) tasks. The best scores are in bold, and the second best scores are underlined. <ref type="table" target="#tab_1">COCO  IR@1 IR@5 IR@10 TR@1 TR@5 TR@10 IR@1 IR@5 IR@10 TR@1 TR@5 TR@10</ref> Pre-trained with &gt;10M images ALBEF (14M) <ref type="bibr" target="#b29">[29]</ref> 85  <ref type="table">Table 9</ref>. Comparisons with models pre-trained with &lt;10M images on Flickr30k and COCO image retrieval (IR) and text retrieval (TR) tasks in the finetuning setting. The best scores are in bold, and the second best scores are underlined.  surpassing the state-of-the-art region-feature-based VinVL model by 1.04%, and outperforming the previous best fully transformer-based model (i.e., ALBEF) by 1.6%. In addition, while ALBEF has specially-designed objectives for retrieval, our model can still outperform ALBEF on text and image retrieval tasks, further demonstrating the effectiveness of METER. Also, as shown in Appendix, we can maintain the fast inference speed of ViT-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Flickr</head><p>Scaling the Model. We also investigate if the METER framework is scalable. To this end, we pre-train our model with more images and larger vision backbone. Specifically, we pre-train the model with COCO, CC, CC12M <ref type="bibr" target="#b4">[5]</ref>, SBU, and VG datasets, consisting of about 14M images and 20M image-caption pairs in total. We use CoSwin-Huge <ref type="bibr" target="#b64">[63]</ref> as our vision backbone and RoBERTa-base as our text backbone. The hidden size of the fusion module remains unchanged. As shown in <ref type="table" target="#tab_1">Table 10</ref>, our model can achieve state-of-the-art performance on VQAv2, surpassing previous models trained with 1.8B images. The results indicate that our METER framework is scalable.</p><p>Further Analysis. We also conduct experiments on image captioning, investigate multi-scale feature fusion, study the model performance on unimodal tasks after VLP, and provide visualization of learned attention maps. All these results are provided in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present METER, through which we systematically investigate how to train a fully-transformer VLP model in an end-to-end manner. Experiments demonstrate that we can achieve competitive performance with state-of-the-art models with only 4M images for pre-training. When further scaled up, METER achieves new state of the art on VQA.  <ref type="table" target="#tab_1">Table 11</ref>. Statistics of the pre-training datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Datasets. The statistics of our pre-training datasets is shown in <ref type="table" target="#tab_1">Table 11</ref>. Following many previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">29]</ref>, we pre-train the models with four datasets, including COCO, Visual Genome, Conceptual Captions and SBU Captions, consisting of about 4M images and 9M imagecaption pairs in total.</p><p>For the downstream tasks, we test the models on VQAv2 <ref type="bibr" target="#b0">[1]</ref> for visual question answering, NLVR 2 <ref type="bibr" target="#b50">[50]</ref> for visual reasoning, COCO <ref type="bibr" target="#b33">[33]</ref> and Flickr30k <ref type="bibr" target="#b40">[40]</ref> for imagetext retrieval, and SNLI-VE <ref type="bibr" target="#b60">[59]</ref> for visual entailment. We use the standard splits for all the datasets except for VQAv2, where we follow standard practice <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">29]</ref> to train the models with both its training and development data, and treat its test-dev set as the development set. Note that we do not use the Visual Genome VQA data for data augmentation in our VQA settings.</p><p>Pre-training Settings. We pre-train our best models using the AdamW optimizer <ref type="bibr" target="#b37">[37]</ref> with the learning rates set to 1e-5 for the bottom image and text encoders and 5e-5 for the cross-modal module. The warm-up ratio is set to 10%, and the learning rate is linearly decayed to 0 after 10% of the total training steps. The batch size, hidden size, and number of heads are set to 4096, 768, 12, respectively. We pre-train the models for 100k steps on 8 NVIDIA A100 GPUs, which takes around 3 days for METER-CLIP-ViT BASE?32 and 8 days for METER-Swin BASE and METER-CLIP-ViT BASE?16 .</p><p>Fine-tuning Settings. For the downstream tasks, we perform grid searches over the learning rates and image resolutions. The learning rates and image resolutions are selected from {1e-6, 2e-6, 5e-6, 1e-5} and {288, 384, 576}, respectively. We apply RandAugment <ref type="bibr" target="#b8">[9]</ref> during finetuning following previous work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inference Time</head><p>We measure the inference time of different models as in <ref type="table" target="#tab_1">Table 12</ref>. First, as shown in <ref type="bibr" target="#b24">[25]</ref>, their ViT-based model is much faster than previous region-feature-based VLP models. In our setting, we measure the average inference time of processing 1 VQA instance on 1 NVIDIA V100 GPU. We find that while our model can be slower than the ViLT model, it is still significantly faster than region-featurebased models and comparable to other ViT-based ones. In addition, we can achieve much stronger performance on downstream tasks than other models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Captioning</head><p>While in this paper we mainly focus on finetuning our models for discriminative downstream tasks such as visual question answering, here we investigate if our models can also be applied to generative tasks. Specifically, we finetune our models on the COCO image captioning task.</p><p>We finetune our METER-CLIP-ViT BASE model for 5 epochs using the standard maximum likelihood estimation objective. At each decoding step, instead of using the causal attention mechanism, the input image and all the text tokens can attend to all the generated text tokens so as to minimize the discrepancy between pre-training and finetuning. We use beam search with the beam size set to 5.</p><p>As shown in <ref type="table" target="#tab_1">Table 13</ref>, we can achieve reasonable performance on image captioning even though our model employs an encoder-only architecture. We expect that an encoderdecoder model would be more suitable for generative tasks, which we leave as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-scale Feature Fusion</head><p>For the pre-trained text and visual encoders, different layers can contain different types of information. For example, <ref type="bibr" target="#b20">[21]</ref> finds that the intermediate layers of BERT encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top. Aggregating the features at different layers has demonstrated to be helpful in both vision <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b62">61]</ref> and language <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>. Therefore, in this part, we investigate if we can use feature fusion techniques to better utilize the information embedded at different layers of the pre-trained encoders.</p><p>Method. Based on some preliminary explorations, here we adopt a simple fusion strategy and only fuse the representations of the text and image encoders but not the cross-modal layers on the top. Specifically, given a text token or image patch x i , we first feed it into a text or image encoder on the bottom of our model (e.g., BERT), and get its representations {h(x j i )} N j=0 at different layers, where N is the number of layers of the encoder. Then, we compute a gate value for each layer and perform a weighted sum to get the final representation of x i :</p><formula xml:id="formula_6">o(x i ) = h(x N i ) + N ?1 j=0 g(h(x j i ))h(x j i ),<label>(2)</label></formula><p>where g is a linear transformation function. We then feed o(x i ) to the top cross-modal layers. Note that the fusion can be done in both the text and visual encoders.</p><p>Results. We pre-train the models using the co-attention model with RoBERTa as the text encoder and Swin Transformer as the visual encoder. We evaluate the models both with and without VLP following the default settings. Because Swin Transformers have different numbers of image representations at different layers, we perform an average pooling so that each layer has 12?12 patch representations. As shown in Tab. 14, while the fusion strategy can improve the model performance by a small margin without VLP, it can degrade the model performance after pre-training. We hypothesize that this is because after the pre-training, the VLP model can learn how to well utilize the representations in the pre-trained encoders and layer fusion is not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Correlation between Vision-and-Language Tasks and Vision or Language Tasks</head><p>In this section, we perform a quantitative analysis of the correlation between model performance on vision-andlanguage tasks and pure vision or language tasks. We vary  <ref type="figure">Figure 6</ref>. Correlation between model performance on vision-andlanguage tasks and pure vision or language tasks. different text and vision encoders, and plot the model performance on the VQAv2 test-dev set and SQuAD or Ima-geNet datasets as in <ref type="figure">Figure 6</ref>. We also compute the Pearson correlations in both cases. We find that the Pearson correlation coefficients and p-values are -0.09 and 0.88 in the VL vs. L setting, and 0.41 and 0.36 in the VL vs. V setting, indicating that there exists little to none correlations between the model performance on VL tasks and V or L tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Unimodal Tasks</head><p>We also investigate the model performance on unimodal tasks after VLP. For text-only tasks, we finetune the bottom text encoders on GLUE tasks; for image-only tasks, we fit a linear classifier on the learned representations of image encoders on CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b26">[27]</ref>.</p><p>We report results in <ref type="table" target="#tab_1">Table 15</ref> and 16. As shown in the tables, our text encoder gets slightly worse performance on the GLUE tasks on average; for image-only tasks, VLP seems to improve the model performance for Swin Transformer but not for CLIP-ViT, possibly because of domain issues. Note that in both sets of the experiments, we only use our text or image encoder and discard the rest of the networks, and how to utilize multi-modal encoder to improve uni-modal performance is an open problem and we leave it as a future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Analysis on Pre-training Datasets</head><p>We also perform analysis on our pre-training datasets. We pre-train our model on each of the pre-training datasets. We choose CLIP-ViT-224/32 as the image encoder and BERT-base-uncased as the text encoder, and employ the coattention fusion module. We pre-train the model for 50k steps on each dataset and report the evaluation results on VQAv2 and Flickr30k zero-shot retrieval tasks.</p><p>As we can see from <ref type="table" target="#tab_1">Table 17</ref>, both data size and domain similarity contribute to the downstream task performance. CC and VG are the largest datasets and COCO most matches the downstream task domains, thus models pre-trained on the three datasets obtain the highest scores.  <ref type="table" target="#tab_1">Table 15</ref>. Performance of text encoders (RoBERTa-base) on GLUE dev sets before and after VLP. The image encoder during VLP is CLIP-ViT-224/16. We report average scores and standard deviations over three runs of different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Encoder</head><p>Before VLP After VLP CF10 CF100 CF10 CF100  <ref type="table" target="#tab_1">Table 17</ref>. Results of models pre-trained with different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Visualization</head><p>In this section, we use Grad-CAM <ref type="bibr" target="#b45">[45]</ref> to visualize our models. Specifically, we visualize the cross-attention maps of the pre-trained models corresponding to individual words when performing masked language modeling. As shown in <ref type="figure" target="#fig_6">Figure 7</ref> and 8, both our Swin Transformer-based and CLIP-ViT-based models can correctly attend to the corresponding regions given different words, suggesting that our models can learn visual grounding implicitly during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Limitations</head><p>While we have demonstrated the effectiveness of our models across different tasks, our models still have several limitations:</p><p>Generative Tasks. We mainly focus on discriminative tasks such as visual question answering and visual reasoning in this paper, while generative tasks such as image captioning are under-investigated. We perform experiments on the COCO image captioning data in Appendix, and will investigate more on this in the future.</p><p>Scalability. In our current settings, we pre-train the models with 4M or 14M images, thus it is unclear how the model performance would be if we pre-train the models with larger datasets and we are actively experimenting in this direction.</p><p>English Data. So far, we only experiment on the English data, and it is worth investigating if our models can generalize to other languages as well, which we leave as a future direction.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of two types of multimodal fusion modules: (a) co-attention, and (b) merged attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the encoder-only and encoder-decoder model architectures for VLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of masked patch classification with in-batch negatives and with discrete code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Section 4.2, (iii) we compare encoder-only and encoderdecoder architectures in Section 4.3, and (iv) we ablate pretraining objectives in Section 4.4. Finally, we compare with state of the arts in Section 4.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Vision-and-Language vs. Vision</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of the attention maps of text tokens in the caption "a display of flowers growing out and over the retaining wall in front of cottages on a cloudy day." The first and second rows correspond to the results of METER-SwinBASE and METER-CLIP-ViTBASE, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of the attention maps of text tokens in the caption "yellow squash, corn on the cob and green beans laid out on a white cloth." The first and second rows correspond to the results of METER-SwinBASE and METER-CLIP-ViTBASE, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Glossary of representative VLP models. OD: objective detector. Xformer: transformer. Emb.: embedding. MLM/MIM: masked language/image modeling. ITM: image-text matching. WRA: word-region alginment. ITC: image-text contrastive learning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Comparisons of different text encoders without VLP. CLIP-ViT-224/32 is used as the vision encoder. All the text encoders are in base model size, except ALBERT, which is xlarge. Emb-only: only using word embeddings as text encoder. IR/TR: Flickr30k image/text retrieval. EM: exact match. The results of SQuAD and MNLI are copied from their corresponding papers. All the results on VL tasks are from their test-dev/val sets.</figDesc><table><row><cell>Text Enc.</cell><cell>VQAv2 Acc.</cell><cell>VE Acc.</cell><cell cols="2">IR R@1 R@1 TR</cell><cell cols="2">SQuAD MNLI EM Acc.</cell></row><row><cell>Emb-only</cell><cell>67.13</cell><cell cols="3">74.85 49.06 68.20</cell><cell>-</cell><cell>-</cell></row><row><cell>ELECTRA</cell><cell>69.22</cell><cell cols="3">76.57 41.80 58.30</cell><cell>86.8</cell><cell>88.8</cell></row><row><cell>CLIP</cell><cell>69.31</cell><cell cols="3">75.37 54.96 73.80</cell><cell>-</cell><cell>-</cell></row><row><cell>DeBERTa</cell><cell>69.40</cell><cell cols="3">76.74 51.50 67.70</cell><cell>87.2</cell><cell>88.8</cell></row><row><cell>BERT</cell><cell>69.56</cell><cell cols="3">76.27 49.60 66.60</cell><cell>76.3</cell><cell>84.3</cell></row><row><cell>RoBERTa</cell><cell>69.69</cell><cell cols="3">76.53 49.86 68.90</cell><cell>84.6</cell><cell>87.6</cell></row><row><cell>ALBERT</cell><cell>69.94</cell><cell cols="3">76.20 52.20 68.70</cell><cell>86.4</cell><cell>87.9</cell></row><row><cell cols="2">Vision Encoder</cell><cell>VQAv2</cell><cell>VE</cell><cell>IR</cell><cell>TR</cell><cell>ImageNet</cell></row><row><cell cols="2">Dis. DeiT B-384/16</cell><cell>67.84</cell><cell cols="3">76.17 34.84 52.10</cell><cell>85.2</cell></row><row><cell cols="2">BEiT B-224/16</cell><cell>68.45</cell><cell cols="3">75.28 32.24 59.80</cell><cell>85.2</cell></row><row><cell cols="2">DeiT B-384/16</cell><cell>68.92</cell><cell cols="3">75.97 33.38 50.90</cell><cell>82.9</cell></row><row><cell cols="2">ViT B-384/16</cell><cell>69.09</cell><cell cols="3">76.35 40.30 59.80</cell><cell>83.97</cell></row><row><cell cols="2">CLIP B-224/32</cell><cell>69.69</cell><cell cols="3">76.53 49.86 68.90</cell><cell>-</cell></row><row><cell cols="2">VOLO 4-448/32</cell><cell>71.44</cell><cell cols="3">76.42 40.90 61.40</cell><cell>86.8</cell></row><row><cell cols="2">CaiT M-384/32</cell><cell>71.52</cell><cell cols="3">76.62 38.96 61.30</cell><cell>86.1</cell></row><row><cell cols="2">CLIP B-224/16</cell><cell>71.75</cell><cell cols="3">77.54 57.64 76.90</cell><cell>-</cell></row><row><cell cols="2">Swin B-384/32</cell><cell>72.38</cell><cell cols="3">77.65 52.30 69.50</cell><cell>86.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Comparisons of different vision encoders without VLP. RoBERTa is used as the default text encoder. IR/TR: Flickr30k image/text retrieval; B: Base. The results of ImageNet classification are copied from their corresponding papers. All the results on VL tasks are from their test-dev/val sets. N and M in ViT-N/M denote the image resolution and patch size, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of different vision and text encoders with VLP. Results on VQAv2 are on test-dev set. ZS: zero-shot.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>4.1.2 Results with VLPNow, we follow the default setting in Section 3.3, and compare different vision/text encoders with VLP. Based on the previous results, we compare Embed-only, BERT, and RoBERTa on the text side, and CLIP-ViT-224/32, CLIP-ViT-224/16, and Swin Transformer on the image side.</figDesc><table><row><cell cols="3">Bottom LR Top LR VQAv2</cell><cell>Flickr-ZS IR TR</cell></row><row><cell>1e-5</cell><cell>1e-5</cell><cell>73.16</cell><cell>48.80 63.70</cell></row><row><cell>2e-5</cell><cell>2e-5</cell><cell>73.66</cell><cell>53.14 67.20</cell></row><row><cell>3e-5</cell><cell>3e-5</cell><cell>73.77</cell><cell>56.48 70.90</cell></row><row><cell>5e-5</cell><cell>5e-5</cell><cell>73.54</cell><cell>52.48 65.90</cell></row><row><cell>1e-5</cell><cell>5e-5</cell><cell>74.98</cell><cell>66.08 78.10</cell></row></table><note>Results. As shown in Table 4, after VLP, the difference be- tween BERT and RoBERTa seems to be diminished, but it is still important to have a pre-trained text encoder on the bottom (Embed-only vs. RoBERTa). For vision encoder,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Using different learning rates for the randomly-initialized and pre-trained parameters is better than using the same learning rate. Results on VQAv2 are on test-dev set. ZS: zero-shot.</figDesc><table><row><cell>VQAv2 Test-Dev Score</cell><cell>73 74 75 76 77</cell><cell></cell><cell>CLIP-ViT-224/32 CLIP-ViT-224/16</cell></row><row><cell></cell><cell>224</cell><cell>384 Resolution</cell><cell>576</cell></row><row><cell cols="4">Figure 5. Increasing the image resolution during finetuning</cell></row><row><cell cols="4">greatly improves the performance on the VQAv2 test-dev set.</cell></row></table><note>both CLIP-ViT-224/16 and Swin Transformer can achieve pretty good performance. Especially, CLIP-ViT-224/16 can achieve a VQA score of 77.19/77.20 on the test-dev/test-std sets, respectively, outperforming the previous state-of-the- art region-based VinVL [65] models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>Co-attention performs better than merged attention in our setting, and adding a decoder is not helpful for our discriminative VL tasks. Results on VQAv2 are on test-dev set. ZS: zero-shot.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 .</head><label>7</label><figDesc>Masked language modeling (MLM) and image-text matching (ITM) can both improve the model performance, but both of our designed masked image modeling (MIM) objectives lead to degraded performance on downstream tasks. Results on VQAv2 are on test-dev set. ZS: zero-shot.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 .</head><label>10</label><figDesc>Pre-training a huge model under the METER framework with 14M images can lead to state-of-the-art performance on VQAv2, surpassing previous models trained with 1.8B images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 .</head><label>12</label><figDesc>Inference time (ms) of different models. We report the inference time measured by<ref type="bibr" target="#b24">[25]</ref> and in our setting. We also list the model performance on the VQAv2 test-std set.</figDesc><table><row><cell cols="5">Model (#Pre-training Images) BLEU METEOR Cider SPICE</cell></row><row><cell>OSCAR BASE (4M)</cell><cell>36.5</cell><cell>30.3</cell><cell>123.7</cell><cell>23.1</cell></row><row><cell>VinVL BASE (5.6M)</cell><cell>38.2</cell><cell>30.3</cell><cell>129.3</cell><cell>23.6</cell></row><row><cell>SimVLM BASE (1.8B)</cell><cell>39.0</cell><cell>32.9</cell><cell>134.8</cell><cell>24.0</cell></row><row><cell>METER-CLIP-ViT BASE (4M)</cell><cell>38.8</cell><cell>30.0</cell><cell>128.2</cell><cell>23.0</cell></row><row><cell cols="5">Table 13. Image captioning results of different models trained</cell></row><row><cell cols="3">with maximum likelihood estimation on COCO.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>Before VLP 91.31?0.15 87.53?0.24 92.61?0.34 94.38?0.20 58.72?0.73 91.03?0.59 90.15?0.18 71.24?3.07 After VLP 91.34?0.08 87.38?0.18 92.67?0.06 93.92?0.50 57.88?0.79 90.57?0.78 89.93?0.46 70.28?2.00</figDesc><table><row><cell>Text Encoder</cell><cell>QQP</cell><cell>MNLI</cell><cell>QNLI</cell><cell>SST2</cell><cell>CoLA</cell><cell>MRPC</cell><cell>STSB</cell><cell>RTE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 16 .</head><label>16</label><figDesc>Linear probe performance on CIFAR-10 and CIFAR-100. The text encoder during VLP is RoBERTa-base.</figDesc><table><row><cell cols="2">Swin-Base-384/32 97.00</cell><cell>89.15</cell><cell>97.99</cell><cell>90.26</cell></row><row><cell>CLIP-ViT-224/16</cell><cell>95.85</cell><cell>82.60</cell><cell>94.92</cell><cell>81.90</cell></row><row><cell cols="3">Pre-training Datasets VQAv2</cell><cell cols="2">Flickr-ZS IR TR</cell></row><row><cell>COCO</cell><cell></cell><cell>72.95</cell><cell cols="2">46.38 60.20</cell></row><row><cell>CC</cell><cell></cell><cell>73.05</cell><cell cols="2">39.84 55.50</cell></row><row><cell>SBU</cell><cell></cell><cell>70.14</cell><cell cols="2">21.52 35.90</cell></row><row><cell>VG</cell><cell></cell><cell>73.54</cell><cell cols="2">39.24 49.30</cell></row><row><cell cols="2">COCO+CC+SBU+VG</cell><cell>74.98</cell><cell cols="2">66.08 78.10</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BEiT: Bert pretraining of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training deeper neural machine translation models with transparent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal pretraining unmasked: Unifying the vision and language BERTs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">UNITER: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting deep representations for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale adversarial training for visionand-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">DeBERTa: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decoupling the role of data, attention, and losses in multimodal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Seeing out of the box: End-toend pre-training for vision-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pixel-BERT: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ishan Misra, and Nicolas Carion. Mdetrmodulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ViLT: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<title level="m">VisualBERT: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flickr30k Entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">How much can clip benefit vision-and-language tasks? arXiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian</forename><forename type="middle">Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">VL-BERT: Pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">LXMERT: Learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<title level="m">Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<title level="m">Visual entailment: A novel task for fine-grained image understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Probing intermodality: Visual parsing with self-attention for visionlanguage pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">A new foundation model for computer vision</title>
		<meeting><address><addrLine>Florence</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volo</surname></persName>
		</author>
		<title level="m">Vision outlooker for visual recognition</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">VinVL: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
