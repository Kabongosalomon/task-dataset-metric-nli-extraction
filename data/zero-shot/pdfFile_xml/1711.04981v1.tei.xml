<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SKIPFLOW: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">C</forename><surname>Phan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luu</forename><forename type="middle">Anh</forename><surname>Tuan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute for Infocomm Research</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Hui</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University School of Computer Science and Engineering</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>1, 2</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SKIPFLOW: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has demonstrated tremendous potential for Automatic Text Scoring (ATS) tasks. In this paper, we describe a new neural architecture that enhances vanilla neural network models with auxiliary neural coherence features. Our new method proposes a new SKIPFLOW mechanism that models relationships between snapshots of the hidden representations of a long short-term memory (LSTM) network as it reads. Subsequently, the semantic relationships between multiple snapshots are used as auxiliary features for prediction. This has two main benefits. Firstly, essays are typically long sequences and therefore the memorization capability of the LSTM network may be insufficient. Implicit access to multiple snapshots can alleviate this problem by acting as a protection against vanishing gradients. The parameters of the SKIPFLOW mechanism also acts as an auxiliary memory. Secondly, modeling relationships between multiple positions allows our model to learn features that represent and approximate textual coherence. In our model, we call this neural coherence features. Overall, we present a unified deep learning architecture that generates neural coherence features as it reads in an end-to-end fashion. Our approach demonstrates state-of-the-art performance on the benchmark ASAP dataset, outperforming not only feature engineering baselines but also other deep learning models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Automated Text Scoring (ATS) systems are targeted at both alleviating the workload of teachers and improving the feedback cycle in educational systems. ATS systems have also seen adoption for several high-stakes assessment, e.g., the e-rater system <ref type="bibr" target="#b1">(Attali and Burstein 2004)</ref> which has been used for TOEFL and GRE examinations. A successful ATS system brings about widespread benefits to society and the education industry. This paper presents a novel neural network architecture for this task.</p><p>Traditionally, the task of ATS has been regarded as a machine learning problem <ref type="bibr" target="#b10">(Larkey 1998;</ref><ref type="bibr" target="#b1">Attali and Burstein 2004)</ref> which learns to approximate the marking process with supervised learning. Decades of ATS research follow the same traditional supervised text regression methods in which handcrafted features are constructed and subsequently passed into a machine learning based classifier. A Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. wide assortment of features may commonly extracted from essays. Simple and intuitive features may include essay length, sentence length. On the other hand, intricate and complex features may also be extracted, e.g.., features such as grammar correctness <ref type="bibr" target="#b1">(Attali and Burstein 2004)</ref>, readability <ref type="bibr" target="#b21">(Zesch, Wojatzki, and Scholten-Akoun 2015)</ref> and textual coherence <ref type="bibr" target="#b3">(Chen and He 2013)</ref>. However, these handcrafted features are often painstakingly designed, require a lot of human involvement and usually require laborious implementation for every new feature.</p><p>Deep learning based ATS systems have recently been proposed <ref type="bibr" target="#b4">(Dong and Zhang 2016;</ref><ref type="bibr" target="#b17">Taghipour and Ng 2016;</ref><ref type="bibr" target="#b0">Alikaniotis, Yannakoudakis, and Rei 2016)</ref>. A comprehensive study has been done in <ref type="bibr" target="#b17">(Taghipour and Ng 2016)</ref> which demonstrated that neural network architectures such as the long short-term memory (LSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber 1997)</ref> and convolutional neural network (CNN) are capable of outperforming systems that extensively require handcrafted features. However, all of these neural models do not consider transition of an essay over time, i.e., logical flow and coherence over time. In particular, mainly semantic compositionality is modeled within the recursive operations in the LSTM model which compresses the input text repeatedly within the recurrent cell. In this case, the relationships between multiple points in the essay cannot be captured effectively. Moreover, essays are typically long sequences which pushes the limits of the memorization capability of the LSTM.</p><p>Hence, the objective of this work is a unified solution to the above mentioned problems. Our method alleviates two problems. The first is targeted at alleviating the inability of current neural network architectures to model flow, coherence and semantic relatedness over time. The second is aimed at easing the burden of the recurrent model. In order to do so, we model the relationships between multiple snapshots of the LSTM's hidden state over time. More specifically, as our model reads the essay, it models the semantic relationships between two points of an essay using a neural tensor layer. Eventually, multiple features of semantic relatedness are aggregated across the essay and used as auxiliary features for prediction.</p><p>The intuition behind our idea is as follows. Firstly, semantic relationships across sentences are commonly used as an indicator of writing flow and textual coherence <ref type="bibr" target="#b20">(Wiemer-Hastings and Graesser 2000;</ref><ref type="bibr" target="#b7">Higgins et al. 2004;</ref><ref type="bibr" target="#b6">Higgins and Burstein 2007;</ref><ref type="bibr" target="#b3">Chen and He 2013;</ref><ref type="bibr">Somasundaran, Burstein, and Chodorow 2014)</ref>. As such, our auxiliary features (generated end-to-end) aim to capture the logical and semantic flow of an essay. This also provides a measure of semantic similarity aside from the flavor of semantic compositionality modeled by the base LSTM model. Secondly, the additional parameters from the external tensor serve as an auxiliary memory for the network. As essays are typically long sequences, modeling the relationship between distant states with additional parameters can enhance memorization and improve performance of the deep architecture by allowing access to intermediate states, albeit implicitly. The semantic relevance scores can then be aggregated by concatenation and passed as an auxiliary feature to a fully-connected dense layer in the final layer of the network. As such, our architecture performs sentence modeling (compositional reading) and semantic matching in a unified end-to-end framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Contributions</head><p>The prime contributions of our paper are as follows:</p><p>? For the first time, we consider neural coherence features within the context of an end-to-end neural framework. Semantic similarity and textual coherence have a long standing history in ATS literature <ref type="bibr" target="#b20">(Wiemer-Hastings and Graesser 2000;</ref><ref type="bibr" target="#b6">Higgins and Burstein 2007;</ref><ref type="bibr" target="#b7">Higgins et al. 2004)</ref>. Our work incorporates this intuition into modern neural architectures.</p><p>? Aside from modeling coherence, our method also alleviates and eases the burden of the recurrent model by implicit access to hidden representations over time. This serves as a protection against vanishing gradient. Moreover, a better performance can be achieved with a smaller LSTM parameterization.</p><p>? We propose SKIPFLOW LSTM, a new neural architecture that incorporates the intuition of logical and semantic flow into the vanilla LSTM model. SKIPFLOW LSTM obtains state-of-the-art performance on the ASAP benchmark dataset. We also achieve an increase of 6% in performance over a strong feature engineering baseline. In the same experimental configuration, we achieve about 10% increase over a baseline LSTM model, outperforming more advanced extensions such as Multi-Layered LSTMs and attention-based LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Automated Text Scoring (ATS) systems have been deployed for high-stakes assessment since decades ago. Early highstakes ATS systems include the Intelligent Essay Assessor (IEA) <ref type="bibr" target="#b5">(Foltz et al. 2013)</ref> and Project Essay Grade <ref type="bibr" target="#b11">(Page 1967;</ref><ref type="bibr" target="#b14">Shermis and Burstein 2003)</ref>. Commercial ATS systems such as the e-rater <ref type="bibr" target="#b1">(Attali and Burstein 2004)</ref> have been also deployed for GRE and TOEFL examinations. Across the rich history of ATS research, supervised learning based ATS systems mainly rely on domain-specific feature engineering whereby lexical, syntactic and semantic features are designed by domain experts and subsequently extracted from essays. Then, a simple machine learning classifier trained on these feature vectors can be used to predict the grades of essays. Early work <ref type="bibr" target="#b10">(Larkey 1998</ref>) treats ATS as a text categorization problem and uses a Naive Bayes model for grading while the e-rater system uses linear regression over handcrafted features. <ref type="bibr" target="#b12">(Phandi, Chai, and Ng 2015)</ref> proposed a Bayesian Linear Ridge Regression approach for domain adaptation of essays.</p><p>The reliance on handcrafted features is a central theme to decades of ATS research. The complexity and ease of implementation of essay scoring features can be diverse. For example, length-based features are intuitive and simple to extract from essays. On the other hand, there are more complex features such as grammar correctness or lexical complexity. Features such as readability <ref type="bibr" target="#b21">(Zesch, Wojatzki, and Scholten-Akoun 2015)</ref>, textual and discourse coherence <ref type="bibr" target="#b3">(Chen and He 2013;</ref><ref type="bibr">Somasundaran, Burstein, and Chodorow 2014)</ref> are also harder to design in which convoluted pipelines have to be built for feature extraction to be performed. As a whole, feature engineering is generally a laborious process, i.e., apart from designing features, custom code has to be written for each additional feature. For a comprehensive review of feature engineering in the context of ATS, we refer interested readers to <ref type="bibr" target="#b21">(Zesch, Wojatzki, and Scholten-Akoun 2015)</ref>.</p><p>Recently, motivated by the success of deep learning in many domains, several deep learning architectures for ATS have been proposed. <ref type="bibr" target="#b17">(Taghipour and Ng 2016;</ref><ref type="bibr" target="#b4">Dong and Zhang 2016)</ref> empirically evaluated the performance of a myriad of deep learning models on the ATS tasks. In their work, models such as the recurrent neural network (RNN) and convolutional neural network (CNN) demonstrated highly competitive results without requiring any feature engineering. On the other hand, an adapted task-specific embedding approach was proposed in (Alikaniotis, Yannakoudakis, and Rei 2016) that learns semantic word embeddings while predicting essay grades. Subsequently, these adapted word embeddings are passed as input to a LSTM network for prediction. The attractiveness of neural text scoring stems from the fact that features are learned end-toend, diminishing the need for laborious feature engineering to be performed.</p><p>Our work extends the vanilla model and enhances with the incorporation of neural coherence features. The concept of semantic similarity between sentences has been used to measure coherence in student essays <ref type="bibr" target="#b6">(Higgins and Burstein 2007;</ref><ref type="bibr" target="#b7">Higgins et al. 2004)</ref>. Textual coherence features have also been adopted in (Chen and He 2013) which measures the semantic similarity between nouns and proper nouns. Lexical chaining (Somasundaran, Burstein, and Chodorow 2014) has also been used for measuring discourse quality in student essays. Our work, however, is the first neural coherence model that incorporates these features into an end-toend fashion. Different from traditional coherence features, our neural features form a part of an overall unified framework.</p><p>Our proposed approach is inspired by the field of semantic matching. In semantic matching, a similarity score is pro-duced between two vectors and is often used in many NLP and IR applications. The usage of tensor layers and bilinear similarity is inspired by many of these works. For example, convolutional neural tensor network (CNTN) <ref type="bibr" target="#b13">(Qiu and Huang 2015)</ref> and NTN-LSTM ) are recently proposed architectures for question-answer pair matching. However, unlike ours, these works are mainly concerned with matching between two sentences and are often trained with two networks. The tensor layer, also known as the Neural Tensor Network (NTN), was first incepted as a compositional operator in Recursive Neural Networks for sentiment analysis <ref type="bibr" target="#b16">(Socher et al. 2013b</ref>). Subsequently, it has also been adopted for rich and expressive knowledge base completion <ref type="bibr" target="#b15">(Socher et al. 2013a</ref>). It has also seen adoption in end-to-end memory networks <ref type="bibr" target="#b19">(Tay, Tuan, and Hui 2017)</ref>. The NTN is parameterized by both a tensor and an ordinary linear layer in which the tensor parameters model multiple instances of second order interactions between two vectors. The adoption of the tensor layer in our framework is motivated by the strong empirical performance of NTN.</p><p>In our approach, we generate neural coherence features by performing semantic matching k times while reading. This can be interpreted as jointly matching and reading. These additional parameters can also be interpreted as an auxiliary memory which can also help and ease the burden of the LSTM memory. LSTMs are known to have difficulty in modeling long term dependencies 1 and due to their compositional nature, measuring relatedness and coherence between two points becomes almost impossible. Moreover, our SKIPFLOW mechanism serves as an additional protection against the vanishing gradient problem by exposing hidden states to deeper layers. In a similar spirit, attention mechanisms (Bahdanau, Cho, and Bengio 2014) learn a weighted combination of hidden states across all time steps and produces a global feature vector. However, our approach learns auxiliary features that are used for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our SKIPFLOW LSTM Model</head><p>In this section, we introduce the overall model architecture of SKIPFLOW. <ref type="figure" target="#fig_0">Figure 1</ref> depicts the proposed architecture of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Layer</head><p>Our model accepts an essay and the target score as a training instance. Each essay is represented as a fixed-length sequence in which we pad all sequences to the maximum length. Let L be the maximum essay length. Subsequently, each sequence is converted into a sequence of lowdimensional vectors via the embedding layer. The parameters of the embedding layer are defined as W e ? R |V |?N where |V | is the size of the vocabulary and N is the dimensionality of the word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Short-Term Memory (LSTM)</head><p>The sequence of word embeddings obtained from the embedding layer is then passed into a long short-term memory 1 Essays are typically long documents spanning 300-800 words on average.</p><p>(LSTM) network <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber 1997)</ref>.</p><formula xml:id="formula_0">h t = LST M (h t?1 , x i )</formula><p>(1) where x t and h t?1 are the input vectors at time t. The LSTM model is parameterized by output, input and forget gates, controlling the information flow within the recursive operation. For the sake of brevity, we omit the technical details of LSTM which can be found in many related works. At every time step t, LSTM outputs a hidden vector h t that reflects the semantic representation of the essay at position t. To select the final representation of the essay, a temporal mean pool is applied to all LSTM outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SKIPFLOW Mechanism for Generating Neural Coherence Features</head><p>In this section, we describe the process of generating neural coherence features within our end-to-end framework.</p><p>Skipping and Relevance Width In our proposed approach, the relationships between two positional outputs of LSTM across time steps are modeled via a parameterized compositional technique that generates a coherence feature. Let ? be a hyperparameter that controls the relevance width of the model. For each LSTM output, we select pairs of sequential outputs of width ?,</p><formula xml:id="formula_1">i.e., {(h i , h i+? ), (h i+? , h i+2? ), (h i+2X , h i+3? ), .</formula><p>.} are the tuples from the outputs that are being composed, h t denotes the output of LSTM at time step t. In our experiments, the starting position 2 is fixed at i = 3. For the sake of simplicity, if the width ? exceeds the max length, we loop back to the beginning of the essay in a circular fashion. The rationale for fixed length matching is as follows. Firstly, we want to limit the amount of preprocessing required as determining important key points such as nouns and pronouns require preprocessing of some sort. Secondly, maintaining specific indices for each essay can be cumbersome in the context of batch-wise training of deep learning models using libraries restricted by static computational graphs. Finally, LSTMs are memory-enabled models and therefore, intuitively, a slight degree of positional impreciseness should be tolerable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Tensor Layer</head><p>We adopt a tensor layer to model the relationship between two LSTM outputs. The tensor layer is a parameterized composition defined as follows:</p><formula xml:id="formula_2">s i (a, b) = ?(u T f (v T a M [1:k] v b + V [v a , v b ] + b))<label>(2)</label></formula><p>where f is a non-linear function such as tanh. M [1:k] ? R n?n?k is a tensor (3d matrix). a, b ? R d are the vector outputs of LSTM at two time steps of ?-width apart where d is the dimensionality of LSTM parameters. For each slice of the tensor M , each bilinear tensor product v T a M k v b returns a scalar to form a k dimensional vector. ? is the sigmoid function which constraints the output to [0, 1]. The other parameters are the standard form of a neural network. In our model, two vectors (outputs of LSTM) are passed through the tensor layer and returns a similarity score  that determines the coherence feature between the two vectors. The parameters of the tensor layer are shared throughout all output pairs. The usage of bilinear product enables dyadic interaction between vectors through a similarity matrix. This enables a rich interaction between hidden representations. Moreover, the usage of multiple slices encourages different aspects of this relation to be modeled.</p><formula xml:id="formula_3">s i (h i , h i+X ) ? [0, 1] ! " ! # ! $ ! $%&amp; ? " ? # ? $ ? $%&amp; ? ? $%#&amp; ? ? ? ? ! $%#&amp; ? ? ? $%$&amp; ! $%$&amp; ? ! )*" ? )*" ! ) ? )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully-connected Hidden Layer</head><p>Subsequently, all the scalar values s 1 , s 2 , ? ? ? , s n that are obtained from the tensor layer are concatenated together to form the neural coherence feature vector. n is the number of times that coherence is being measured, depending on the relevance width ? and maximum sequence length L. Recall that the essay representation is obtained from a mean pooling over all hidden states. This essay vector is then concatenated with the coherence feature vector. This vector is then passed through a fully connected hidden layer defined as follows:</p><formula xml:id="formula_4">h out = f (W h ([e, s 1 , s 2 , ...., s n ])) + b h<label>(3)</label></formula><p>where f (.) is a non-linear activation such as tanh or relu, W h and b h are the parameters of the hidden layer. e is the final essay representation obtained from temporal mean pooling and s 1 , s 2 , ..., s n are the scalar values obtained from the neural tensor layer, i.e., each scalar value is the matching score</p><formula xml:id="formula_5">from {(h i , h i+? ), (h i+? , h i+2? ), (h i+2? , h i+3? ), ..}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Layer with Sigmoid</head><p>Finally, we pass h out into a final linear regression layer. The final layer is defined as follows:</p><formula xml:id="formula_6">y out = ? (W f ([h out ])) + b f<label>(4)</label></formula><p>where W f , b f are parameters of the final linear layer, ? is the sigmoid function and y out ? [0, 1]. The output at this final layer is the normalized score of the essay. Following <ref type="bibr" target="#b17">(Taghipour and Ng 2016)</ref>, the bias is set to the mean expected score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning and Optimization</head><p>Our network optimizes the mean-square error which is defined as:</p><formula xml:id="formula_7">M SE(z, z * ) = 1 N N i=1 (z i ? z * i ) 2<label>(5)</label></formula><p>where z * i is the gold standard score and z i is the model output. The parameters of the network are then optimized using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Evaluation</head><p>In this section, we describe our experimental procedure, dataset and empirical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>We use the ASAP (Automated Student Assessment Prize) dataset for experimental evaluation. This comes from the competition which was organized and sponsored by the William and Flora Hewlett Foundation (Hewlett) and ran on Kaggle from 10/2/12 to 30/4/12. This dataset contains 8 essay prompts as described in <ref type="table">Table 1</ref>. Each prompt can be interpreted as a different essay topic along with a different genre such as argumentative or narrative. <ref type="table" target="#tab_3">1  1783  350  2-12  2  1800  350  1-6  3  1726  150  0-3  4  1772  150  0-3  5  1805  150  0-4  6  1800  150  0-4  7  1569  250  0-30  8  723  650  0-60   Table 1</ref>: Statistics of ASAP dataset. Scores denote the range of possible marks in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt #Essays Avg Length Scores</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>We use 5-fold cross validation to evaluate all systems with a 60/20/20 split for train, development and test sets. The splits are provided by <ref type="bibr" target="#b17">(Taghipour and Ng 2016)</ref> and the experimental procedure is followed closely. We train all models for 50 epochs and select the best model based on the performance on the development set. The vocabulary is restricted to the 4000 most frequent words. We tokenize and lowercase text using NLTK 3 , and normalize all score range to within [0,1]. The scores are rescaled back to the original prompt-specific scale for calculating Quadratic Weighted Kappa (QWK) scores. Following <ref type="bibr" target="#b17">(Taghipour and Ng 2016)</ref>, the evaluation is conducted in prompt-specific fashion. Even though training prompts together might seem ideal, it is good to note that each prompt can contain genres of essays that are very contrastive such as narrative or argumentative essays. Additionally, prompts can have different marking schemes and level of students. As such, it would be extremely difficult to train prompts together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metric</head><p>The evaluation metric used is the Quadratic Weighted Kappa (QWK) which measures agreement between raters and is a commonly used metric for ATS systems. The QWK score ranges from 0 to 1 but becomes negative if there is less agreement than expected by chance. The QWK score is calculated as follows. First, an N ? N histogram matrix O is constructed. Next, a weight matrix W i,j = (i?j) 2 (N ?1) 2 is calculated that corresponds to the difference between rater's scores where i and j are reference ratings by the annotator and the ATS system. Finally, another N ? N histogram matrix E is constructed assuming no correlation between rating scores. This is done using an outer product between each rater's histogram vector and normalized such that sum(E) = sum(O). Finally, the QWK score is calculated as ? = 1 ? i,j wi,j Oi,j i,j wi,j Ei,j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines and Implementation Details</head><p>In this section, we discuss the competitor algorithms that are used as baselines for our model. ? CNN -We implemented a CNN model using 1D convolutions similar to <ref type="bibr" target="#b17">(Taghipour and Ng 2016)</ref>. We use a filter width of 3 and a final embedding dimension of 50. The outputs from the CNN model are passed through a mean pooling layer and finally through the final linear layer.</p><p>? RNN / GRU / LSTM -Similar to <ref type="bibr" target="#b17">(Taghipour and Ng 2016)</ref>, we implemented and tested all RNN variants, namely the vanilla RNN, GRU (Gated Recurrent Unit) and LSTM. We compare mainly on two settings of mean pooling and last. In the former, the average vector of all outputs from the model is used. In the latter, only the last vector is used for prediction. A fully connected linear layer connects this feature vector to the final sigmoid activation function. We use a dimension of 50 for all RNN/GRU/LSTM models.</p><p>? LSTM Variants -Additionally, we also compare with multiple LSTM variants such as the Attention Mechanism (ATT-LSTM), Bidirectional LSTM (BI-LSTM) and the Multi-Layer LSTM (ML-LSTM). We use the Atten-tionCellWrapper implementation in TensorFlow with an attention width of 10.</p><p>Our Models We compare two settings of our model, namely the bilinear and tensor composition. They are denoted as SKIPFLOW LSTM (Bilinear) and SKIPFLOW LSTM (Tensor) respectively. The bilinear setting is formally described as s(a, b) = a T M b, where a, b are vectors of two distant LSTM outputs and M is a similarity matrix. The bilinear setting produces a scalar value, similar to the output of the tensor layer. The tensor layer, aside from the combination of multiple bilinear products, also includes a separate linear layer along with a non-linear activation function. For the tensor setting, the number of slices of the tensor is tuned amongst {2, 4, 6, 8}. For both models, the hidden layer is set to 50. There is no dropout for this layer and the bias vector is set to 0. The relevance width of our model ? is set amongst {20, 50, 100}. In addition, to demonstrate the effectiveness and suitability of the LSTM model for joint modeling of semantic relevance, we conduct further experiments with the SKIPFLOW extension of the CNN model which we call the SKIPFLOW CNN. Similarly, we apply the same procedure on the convolved representations. Aside from swapping the LSTM for a CNN, the entire architecture remains identical.</p><p>To facilitate fair comparison, we implemented and evaluated all deep learning models ourselves in TensorFlow. We also implemented the architectures of (Taghipour and Ng 2016) which we denote with ?. For training, the ADAM  Results are sorted by average performance. ? denotes our implementation of a model from <ref type="bibr" target="#b17">(Taghipour and Ng 2016)</ref>, ? denotes the baseline for statistical significance testing, * denotes statistically significant improvement. denotes non deep learning baselines.</p><p>optimizer <ref type="bibr" target="#b9">(Kingma and Ba 2014</ref>) was adopted with a learning rate amongst {0.01, 0.001, 0.0001} and mini-batch size amongst {64, 128, 256}. The gradient of the norm is clipped to 1.0. The sequences are all padded with zero vectors up till the total maximum length 5 . We use the same embeddings from <ref type="bibr" target="#b17">(Taghipour and Ng 2016)</ref> and set them to trainable parameters. All experiments are conducted on a Linux machine running two GTX1060 GPUs. <ref type="table" target="#tab_3">Table 2</ref> reports the empirical results of all deep learning models. First, it is clear that the mean pooling is significantly more effective as compared to the last LSTM output. In the last setting, the performance of RNN is significantly worse compared to LSTM and GRU possibly due to the weaker memorization ability. However, the performance of LSTM, GRU and RNN are similar using the mean pooling setting. This is indeed reasonable because the adoption of a mean pooling layer reduces the dependency of the model's memorization ability due to implicit access to all intermediate states. Overall, we observe that the performance of LSTM and GRU is quite similar with either mean pooling or last setting. Finally, we note that the performance of CNN is considerably better than RNN-based models. We also observe that a multi-layered LSTM performs considerably better than a single-layered LSTM. We also observe that adding layers also increases the performance. On the <ref type="bibr">5</ref> We used the dynamic RNN in TensorFlow in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>other hand, the bidirectional LSTM did not yield any significant improvements in performance. The performance of ATT-LSTM is notably much higher than the base LSTM.</p><p>The best performing LSTM model is a multi-layered LSTM with 4 layers. Additionally, we observe that SKIPFLOW LSTM (Tensor) outperforms the baseline LSTM (Mean) by almost 10% in QWK score. Evidently, we see the effectiveness of our proposed approach. The tensor setting of SKIPFLOW LSTM is considerably better than the bilinear setting which could be due to the richer modeling capability of the tensor layer. On the other hand, we also note that the SKIPFLOW extension of CNN model did not increase the performance of CNN. As such, we see that the SKIPFLOW mechanism seems to only apply to the compositional representations of recurrentbased models. Moreover, the width of the CNN is 3 which might be insufficient to offset the impreciseness of our fixed width matching.</p><p>Finally, we compare SKIPFLOW LSTM with deep learning models 6 of <ref type="bibr" target="#b17">(Taghipour and Ng 2016)</ref>. The key difference is that these models (denoted with ? in <ref type="table" target="#tab_3">Table 2</ref>) have a higher dimensionality of d = 300. First, we observe that a higher dimensionality improves performance over d = 50. Our SKIPFLOW LSTM (Tensor) outperforms LSTM ? (d = 300) significantly by 5%. The performance of LSTM ? (d = 300) and GRU ? (d = 300) are in fact iden-tical and are only slightly better than feature engineering baselines such as EASE (BLRR). We also observe that ATT-LSTM and ML-LSTM (L=4) with both d = 50 also consistently outperform LSTM ? and GRU ? . Conversely, our SKIPFLOW LSTM (Tensor) model outperforms the best feature engineering baseline (EASE) by about 6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison against Published Results</head><p>Finally we compare with published state-of-the-art results from <ref type="bibr" target="#b17">(Taghipour and Ng 2016)</ref>. While our reproduction of the vanilla LSTM (d = 300)could not achieve similar results, our SKIPFLOW model still outperforms the reported results with a much smaller parameterization (d = 50 instead of d = 300). SKIPFLOW also remains competitive to an ensemble of 20 models (CNN + LSTM) with just a single model.   <ref type="table" target="#tab_7">Table 4</ref> reports the runtime and parameters of several LSTM variants. We observe that the runtime of our models only incur a small cost of 1-2 seconds over the baseline LSTM model. Our model also not only outperforms LSTM ? and ML-LSTM (L=4) in terms of QWK score but also in terms of memory footprint and runtime. SKIPFLOW is also faster then the attention mechanism (ATT-LSTM).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime and Memory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Hyperparameters</head><p>In this section, we investigate the effect of hyperparameters, namely the number of tensor slices k and the relevance width ?. While we report the results on the test set, it is good to note that the curves on the development set follow exactly the same rank and pattern.</p><p>Effect of Tensor Slices on Performance <ref type="figure" target="#fig_1">Figure 2</ref> shows the effect of the number of tensor slices (k) on performance. The prompts 7 are separated into two graphs due to the different ranges of results. The optimal k value is around 4 to 6 across all prompts. Intuitively, a small k (2) and an overly large k (8) often result in bad performance. The exception lies in prompts 5 and 6 where increasing the number of slices to k = 8 either improved or maintained the QWK score. Effect of Relevance Width ? on Performance <ref type="figure" target="#fig_2">Figure 3</ref> shows the influence of the hyperparameter relevance width ? on the performance. We observe that a small width produces worse results as compared to a large width. This is possibly due to insufficient tensor parameters or underfitting in lieu of a large number of matches is required with a small width. For example, consider prompt 8 that has the longest essays. Adopting ? = 20 for prompt 8 requires about ? 300 to 400 comparisons that have to be modeled by a fixed number of tensor parameters. A quick solution is to increase the size of the tensor. However, raising both ? and k would severely increase computational costs. Hence, a trade-off has to be made between ? and k. Empirical results show that a value from 50 to 100 for ? works best with k = 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed a new deep learning model for Automatic Text Scoring (ATS). We incorporated the intuition of textual coherence in neural ATS systems. Our model, SKIPFLOW LSTM, adopts parameterized tensor compositions to model the relationships between different points within an essay, generating neural coherence features that can support predictions. Our approach outperforms a baseline LSTM on the same setting by approximately 10% and also produces significantly better results as compared to multi-layered and attentional LSTMs. In addition, we also achieve a significant 6% improvement over feature engineering baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of our proposed SKIPFLOW LSTM model with width ?. Note that tensors depicted are shared parameters and there is only one tensor parameter in the entire architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Effect of tensor slices on performance with ? = 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Effect of relevance width ? on performance with tensor slices k = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This system is publicly available, open-source 4 and also took part in the ASAP competition and ranked third amongst 154 participants. EASE uses manual feature engineering and applies different regression techniques over the handcrafted features. Examples of the features of EASE include length-based features, POS tags and word overlap. We report the results of EASE with the settings of Support Vector Regression (SVR) and Bayesian Linear Ridge Regression (BLRR).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of all compared models on the ASAP dataset. Best result is in bold and 2nd best is underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparision against published works. Single model of SKIPFLOW outperforms model ensembles.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of runtime and parameter size on prompt 1. All models are d = 50 unless stated otherwise.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We set the starting position i &gt; 0 to avoid matching against the initial state.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? EASE -The major non deep learning system that we compare against is the Enhanced AI Scoring Engine (EASE).3 http://www.nltk.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://github.com/edx/ease</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For fair comparison, we only compare against single neural models and not against ensemble approaches.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We omit prompt 1 because it has a much higher score which distorts the visualization of our graphs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank anonymous reviewers of AAAI 2018, EMNLP 2017 and ACL 2017 whom have helped improve this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic text scoring using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alikaniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated essay scoring with e-rater R v. 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETS Research Report Series</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated essay scoring by maximizing human-machine agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1741" to="1752" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic features for essay scoring -an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="1072" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Implementation and applications of the intelligent essay assessor. Handbook of automated essay evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Streeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Lochbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="68" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sentence similarity measures for essay coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Evaluating multiple aspects of coherence in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic essay grading using text categorization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Larkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grading essays by computer: Progress report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the invitational Conference on Testing Problems</title>
		<meeting>the invitational Conference on Testing Problems</meeting>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flexible domain adaptation for automated essay scoring using correlated linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M A</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural tensor network architecture for community-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-25" />
			<biblScope unit="page" from="1305" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Automated essay scoring: A cross-disciplinary perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Shermis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Burstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">S</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lexical chaining for measuring discourse coherence quality in test-taker essays</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A neural approach to automated essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="1882" to="1891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to rank question answer pairs with holographic dual LSTM architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan, Au</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dyadic memory networks for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11-06" />
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Select-akibitzer: A computer tool that gives meaningful feedback on student compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wiemer-Hastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interactive learning environments</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="169" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Task-independent features for automated essay grading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wojatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scholten-Akoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, BEA@NAACL-HLT 2015</title>
		<meeting>the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, BEA@NAACL-HLT 2015<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-04" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
