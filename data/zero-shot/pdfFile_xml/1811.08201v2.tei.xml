<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CGNet: A Light-weight Context Guided Network for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
							<email>wutianyi@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<email>zhangrui@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CGNet: A Light-weight Context Guided Network for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The demand of applying semantic segmentation model on mobile devices has been increasing rapidly. Current state-of-the-art networks have enormous amount of parameters hence unsuitable for mobile devices, while other small memory footprint models follow the spirit of classification network and ignore the inherent characteristic of semantic segmentation. To tackle this problem, we propose a novel Context Guided Network (CGNet), which is a lightweight and efficient network for semantic segmentation. We first propose the Context Guided (CG) block, which learns the joint feature of both local feature and surrounding context, and further improves the joint feature with the global context. Based on the CG block, we develop CGNet which captures contextual information in all stages of the network and is specially tailored for increasing segmentation accuracy. CGNet is also elaborately designed to reduce the number of parameters and save memory footprint. Under an equivalent number of parameters, the proposed CGNet significantly outperforms existing segmentation networks. Extensive experiments on Cityscapes and CamVid datasets verify the effectiveness of the proposed approach. Specifically, without any post-processing and multi-scale testing, the proposed CGNet achieves 64.8% mean IoU on Cityscapes with less than 0.5 M parameters. The source code for the complete system can be found at https://github.com/wutianyiRosun/CGNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent interest in autonomous driving and robotic systems has a strong demand for deploying semantic segmentation models on mobile devices. It is significant and challenging to design a model with both small memory footprint and high accuracy. <ref type="figure" target="#fig_6">Fig. 1</ref> shows the accuracy and the number of parameters of different frameworks on Cityscapes <ref type="bibr" target="#b10">[11]</ref> dataset. High-accuracy methods, marked * Corresponding author: Sheng Tang (ts@ict.ac.cn)  <ref type="figure" target="#fig_6">Figure 1</ref>. Accuracy vs. the number of parameters on Cityscapes <ref type="bibr" target="#b10">[11]</ref> 1 . Blue points: high-accuracy methods. Red points: methods with small memory footprint. Compared with methods of small memory footprint, the proposed CGNet locates in the left-top since it has a lower number of parameters while achieving higher accuracy.</p><p>as blue points in <ref type="figure" target="#fig_6">Fig. 1</ref>, are transferred from deep image classification networks and have a huge amount of parameters, e.g. DFN <ref type="bibr" target="#b35">[36]</ref> of 44.8 M, DeepLabv3+ <ref type="bibr" target="#b9">[10]</ref> of 54.6 M and DenseASPP <ref type="bibr" target="#b33">[34]</ref> of 28.6 M. Therefore, most of these high-accuracy methods are unfit for being deployed on mobile devices. There are some models with small memory footprint, marked as red points in <ref type="figure" target="#fig_6">Fig. 1</ref>. Unfortunately, these small footprint methods get low segmentation accuracy, because they only follow the design principle of image classification but ignore the inherent property of semantic segmentation. To address the above issue, we propose a light-weight network specially tailored for semantic segmentation, named as Context Guided Network (CGNet). In order to improve the accuracy, we design a novel CGNet to exploit the inherent property of semantic segmentation. Spatial dependency and contextual information play <ref type="bibr" target="#b0">1</ref> The methods involved are Dilation8 <ref type="bibr" target="#b36">[37]</ref>, DeepLabv2 <ref type="bibr" target="#b6">[7]</ref>, SQNet <ref type="bibr" target="#b30">[31]</ref>, ENet <ref type="bibr" target="#b23">[24]</ref>, PSPNet <ref type="bibr" target="#b43">[44]</ref>, RefineNet <ref type="bibr" target="#b19">[20]</ref>, FRRN <ref type="bibr" target="#b24">[25]</ref>, FCN-8s <ref type="bibr" target="#b28">[29]</ref>, SegNet <ref type="bibr" target="#b1">[2]</ref>, ESPNet <ref type="bibr" target="#b21">[22]</ref>, ERFNet <ref type="bibr" target="#b26">[27]</ref>, ICNet <ref type="bibr" target="#b42">[43]</ref>, DenseASPP <ref type="bibr" target="#b33">[34]</ref>, DeepLabv3+ <ref type="bibr" target="#b9">[10]</ref>, DFN <ref type="bibr" target="#b35">[36]</ref>, BiSeNet <ref type="bibr" target="#b34">[35]</ref> and the proposed CGNet. an important role to improve accuracy, since semantic segmentation involves both pixel-level categorization and object localization. Thus, we present Context Guided (CG) block, which is the basic unit of CGNet, to model the spatial dependency and the semantic contextual information effectively and efficiently. Firstly, CG block learns the joint feature of both local feature and surrounding context. Thus, CG block learns the representation of each object from both itself and its spatially related objects, which contains rich co-occurrence relationship. Secondly, CG block employs the global context to improve the joint feature. The global context is applied to channel-wisely re-weight the joint feature, so as to emphasize useful components and suppress useless ones. Thirdly, the CG block is utilized in all stages of CGNet, from bottom to top. Thus, CGNet captures contextual information from both the semantic level (from deep layers) and the spatial level (from shallow layers), which is more fit for semantic segmentation compared with existing methods. Existing segmentation frameworks can be divided into two types: (1) Some methods named FCN-shape models follow the design principle of image classification and ignore the contextual information, e.g. ESPNet <ref type="bibr" target="#b21">[22]</ref>, ENet <ref type="bibr" target="#b34">[35]</ref> and FCN <ref type="bibr" target="#b28">[29]</ref>, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref> (a). (2) Other methods named FCN-CM models only capture contextual information from the semantic level by performing context module after the encoding stage, e.g. DPC <ref type="bibr" target="#b5">[6]</ref>, DenseA-SPP <ref type="bibr" target="#b33">[34]</ref>, DFN <ref type="bibr" target="#b35">[36]</ref> and PSPNet <ref type="bibr" target="#b43">[44]</ref>, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref> (b). In contrast, the structure of capturing context feature in all stages are more effective and efficient, as shown in <ref type="figure" target="#fig_2">Fig. 2 (c)</ref>. Fourthly, current mainstream segmentation networks have five down-sampling stages which learn too abstract features of objects and missing lots of the discriminative spatial information, causing over-smoothed segmentation boundaries. Differently, CGNet has only three downsampling stages, which is helpful for preserving spatial information.</p><p>Additionally, CGNet is elaborately designed to reduce the number of parameters. Firstly, it follows the principle of "deep and thin" to save memory footprint as much as possible. CGNet only contains 51 layers, and the number of channels in the three stages is 32, 64, 128, respectively. Compared with frameworks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b33">34]</ref> transferred from ResNet <ref type="bibr" target="#b12">[13]</ref> and DenseNet <ref type="bibr" target="#b15">[16]</ref> which contain hundreds of layers and thousands of channel numbers, CGNet is a light-weighted neural network. Secondly, to further reduce the number of parameters and save memory footprint, CG block adopts channel-wise convolutions, which removes the computational cost across channels. Finally, experiments on Cityscapes <ref type="bibr" target="#b10">[11]</ref> and CamVid <ref type="bibr" target="#b3">[4]</ref> verify the effectiveness and efficiency of the proposed CGNet. Without any preprocessing, post-processing, or complex upsampling, our model achieves 64.8% mean IoU on Cityscapes test set with less than 0.5 M parameters. We will release the code soon.  Our main contributions could be concluded as:</p><p>? We analyze the inherent property of semantic segmentation and propose CG block which learns the joint feature of both local feature and surrounding context and further improves the joint feature with global context.</p><p>? We design CGNet, which applies CG block to effectively and efficiently capture contextual information in all stages. The backbone of CGNet is particularly tailored for increasing segmentation accuracy.</p><p>? We elaborately design the architecture of CGNet to reduce the number of parameters and save memory footprint. Under an equivalent number of parameters, the proposed CGNet significantly outperforms existing segmentation networks (e.g., ENet and ESPNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we introduce related work on semantic segmentation, including small semantic segmentation models and contextual information models, as well as related work on attention models.</p><p>Small semantic segmentation models: Small semantic segmentation models require making a good trade-off between accuracy and model parameters or memory footprint. ENet <ref type="bibr" target="#b23">[24]</ref> proposes to discard the last stage of the model and shows that semantic segmentation is feasible on embedded devices. However, ICNet <ref type="bibr" target="#b42">[43]</ref> proposes a * *  compressed-PSPNet-based image cascade network to speed up the semantic segmentation. More recent ESPNet <ref type="bibr" target="#b22">[23]</ref> introduces a fast and efficient convolutional network for semantic segmentation of high-resolution images under resource constraints. Most of them follow the design principles of image classification, which makes them have poor segmentation accuracy. Contextual information models: Recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref> have shown that contextual information is helpful for models to predict high-quality segmentation results. One direction is to enlarge the receptive field of filter or construct a specific module to capture contextual information. Dilation8 <ref type="bibr" target="#b36">[37]</ref> employs multiple dilated convolutional layers after class likelihood maps to exercise multi-scale context aggregation. SAC <ref type="bibr" target="#b41">[42]</ref> proposes a scale-adaptive convolution to acquire flexible-size receptive fields. DeepLab-v3 <ref type="bibr" target="#b7">[8]</ref> employs Atrous Spatial Pyramid Pooling <ref type="bibr" target="#b6">[7]</ref> to capture useful contextual information with multiple scales.</p><p>Following this, the work <ref type="bibr" target="#b33">[34]</ref> introduces DenseASPP to connect a set of atrous convolutional layers for generating multi-scale features. However, the work <ref type="bibr" target="#b39">[40]</ref> proposes a Global-residual Refinement Network through exploiting global contextual information to predict the parsing residuals. PSPNet <ref type="bibr" target="#b43">[44]</ref> introduces four pooling branches to exploit global information from different subregions. By contrast, some other approaches directly construct information propagation model. SPN <ref type="bibr" target="#b20">[21]</ref> constructs a row/column linear propagation model to capture dense and global pairwise relationships in an image, and PSANet <ref type="bibr" target="#b44">[45]</ref> proposes to learn the adaptively point-wise context by employing bi-directional information propagation. Another direction is to use Conditional Random Field (CRF) to model the long-range dependencies. CRFasRNN <ref type="bibr" target="#b45">[46]</ref> reformulates DenseCRF with pairwise potential functions and unrolls the mean-field steps as recurrent neural networks, which composes a uniform framework and can be learned end-to-end. Differently, DeepLab frameworks <ref type="bibr" target="#b6">[7]</ref> use DenseCRF <ref type="bibr" target="#b18">[19]</ref> as post-processing. After that, many approaches combine CRFs and DCNNs in a uniform framework, such as combining Gaussian CRFs <ref type="bibr" target="#b4">[5]</ref> and specific pairwise potentials <ref type="bibr" target="#b16">[17]</ref>. More recently, CCL <ref type="bibr" target="#b11">[12]</ref> proposes a novel context contrasted local feature that not only leverages the informative context but also spotlights the local information in contrast to the context. DPC <ref type="bibr" target="#b5">[6]</ref> proposes to search for efficient multi-scale architectures by using architecture search techniques. Most of these works explore context information in the decoder phase and ignore surrounding context, since they take classification network as the backbone of the segmentation model. In contrast, our approach proposes to learn the joint feature of both local feature and surrounding context feature in the encoder phase, which is more rep-resentative for semantic segmentation than the feature extracted by the classification network.</p><p>Attention models: Recently, attention mechanism has been widely used for increasing model capability. RNNsearch <ref type="bibr" target="#b2">[3]</ref> proposes an attention model that softly weighs the importance of input words when predicting a target word for machine translation. Following this, SA <ref type="bibr" target="#b8">[9]</ref> proposes an attention mechanism that learns to softly weigh the features from different input scales when predicting the semantic label of a pixel. SENet <ref type="bibr" target="#b14">[15]</ref> proposes to recalibrate channel-wise feature responses by explicitly modeling interdependencies between channels for image classification. More recently, NL <ref type="bibr" target="#b31">[32]</ref> proposes to compute the response at a position as a weighted sum of the features at all positions for video classification. In contrast, we introduce the attention mechanism into semantic segmentation. Our proposed CG block uses the global contextual information to compute a weight vector, which is employed to refine the joint feature of both local feature and surrounding context feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this work, we develop CGNet, a light-weight neural network for semantic segmentation on mobile devices. In this section, we first elaborate the important component CG block. Then we present the details of the proposed CGNet. Finally, we compare CG block with similar units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Context Guided Block</head><p>The CG block is inspired by the human visual system, which depends on contextual information to understand the scene. As shown in <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>, suppose the human visual system tries to recognize the yellow region, which is difficult if we only pay attention to this region itself. In <ref type="figure" target="#fig_3">Fig. 3</ref> (b), we define the red region as the surrounding context of the yellow region. If both the yellow region and its surrounding context are obtained, it is easier to assign the category to the yellow region. Therefore, the surrounding context is helpful for semantic segmentation. For <ref type="figure" target="#fig_3">Fig. 3 (c)</ref>, if the human visual system further captures the global context of the whole scene (purple region) along with the yellow region and its surrounding context (red region), it has a higher degree of confidence to categorize the yellow region. Therefore, both surrounding context and global context are helpful for improving the segmentation accuracy.</p><p>Based on the above analysis, we introduce CG block to take full advantage of local feature, surrounding context and global context. CG block consists of a local feature extractor f loc ( * ), a surrounding context extractor f sur ( * ) , a joint feature extractor f joi ( * ), and a global context extractor f glo ( * ), as shown in <ref type="figure" target="#fig_3">Fig. 3 (d)</ref>. CG block contains two main steps. In the first step, f loc ( * ) and f sur ( * ) is employed to learn local feature and the corresponding surrounding context respectively. f loc ( * ) is instantiated as 3 ? 3 standard convolutional layer to learn the local feature from the 8 neighboring feature vectors, corresponding to the yellow region in <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>. Meanwhile, f sur ( * ) is instantiated as a 3 ? 3 atrous/dilated convolutional layer since atrous/dilated convolution has a relatively large receptive field to learn the surrounding context efficiently, corresponding to the red region in <ref type="figure" target="#fig_3">Fig. 3 (b)</ref>. Thus, f joi ( * ) obtains the joint feature from the output of f loc ( * ) and f sur ( * ). We simply design f joi ( * ) as a concatenation layer followed by the Batch Normalization (BN) and Parametric ReLU (PReLU) operators. In the second step, f glo ( * ) extracts global context to improve the joint feature. Inspired by SENet <ref type="bibr" target="#b14">[15]</ref>, the global context is treated as a weighted vector and is applied to channel-wisely refine the joint feature, so as to emphasize useful components and suppress useless one. In practice, we instantiate f glo ( * ) as a global average pooling layer to aggregate the global context corresponding to the purple region in <ref type="figure" target="#fig_3">Fig. 3 (c)</ref>, followed by a multilayer perceptron to further extract the global context. Finally, we employ a scale layer to re-weight the joint feature with the extracted global context. Note that the refining operation of f glo ( * ) is adaptive for the input image since the extracted global context is generated from the input image. Furthermore, the proposed CG block employs residual learning <ref type="bibr" target="#b12">[13]</ref> which helps to learn highly complex features and to improve gradient back-propagation during training. There are two types of residual connection in the proposed CG block. One is local residual learning (LRL), which connects input and the joint feature extractor f joi ( * ). The other is global residual learning (GRL), which bridges input and the global feature extractor f glo ( * ). <ref type="figure" target="#fig_4">Fig. 4 (a)</ref> and (b) show these two cases, respectively. Intuitively, GRL has a stronger capability than LRL to promote the flow of information in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Context Guided Network</head><p>Based on the proposed CG block, we elaborately design the structure of CGNet to reduce the number of parameters, as shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. CGNet follows the major principle of   "deep and thin" to save memory footprint as much as possible. Different from frameworks transferred from deep image classification networks which contain hundreds of layers and thousands of channel numbers, CGNet only consists of 51 convolutional layers with small channel numbers. In order to better preserve the discriminative spatial information, CGNet has only three down-sampling stages and obtains 1/8 feature map resolution, which is much different from mainstream segmentation networks with five downsampling stages and 1/32 feature map resolution. The detailed architecture of our proposed CGNet is presented in Tab. 1. In stage 1, we stack only three standard convolutional layers to obtain the feature map of 1/2 resolution, while in stage 2 and 3, we stack M and N CG blocks to downsample the feature map to 1/4 and 1/8 of the input image respectively. For stage 2 and 3, the input of their first layer are gained from combining the first and last blocks of their previous stages, which encourages feature reuse and strengthen feature propagation. In order to improve the flow of information in CGNet, we take the input injection mechanism which additionally feeds 1/4 and 1/8 downsampled input image to stage 2 and stage 3 respectively. Finally, a 1 ? 1 convolutional layer is employed to produce the segmentation prediction. Note that CG block is employed in all units of stage 2 and 3, which means CG block is utilized almost in all the stages of CGNet. Therefore, CGNet has the capability of aggre-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Input Injection mIoU (%) CGNet M3N15 w/o 59.4 CGNet M3N15 w 59.7 gating contextual information from bottom to top, in both semantic level from deep layers and spatial level from shallow layers. Compared with existing segmentation frameworks which ignore the contextual information or only capture contextual information from the semantic level by performing context module after the encoding stage, the structure of CGNet is elaborately tailored for semantic segmentation to improve the accuracy. Furthermore, in order to further reduce the number of parameters, feature extractor f loc ( * ) and f sur ( * ) employ channel-wise convolutions, which remove the computational cost across channels and save much memory footprint. The previous work <ref type="bibr" target="#b13">[14]</ref> employs 1 ? 1 convolutional layer followed channel-wise convolutions for promoting the flow of information between channels. However, this design is not suitable for the proposed CG block, since the local feature and the surrounding context in CG block need to maintain channel independence. Additional experiments also verify this observation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparision with Similar Works</head><p>ENet unit <ref type="bibr" target="#b23">[24]</ref>  unit. When f sur ( * ) = 0 and f glo ( * ) = 1, our proposed CG block will degenerate to MobileNet unit. ESP unit <ref type="bibr" target="#b21">[22]</ref> employs K parallel dilated convolutional kernels with different dilation rates to learn multi-scale features. Inception unit <ref type="bibr" target="#b29">[30]</ref> is proposed to approximate a sparse structure and process multi-scale visual information for image classification. CCL unit <ref type="bibr" target="#b11">[12]</ref> leverages the informative context and spotlights the local information in contrast to the context, which is proposed to learn locally discriminative features form block3, block4 and block5 of ResNet-101, and fuse different scale features through gated sum scheme in the decoder phase. In contrast to them, the CG block is proposed to learn the joint feature of both local feature and surrounding context feature in the encoder phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the proposed CGNet on Cityscapes <ref type="bibr" target="#b10">[11]</ref> and CamVid <ref type="bibr" target="#b3">[4]</ref>. Firstly, we introduce the datasets and the implementation protocol. Then the contributions of each component are investigated in ablation experiments on Cityscapes validation set. Finally, we perform comprehensive experiments on Cityscapes and CamVid benchmarks and compare with the state-of-the-art works to verify the effectiveness of CGNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Cityscapes Dataset The Cityscapes dataset contains 5, 000 images collected in street scenes from 50 different cities. The dataset is divided into three subsets, including 2, 975 images in training set, 500 images in validation set and 1, 525 images in testing set. High-quality pixellevel annotations of 19 semantic classes are provided in this dataset. Segmentation performances are reported using the commonly Intersection-over-Union (IoU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CamVid Dataset</head><p>The CamVid is a road scene dataset from the perspective of a driving automobile. The dataset involves 367 training images, 101 validation images and 233 testing images. The images have a resolution of 480 ? 360. The performance is measured by pixel intersectionover-union (IoU) averaged across the 11 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Residual connections mIoU (%) CGNet M3N21 LRL 57.2 CGNet M3N21 GRL 63.5 Implementation protocol All the experiments are performed on the PyTorch platform with 2? V100 GPU. We employ the "poly" learning rate policy, in which we set base learning rate to 0.001 and power to 0.9. For optimization, we use ADAM <ref type="bibr" target="#b17">[18]</ref> with batch size 14, be-tas=(0.9, 0.999), and weight decay 0.0005 in training. For data augmentation, we employ random mirror, the mean subtraction and random scale on the input images to augment the dataset during training. The random scale contains {0.5, 0.75, 1, 1.5, 1.75, 2.0}. The iteration number is set to 60K for Cityscapes and CamVid. For all experiments, we use a single-scale evaluation to compute mean IoU. Our loss function is the sum of cross-entropy terms for each spatial position in the output score map, ignoring the unlabeled pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>Ablation Study for Surrounding Context Extractor We adopt three schemes to evaluate the effectiveness of surrounding context extractor f sur ( * ). It is clear that the second and third schemes can improve the accuracy by 0.8% and 5.1% respectively, which shows surrounding context is very beneficial for increasing segmentation accuracy and should be employed in all blocks of the framework.</p><p>Ablation Study for Global Context Extractor We use global context to refine the joint feature learned by f joi ( * ).</p><p>As shown in Tab. 3, global context extractor can improve the accuracy from 58.9% to 59.7%, which demonstrates that the global context extractor f glo ( * ) is desirable for the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Year  <ref type="table">Table 9</ref>. FLOPS, parameter, memory, and accuracy analysis on Cityscapes test set. FLOPS and Memory are estimated for an input of 3?640?360. The running times are computed with input size of 2048?1024. "-" indicates the approaches do not report the corresponding results."Ms" indicates employing multi-scale inputs with average fusion during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study for the Input Injection Mechanism</head><p>We take input injection mechanism that refers to downsampling the input image to the resolution of stage 2 and stage 3, and injecting them into the corresponding stage. As shown in Tab. 4, this mechanism can improve the accuracy from 59.4% to 59.7%. Intuitively, this performance improvement comes from Input Injection mechanism which increases the flow of information on the network.</p><p>Ablation Study for Activation Function We compare ReLU and PReLU in CGNet M3N15, as shown in Tab. 5. Using PReLU can improve performance from 58.1% to 59.7%. Therefore, we choose PReLU as the activation function of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study for Network Depth</head><p>We train the proposed CGNet with different block nums at each stage and shows the trade-offs between the accuracy and the number of parameters, as shown in Tab. 6. In general, deep networks perform better than shallow ones at the expense of increased computational cost and model size. From Tab. 6, we can find that segmentation accuracy does not increase as M increases when fixing N. For example, we fix N = 12 and change M from 3 to 6, the mean IoU drops by 0.2 points. So we set M = 3 (the number of CG blocks in stage 2) for CGNet. Furthermore, we compromise between accuracy and model size by setting different N (the number of CG blocks in stage 3). Our approach achieves the highest mean IoU of 63.5% on Cityscapes validation set when M=3, N=21.</p><p>Ablation Study for Residual Learning Inspired by <ref type="bibr" target="#b12">[13]</ref>, residual learning is employed in CG block to further improve the information flow. From Tab. 7, compared with LRL, we can find that GRL can improve the accuracy from 57.2% to 63.5%. One possible reason is that the GRL has a stronger ability to promote the flow of information in the network, so we choose GRL in the proposed CG block.</p><p>Ablation Study for Inter-channel Interaction Previous work <ref type="bibr" target="#b13">[14]</ref> employs a 1?1 convolution followed by channel-wise convolution to improve the flow of information between channels and promote inter-channel interaction. Here, We try the 1?1 convolution in CG block but find it damage the segmentation accuracy. As shown in Tab. 8, we can improve the accuracy from 53.3% to 63.5% by removing the 1?1 convolutions. In other words, this interaction mechanism in our CG block hampers the accuracy of our models severely. One possible reason is that the local feature and the surrounding context feature need to maintain channel independence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with state-of-the-arts</head><p>Efficiency analysis Tab. 9 reports a comparison of FLOPS (floating point operations), memory footprint, running time and parameters of different models. FLOPS and Memory are estimated with an input size of 3 ? 640 ? 360, and the running times are computed with an input size of 2048 ? 1024. According to the 3 th and 4 th columns, the FLOPS and Parameters are very close to ENet which is the current smallest semantic segmentation model, yet our method has 6.5% improvement over ENet. Furthermore, the accuracy of our approach is 4.5% higher than the very recent model ESPNet <ref type="bibr" target="#b21">[22]</ref> which is based on an efficient spatial pyramid module. With such a few parameters and FLOPS, the proposed CGNet is very suitable to be deployed in mobile devices. Furthermore, compared with deep and state-of-the-art semantic segmentation networks, CGNet M3N21 is 131 and 57 times smaller than PSPNet <ref type="bibr" target="#b43">[44]</ref> and DenseASPP <ref type="bibr" target="#b33">[34]</ref>, while its category-wise accuracy is only 5.4% and 5.5% less respectively. On the other hand, the memory requirement of the proposed model is 334.0 M, which is 10? less than DenseASPP <ref type="bibr" target="#b33">[34]</ref>   <ref type="table" target="#tab_0">Table 11</ref>. Accuracy comparison of our method against other semantic segmentation methods on Camvid test set. "-" indicates that the approaches do not report the corresponding results.</p><p>Accuracy analysis We report the evaluation results of the proposed CGNet M3N21 on Cityscapes test set and compare to other state-of-the-art methods in Tab. 10. Without any pre-processing, post-processing, or any decoder modules (such as ASPP <ref type="bibr" target="#b6">[7]</ref>, PPM <ref type="bibr" target="#b43">[44]</ref>), our CGNet M3N21 achieves 64.8% in terms of mean IoU (only training on fine annotated images). Note that we do not employ any testing tricks, like multi-scale or complex upsampling. We list the number of model parameters and the segmentation accuracy in Tab. 10. Compared with the methods that do not require pretraining on ImageNet, our CGNet M3N21 achieves a relatively large accuracy gain. For example, the mean IoU of proposed CGNet M3N21 is about 6.5% and 4.5% higher than ENet <ref type="bibr" target="#b23">[24]</ref> and ESPNet <ref type="bibr" target="#b21">[22]</ref> with almost no increase of the model parameters. Besides, it is even quantitatively better than the methods that are pretrained on ImageNet without consideration of memory footprint and speed, such as SegNet <ref type="bibr" target="#b1">[2]</ref>, and the model parameters of CGNet M3N21 is about 60 times smaller than it. We visualize some segmentation results on the validation set of  <ref type="figure" target="#fig_7">Fig. 6</ref>. Tab. 11 shows the accuracy result of the proposed CGNet M3N21 on CamVid dataset. We use the training set and validation set to train our model. Here, we use 480?360 resolution for training and evaluation. The number of parameters of CGNet M3N21 is close to the current smallest semantic segmentation model ENet <ref type="bibr" target="#b23">[24]</ref>, and the accuracy of our method is 14.3% higher than it. The proposed CGNet is just lower 1.9% than FCN (Res101) with output stride of 8, and the parameters of CGNet is 110? less than it (0.5 M vs. 56.8 M), which further verifies the effectiveness and efficiency of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cityscapes in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we rethink semantic segmentation from its characteristic which involves image recognition and object localization. Furthermore, we propose a novel Context Guided block for learning the joint feature of both local fea-ture and the surrounding context. Based on Context Guided block, we develop a light-weight Context Guided Network for semantic segmentation, and our model allows very memory-efficient inference, which significantly enhances the practicality of semantic segmentation in real-world scenarios. Experiments on the Cityscapes and CamVid show that the proposed CGNet provides a general and effective solution for achieving high-quality segmentation results under the case of resource limited.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Alternative architectures for semantic segmentation. CM: context modules, CF: context features. (a) FCN-shape models follow the design principle of image classification and ignore contextual information. (b) FCN-CM models only capture contextual information from the semantic level by performing a context module after the encoding stage. (c) The proposed CGNet captures context features in all stages, from both semantic level and spatial level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>An overview of the Context Guided block. (a) It is difficult to categorize the yellow region when we only pay attention to the yellow region itself. (b) It is easier to recognize the yellow region with the help of its surrounding context (red region). (c) Intuitively, we can categorize the yellow region with a higher degree of confidence when we further consider the global contextual information (purple region). (d) The structure of Context Guided block, which consists of local feature extractor f loc ( * ), surrounding context extractor fsur( * ), joint feature extractor fjoi( * ), and global context extractor f glo ( * ). (?) represents element-wise multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Structure of Local Residual Learning (LRL) and Global Residual Learning (GRL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The architecture of the proposed Context Guided Network. "M"and "N" are the number of CG blocks in stage 2 and stage 3 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 1 )</head><label>1</label><figDesc>No: CGNet M3N15 model does not employ f sur ( * ), and is configured with the same number of parameters by increasing the number of channels. (2) Single: the surrounding context extractor f sur ( * ) is employed only in the last block of the framework. (3) Full: the surrounding context extractor f sur ( * ) is employed in all blocks of the framework. Results are shown in Tab. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Result illustration of CGNet on Cityscapes validation set. From left to right: Input image, prediction of CGNet M3N21 without surrounding context feature extractor fsur( * ), prediction of CGNet M3N21 without global context feature extractor f glo ( * ), prediction of CGNet M3N21 and ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The CGNet architecture for Cityscapes. Input size is 3 ? 680 ? 680. "Conv" represents the operators of Conv-BN-PReLU. "r" is the rate of Atrous/dilated convolution in surrounding context extractor fsur( * ). "M" and "N" are the number of CG blocks in stage 2 and stage 3 respectively.</figDesc><table><row><cell>Name</cell><cell>Type</cell><cell cols="2">Channel Output size</cell></row><row><cell></cell><cell>3?3 Conv (stride=2)</cell><cell>32</cell><cell>340 ? 340</cell></row><row><cell cols="2">stage 1 3?3 Conv (stride=1)</cell><cell>32</cell><cell>340 ? 340</cell></row><row><cell></cell><cell>3?3 Conv (stride=1)</cell><cell>32</cell><cell>340 ?340</cell></row><row><cell cols="2">stage 2 CG block (r=2) ? M</cell><cell>64</cell><cell>170 ? 170</cell></row><row><cell cols="2">stage 3 CG block (r=4) ? N</cell><cell>128</cell><cell>85 ? 85</cell></row><row><cell></cell><cell>1?1 Conv(stride=1)</cell><cell>19</cell><cell>85 ? 85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Evaluation results of global context extractor on Cityscapes validation set. Here we set M=3, N=15.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The effectiveness of Input Injection mechanism.</figDesc><table><row><cell>Here we</cell></row></table><note>Table 5. The effectiveness of ReLU and PReLU. Here we set M=3, N=15.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>employs a main convolutional layer to extract single-scale feature, which results in lacking of local features in the deeper layers of the network and lacking surrounding context at the shallow layers of the network. MobileNet unit<ref type="bibr" target="#b13">[14]</ref> employs a depth-wise separable convolution that factorizes standard convolutions into depth-wise convolutions and point-wise convolutions. Our proposed CG block can be treated as the generalization of MobileNet</figDesc><table><row><cell cols="4">M N Parameters (M) mIoU (%)</cell></row><row><cell>3</cell><cell>9</cell><cell>0.34</cell><cell>56.5</cell></row><row><cell>3</cell><cell>12</cell><cell>0.38</cell><cell>58.1</cell></row><row><cell>6</cell><cell>12</cell><cell>0.39</cell><cell>57.9</cell></row><row><cell>3</cell><cell>15</cell><cell>0.41</cell><cell>59.7</cell></row><row><cell>6</cell><cell>15</cell><cell>0.41</cell><cell>58.4</cell></row><row><cell>3</cell><cell>18</cell><cell>0.45</cell><cell>61.1</cell></row><row><cell>3</cell><cell>21</cell><cell>0.49</cell><cell>63.5</cell></row><row><cell cols="4">Table 6. Evaluation results of CGNet with different M and N on</cell></row><row><cell cols="4">Cityscapes validation set. M: the number of CG blocks in stage 2;</cell></row><row><cell cols="3">N: the number of CG blocks in stage 3.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>The effectiveness of local residual learning (LRL) and global residual learning (GRL). Here we set M=3, N=21.</figDesc><table><row><cell>Method</cell><cell cols="2">1x1 Conv mIoU (%)</cell></row><row><cell>CGNet M3N21</cell><cell>w/</cell><cell>53.3</cell></row><row><cell>CGNet M3N21</cell><cell>w/o</cell><cell>63.5</cell></row><row><cell cols="3">Table 8. The effectiveness of Inter-channel interaction. Here we</cell></row><row><cell>set M=3, N=21.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>FLOPS (G) ? Parameters (M) ? Memory (M) ? mIoU (%) ? Time (ms) ?</figDesc><table><row><cell>RefineNet [20]</cell><cell>'17</cell><cell>428.3</cell><cell>118.4</cell><cell>1986.5</cell><cell>73.6</cell><cell>&gt;1000</cell></row><row><cell>PSPNet Ms [44]</cell><cell>'17</cell><cell>453.6</cell><cell>65.6</cell><cell>2180.6</cell><cell>78.4</cell><cell>&gt;1000</cell></row><row><cell cols="2">DenseASP Ms [34] '18</cell><cell>214.7</cell><cell>28.6</cell><cell>3997.5</cell><cell>80.6</cell><cell>&gt;500</cell></row><row><cell>SegNet [2]</cell><cell>'15</cell><cell>286.0</cell><cell>29.5</cell><cell>-</cell><cell>56.1</cell><cell>89.2</cell></row><row><cell>ENet [24]</cell><cell>'16</cell><cell>3.8</cell><cell>0.4</cell><cell>-</cell><cell>58.3</cell><cell>19.3</cell></row><row><cell>ERFNet [26]</cell><cell>'17</cell><cell>21.0</cell><cell>2.1</cell><cell>-</cell><cell>68.0</cell><cell>-</cell></row><row><cell>ESPNet [22]</cell><cell>'18</cell><cell>4.0</cell><cell>0.4</cell><cell>-</cell><cell>60.3</cell><cell>-</cell></row><row><cell>MobileNetV2 [28]</cell><cell>'18</cell><cell>9.1</cell><cell>2.1</cell><cell>-</cell><cell>70.2</cell><cell>-</cell></row><row><cell>HRFR [41]</cell><cell>'18</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.4</cell><cell>778.6</cell></row><row><cell>CGNet M3N21</cell><cell>-</cell><cell>6.0</cell><cell>0.5</cell><cell>334.0</cell><cell>64.8</cell><cell>56.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 .</head><label>10</label><figDesc>Accuracy comparison of our method against other small or high-accuracy semantic segmentation methods on Cityscapes test set, only training with the fine set. 'Pretrain" refers to the models that have been pretrained using external data like ImageNet, and "-" indicates that the approaches do not report the corresponding results.</figDesc><table><row><cell>(334.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multi-scale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning sparse high dimensional filters: Image filtering, dense crfs and bilateral neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Refinenet: Multipath refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">De</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06815</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A deep cnn-based framework for enhanced aerial imagery registration with applications to uav geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Elhakim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhelw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fullresolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TITS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schuberth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Tree-structured kronecker convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04945</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00916</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Global-residual and local-boundary refinement networks for rectifying scene parsing predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">High resolution feature recovering for accelerating urban scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Scaleadaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
