<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
							<email>miyazawa.kazuyuki@cw.mitsubishielectric.co.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Mitsubishi Electric</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action localization is an important yet challenging problem. Given a long, untrimmed video consisting of multiple action instances and complex background contents, we need not only to recognize their action categories, but also to localize the start time and end time of each instance. Many state-of-the-art systems use segmentlevel classifiers to select and rank proposal segments of predetermined boundaries. However, a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. To this end, we design a novel Convolutional-De-Convolutional (CDC) network that places CDC filters on top of 3D ConvNets, which have been shown to be effective for abstracting action semantics but reduce the temporal length of the input data. The proposed CDC filter performs the required temporal upsampling and spatial downsampling operations simultaneously to predict actions at the frame-level granularity. It is unique in jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network in an end-toend manner efficiently. Our model not only achieves superior performance in detecting actions in every frame, but also significantly boosts the precision of localizing temporal boundaries. Finally, the CDC network demonstrates a very high efficiency with the ability to process 500 frames per second on a single GPU server. Source code and trained models are available online at https://bitbucket. org/columbiadvmm/cdc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, temporal action localization has drawn considerable interest in the computer vision community [25, <ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b36">36]</ref>. This task involves two components: <ref type="bibr" target="#b0">(1)</ref> determining whether a video contains specific actions (such as diving, jump, etc.) and (2) identifying temporal boundaries (start time and end time) of each action instance.</p><p>A typical framework used by many state-of-the-art systems <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b26">26]</ref> is fusing a large set of features and training classifiers that operate on sliding windows or segment proposals. Recently, an end-to-end deep learning framework called Segment-CNN (S-CNN) <ref type="bibr" target="#b47">[47]</ref> based on 3D ConvNets <ref type="bibr" target="#b61">[61]</ref> demonstrated superior performances both in efficiency and accuracy on standard benchmarks such as THUMOS <ref type="bibr">'14 [25]</ref>. S-CNN consists of a proposal <ref type="bibr">Figure 1.</ref> Our framework for precise temporal action localization. Given an input raw video, it is fed into our CDC localization network, which consists of 3D ConvNets for semantic abstraction and a novel CDC network for dense score prediction at the framelevel. Such fine-granular score sequences are combined with segment proposals to detect action instances with precise boundaries. network for generating candidate video segments and a localization network for predicting segment-level scores of action classes. Although the localization network can be optimized to select segments with high overlaps with ground truth action instances, the detected action boundaries are still retained and thus are restricted to the pre-determined boundaries of a fixed set of proposal segments.</p><p>As illustrated in <ref type="figure">Figure 1</ref>, our goal is to refine temporal boundaries from proposal segments to precisely localize boundaries of action instances. This motivates us to move beyond existing practices based on segment-level predictions, and explicitly focus on the issue of fine-grained, dense predictions in time. To achieve this goal, some existing techniques can be adapted: <ref type="bibr" target="#b0">(1)</ref> Single-frame classifiers operate on each frame individually; (2) Recurrent Neural Networks (RNN) further take into account temporal dependencies across frames. But both of them fail to explicitly model the spatio-temporal information in raw videos.</p><p>3D CNN <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b47">47]</ref> has been shown that it can learn spatiotemporal abstraction of high-level semantics directly from raw videos but loses granularity in time, which is important for precise localization, as mentioned above. For example, layers from conv1a to conv5b in the well-known C3D architecture <ref type="bibr" target="#b61">[61]</ref> reduce the temporal length of an input video by a factor of 8. In pixel-level semantic segmentation, deconvolution proves to be an effective upsampling method in both image <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b45">45]</ref> and video <ref type="bibr" target="#b62">[62]</ref> for producing output of the same resolution as the input. In our temporal localization problem, the temporal length of the output should be the same as the input video, but the spatial size should be reduced to 1x1. Therefore, we not only need to upsample in time but also need to downsample in space. To this end, we propose a novel Convolutional-De-Convolutional (CDC) filter, which performs convolution in space (for semantic abstraction) and de-convolution in time (for framelevel resolution) simultaneously. It is unique in jointly modeling the spatio-temporal interactions between summarizing high-level semantics in space and inferring fine-grained action dynamics in time. On top of 3D ConvNets, we stack multiple CDC layers to form our CDC network, which can achieve the aforementioned goal of temporal upsampling and spatial downsampling, and thereby can determine action categories and can refine boundaries of proposal segments to precisely localize action instances.</p><p>In summary, this paper makes three novel contributions: <ref type="bibr" target="#b0">(1)</ref> To the best of our knowledge, this is the first work to combine two reverse operations (i.e. convolution and deconvolution) into a joint CDC filter, which simultaneously conducts downsampling in space and upsampling in time to infer both high-level action semantics and temporal dynamics at a fine granularity in time.</p><p>(2) We build a CDC network using the proposed CDC filter to specifically address precise temporal action local-ization. The CDC network can be efficiently trained end-toend from raw videos to produce dense scores that are used to predict action instances with precise boundaries.</p><p>(3) Our model outperforms state-of-the-art methods in video per-frame action labeling and significantly boosts the precision of temporal action localization over a wide range of detection thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Action recognition and detection. Early works mainly focus on simple actions in well-controlled environments and can be found in recent surveys <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b2">3]</ref>. Recently, researchers have started investigating untrimmed videos in the wild and have designed various features and techniques. We briefly review the following that are also useful in temporal action localization: frame-level Convolutional Neural Networks (CNN) trained on ImageNet <ref type="bibr" target="#b44">[44]</ref> such as AlexNet <ref type="bibr" target="#b29">[29]</ref>, VGG <ref type="bibr" target="#b51">[51]</ref>, ResNet <ref type="bibr" target="#b16">[16]</ref>, etc.; 3D CNN architecture called C3D <ref type="bibr" target="#b61">[61]</ref> trained on a large-scale sports video dataset <ref type="bibr" target="#b27">[27]</ref> ; improved Dense Trajectory Feature (iDTF) <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b65">65]</ref> consisting of HOG, HOF, MBH features extracted along dense trajectories with camera motion influences eliminated; key frame selection <ref type="bibr" target="#b13">[13]</ref>; ConvNets adapted for using motion flow as input <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b67">67]</ref>; feature encoding with Fisher Vector (FV) <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b38">38]</ref> and VLAD <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b72">72]</ref>.</p><p>There are also studies on spatio-temporal action detection, which aim to detect action regions with bounding boxes over consecutive frames. Various methods have been developed, from the perspective of supervoxel merging <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b56">56]</ref>, tracking <ref type="bibr" target="#b70">[70,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b53">53]</ref>, object detection and linking <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b63">63]</ref>, spatio-temporal segmentation <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b71">71]</ref>, and leveraging still images <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>Temporal action localization. Gaidon et al. <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref> introduced the problem of temporally localizing actions in untrimmed videos, focusing on limited actions such as "drinking and smoking" <ref type="bibr" target="#b30">[30]</ref> and "open door and sit down" <ref type="bibr" target="#b8">[8]</ref>. Later, researchers worked on building largescale datasets consisting of complex action categories, such as THUMOS <ref type="bibr">[25,</ref><ref type="bibr" target="#b15">15]</ref> and MEXaction2 <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b58">58]</ref>, and datasets focusing on fine-grained actions <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b48">48]</ref> or activities of high-level semantics <ref type="bibr" target="#b17">[17]</ref>. The typical approach used in most systems <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b26">26]</ref> is extracting a pool of features, which are fed to train SVM classifiers, and then applying these classifiers on sliding windows or segment proposals for prediction. In order to design a model specific to temporal localization, Richard and Gall <ref type="bibr" target="#b43">[43]</ref> proposed using statistical length and language modeling to represent temporal and contextual structures. Heilbron et al. <ref type="bibr" target="#b18">[18]</ref> introduced a sparse learning framework for generating segment proposals of high recall.</p><p>Recently, deep learning methods showed improved performance in localizing action instances. RNN has been widely used to model temporal state transitions over frames: Escorcia et al. <ref type="bibr" target="#b9">[9]</ref> built a temporal action proposal system based on Long-Short Term Memory (LSTM); Yeung et al. <ref type="bibr" target="#b74">[74]</ref> used REINFORCE to learn decision policies for a RNN-based agent; Yeung et al. <ref type="bibr" target="#b73">[73]</ref> introduced MultiTHU-MOS dataset of multi-label annotations for every frame in THUMOS videos and defined a LSTM network to model multiple input and output connections; Yuan et al. <ref type="bibr" target="#b77">[77]</ref> proposed a pyramid of score distribution feature at the center of each sliding window to capture the motion information over multiple resolutions, and utilized RNN to improve inter-frame consistency; Sun et al. <ref type="bibr" target="#b60">[60]</ref> leveraged web images to train LSTM model when only video-level annotations are available. In addition, Lea et al. <ref type="bibr" target="#b31">[31]</ref> used temporal 1D convolution to capture scene changes when actions were being performed. Although RNN and temporal 1D convolution can model temporal dependencies among frames and make frame-level predictions, they are usually placed on top of deep ConvNets, which take a single frame as input, rather than directly modeling spatio-temporal characteristics in raw videos. Shou et al. <ref type="bibr" target="#b47">[47]</ref> proposed an end-to-end Segment-based 3D CNN framework (S-CNN), which outperformed other RNN-based methods by capturing spatio-temporal information simultaneously. However, S-CNN lacks the capability to predict at a fine time resolution and to localize precise temporal boundaries of action instances.</p><p>De-convolution and semantic segmentation. Zeiler et al. <ref type="bibr" target="#b79">[79]</ref> originally proposed de-convolutional networks for image decomposition, and later Zeiler and Fergus <ref type="bibr" target="#b78">[78]</ref> repurposed de-convolutional filter to map CNN activations back to the input to visualize where the activations come from. Long et al. <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b45">45]</ref> showed that deep learning based approaches can significantly boost performance in image semantic segmentation. They proposed Fully Convolutional Networks (FCN) to output feature maps of reduced dimensions, and then employed de-convolution for upsampling to make dense, pixel-level predictions. The fully convolutional architecture and learnable upsampling method are efficient and effective, and thus inspired many extensions <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b80">80,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b75">75]</ref>.</p><p>Recently, Tran et al. <ref type="bibr" target="#b62">[62]</ref> extended de-convolution from 2D to 3D and achieved competitive results on various voxellevel prediction tasks such as video semantic segmentation. This shows that de-convolution is also effective in the video domain and has the potential to be adapted for making dense predictions in time for our temporal action localization task. However, unlike the problem of semantic segmentation, we need to upsample in time but maintain downsampling in space. Instead of stacking a convolutional layer and a de-convolutional layer to conduct upsampling and downsampling separately, our proposed CDC filter learns a joint model to perform these two operations simultaneously, and proves to be more powerful and easier to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Convolutional-De-Convolutional networks</head><p>3.1. The need of downsampling and upsampling C3D architecture, consisting of 3D ConvNets followed by three Fully Connected (FC) layers, has achieved promising results in video analysis tasks such as recognition <ref type="bibr" target="#b61">[61]</ref> and localization <ref type="bibr" target="#b47">[47]</ref>. Further, Tran et al. <ref type="bibr" target="#b62">[62]</ref> experimentally demonstrated the 3D ConvNets, i.e. from conv1a to conv5b, to be effective in summarizing spatio-temporal patterns from raw videos into high-level semantics.</p><p>Therefore, we build our CDC network upon C3D. We adopt from conv1a to conv5b as the first part of our CDC network. For the rest of layers in C3D, we keep pool5 to perform max pooling in height and width by a factor of 2 but retain the temporal length. Following conventional settings <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b62">62]</ref>, we set the height and width of the CDC network input to 112x112. Given an input video segment of temporal length L, the output data shape of pool5 is (512, L/8, 4, 4) 1 . Now in order to predict the action class scores at the original temporal resolution (frame-level), we need to upsample in time (from L/8 back to L), and downsample in space (from 4x4 to 1x1). To this end, we propose the CDC filter and design a CDC network to adapt the FC layers from C3D to perform the required upsample and downsample operations. Details are described in Sections 3.2 and 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CDC filter</head><p>In this section, we walk through a concrete example of adapting FC6 layer in C3D to perform spatial downsampling by a factor of 4x4 and temporal upsampling by a factor of 2. For the sake of clarity, we focus on how a filter operates within one input channel and one output channel.</p><p>As explained in <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b45">45]</ref>, the FC layer is a special case of a convolutional layer (when the input data and the kernel have the same size and there is no striding and no padding). So we can transform FC6 into conv6, which is shown in <ref type="figure" target="#fig_0">Figure 2</ref> (a). Previously, a filter in FC6 takes a 4x4 feature map from pool5 as input and outputs a single value. Now, a filter in conv6 can slide on L/8 feature maps of size 4x4 stacked in time and respectively output L/8 values in time. The kernel size of conv6 is 4x4=16.</p><p>Although conv6 performs spatial downsampling, the temporal length remains unchanged. To upsample in time, as shown in <ref type="figure" target="#fig_0">Figure 2</ref> (b), a straightfoward solution adds a de-convolutional layer deconv6 after conv6 to double the temporal length while maintaining the spatial size. The kernel size of deconv6 is 2. Therefore, the total number of pa-rameters for this solution (separated conv6 and deconv6) is 4x4+2=18.</p><p>However, this solution conducts temporal upsampling and spatial downsampling in a separate manner. Instead, we propose the CDC filter CDC6 to jointly perform these two operations. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> (c), a CDC6 filter consists of two independent convolutional filters (the red one and the green one) operating on the same input 4x4 feature map. Each of these convolutional filters has the same kernel size as the filter in conv6 and separately outputs one single value. So each 4x4 feature map results in 2 outputs in time. As the CDC filter slides on L/8 feature maps of size 4x4 stacked in time, this input feature volume of temporal length L/8 is upsampled in time to L/4, and its spatial size is reduced to 1x1. Consequently, in space this CDC filter is equivalent to a 2D convolutional filter of kernel size 4x4; in time it has the same effect as a 1D de-convolutional filter of kernel size 2, stride 2, padding 0. The kernel size of such a joint filter in CDC6 is 2x4x4=32, which is larger than the separate convolution and de-convolution solution <ref type="bibr" target="#b18">(18)</ref>.</p><p>Therefore, a CDC filter is more powerful for jointly modeling high-level semantics and temporal dynamics: each output in time comes from an independent convolutional kernel dedicated to this output (the red/green node corresponds to the red/green kernel); however, in the separate convolution and de-convolution solution, different outputs in time share the same high-level semantics (the blue node) outputted by one single convolutional kernel (the blue one).</p><p>Having more parameters makes the CDC filter harder to learn. To remedy this issue, we propose a method to adapt the pre-trained FC6 layer in C3D to initialize CDC6. After we convert FC6 to conv6, conv6 and CDC6 have the same number of channels (i.e. 4,096) and thus the same number of filters. Each filter in conv6 can be used to initialize its corresponding filter in CDC6: the filter in conv6 (the blue one) has the same kernel size as each of these two convolutional filters (the red one and the green one) in the CDC6 filter and thus can serve as the initialization for them both.</p><p>Generally, assume that a CDC filter F of kernel size (k l , k h , k w ) takes the input receptive field X of height k h and width k w , and produces Y that consists of k l successive outputs in time. For the example given in <ref type="figure" target="#fig_0">Figure 2</ref> (c), we have k l = 2, k h = 4, k w = 4. Given the indices a ? {1, ..., k h } and b ? {1, ..., k w } in height and width respectively for X and the index c ? {1, ..., k l } in time for Y : during the forward pass, we can compute Y by</p><formula xml:id="formula_0">Y [c] = k h a=1 kw b=1 F [c, a, b] ? X [a, b];<label>(1)</label></formula><p>during the back-propagation, our CDC filter follows the chain rule and propagates gradients from Y to X via</p><formula xml:id="formula_1">X [a, b] = k l c=1 F [c, a, b] ?Y [c] .<label>(2)</label></formula><p>A CDC filter F can be regarded as coupling a series of convolutional filters (each one has kernel size k h in height and k w in width) in time with a shared input receptive field X, and at the same time, F performs 1D de-convolution with kernel size k l in time. In addition, the cross-channel mechanisms within a CDC layer and the way of adding biases to the outputs of the CDC filters follow the conventional strategies used in convolutional and de-convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Design of CDC network architecture</head><p>In <ref type="figure">Figure 3</ref>, we illustrate our CDC network for labeling every frame of a video. The final output shape of the CDC network is (K+1, L, 1, 1), where K+1 stands for K action categories plus the background class. As described in Section 3.1, from conv1a to pool5, the temporal length of an input segment has been reduced from L to L/8. On top of pool5, in order to make per-frame predictions, we adapt FC layers in C3D as CDC layers to perform temporal upsampling and spatial downsampling operations. Following previous de-convolution works <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b45">45]</ref>, we upsample in time by a factor of 2 in each CDC layer, to gradually increase temporal length from L/8 back to L.</p><p>In the previous Section 3.2, we provide an example of how to adapt FC6 as CDC6, performing temporal 1D deconvolution of kernel size 2, stride 2, padding 0. For CDC6 in the CDC network, we construct a CDC filter with 4 convolutional filters instead of 2, and thus its temporal kernel </p><formula xml:id="formula_2">Figure 3.</formula><p>Architecture of a typical CDC network. Following the notations indicated in the footnote 1, the top row lists the shape of output data at each layer. (1) A video segment is first fed into 3D ConvNets and the temporal length reduces from L to L/8. (2) CDC6 has kernel size <ref type="bibr" target="#b4">(4,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b4">4)</ref>, stride (2, 1, 1), padding (1, 0, 0), and therefore reduces both height and width to 1 while increases the temporal length from L/8 to L/4. Both CDC7 and CDC8 have kernel size (4, 1, 1), stride (2, 1, 1), padding (1, 0, 0), and hence both CDC7 and CDC8 further perform upsampling in time by a factor of 2, and thus the temporal length is back to L.</p><p>(3) A frame-wise softmax layer is added on top of CDC8 to obtain confidence scores for every frame. Each channel stands for one class. size in time increases from 2 to 4. We set the corresponding stride to 2 and padding to 1. Now each 4x4 feature map produces 4 output nodes, and every two consecutive feature maps have 2 nodes overlapping in time. Consequently, the temporal length of input is still upsampled by CDC6 from L/8 to L/4, but each output node sums contributions from two consecutive input feature maps, allowing temporal dynamics in input to be taken into account. Likewise, we can adapt FC7 as CDC7, as indicated in <ref type="figure">Figure 3</ref>. Additionally, we retain the Relu layers and the Dropout layers with 0.5 dropout ratio from C3D to attach to both CDC6 and CDC7. CDC8 corresponds to FC8 but cannot be directly adapted from FC8 because the classes in FC8 and CDC8 are different. Since each channel stands for one class, CDC8 has K+1 channels. Finally, the CDC8 output is fed into a frame-wise softmax layer Softmax to produce per-frame scores. During each mini-batch with N training segments, for the n-th segment, the CDC8 output O n has the shape (K+1, L, 1 ,1). For each frame, performing the conventional softmax operation and computing the softmax loss and gradient are independent of other frames. Corresponding to the t-th frame, the CDC8 output O n [t] and Softmax output P n [t] both are vectors of K+1 values. Note that for the i-th class, P (i)</p><formula xml:id="formula_3">n [t] = e O (i) n [t] K+1 j=1 e O (j) n [t] .</formula><p>The total loss L is defined as:</p><formula xml:id="formula_4">L = 1 N N n=1 L t=1 ? log P (zn) n [t] ,<label>(3)</label></formula><p>where z n stands for the ground truth class label for the n-th segment. The total gradient w.r.t the output of i-th channel/class and t-th frame in CDC8 is the summation over all N training segments of:</p><formula xml:id="formula_5">?L ?O (i) n [t] = 1 N ? P (zn) n [t] ? 1 if i = z n 1 N ? P (i) n [t] if i = z n .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and prediction</head><p>Training data construction. In theory, because both the convolutional filter and the CDC filter slide over the input, they can be applied to input of arbitrary size. Therefore, our CDC network can operate on videos of variable lengths. Due to GPU memory limitations, in practice we slide a temporal window of 32 frames without overlap on the video and feed each window individually into the CDC network to obtain dense predictions in time. From the temporal boundary annotations, we know the label of every frame. Frames in the same window can have different labels. To prevent including too many background frames for training, we only keep windows that have at least one frame belonging to actions. Therefore, given a set of training videos, we obtain a training collection of windows with frame-level labels.</p><p>Optimization. We use stochastic gradient descent to train the CDC network with the aforementioned frame-wise softmax loss. Our implementation is based on Caffe <ref type="bibr" target="#b24">[24]</ref> and C3D <ref type="bibr" target="#b61">[61]</ref>. The learning rate is set to 0.00001 for all layers except for CDC8 layer where the learning rate is 0.0001 since CDC8 is randomly initialized. Following conventional settings <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b47">47]</ref>, we set momentum to 0.9 and weight decay to 0.005. C3D <ref type="bibr" target="#b61">[61]</ref> is trained on Sports-1M <ref type="bibr" target="#b27">[27]</ref> and can be used to directly initialize conv1a to conv5b. CDC6 and CDC7 are initialized by FC6 and FC7 respectively using the strategy described in the Section 3.2. In addition, since FC8 in C3D and CDC8 in the CDC network have the different number of channels, we randomly initialize CDC8. With such initialization, our CDC network turns out to be very easy to train and converges quickly, i.e. 4 training epochs (within half a day) on THUMOS'14 .</p><p>Fine-grained prediction and precise localization. During testing, after applying the CDC network on the whole video, we can make predictions for every frame of the video. Through thresholding on confidence scores and grouping adjacent frames of the same label, it is possible to cut the video into segments and produce localization results. But this method is not robust to noise, and designing temporal smoothing strategies turns out to be ad hoc and nontrivial. Recently, researchers developed some efficient segment proposal methods <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b9">9]</ref> to generate a small set of candidate segments of high recall. Utilizing these proposals for our localization model not only bypasses the challenge of grouping adjacent frames, but also achieves considerable speedup during testing, because we only need to apply the CDC network on the proposal segments instead of the whole video.</p><p>Since these proposal segments only have coarse boundaries, we propose using fine-grained predictions from the CDC network to localize precise boundaries. First, to look at a wider interval, we extend each proposal segment's boundaries on both sides by the percentage ? of the original segment length. We set ? to 1/8 for all experiments. Then, similar to preparing training segments, we slide temporal windows without overlap on the test videos. We only need to keep test windows that overlap with at least one extended proposal segment. We feed these windows into our CDC network and generate per-frame action classes scores.</p><p>The category of each proposal segment is set to the class with the maximum average confidence score over all frames in the segment. If a proposal segment does not belong to the background class, we keep it and further refine its boundaries. Given the score sequence of the predicted class in the segment, we perform Gaussian kernel density estimation and obtain its mean ? and standard deviation ?. Starting from the boundary frame at each side of the extended segment and moving towards its middle, we shrink its temporal boundaries until we reach a frame with the confidence score no lower than ??. Finally, we set the prediction score of the segment to the average confidence score of the predicted class over frames in the refined segment of boundaries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Per-frame labeling</head><p>We first demonstrate the effectiveness of our model in predicting accurate labels for every frame. Note that this task can accept an input of multiple frames to take into account temporal information. We denote our model as CDC.</p><p>THUMOS <ref type="bibr">'14 [25]</ref>. The temporal action localization task in THUMOS Challenge 2014 involves 20 actions. We use 2,755 trimmed training videos and 1,010 untrimmed validation videos (3,007 action instances) to train our model. For testing, we use all 213 test videos (3,358 action instances) which are not entirely background videos.</p><p>Evaluation metrics. Following conventional metrics <ref type="bibr" target="#b73">[73]</ref>, we treat the per-frame labeling task as a retrieval problem. For each action class, we rank all frames in the test set by their confidence scores for that class and compute Average Precision (AP). Then we average over all classes to obtain mean AP (mAP). <ref type="table">Table 1</ref>, we first compare our CDC network (denoted by CDC) with some state-of-the-art models (results are quoted from <ref type="bibr" target="#b73">[73]</ref>): (1) Single-frame CNN: the frame-level 16-layer VGG CNN model <ref type="bibr" target="#b51">[51]</ref>; (2) Twostream CNN: the frame-level two-stream CNN model proposed in <ref type="bibr" target="#b50">[50]</ref>, which has one stream for pixel and one stream for optical flow; (3) LSTM: the basic per-frame labeling LSTM model of 512 hidden units <ref type="bibr" target="#b7">[7]</ref> on the top of VGG CNN FC7 layer; (4) MultiLSTM: a LSTM model developed by Yeung et al. <ref type="bibr" target="#b73">[73]</ref> to process multiple input frames together with temporal attention mechanism and output predictions for multiple frames. Single-frame CNN only takes into account appearance information. Twostream CNN models appearance and motion information separately. LSTM based models can capture temporal dependencies across frames but do not model motion explicitly. Our CDC model is based on 3D convolutional layers and CDC layers, which can operate on spatial and temporal dimensions simultaneously, achieving the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons. In</head><p>In addition, we compare CDC with other C3D based approaches that use different upsampling methods. (1) C3D + LinearInterp: we train a segment-level C3D using the same set of training segments whose segment-level labels are determined by the majority vote. During testing we perform linear interpolation to upsample segment-level predictions as frame-level. (2) Conv &amp; De-conv: CDC7 and CDC8 in our CDC network keep the spatial data shape unchanged and therefore can be also regarded as de-convolutional layers. For CDC6, we replace it with a convolutional layer conv6 and a separate de-convolutional layer deconv6 as shown in  <ref type="figure" target="#fig_0">Figure 2</ref> (b). The CDC model outperforms these baselines because the CDC filter can simultaneously model high-level semantics and temporal action dynamics. We also evaluate the CDC network with fixed weights in 3D ConvNets and only fine-tune CDC layers, resulting in a minor performance drop. This implies that it is helpful to train CDC networks in an end-to-end manner so that the 3D ConvNets part can be trained to summarize more discriminative information for CDC layers to infer more accurate temporal dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Temporal action localization</head><p>Given per-frame labeling results from the CDC network, we generate proposals, determine class category, and predict precise boundaries following Section 3.4. Our approach is applicable to any segment proposal method. Here we conduct experiments on THUMOS'14, and thus employ the publicly available proposals generated by the S-CNN proposal network <ref type="bibr" target="#b47">[47]</ref>, which achieves high recall on THU-MOS'14 . Finally, we follow <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b47">47]</ref> to perform standard post-processing steps such as non-maximum suppression.</p><p>Evaluation metrics. Localization performance is also evaluated by mAP. Each item in the rank list is a predicted segment. The prediction is correct when it has the correct category and its temporal overlap IoU with the ground truth is larger than the threshold. Redundant detections for the same ground truth instance are not allowed. <ref type="table">Table 2</ref>, CDC achieves much better results than all the other state-of-the-art methods, which have been reviewed in Section 2. Compared to the proposed CDC model: the typical approach of extracting a set of features to train SVM classifiers and then applying the trained classifiers on sliding windows or segment proposals (Karaman et al. <ref type="bibr" target="#b26">[26]</ref>, Wang et al. <ref type="bibr" target="#b66">[66]</ref>, Oneata et al. <ref type="bibr" target="#b39">[39]</ref>, Escorcia et al. <ref type="bibr" target="#b9">[9]</ref>) does not directly address the temporal localization problem. Systems encoding iDTF with FV (Heilbron et al. <ref type="bibr" target="#b18">[18]</ref>, Richard and Gall <ref type="bibr" target="#b43">[43]</ref>) cannot learn spatio-temporal patterns directly from raw videos to make predictions. RNN/LSTM based methods (Yeung et al. <ref type="bibr" target="#b74">[74]</ref>, Yuan et al. <ref type="bibr" target="#b77">[77]</ref>) are unable to explicitly capture motion information beyond temporal dependencies. S-CNN can effectively capture spatio-temporal patterns from raw videos but lacks the ability of adjusting boundaries from proposal candidates. With the proposed CDC filter, the CDC network can determine confidence scores at a fine granularity, beyond segment-level prediction, and hence precisely localize temporal boundaries. In addition, we employ per-frame predictions of other methods indicated in <ref type="table">Table 1</ref> (C3D + LinearInterp, Conv &amp; De-conv, CDC with fixed 3D Con-vNets ) to perform temporal localization based on S-CNN proposal segments. As shown in <ref type="table">Table 2</ref>, the performance of the CDC network is still better, because more accurate predictions at the same temporal granularity can be used to predict more accurate label and more precise boundaries for the same input proposal segment. In <ref type="figure" target="#fig_1">Figure 4</ref>, we illustrate how our model refines boundaries from segment proposal to precisely localize action instance in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons. As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussions</head><p>The necessity of predicting at a fine granularity in time.</p><p>In <ref type="figure">Figure 5</ref>, we compare CDC networks predicting action scores at different temporal granularities. When the temporal granularity increases, mAP increases accordingly. This demonstrates the importance of predicting at a finegranularity for achieving precise localization.</p><p>Efficiency analysis. The CDC network is compact and demands little storage, because it can be trained from raw videos directly to make fine-grained predictions in an endto-end manner without the need to cache intermediate features. A typical CDC network such as the example in <ref type="figure">Figure  3</ref> only requires around 1GB storage.</p><p>Our approach is also fast. Compared with segment-level prediction methods such as S-CNN localization network <ref type="bibr" target="#b47">[47]</ref>, CDC has to perform more operations due to the need of making predictions at every frame. Therefore, when the proposal segment is long, CDC is less efficient for the sake of achieving more accurate boundaries. But in the case of short proposal segments, since these proposals usually are densely overlapped, segment-level methods have to process a large number of segments one by one. However, CDC networks only need to process each frame once, and thus it can avoid redundant computations. On a NVIDIA Titan X GPU of 12GB memory, the speed of a CDC network is around 500 Frames Per Second (FPS), which means it can <ref type="bibr" target="#b76">76</ref>    <ref type="figure">Figure 5</ref>. mAP gradually increases when the temporal granularity of CDC network prediction increases from x1 (one label for every 8 frames) to x8 (one label per frame). Each point corresponds to x total upscaling factor (x CDC6 upscaling factor x CDC7 upscaling factor x CDC8 upscaling factor) in time. We conduct the evaluation on THUMOS'14 with IoU 0.5. process a 20s long video clip of 25 FPS within one second.</p><p>Temporal activity localization. Furthermore, we found that our approach is also useful for localizing activities of high-level semantics and complex components. We conduct experiments on ActivityNet Challenge 2016 dataset <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b1">2]</ref>, which involves 200 activities, and contains around 10K training videos (15K instances) and 5K validation videos (7.6K instances). Each video has an average of 1.65 instances with temporal annotations. We train on the training videos and test on the validation videos. Since no activity proposal results of high quality exist, we apply the mAP 0.5 0.75 0.95 Average-mAP before 45.  <ref type="table">Table 3</ref>. Temporal localization mAP on ActivityNet Challenge 2016 <ref type="bibr" target="#b1">[2]</ref> of Wang and Tao <ref type="bibr" target="#b68">[68]</ref> before and after the refinement step using our CDC network. We follow the official metrics used in <ref type="bibr" target="#b1">[2]</ref> to evaluate the average mAP.</p><p>trained CDC network to the results of the first place winner <ref type="bibr" target="#b68">[68]</ref> in this Challenge to localize more precise boundaries. As shown in <ref type="table">Table 5</ref>, they achieve high mAP when the IoU in evaluation is set to 0.5, but mAP drops rapidly when the evaluation IoU increases. After using the perframe predictions of our CDC network to refine temporal boundaries of their predicted segments, we gain significant improvements particularly when the evaluation IoU is high (i.e. 0.75). This means that after the refinement, these segments have more precise boundaries and have larger overlap with ground truth instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future works</head><p>In this paper, we propose a novel CDC filter to simultaneously perform spatial downsampling (for spatio-temporal semantic abstraction) and temporal upsampling (for precise temporal localization), and design a CDC network to predict actions at frame-level. Our model significantly outperforms all other methods both in the per-frame labeling task and the temporal action localization task. Supplementary descriptions of the implementation details and additional experimental results are available in <ref type="bibr" target="#b46">[46]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Additional justification of the motivation</head><p>As mentioned in the paper, the traditional approaches use segment-level detection, in which segment proposals are analyzed to predict the action class in each segment. Such approaches are limited by the fixed segment lengths and boundary locations, and thus inadequate for finding precise action boundaries. Here we proposed a novel model to first predict actions at fine-level and then use such fine-grained score sequences to accurately detect the action boundaries. The fine-grained score sequence also offers natural ways to determine the score threshold needed in refining boundaries at the frame level. Also, though not emphasized in the paper, the fine-level score sequence can also be used to select precise keyframes or discover sub-actions within an action.</p><p>Following the reviewer's suggestion, we also computed the frame-to-frame score gradient using the frame-level detection results. As shown in <ref type="figure">Figure 6</ref>, the frame-level gradient peaks nicely correlate with the action boundaries, confirming the intuition of using the fine-level detection results. Also, as shown in <ref type="figure">Figure 5</ref> in the paper, when the temporal granularity increases, localization performance increases accordingly. Finally, our motivation is quantitatively justified by the good results on two standard benchmarks as shown in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Additional implementation details</head><p>Temporal boundary refinement. Here, we provide details and pseudo-codes for temporal boundary refinement presented in Section 3.4. Algorithm 1 is used to refine boundaries of each proposal segment. Also, our source codes can be found at https://bitbucket.org/ columbiadvmm/cdc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Temporal Boundary Refinement</head><p>Input: A proposal segment of starting frame index t s and ending frame index t e , the percentage parameter of segment length expansion ?, the first frame index v s and the last frame index v e of the video containing the proposal segment, the total number of categories K Output: the refined starting frame index t s and ending frame index t e , the predicted category c, the predicted confidence score s 1. // Extend boundaries on both sides by the percentage of the original segment length 2. t s = max (v s , t s ? ? ? (t e ? t s + 1)) 3. t e = min (v e , t e + ? ? (t e ? t s + 1)) 4. // Feed frames into the CDC network to produce the confidence score matrix P ? (te?ts+1)?K 5. P = CDC(frames from t s to t e ) 6. assign c as the category with the maximum average confidence score over all frames from t s to t e 7. // Estimate the mean ? and the standard deviation Discussions about the window length used during creating mini-batches. During mini-batch construction, ideally we would like to set the window length as longer as possible. Therefore, when CDC processes each window, it can take into account more temporal contextual information. However, due to the limitation of the GPU memory, if the window length is too high, we have to set the number of training samples for each mini-batch to be very small, which will make the optimization unstable and thus the training procedure cannot converge well. Also, a long window usually contains much more background frames than action frames and thus we need to further handle the data imbalance issue. During experiments, we conduct a CliffDiving BG BG <ref type="figure">Figure 6</ref>. We use the frame-level detection scores (3rd row) to compute the absolute frame-to-frame score differences (4th row), which show high correlations with the true action boundaries.</p><p>grid search of window length in <ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512</ref> and empirically found that setting the window length to 32 frames is a good trade-off on a single NVIDIA Titan X GPU of 12GB memory: (1) we can include sufficient temporal contextual information to achieve good accuracy and <ref type="formula" target="#formula_1">(2)</ref> we can set the batch size as 8 to guarantee stable optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Additional experiments</head><p>Sensitivity analysis. When we extend the segment proposal, the percentage ? of the original proposal length should not be too small so that our model can consider a wider interval and not be too large to include too many irrelevant frames. As shown in <ref type="table" target="#tab_5">Table 4</ref>, the system has stable performances when ? varies within a reasonable range. Additional results on ActivityNet. We expand the comparisons on ActivityNet validation set to include results provided by additional top performers <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref> in ActivityNet Challenge 2016. As shown in <ref type="table">Table 5</ref>, our method CDC outperforms all other methods. As shown in <ref type="table">Table 6</ref>, CDC also performs the best on ActivityNet test set.</p><p>Discussions about other proposal methods. As shown in <ref type="table">Table 7</ref>, we evaluate temporal localization performances of CDC based on other proposals on THUMOS'14.</p><p>On ActivityNet, the proposals currently used in Section 4 from <ref type="bibr" target="#b68">[68]</ref> is a reasonable choice -its recall is 0.681 with 56K proposals when evaluate at IoU=0.5 on the validation set. We also have considered using other state-of-the-art proposal methods: (1) The ActivityNet challenge provides proposals computed by <ref type="bibr" target="#b18">[18]</ref>, but it has a low recall at 0.527 on the validation set with 441K proposals, which contain a lot of false alarms.</p><p>(2) DAPs <ref type="bibr" target="#b9">[9]</ref> advocates that train proposal model on THUMOS and then generalize the model to ActivityNet. Due to lack training data from ActivityNet, DAPs has a quite low recall at around 0.23 and is not a reasonable proposal candidate. (3) S-CNN <ref type="bibr" target="#b47">[47]</ref> is designed for instance-level detection. However, ground truth annotations in ActivityNet do not distinguish consecutive instancesone ground truth interval can contain multiple activity instances. Also, for activities of high-level semantics, it is ambiguous to define what is an individual activity instance.  <ref type="figure">Figure 7</ref>. Visualization of the process of refining temporal boundaries for an action proposal segment. Horizontal axis stands for time.</p><p>From the top to the bottom: (1) frame-level ground truths for a SoccerPenalty action instance in a test video with some representative frames; (2) a corresponding proposal segment; (3) the proposal segment after extension; (4) the per-frame score of being SoccerPenalty predicted by the CDC network; (5) the precisely predicted action instance after the refinement step using CDC.  Therefore, S-CNN does not suit ActivityNet.</p><p>Additional discussions about speed. For the sake of avoiding confusions, we would like to emphasize that the CDC network is end-to-end while the task of temporal localization is not end-to-end due to the need of combing with proposals and performing post-processing. Throughout the paper, the speed is also computed for the CDC network itself. Following C3D <ref type="bibr" target="#b61">[61]</ref>, each input frame has spatial resolution 128 ? 171 and will be cropped into 112 ? 112 as network input (random cropping during training and center cropping during testing). As indicated in <ref type="figure">Figure 3</ref>, each input video of L frames has the shape of (3, L, 112, 112). As aforementioned, on a single NVIDIA Titan X GPU of 12GB memory, the speed of a CDC network is around 500 Frames Per Second (FPS), which means it can process a 20s long video clip of 25 FPS within one second.</p><p>Additional visualization examples. As supplementary material to <ref type="figure" target="#fig_1">Figure 4</ref>, we provide additional examples to show the process of using Convolutional-De-Convolutional (CDC) model to refine the boundaries of proposal segments and achieve precise temporal action localization on THU-MOS'14 <ref type="bibr">[25]</ref>. As shown in <ref type="figure">Figure 7</ref> and <ref type="figure" target="#fig_6">Figure 8</ref>, the combination of the segment proposal and the CDC frame-level score prediction is powerful. The segment proposal allows for leveraging candidates of coarse boundaries to help handle the noisy outliers in the dipped score intervals such as shown in <ref type="figure" target="#fig_6">Figure 8</ref>. The proposed CDC model allows for fine-grained predictions at the frame level to help refine the segment boundaries in frame-level for precise localization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of how a filter in conv6, deconv6, CDC6 operates on pool5 output feature maps (grey rectangles) stacked in time. In each panel, dashed lines with the same color indicate the same filter sliding over time. Nodes stand for outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of the process of refining temporal boundaries for a proposal segment. Horizontal axis stands for time. From the top to the bottom: (1) frame-level ground truths for a CliffDiving instance in an input video with some representative frames; (2) a corresponding proposal segment; (3) the proposal segment after extension; (4) the per-frame score of detecting CliffDiving predicted by the CDC network; (5) the predicted action instance after the refinement using CDC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The project was supported by Mitsubishi Electric, and also by Award No. 2015-R2-CX-K025, awarded by the National Institute of Justice, Office of Justice Programs, U.S. Department of Justice. The opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect those of the Department of Justice. The Tesla K40 used for this research was donated by the NVIDIA Corporation. We thank Wei Family Private Foundation for their support for Zheng Shou, and anonymous reviewers for their valuable comments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>? 8. ?, ? = Gaussian Kernel Density Estimation(P [:, c]) 9. ? = ? ? ? // Compute the score threshold 10. // Refine the starting time 11. for i s = 1, 2, . . . , (t e ? t s + 1) do 12. if P [i s , c] &gt;= ? then 13. break 14. end if 15. end for 16. // Refine the ending time 17. for i e = (t e ? t s + 1) , . . . , 2, 1 do 18. if P [i e , c] &gt;= ? then 19. break 20. end if 21. end for 22. t e = t s + i e ? 1 23. t s = t s + i s ? 1 24. s = ie i=is P[i,c] te?ts+1 // Compute the average score 25. return t s , t e , c, s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of the process of refining temporal boundaries for an action proposal segment. Horizontal axis stands for time. From the top to the bottom: (1) frame-level ground truths for a JavelinThrow action instance in a test video with some representative frames; (2) a corresponding proposal segment; (3) the proposal segment after extension; (4) the per-frame score of being JavelinThrow predicted by the CDC network; (5) the precisely predicted action instance after the refinement step using CDC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>mAP on THUMOS'14 with the evaluation IoU set to 0.5 when we vary the extension percentage ? of the original proposal length from 1/8 to 1/4.</figDesc><table><row><cell>?</cell><cell>1/8</cell><cell>1/7</cell><cell>1/6</cell><cell>1/5</cell><cell>1/4</cell></row><row><cell cols="6">mAP 23.3 23.2 23.1 23.1 23.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Additional baseline results of temporal localization mAP on ActivityNet Challenge 2016<ref type="bibr" target="#b1">[2]</ref> validation set. The baseline results are kindly provided by the authors of<ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b68">68]</ref>. Comparisons of temporal localization mAP on Activi-tyNet Challenge 2016<ref type="bibr" target="#b1">[2]</ref> test set. The baseline results are quoted from the ActivityNet Challenge 2016 leaderboard<ref type="bibr" target="#b1">[2]</ref>. CDC (train) is training the CDC model on the training set only and CDC (train+val) uses the training set and the validation set together to train the CDC model.</figDesc><table><row><cell>IoU threshold</cell><cell cols="3">0.5 0.75 0.95 Ave-mAP</cell></row><row><cell cols="3">Singh and Cuzzolin [54] 22.7 10.8 0.3</cell><cell>11.3</cell></row><row><cell>Singh [52]</cell><cell cols="2">26.0 15.2 2.6</cell><cell>14.6</cell></row><row><cell>Wang and Tao [68]</cell><cell>45.1 4.1</cell><cell>0.0</cell><cell>16.4</cell></row><row><cell>CDC</cell><cell cols="2">45.3 26.0 0.2</cell><cell>23.8</cell></row><row><cell>IoU threshold</cell><cell cols="3">0.5 0.75 0.95 Ave-mAP</cell></row><row><cell cols="3">Singh and Cuzzolin [54] 36.4 11.1 0.1</cell><cell>17.8</cell></row><row><cell>Singh [52]</cell><cell cols="2">28.7 17.8 2.9</cell><cell>17.7</cell></row><row><cell>Wang and Tao [68]</cell><cell>42.5 2.9</cell><cell>0.1</cell><cell>14.6</cell></row><row><cell>CDC (train)</cell><cell cols="2">43.1 25.6 0.2</cell><cell>22.9</cell></row><row><cell>CDC (train+val)</cell><cell cols="2">43.0 25.7 0.2</cell><cell>22.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We denote the shape of data in the networks using the form of (number of channels, temporal length, height, width) and the size of feature map, kernel, stride, zero padding using (temporal length, height, width).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://mexculture.cnam.fr/xwiki/bin/view/Datasets/Mex+action+dataset" />
		<title level="m">Mexaction2</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://activity-net.org/challenges/2016/" />
	</analytic>
	<monogr>
		<title level="j">Activitynet challenge</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Computing Surveys</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Temporal action localization mAP on THUMOS&apos;14 as the overlap IoU threshold used in evaluation varies from 0.3 to 0.7. We evaluate our CDC model based on different proposal methods</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Actom sequence models for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal localization of actions with actoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://www.thumos.info/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ob-jects2action: Classifying and localizing actions without any video example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast saliency based pooling of fisher encoded dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human focused action localization in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsza?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends and Topics in Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Retrieving actions in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-scale patch aggregation (mpa) for simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spot on: Action localization from pointly-supervised proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The lear submission at thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and vision computing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised tube extraction using transductive learning and dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01515</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Much ado about time: Exhaustive annotation of temporal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HCOMP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR ActivityNet Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for finegrained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Untrimmed classification for activity detection: submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR ActivityNet Workshop</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Action localization in videos through context walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Predicting the where and what of actors and actions through online action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fast action localization in large scale video archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stoian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Crucianu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TCSVT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Scalable action localization with kernel-space hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stoian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benois-Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Crucianu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">What if we do not have multiple videos of the same action? -video action localization using web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep end2end voxel2voxel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Deep Learning in Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Apt: Action localization proposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Action Recognition by Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV THUMOS Workshop</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Uts at activitynet 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR ActivityNet Workshop</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A survey of visionbased methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Actor-action semantic segmentation with grouping process models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A discriminative cnn video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05738</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
