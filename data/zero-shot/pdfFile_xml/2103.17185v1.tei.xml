<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Kotovenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Heidelberg Collaboratory for Image Processing</orgName>
								<orgName type="institution" key="instit1">IWR</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Wright</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Heidelberg Collaboratory for Image Processing</orgName>
								<orgName type="institution" key="instit1">IWR</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Heimbrecht</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Heidelberg Collaboratory for Image Processing</orgName>
								<orgName type="institution" key="instit1">IWR</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Heidelberg Collaboratory for Image Processing</orgName>
								<orgName type="institution" key="instit1">IWR</orgName>
								<orgName type="institution" key="instit2">Heidelberg University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There have been many successful implementations of neural style transfer in recent years. In most of these works, the stylization process is confined to the pixel domain. However, we argue that this representation is unnatural because paintings usually consist of brushstrokes rather than pixels. We propose a method to stylize images by optimizing parameterized brushstrokes instead of pixels and further introduce a simple differentiable rendering mechanism. Our approach significantly improves visual quality and enables additional control over the stylization process such as controlling the flow of brushstrokes through user input. We provide qualitative and quantitative evaluations that show the efficacy of the proposed parameterized representation. Code is available at https://github. com / CompVis / brushstroke -parameterizedstyle-transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Style and texture transfer have been research topics for decades <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11]</ref>. More recently, the seminal work by Gatys et al. <ref type="bibr" target="#b12">[13]</ref> reformulated style transfer as the synthesis of an image combining content of one image with style of another image. Since then, a plethora of approaches have explored different aspects of the original problem. There are papers on feed-forward architectures <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51]</ref>, universal feed-forward models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b33">34]</ref>, disentanglement of style and content <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, ultra-resolution models <ref type="bibr" target="#b52">[53]</ref>, meta-learning techniques <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b58">59]</ref>, and video style transfer <ref type="bibr" target="#b5">[6]</ref>. Yet, the initial approach suggested by Gatys et al. <ref type="bibr" target="#b12">[13]</ref> remains one of the best in terms of image quality, especially in the artistic style transfer scenario, with one style image and one content image.</p><p>Recent works have advanced the field of style transfer and produced impressive results by introducing novel losses <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, adopting more suitable architectures <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>, imposing regularizations on the final image and intermediate latent representation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b49">50]</ref>, * Both authors contributed equally to this work. and even using different training paradigms <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b58">59]</ref>. However, they share a key commonality: the stylization pro-cess is confined to the pixel domain, almost as if style transfer is a special case of image-to-image translation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref>. We argue that the pixel representation is unnatural for the task of artistic style transfer: artists compose their paintings with brushstrokes, not with individual pixels. While position, color, shape, placement and interaction of brushstrokes play an important role in the creation of an artwork, small irregularities appearing on the pixel level like bristle marks, canvas texture or pigments are to some extent arbitrary and random.</p><p>With this in mind, we take a step back and rethink the original approach by suggesting a representation that inherently aligns with these characteristics by design. Just like learning to walk in the reinforcement learning setting starts with defining the set of constraints and degrees of freedom for individual joints, we restrict our representation to a collection of brushstrokes instead of pixels. Specifically, we parameterize a brushstroke with a B?zier curve and additional parameters for color, width, and location. To map these parameterized brushstrokes into the pixel domain we propose a lightweight, explicit, differentiable renderer which serves as a mapping between brushstroke parameters and pixels. Thus, this reparameterization can be seamlessly combined with other style transfer approaches. One crucial property that this rendering mechanism offers is a spatial relocation ability of groups of pixels. Standard optimization on the pixel level cannot directly move pixels across the image -instead it dims pixels in one area and highlights them in another area. Our model, however, parameterizes brushstrokes with location and shape, thus moving brushstrokes becomes a more natural transformation.</p><p>We validate the effectiveness of this reparameterization by coupling the renderer with the model Gatys et al. <ref type="bibr" target="#b12">[13]</ref> have suggested, see <ref type="figure" target="#fig_2">Fig. 4</ref>. We show that this simple shift of representation along with our rendering mechanism can outperform modern style transfer approaches in terms of stylization quality. This is measured using 1) the deception rate -how similar is the stylized image to the style of an artist 2) human deception rate -whether a human subject can distinguish cropouts of real artworks from cropouts of our stylization. In addition, we illustrate that the brushstroke representation offers more control. A user can control brushstrokes, change the flow of strokes in a neighbourhood.</p><p>We further conduct experiments on reconstructions of an image using our rendering mechanism. Huang et al. <ref type="bibr" target="#b22">[23]</ref> train a neural network that successively fits colored quadratic B?zier curves (brushstrokes) that approximate a target image. Our renderer can be applied to this task as well. It achieves almost 2 times smaller mean squared error (MSE) in the pixel space for a large number of strokes (1000 strokes) and 20% smaller MSE using 200 strokes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Style Transfer. Initially, Efros and Freeman <ref type="bibr" target="#b10">[11]</ref> performed texture synthesis and transfer using image quilting and Hertzmann et al. <ref type="bibr" target="#b19">[20]</ref> used a pair of images -one being a filtered version of the other -to learn a filter, which can then be applied to a new image. Wang et al. <ref type="bibr" target="#b51">[52]</ref> introduced a method for synthesizing directional textures. Besides that, there are works studying shape and morphology of images <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b41">42]</ref>. More recently, Gatys et al. <ref type="bibr" target="#b12">[13]</ref> proposed an iterative method for combining the content of one image with the style of another by jointly minimizing content and style losses, where the content loss compares the features of a pretrained VGG network <ref type="bibr" target="#b48">[49]</ref> and the style loss compares the feature correlations as given by the Gram matrices.</p><p>Several works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51]</ref> have proposed feed-forward networks to approximate the optimization problem posed by Gatys et al. <ref type="bibr" target="#b12">[13]</ref> for a fixed style image. Li et al. <ref type="bibr" target="#b36">[37]</ref> showed that matching the Gram matrices of feature maps corresponds to minimizing the Maximum Mean Discrepancy with the second order polynomial kernel and also proposed alternative style representations to the Gram matrix such as mean and variance. Dumoulin et al. <ref type="bibr" target="#b9">[10]</ref> introduced conditional instance normalization, which enables the model to learn multiple styles. Huang and Belongie <ref type="bibr" target="#b20">[21]</ref> performed arbitrary real-time style transfer by training a feed-forward network to align the channel-wise mean and standard deviation of the VGG features of a content image to match those of a given style image. Li et al. <ref type="bibr" target="#b34">[35]</ref> extend this approach by replacing the moment matching between the encoder and decoder with whitening and colouring transformations.</p><p>Li et al. <ref type="bibr" target="#b35">[36]</ref> propose a closed-form solution for photorealistic image stylization and Li et al. <ref type="bibr" target="#b33">[34]</ref> learn linear transformations for fast arbitrary style transfer. Sanakoyeu et al. <ref type="bibr" target="#b46">[47]</ref> and <ref type="bibr">Kotovenko et al. [31]</ref> propose a style-aware content loss, which also has been used for disentanglement of style and content <ref type="bibr" target="#b31">[32]</ref>.</p><p>Another line of work draws on meta learning to handle the trade-off between speed, flexibility, and quality <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b58">59]</ref>. Wang et al. <ref type="bibr" target="#b52">[53]</ref> incorporate model compression to enable ultra-resolution style transfer <ref type="bibr" target="#b52">[53]</ref>, Xia et al. perform photorealistic style transfer using local affine transforms <ref type="bibr" target="#b55">[56]</ref>, Chang et al. <ref type="bibr" target="#b4">[5]</ref> employ domain-specific mappings for style transfer, Chiu and Gurari <ref type="bibr" target="#b6">[7]</ref> propose an iterative and analytical solution to the style transfer problem, and Kim et al. <ref type="bibr" target="#b28">[29]</ref> suggest a method for deformable style transfer that is not restricted to a particular domain. Yim et al. <ref type="bibr" target="#b57">[58]</ref> introduce filter style transfer, Wang et al. <ref type="bibr" target="#b54">[55]</ref> propose deep feature perturbation, Svoboda et al. <ref type="bibr" target="#b49">[50]</ref> perform style transfer with a custom graph convolutional layer, and Chen et al. <ref type="bibr" target="#b5">[6]</ref> employ optical flow to stylize videos.</p><p>Stroke Based Rendering. Stroke based rendering (SBR) aims to represent an image as a collection of parameterized strokes or other shapes that can be explicitly defined by a finite set of parameters. In accordance with other non-photorealistic rendering techniques, the goal is not to reconstruct but rather to render the image into an artistic style. Early works include an interactive method by Haeberli <ref type="bibr" target="#b16">[17]</ref>, where the program follows the cursor across the canvas, obtains a color by point sampling the source image, and then paints a brush of that color. Hertzmann <ref type="bibr" target="#b17">[18]</ref> extended this line of research by proposing an automated algorithm that takes a source image and a list of brush sizes, and then paints a series of layers, one for each brush size, on a canvas in order to recreate the source image with a handpainted appearance. Similar approaches employ segmentation <ref type="bibr" target="#b13">[14]</ref> or relaxation <ref type="bibr" target="#b18">[19]</ref>. SBR methods are not constrained to static images and have also been used to transform ordinary video segments into animations that possess a hand-painted appearance <ref type="bibr" target="#b37">[38]</ref>. Brush Stroke Extraction. Conversely to SBR methods, there have been attempts to detect and extract brush strokes from a given painting. These methods generally utilize edge detection and clustering-based segmentation <ref type="bibr" target="#b32">[33]</ref> or other classical computer vision techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45]</ref> and have been used to analyze paintings. Drawing Networks. Recent work relies on neural net-works to predict brush stroke parameters that approximate a given image, using a variety of architectures and training paradigms. These range from supervised training of feedforward and recurrent architectures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b43">44]</ref> to deep reinforcement learning, using recurrent <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref> and feedforward models <ref type="bibr" target="#b22">[23]</ref>. Note that our work is orthogonal to this line of research because we focus on performing style transfer on the level of parameterized brushstrokes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>In the original style transfer formulation, Gatys et al. <ref type="bibr" target="#b12">[13]</ref> propose an iterative method for combining the content of one image with the style of another by jointly minimizing content and style losses. The content loss is the Euclidean distance between the rendered image I r and the content image I c in the VGG feature space:</p><formula xml:id="formula_0">L content = ||? l (I r ) ? ? l (I c )|| 2 ,<label>(1)</label></formula><p>where ? l (?) denotes the l-th layer of the VGG-19 network. The style loss is defined as:</p><formula xml:id="formula_1">L style = L l=0 w l E l<label>(2)</label></formula><p>Gatys et al.</p><p>Ours <ref type="figure">Figure 3</ref>: For Gatys et al. <ref type="bibr" target="#b12">[13]</ref>, the pixels are adjusted to match the brushstroke pattern. In our approach, the brushstroke pattern is occurring by design. Style image: "Starry Night" by Vincent van Gogh. Content image: original image of Tuebingen from the paper <ref type="bibr" target="#b12">[13]</ref>. Same region of the sky is cropped.</p><formula xml:id="formula_2">with E l = 1 N 2 l M 2 l ||G l r ? G l s || F<label>(3)</label></formula><p>where G l r and G l s are the Gram matrices of I r and I c respectively, computed from the l-th layer of the VGG-19 network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>The method by Gatys et al. <ref type="bibr" target="#b12">[13]</ref> adjusts each pixel individually to minimize the content and style losses. However, artworks generally consist of brushstrokes, not pixels. Instead of optimizing on pixels, we therefore optimize directly on parameterized brushstrokes, using the same content and style losses defined in Eq. 1 and Eq. 2, respectively. See <ref type="figure" target="#fig_2">Fig. 4</ref> for an overview of our method and <ref type="figure">Fig. 3</ref> for a comparison of the synthesized brushstroke patterns. Our brushstrokes are parameterized by location, color, width, and shape. The shape of a brushstroke is modelled as a quadratic B?zier curve <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>, which can be parameterized by:</p><formula xml:id="formula_3">B(t) = (1 ? t) 2 P 0 + 2(1 ? t)tP 1 + t 2 P 2 , 0 ? t ? 1. (4)</formula><p>A key difficulty here is to find an efficient and differentiable mapping from the brushstroke parameter space into the pixel domain. To this end, we propose a mechanism to construct this mapping explicitly. See Sec. 4.2 for details. Using our rendering mechanism we can backpropagate gradients from the style and content losses through the rendered pixels directly to the brushstroke parameters. After the optimization is finished, we render the optimized brushstroke parameters to obtain an image I and then apply the standard Gatys et al. <ref type="bibr" target="#b12">[13]</ref> approach on the pixel level using I s as style image and I as content image. This final step blends the brushstrokes together and adds some texture. <ref type="figure" target="#fig_5">Fig. 7</ref> shows the effect of this pixel optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Similar to Gatys et al. <ref type="bibr" target="#b12">[13]</ref>, we use layers "conv4 2" and "conv5 2" for the content loss and layers "conv1 1", "conv2 1", "conv3 1", "conv4 1", and "conv5 1" for the style loss. We use Adam <ref type="bibr" target="#b29">[30]</ref> with learning rate 0.1 for optimization. Similar to Johnson et al. <ref type="bibr" target="#b25">[26]</ref>, we employ a total variation regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Differentiable Renderer</head><p>Nowadays, generative models have reached unmatched image quality on a variety of datasets <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4]</ref>. Thus, our first attempt to generate brushstrokes followed this line of work. We generated a dataset of brushstrokes simulated in the FluidPaint environment 1 and trained a network inspired by StyleGAN <ref type="bibr" target="#b26">[27]</ref> to generate images conditioned on brushstroke parameters. Despite achieving satisfactory visual quality, the main limitation of this approach is that it is memory-intensive and can not be efficiently scaled to process a large number of brushstrokes in parallel. This is critical for us since our method relies on an iterative optimization procedure.</p><p>Therefore, instead of training a neural network to generate brushstrokes, we explicitly construct a differentiable function which transforms a collection of brushstrokes parameterized by location, shape, width and color into pixel values on a canvas. Formally, the renderer is a function:</p><formula xml:id="formula_4">R : R N ?F ? R H?W ?3 ,<label>(5)</label></formula><p>where N denotes the number of brushstrokes, F the number of brushstroke parameters (12 in our case), and H and W are the height and width of the image to render. This renderer requires less memory and is also not constrained by the limitations of a brushstroke dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Motivation and Idea</head><p>Before explaining how our render works, let us start with a simple example. Assume we have a flat disk parameterized with color, radius, and location (1, 1, and 2 scalars respectively) and we want to draw it on a canvas. For the sake of brevity, we assume our images are grayscale but the algorithm trivially generalizes to the RGB space. A grayscale image is a 2D matrix of pixel values. First, we need to decide for every pixel whether or not it belongs to the disk. For this, we simply subtract the disk location from each pixel coordinate and compute the L 2 norm to obtain distances D from each pixel to the disk center. Now we have to check if the distance D is smaller then the radius to get a binary mask M . To incorporate color, it suffices to multiply the mask by a color value.  <ref type="bibr" target="#b12">[13]</ref> optimize pixels to minimize style and content loss. We directly optimize parameters of the brushstrokes. To do that we have designed a differentiable rendering mechanism that maps brushstrokes onto the canvas. Each brushstroke is parameterized by color, location, width and shape. Brushstroke parameters are updated by gradient backpropagation (red, dashed arrows).</p><p>If we have two disks, we simply repeat the procedure above for each disk separately and obtain two separate images with disks, namely I 1 , I 2 ? R H?W ?3 . Now, how do we blend I 1 , I 2 together? If they do not overlap we can sum the pixel values across disks I 1 + I 2 . However, if the disks overlap, adding them together will produce artifacts. Therefore, in the overlapping regions, we will assign each pixel to the nearest disk. This can be done by computing the distances D 1 , D 2 ? R H?W from each pixel to each disk center and determine for every pixel the closer disk. We call this object an assignment matrix A := {1 if D 1 ? D 2 , 0 otherwise} ? R H?W . Now the final image I can be computed using the matrices I 1 , I 2 and A: I := I 1 * A + I 2 * (1 ? A). The assignment matrix A naturally generalizes to N objects:</p><formula xml:id="formula_5">A(i, j, n) := 1 if D n (i, j) &lt; D k (i, j) ?k = n, 0 otherwise.<label>(6)</label></formula><p>It indicates which object is the nearest to the coordinate (i, j). The final image computation for N images of disks I 1 , .., I N then corresponds to:</p><formula xml:id="formula_6">I(i, j) := N n=1 I n (i, j) * A(i, j, n)<label>(7)</label></formula><p>Hence, the final image is computed by the weighted sum of renderings weighted according to the assignment matrix A. Both the assignment matrix and the individual renderings I 1 , ..., I N originate from the distance matrices D 1 , .., D N from each pixel location to the object. Indeed, to render a single object we take its distance matrix, threshold with radius/width and multiply by a color value. The assignment matrix is an indicator function of the smallest distance across distances D 1 , .., D N . Thus, the matrix of distances is a cornerstone of our approach. We can effectively render any object for which we can compute the distances from each pixel to the object.</p><p>Our initial goal was to render brushstrokes. To render a disk we take a distance matrix D, get a mask of points that are closer than the radius and multiply this mask by a color value. The same holds for a B?zier curve. First, we compute a matrix of distances to the curve D B (matrix of distances from every point in a 2D image to the nearest point on the B?zier curve). Then, we mask points that are closer than the brushstroke width and multiply them by a color value. We approximate the distance from a point p to a B?zier curve by sampling S equidistant points p 1 , .., p S along the curve and computing the minimum pairwise distance between p and p 1 , .., p S . Note that there exists an analytical solution of this distance for a quadratic B?zier curve, however, the approximated distance allows the use of arbitrary parametric curves. In the final step, we can compute the individual renderings of brushstrokes and the assignment matrix as in Eq. 6 and blend them together into the final rendering with Eq. 7.</p><p>For the sake of clarity, we have left out two important details in the above explanation. First, the renderer should be differentiable, yet, the compu-tation of the assignment matrix and the masking operation are both discontinuous. To alleviate this problem, we implement a masking operation with a sigmoid function. To make the assignment matrix computation differentiable we replace it with a softmax operation with high temperature. Second, the computation of distances between every brushstroke and every pixel on the canvas is computationally expensive, memory-intensive and also redundant because a brushstroke only affects the nearby area of the canvas. Therefore, we limit the computation of distances from a pixel to all the brushstrokes to only the K nearest brushstrokes, see Sec. C.2 for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Deception Rate</head><p>To evaluate the quality of the stylization we use a deception rate proposed by Sanakoyeu et al. <ref type="bibr" target="#b46">[47]</ref>. The method is based on a network trained to classify paintings into artists. The deception rate is the fraction of stylized images that the network has assigned to the artist, whose artwork has been used for stylization. However, a high deception score indicates high similarity to the target image. But this metric does not indicate how plausible a stylized image is. To measure this quality we conduct the following experiment: we show to a human subject four crop outs. Each one can be either taken from a real artwork or from a generated image. The task is to detect all real crop outs. The experiment is conducted with 10 human subjects, each participant evaluates 200+ tuples. Fake images are randomly sampled from one of three methods: ours, Gatys et al. <ref type="bibr" target="#b12">[13]</ref>, and AST <ref type="bibr" target="#b46">[47]</ref>. For each method we report the proportion of ranking this image as real, see Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Differentiable Renderer</head><p>We compare our simple explicitly constructed renderer to the rendering mechanism proposed by Huang et al. <ref type="bibr" target="#b22">[23]</ref>. Our approach is slower, but it requires no pretraining on specific datasets as opposed to Huang et al. <ref type="bibr" target="#b22">[23]</ref>. We achieve 20% lower mean squared error (MSE) using 200 strokes, and 49% lower MSE on 1000 strokes. The comparison has been conducted on the CelebA dataset. See <ref type="figure" target="#fig_4">Fig. 6</ref> for a visual comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Fitting Brushstrokes to Artwork</head><p>We can fit brushstrokes not only to a photograph but also to an artwork. This procedure is useful if we want to study the distribution of brushstrokes in an artwork. It has been shown by Li et al. <ref type="bibr" target="#b32">[33]</ref> that this information may be helpful to detect forgeries and analyze the style of an artist. In <ref type="figure">Fig.  5</ref> we show reconstructions of "Self-Portrait" by Vincent van Gogh obtained using our renderer.</p><p>We additionally trained a neural network that receives brushstroke parameters as input and generates the corresponding brushstrokes. The network employs an architecture inspired by StyleGAN <ref type="bibr" target="#b26">[27]</ref> and was trained on a dataset obtained using the FluidPaint environment. The brushstroke Trained Renderer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Our Renderer <ref type="figure">Figure 5</ref>: Reconstructions of "Self-Portrait" by Vincent van Gogh using our brushstroke renderer and a trained renderer.</p><p>In either case we use 10.000 brushstrokes. [23] on the image reconstruction task. Our method directly minimizes l 2 distance between the input target image and image rendered as a collection of brushstrokes. Using our renderer we achieve 20% lower Mean Squared Error (MSE) for 200 strokes and 49% lower MSE for 1000 strokes. Please zoom in for details.</p><p>parameterization is as described in this paper. The trained renderer yields results comparable to our simple renderer but requires more precise hyperparameter tuning an takes more time to optimize on. Since the trained renderer is based on the StyleGAN <ref type="bibr" target="#b26">[27]</ref> architecture, it consumes much more memory and thus fitting hundreds or thousands of brushstrokes cannot be run in parallel. In <ref type="figure">Fig. 5</ref> we present results of our renderer and the trained renderer. See Sec. D for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Controlling Brushstrokes</head><p>To highlight the additional control our brushstroke representation enables over the stylization process, we show how users can control the flow of brushstrokes in the stylized image, see <ref type="figure" target="#fig_1">Fig. 2</ref>. A user can draw arbitrary curves on the content image and the brushstrokes in the stylized image will follow these curves. This can be achieved by adding a simple projection loss that enforces brushstrokes along the drawn paths to align with the tangent vectors of the paths. See Sec. B for details. <ref type="figure" target="#fig_1">Fig. 2</ref> further shows the effect the number of brushstrokes has on the stylization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we have proposed to switch the representation for style transfer from pixels to parameterized brushstrokes. We argue that the latter representation is more natural for artistic style transfer and show how it benefits the visual quality of the stylizations and enables additional control.</p><p>We have further introduced an explicit rendering mechanism and show that it can be applied even beyond the field of style transfer.</p><p>A limitation of our approach is that it performs best for artistic styles where brushstrokes are clearly visible. This can potentially be alleviated with more sophisticated brushstroke blending procedures and should be investigated in future endeavors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>This work has been supported in part by the German Research Foundation (DFG) within project 421703927.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Ours</head><p>Svoboda et al. <ref type="bibr" target="#b49">[50]</ref> Gatys et al. <ref type="bibr" target="#b12">[13]</ref> AST <ref type="bibr" target="#b46">[47]</ref> Figure 8: Comparison with other methods for images used by Svoboda et al. <ref type="bibr" target="#b49">[50]</ref> and AST <ref type="bibr" target="#b46">[47]</ref>, please zoom in for details. See Sec. F for full size images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Videos</head><p>In order to give insights into the stylization procedure, we provide two videos, see:</p><p>? https://heibox.uni-heidelberg.de/f/ 77c92a1355904de6a6be/</p><p>? https://heibox.uni-heidelberg.de/f/ c17c112570f646bab081/ In these videos we show how the stylization evolves over time, before and after pixel optimization. We also compare to Gatys et al. <ref type="bibr" target="#b12">[13]</ref> and show how the user input (Sec. B) influences the stylization. See <ref type="figure" target="#fig_6">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Controlling the Flow of Brushstrokes with User Input</head><p>In Sec. 5.4, we showed how our method enables us to control the flow of brushstrokes. A user can draw arbitrary curves on the content image through a user interface and the brushstrokes in the stylized image will follow these curves. This can be achieved by adding a simple projection loss, which we will explain in this section. Each drawn curve is represented as a set of points P 1 , P 2 , ..., P M (do not confuse those with the control points for a B?zier curve). For each point P i the approximate tangent vector v i is computed as follows:</p><formula xml:id="formula_7">v i =? i ||? i || ,? i = ? ? 1 Q Q j=1 P i+j ? ? ? P i ,<label>(8)</label></formula><p>where Q = 3 in all our experiments. <ref type="figure" target="#fig_0">Fig. 10</ref> shows user drawn curves with corresponding tangent vectors.  As explained in Sec. 4, each brushstroke is represented as a quadratic B?zier curve with additional parameters for location, width, and color. A quadratic B?zier curve is parameterized by three points: a start point, an end point, and a control point. Roughly speaking, the start and end points determine the curves orientation and the control point determines the curvature.</p><p>The projection loss is computed as follows:</p><p>1. Compute for each brushstroke the vector from the start point to the end point of the B?zier curve, see <ref type="figure" target="#fig_0">Fig. 12</ref>.</p><p>We will refer to this vector as the orientation vector of a brushstroke.</p><p>2. For each tangent vector, compute the L nearest brushstrokes on the canvas. L is a hyperparameter and determines the range of the brushstrokes that will be affected by the drawn curves. <ref type="figure" target="#fig_0">Fig. 14</ref> shows the influence of L on the stylization.</p><p>3. For each tangent vector, compute the projection of the orientation vectors from the nearest brushstrokes onto the tangent vector. Both the tangent vectors and the orientation vectors are normalized to unit length.</p><p>4. The projection loss encourages the absolute value of these projections to be 1. Since all vectors are normalized, the absolute value of the projections will be 1 if and only if the orientation vectors are parallel to the tangent vector. See <ref type="figure" target="#fig_0">Fig. 11</ref> for an overview of the whole computation.</p><p>See <ref type="figure" target="#fig_0">Fig. 15 and 16</ref> for more results.</p><p>C. Renderer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Brushstroke Parameterization</head><p>Each brushstroke is parameterized by color rgb ? R 3 , B?zier curve B(t) with t ? [0; 1] and width w ? R. B?zier curve B(t) is introduced in Eq. 4. From the formulation we see that B(t) depends on three points P 0 , P 1 , P 2 ? R 2 which we further call start point, control point and end point, respectively. We additionally define the direction (orientation) d ? R 2 of a stroke as d := P 2 ? P 0 . This vector will be used in Sec. B to control the flow of the brushstrokes. This parameterization of a brushstrokes is illustrated in <ref type="figure" target="#fig_0">Fig. 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Tensor of Distances</head><p>As described in Alg. 1, the cornerstone of our rendering mechanism is a tensor of distances D between every sampled point on a brushstroke and each point on a canvas. We use a canvas C of size H ? W , where H = W = 256. We typically draw N = 5000 brushstrokes and on every brushstrokes we sample S = 10 points. This results in a tensor of shape H ? W ? N ? S. If we use float32 data type taking 4 Bytes, then the distances tensor has size 256 * 256 * 5000 * 10 * 4 = 13107200000 ? 13GB, which is infeasible in practice. However, we actually do not need to compute the distances between every pixel and every brushstroke since a pixel is only affected by a few nearby brushstrokes, say by K nearest brushstrokes (in our implementation we set K = 20). With this in mind, we can reduce the tensor of distances D of shape H ? W ? N ? S to the size H ? W ? K ? S which requires N K = 5000 20 = 250 times less memory, roughly 52MB.</p><p>In order to accomplish the tensor size reduction, we need to assign the K nearest brushstrokes to each pixel. However, for this we would need the tensor of distances of size H ? W ? N , which is not feasible for large values of N . This problem can be circumvented if we compute the distances from each of the N brushstrokes to a sparse subset of "anchor" points on the tensor of locations C ? R H?W . We create a tensor C coarse of size H ? W where H &lt; H and W &lt; W containing subset of tensor C, in our case we set H = 0.1 ? H and W = 0.1 ? W . Now we can effectively compute the tensor of distances between C coarse and each brushstroke, it will have shape H ? W ? N . We left out dimension S because we only need to roughly estimate the distances of the brushstrokes that are close to the location, so we use the location coordinates of the whole brushstroke. Now we extract the K nearest strokes at every location and obtain a tensor of indices idcs having shape H ? W ? K. We then apply nearest neighbor upsampling to idcs across dimensions H and W and obtain idcs, a tensor of shape H ? W ? K. Thus, every pixel will have the same nearest neighbors as the nearest "anchor" point. This tensor of indices indicates the K nearest brushstrokes for each pixel of the canvas. Now using this tensor of indices idcs and the tf.gather operation in TensorFlow we can effectively assign to every location only the K nearest strokes. We note that the same stroke is assigned to multiple locations but this does not hinder the optimization process because the stroke will just receive more gradient information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Hardware, Runtime, Memory</head><p>Our stylization process consists of two stages. At the first stage we optimize brushstroke parameters, at the second stage we optimize individual pixels.</p><p>Brushstroke parameters optimization. We use a canvas of size H ? W , where H = W = 256 and optimize using our renderer for 1000 steps using the Adam Optimizer <ref type="bibr" target="#b29">[30]</ref>. It takes around 3 minutes.</p><p>Pixel optimization. Now we upsample the canvas with fitted strokes to have the smallest image side of 1024px and keep the input content image aspect ratio. This image with fitted brushstrokes is used as both content image and initialization for the standard Gatys et al. stylization routine. We optimize for another 1000 steps using the Adam optimizer. It takes 4 more minutes to converge. All the experiments are conducted on NVIDIA TitanXP or NVIDIA 2080Ti graphic cards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.1 Number of Brushstrokes</head><p>The memory consumption does not depend on the number of strokes, see Sec. C.2. However, the run-time reduces linearly as the number of strokes increases, see Tab. 2.  <ref type="figure" target="#fig_0">Figure 11</ref>: Overview of projection loss for a specific tangent vector. For each tangent vector (green), we select the closest brushstrokes (gray). We then take the orientation vector (black) for each brushstroke and compute the projection (yellow) onto the tangent vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Trained Renderer</head><p>In order to ablate our renderer, we trained a neural network that receives brushstroke parameters as input and generates the corresponding brushstrokes. The brushstrokes are parameterized as described in Sec. 4. The network gener-ates an RGB image of a brushstroke as well as an alpha mask. In order to render N brushstrokes onto a canvas, we first have to generate each brushstroke individually and then blend them together using the alpha masks. See <ref type="figure" target="#fig_0">Fig. 13</ref> for some generated brushstrokes. <ref type="figure" target="#fig_0">Figure 12</ref>: The brushstroke is parameterized by color rgb ? R 3 , width w ? R and B?zier curve B(t). The B?zier curve is defined by points P 0 , P 1 , P 2 ? R 2 and position on a curve t ? [0; 1]. The direction (orientation) vector d ? R 2 is used to simplify the strokes. <ref type="figure" target="#fig_0">Figure 13</ref>: Generated brushstrokes using the trained renderer. Top row: generated brushstrokes. Bottom row: ground truth simulated in the FluidPaint environment. The alpha mask is always white and located to the right of the brushstroke.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Architecture</head><p>The brushstroke generator follows the StyleGAN architecture <ref type="bibr" target="#b26">[27]</ref> and consists of a mapping network f and a synthesis network g, here we adopt the notation from the StyleGAN paper. The mapping network f takes in the brushstroke parameters z and processes them using 4 fullyconnected layers to create the latent vector w. The synthesis network g consists of 4 blocks, each consisting of an upsampling layer, a 3x3 convolutional layer, and an AdaIN layer <ref type="bibr" target="#b20">[21]</ref>. The latent vector w is injected into the AdaIN layers using learned affine transformations. After every convolutional layers we also inject noise. The discriminator follows the StyleGAN architecture <ref type="bibr" target="#b26">[27]</ref> as well, however, it only consists of 8 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Training</head><p>For training, we used the Wasserstein GAN loss <ref type="bibr" target="#b1">[2]</ref> with gradient penalty <ref type="bibr" target="#b14">[15]</ref> and a L2 loss (equally weighted). We used the Adam optimizer <ref type="bibr" target="#b29">[30]</ref> with learning rate 0.0002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. User Study</head><p>To evaluate the quality of the synthesized images we use two methods. First, we compute the deception score, as suggested by Sanakoyeu et al. <ref type="bibr" target="#b46">[47]</ref>. This score indicates how similar is a given stylization to the actual style of the artist. Another way to evaluate the quality of images is to perform a user study. We show to a human subject cropouts from images obtained using different stylization approaches or real artworks and ask them to pick crops from a real artwork. Among the stylization approaches we have Gatys et al. <ref type="bibr" target="#b12">[13]</ref>, AST by Sanakoyeu et al. <ref type="bibr" target="#b46">[47]</ref>, WCT <ref type="bibr" target="#b34">[35]</ref>, and AdaIN <ref type="bibr" target="#b20">[21]</ref>. At once we show 4 images: each drawn randomly and independently. In <ref type="figure">Fig. 38</ref> we present randomly drawn example trials. Note that in <ref type="figure">Fig. 38</ref> we do not provide images for WCT <ref type="bibr" target="#b34">[35]</ref> and AdaIN <ref type="bibr" target="#b20">[21]</ref> since those are easy to spot in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Additional Results</head><p>We provide additional stylization examples in <ref type="figure" target="#fig_0">Fig. 17</ref>, <ref type="bibr">18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Fitting Brushstrokes to Artwork</head><p>In Sec. 5.3 we showed how we can use our renderer to fit brushstrokes to paintings. Note that we use SLIC superpixels <ref type="bibr" target="#b0">[1]</ref> to initialize the brushstrokes for this experiment. We only optimize the brushstroke parameters and did not apply pixel level optimization for this experiment. See <ref type="figure" target="#fig_29">Fig. 35</ref>  Note that for the stylization with user input we also used (a) as content image. The control is imposed on the brushstroke parameters, not the pixels. Note that for the stylization with user input we also used (a) as content image. The control is imposed on the brushstroke parameters, not the pixels.                      <ref type="figure">Figure 38</ref>: Randomly sampled patches from the user study. In each round we show one row and ask the users to mark all the patches cropped out of real artworks. Each crop in a row is drawn from either a real artwork ('real'), a stylization by Gatys et al. <ref type="bibr" target="#b12">[13]</ref> ('Gatys'), a stylization by Sanakoyeu et al. <ref type="bibr" target="#b46">[47]</ref> ('AST'), or from our method ('ours'). In this table we have restricted ourselves to only those 4 classes to make this quiz more difficult for the reader. Try to guess which are real. The answers are on the last page.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Stylization results. Top artwork: "Girl on a Divan" by Ernst Ludwig Kirchner. Bottom artwork: "Red Cabbages and Onions" by Vincent van Gogh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Content with User Input (e) 2000 Brushstrokes and User Input (f) 5000 Brushstrokes and User Input A user can draw curves on the content image and thus control the flow of the brushstrokes in the stylized image. Note that for the stylizations with user input we also used (a) as content image. The control is imposed on the brushstroke parameters, not the pixels. Images in the middle column are synthesized using 2000 brushstrokes and images in the right column are synthesized with 5000 brushstrokes. See Sec. B and F for more experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of our method (bottom row) with Gatys et al. [13] (top row). Gatys et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 7 See Alg. 1 .</head><label>171</label><figDesc>Renderer Input: Brushtroke parameters B = {B 1 , B 2 , ..., B N }, temperature parameter t, number samples per curve S Output: Image I ? R H?W ?3 init C ? R H?W ?2 ; // coordinates tensor, C(x, y) = (x, y)init tensor of brushtrokes colors c strokes from B parameters ;// shape=[N,<ref type="bibr" target="#b2">3]</ref> init tensor of brushtrokes widths w strokes from B parameters ;// shape=[N] sample S points t ? [0; 1] sample points t at each brushtroke B sampled := {compute B i (t j ) withEq.4 |?i, j} ; // shape=[N,S,2] D ( x, y, n, s) := ||C(x, y) ? B sampled (n, s)|| 2 ; // Distances from each sampled point on a stroke to each coordinate, shape=[H,W,N,S] D strokes := min(D, axis = 4) ; // distance from a coordinate x, y to the nearest point on a curve. shape=[H, W, N] M strokes := sigm(max(t ? ||w strokes ? D strokes || 2 , axis = 4) ; // mask of each stroke, shape=[H, W, N] I strokes := M strokes ? c strokes ; // rendering of each stroke, shape=[H, W, N, 3] A := softmax(t ? D strokes , axis = 3) ; // assignment, shape=[H, W, N] I := einsum( xyn, xync? &gt; xyc , A, I) ; // final rendering, see Eq.See Sec. C for additional technical details of the implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison to the Learning to Paint (LTP) by Huang et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>The effect of the pixel optimization. Brushstrokes are blended together and texture is added. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>We provide videos that show how the stylization evolves over time. Moreover, we show how flow constraints change the stylization. See Sec. A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Top: A user draws arbitrary curves through a user interface. Bottom: A curve is represented as a set of points. For each point we can compute an approximate tangent vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>Controlling the flow of brushstrokes with different values for L. The larger the value L the more strokes around the user input are affected. User input is porvided inFig. 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 :</head><label>15</label><figDesc>Stylized without user input (d) Content with user input (e) Stylized with user input A user can draw curves on the content image and thus control the flow of the brushstrokes in the stylized image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 16 :</head><label>16</label><figDesc>A user can draw curves on the content image and thus control the flow of the brushstrokes in the stylized image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 17 :</head><label>17</label><figDesc>Style image: "The Weeping Woman" by Pablo Picasso.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 18 :</head><label>18</label><figDesc>Style image: "Ile De Br?hat" by Samuel John Peploe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 19 :</head><label>19</label><figDesc>Style image: "Road with Cypress and Star" by Vincent van Gogh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 20 :</head><label>20</label><figDesc>Style image: "Antibes, the Pink Cloud" by Paul Signac.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 21 :</head><label>21</label><figDesc>Style image: "Girl on a Divan" by Ernst Ludwig Kirchner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 22 :</head><label>22</label><figDesc>Style image: "A Pair of Leather Clogs" by Vincent van Gogh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 23 :</head><label>23</label><figDesc>Style image: "La Corne d'Or" by Paul Signac.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 24 :</head><label>24</label><figDesc>Style image: "Self Portrait" by Pablo Picasso.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 25 :</head><label>25</label><figDesc>Style image: "Murnau Street With Women" by Wassily Kandinsky.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 26 :</head><label>26</label><figDesc>Style image: "Barges on the Seine" by Maurice de Vlaminck.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 27 :</head><label>27</label><figDesc>Style image: "Red Cabbages and Onions" by Vincent van Gogh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 28 :</head><label>28</label><figDesc>Style image: "The Scream" by Edvard Munch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 29 :</head><label>29</label><figDesc>Style image: "The Olive Trees" by Vincent van Gogh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 30 :</head><label>30</label><figDesc>Style image: "Spring in the Elm Forest" by Edvard Munch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 31 :</head><label>31</label><figDesc>Style image: "Les Alyscamps" by Vincent van Gogh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 32 :</head><label>32</label><figDesc>Style image: "The Olive Trees" by Vincent van Gogh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 33 :</head><label>33</label><figDesc>Style image: "Olive Trees with Yellow Sky and Sun" by Vincent van Gogh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 34 :</head><label>34</label><figDesc>Style image: "Red Cabbages and Onions" by Vincent van Gogh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 35 :</head><label>35</label><figDesc>Brushstroke approximation of "Starry Night" by Vincent van Gogh using 12.000 brushstrokes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 36 :</head><label>36</label><figDesc>Brushstroke approximation of "Road with Cypress and Star" by Vincent van Gogh using 12.000 brushstrokes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 37 :</head><label>37</label><figDesc>Brushstroke approximation of "Iris" by Vincent van Gogh using 12.000 brushstrokes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>(Left) Deception score. Wikiart test gives the accuracy on real artworks from the test set. Photos correspond to the content images used by each of the methods for style transfer. (Right) Human deception rate. The probability of labeling randomly sampled crop out of a specified class as real. Both scores are averaged over 8 styles.</figDesc><table><row><cell>Method</cell><cell>Mean deception score ?</cell><cell>Mean human deception rate ?</cell></row><row><cell>AdaIN [21]</cell><cell>0.08</cell><cell>0.035</cell></row><row><cell>WCT [35]</cell><cell>0.11</cell><cell>0.091</cell></row><row><cell>Gatys et al. [13]</cell><cell>0.389</cell><cell>0.139</cell></row><row><cell>AST [47]</cell><cell>0.451</cell><cell>0.146</cell></row><row><cell>Ours</cell><cell>0.588</cell><cell>0.268</cell></row><row><cell>Wikiart test</cell><cell>0.687</cell><cell>-</cell></row><row><cell>Photos</cell><cell>0.002</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Run-time and memory analysis. Experiment conducted on a TITAN Xp GPU.</figDesc><table><row><cell># Strokes</cell><cell>1K</cell><cell>5K</cell><cell>10K</cell><cell>15K</cell><cell>20K</cell></row><row><cell cols="6">Speed [iter/s] 1.17 ? 0.01 1.16 ? 0.01 1.08 ? 0.01 1.0 ? 0.01 0.93 ? 0.01</cell></row><row><cell>Memory [GB]</cell><cell>9550</cell><cell>9550</cell><cell>9550</cell><cell>9550</cell><cell>9550</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://david.li/paint/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic extraction of brushstroke orientation from paintings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><forename type="middle">E</forename><surname>Berezhnoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">O</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Jaap</forename><surname>Herik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vision Appl</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain-specific mappings for generative adversarial style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optical flow distillation: Towards efficient and stable video style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative feature transformation for fast and versatile universal style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Yin</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stargan v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;01</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Synthesizing programs for images using reinforced adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Artistic vision: Painterly rendering using computer vision techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Coombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Symposium on Non-Photorealistic Animation and Rendering, NPAR &apos;02</title>
		<meeting>the 2nd International Symposium on Non-Photorealistic Animation and Rendering, NPAR &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A neural representation of sketch drawings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Haeberli</surname></persName>
		</author>
		<title level="m">Paint by numbers: Abstract image representations. SIGGRAPH &apos;90</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="207" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Painterly rendering with curved brush strokes of multiple sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;98</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Paint by relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics International 2001, CGI &apos;01</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Image analogies. SIG-GRAPH &apos;01</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuria</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="327" to="340" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning to paint with model-based deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Paintbot: A reinforcement learning approach for natural media painting. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungmoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-gat-it: Unsupervised generative attentional networks with adaptive layer-instance normalization for imageto-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Jason Salavon, and Gregory Shakhnarovich. Deformable style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Sunnie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using a transformation content block for image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Kotovenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingchuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Content and style disentanglement for artistic style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Kotovenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rhythmic brushstrokes distinguish van gogh from his contemporaries: Findings via automated brushstroke extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1159" to="1176" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning linear transformations for fast arbitrary style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Universal style transfer via feature transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A closed-form solution to photorealistic image stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Demystifying neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2230" to="2236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Processing images and video for an impressionist effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Litwinowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;97</title>
		<meeting>the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;97<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unsupervised doodling and painting with improved spiral. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eslami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Morphological analysis for investigating artistic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Monroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="414" to="423" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reconstructing the drawing process of reproductions from medieval images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Monroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Carqu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2917" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Neural painters: A learned differentiable constraint for generating brushstroke paintings. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Artistic style characterization of vincent van gogh s paintings using extracted features from visible brush strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieta</forename><surname>Putri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishnan</forename><surname>Mukundan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kourosh</forename><surname>Neshatian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Pattern Recognition Applications and Methods (ICPRAM)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stable and controllable neural texture synthesis and style transfer using histogram losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Risser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Wilmot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<idno>abs/1701.08893</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A style-aware content loss for real-time hd style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artsiom</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Kotovenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural style transfer via meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8061" to="8069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Two-stage peer-regularized feature recombination for arbitrary image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asha</forename><surname>Anoosheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">of Proceedings of Machine Learning Research</title>
		<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient example-based painting and synthesis of 2d directional texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaiping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2004-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Collaborative distillation for ultra-resolution universal style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoji</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Diversified arbitrary style transfer via deep feature perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongming</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Joint bilateral learning for real-time universal photorealistic style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">From meaningful contours to discriminative object shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Yarlagadda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2012</title>
		<editor>Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="766" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Filter style transfer between photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwa</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisung</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomsu</forename><surname>Won Joon Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Metastyle: Three-way trade-off among speed, flexibility, and quality in neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Strokenet: A neural painting environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ningyuan Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding Jiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
