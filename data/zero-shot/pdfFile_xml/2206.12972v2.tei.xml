<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VLCAP: VISION-LANGUAGE WITH CONTRASTIVE LEARNING FOR COHERENT VIDEO PARAGRAPH CAPTIONING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashu</forename><surname>Yamazaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72701</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Truong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72701</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Vo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72701</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kidd</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72701</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chase</forename><surname>Rainwater</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72701</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Luu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72701</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngan</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Arkansas</orgName>
								<address>
									<postCode>72701</postCode>
									<settlement>Fayetteville</settlement>
									<region>AR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VLCAP: VISION-LANGUAGE WITH CONTRASTIVE LEARNING FOR COHERENT VIDEO PARAGRAPH CAPTIONING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Contrastive Learning</term>
					<term>Video Captioning</term>
					<term>Vision</term>
					<term>Language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we leverage the human perceiving process, that involves vision and language interaction, to generate a coherent paragraph description of untrimmed videos. We propose vision-language (VL) features consisting of two modalities, i.e., (i) vision modality to capture global visual content of the entire scene and (ii) language modality to extract scene elements description of both human and non-human objects (e.g. animals, vehicles, etc), visual and non-visual elements (e.g. relations, activities, etc). Furthermore, we propose to train our proposed VLCap under a contrastive learning VL loss. The experiments and ablation studies on ActivityNet Captions and YouCookII datasets show that our VLCap outperforms existing SOTA methods on both accuracy and diversity metrics. Source code: https://github.com/UARK-AICV/VLCAP</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Video paragraph captioning (VPC) aims to generate a paragraph description of untrimmed videos with several temporal event locations in a coherent storytelling. VPC can be considered as a simplified version of dense video captioning by eliminating the requirements for generating event proposals. VPC takes a video with its corresponding event proposals as the input and returns a coherent paragraph as the output. A typical VPC contains two components corresponding to (i) feature extraction to encode each event into a feature and (ii) caption generation to decode features into a list of sentences. An essential requirement of VPC is maintaining the intraevent coherence between words within a sentence describing an event and inter-event coherence between sentences within a paragraph describing an entire video.</p><p>Zhou, et al. <ref type="bibr" target="#b3">[4]</ref> first leveraged the success of Transformer <ref type="bibr" target="#b4">[5]</ref> to dress VPC task, known as Vanilla Transformer VPC. In their approach, intra-event coherence is decoded by a Transformer but there is no mechanism to model the inter-event coherence i.e., each event is decoded individually. Later, <ref type="bibr" target="#b5">[6]</ref> tackled this limitation and proposed MFT by utilizing LSTM <ref type="bibr" target="#b6">[7]</ref>. In MFT, the last hidden state of the current sentence is used as an initial hidden state for the next sentence. However, This material is based upon work supported in part by the US National Science Foundation, under Award No. OIA-1946391, NSF 1920920.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Caption:</head><p>A fire is burning out in a fire pit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Caption:</head><p>A man is fueling the fire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current Event</head><p>Previous Event GT Caption: The fire dies down a bit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Next Event</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoders Decoders</head><p>Event Feature CNN-based backbone Event Feature CNN-based backbone</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-attention Decoder</head><p>The person continues cutting wood with the stick and ends by throwing it into a pile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hidden State</head><p>The person then begins cutting the fire with a stick . the coherence between sentences in MFT is ill-favored, facing the gradient vanishing problem <ref type="bibr" target="#b7">[8]</ref> and unable to model long-term dependencies <ref type="bibr" target="#b8">[9]</ref>. Being inspired by the recent transformer language model, Transformer-XL <ref type="bibr" target="#b0">[1]</ref>, which is able to resolve context fragmentation for language modeling, <ref type="bibr" target="#b1">[2]</ref> proposed MART. While Transformer-XL directly uses hidden states from previous segments, MART is designed as a unified encoder-decoder to prevent overfitting and reduce memory usage. Clearly, to understand and describe a video, we not only observe the entire scene but also pay attention to both element scenes such as human and non-human objects (e.g., vehicles, animals, tools, etc.), visual and non-visual elements (e.g., actions, relations, etc). Furthermore, vision and language are two primary capabilities of humans language influences basic perceptual processing <ref type="bibr" target="#b9">[10]</ref>. However, most of the existing VPC  </p><formula xml:id="formula_0">i } L i=1</formula><p>extracted from L snippets as its input and returns a predicted caption, which is then compared to the groundtruth caption by our proposed VL loss</p><formula xml:id="formula_1">L V L = L M LE + L vl .</formula><p>approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13]</ref> decode caption description by applying a backbone, e.g. C3D <ref type="bibr" target="#b13">[14]</ref>, I3D <ref type="bibr" target="#b14">[15]</ref>, 2Stream <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, or Slowfast <ref type="bibr" target="#b17">[18]</ref> to extract global visual information of the entire scene. By doing that, they ignore the interaction between the entire scene and relevant elements as well as disregard the fact that language and perception are two central cognitive systems.</p><p>In this paper, we propose a multi-modal VL representation consisting of the global visual feature of the entire scene and linguistics relevant scene elements. While maximum likelihood estimation (MLE) is the most widely used loss function for supervised learning VPC, it does not guarantee that the learnt latent features represent the groundtruth captions. In this paper, we leverage contrastive learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> and propose VL Loss, which consists of two terms corresponding to captioning loss (L cap. ) and a contrastive contextual loss (L vl ). The network comparison between our proposed VLCap with other existing VPC networks is shown in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED METHOD</head><p>Our proposed VLCap is designed as a unified encoder-decoder architecture and contains two main modules, i.e., VLCap Encoder and VLCap Decoder. Both modules are trained in an end-to-end framework by our proposed VL loss function. The entire architecture of VLCap is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>In this section, we first introduce all notations and VPC problem formulation as follows: Given an untrimmed video</p><formula xml:id="formula_2">V = {v i } |V| i=1</formula><p>, where |V| is the number of frames, and a list of its important events</p><formula xml:id="formula_3">E = {e i = (e s i , e e i )} |E| i=1</formula><p>, where |E| is the number of events within a video and event e i is defined by a pair of beginning and ending timestamps (e s i , e e i ). Our objective is to generate a coherent paragraph P = {s i } |E| i=1 that describes the whole video V. In this setup, a sentence s i aims to describe its corresponding event e i . We use notation e = (e s , e e ) to denote an event and it is presented by a sequence of frames V e = {v i |e s ? i ? e e }. </p><formula xml:id="formula_4">{S i } L i=1 , each snippet S i consists of ? consecutive frames, where L = |Ve| ?</formula><p>and |V e | is the number frames in V e . VL-Cap Encoder processes a snippet S i to extract f V L i . As a result, VLCap Encoder processes the event e to extract feature</p><formula xml:id="formula_5">F V L = {f V L i } L i=1</formula><p>as shown in the <ref type="figure" target="#fig_0">Fig.2 (left)</ref>. The VLCap Encoder contains three modalities as follows: i. Vision Modality This modality aims to extract visual content by applying a C3D network <ref type="bibr" target="#b13">[14]</ref> into snippet S i . The output feature map of C3D network ? is processed by average pooling to reduce the entire spatial dimension followed by channel multilayer perceptron (MLP). As a result, each snippet S i is represented by a feature f v i .</p><formula xml:id="formula_6">f v i = average pool(?(S i ))<label>(1)</label></formula><p>ii. Language Modality This modality aims to extract elementlevel linguistic details of each snippet S i . We leverage the success of recent works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> which have proved the effectiveness of feature representation learned via Contrastive Language-Image Pre-training (CLIP) <ref type="bibr" target="#b25">[26]</ref>. Given a snippet S i , the linguistic feature f l i is extracted by the following steps: (i) -Word embedding: We construct a vocabulary W = {w 1 , . . . w N } using the groundtruth captions from training dataset. Each word w i ? W is encoded by a Transformer network <ref type="bibr" target="#b4">[5]</ref> into a text feature f w i . We then project feature </p><formula xml:id="formula_7">. w e = W t ? f w , where f w = {f w i } m i=1 .</formula><p>(ii) -Language-based frame embedding: We choose the middle frame I to present each snippet S i . We first encode frame I by a pre-trained Vision Transformer <ref type="bibr" target="#b26">[27]</ref> to extract visual feature f I . We then project feature f I onto visual projection matrix W i pre-trained by CLIP to obtain image embedding I e = W i ? f I . The pairwise cosine similarity between embedded I e and w e is then computed. Top k similarity scores are chosen as language-based frame embedding feature F l i . (iii) -language feature extraction: In this step, we employ Adaptive Attention Mechanism (AAM) <ref type="bibr" target="#b2">[3]</ref> to select the most relevant representative language features:</p><formula xml:id="formula_8">f l i = AAM(F l i ) = AAM(cosine(I e , w e ))<label>(2)</label></formula><p>iii. Fused Modality This modality aims to fuse the visual feature f e i and linguistic feature f l i given a snippet S i . We first extract the inter-feature relationships by utilizing a selfattention layer <ref type="bibr" target="#b4">[5]</ref>. We then merge them by a mean operation:</p><formula xml:id="formula_9">f V L i = mean(Self-Attention([f v i ; f l i ]))<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">VLCap Decoder</head><p>Leveraging the recent success of Transformer vision-language models <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref>, we adopt the unified encoder-decoder Transformer to generate a caption of a given event e i consisting of L snippets, i.e., e i = {S i } L i=1 . By applying VLCap Encoder,</p><formula xml:id="formula_10">each S i is presented by a VL feature f V L i , thus the event e i is presented by F V L = {f V L i } L i=1 .</formula><p>Let F text denotes textual tokens. Both the video features F V L and textual tokens F text are taken as a unified input for the Transformer layers i.e.,</p><formula xml:id="formula_11">H 0 t = [F V L , F text ]<label>(4)</label></formula><p>To model inter-event coherence, our unified encoderdecoder Transformer is equipped with GRU-like memory to remember history information. Inspired by MART <ref type="bibr" target="#b1">[2]</ref>, at step time t, decoding the t th event, the t th layer aggregates the information from both its intermediate hidden statesH l t and the memory states M l t?1 from the last step, using a multi-head attention. The input key, value, query matrices are K, V = [M l t?1 ;H l t ], Q =H l t . A feed forward layer is then used to encode the memory augmented hidden states. The output is then merged withH l t using a residual connection and layer norm to obtain the hidden states output H l t . The process is as follows:</p><formula xml:id="formula_12">U l t = MultiHeadAtt(M l t?1 ,H l t ,H l t ) R l t = tanh(W l mr M l t?1 + W l ur U l t + b l r ) Z l t = sigmoid(W l mz M l t?1 + W l uz U l t + b l z ) M l t = (1 ? Z l t ) R l t + Z l t M l t?1<label>(5)</label></formula><p>where is Hadamard product and W mr , W ur , W mz , W uz are network parameters and b l r , b l z are bias. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">VL Loss</head><p>Maximum likelihood estimation (MLE) loss, which is trained to increase the likelihood between predicted captions groundtruth, is the most common in VPC. However, it is unable to address the question of how well the learnt latent features represent the groundtruth captions. In this paper, we proposed Visual-Linguistic (VL) Loss, which tackles the aforementioned concerns while maintaining the likelihood between predicted caption groundtruth. Particularly, we leverage the recent advantages of constractive learning to propose L vl to pull all snippets of the same event and push snippets of different events. Let consider a set of N events</p><formula xml:id="formula_13">{e i } N i=1 , each event e i consists of L snippets {S i } L i=1 .</formula><p>Each event e i has its corresponding groundtruth caption c, which is then presented as f T by the pretrained Text Transformer from CLIP <ref type="bibr" target="#b25">[26]</ref>. Apply our proposed VLCap network into e i , we obtain the event embeddings F i which is then processed as a vector f i = mean(F i ). L vl is computed as follows:</p><formula xml:id="formula_14">L vl = ? N i,j=1 1 i=j log e ? (f i ? f T j ) + 1 i =j (1 ? log e ? (f i ? f T j )) (6)</formula><p>where ? is a learnable temperature parameter, which is initialized to log(1/0.07), to prevent scaling of the dot product values and reduce training instability.</p><p>Our VL loss L V L consists of two terms corresponding to caption-caption loss (L M LE ) and a vision-language loss (L vl ) as follows:</p><formula xml:id="formula_15">L V L = L M LE + L vl<label>(7)</label></formula><p>3. EXPERIMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets, Metrics and Implementation Details</head><p>We benchmark our VLCap on two popular VPC datasets, YouCookII <ref type="bibr" target="#b21">[22]</ref> and ActivityNet Captions <ref type="bibr" target="#b20">[21]</ref>. Information of those datsets are summarized in <ref type="table" target="#tab_2">Table 1</ref>. We follow the previous works <ref type="bibr" target="#b1">[2]</ref> to split the original validation set into two subsets: ae-val with 2,460 videos for validation and ae-test with 2,457 videos for test. We benchmark VLCap on four standard accuracy metrics, i.e., BLEU@4 (B@4) <ref type="bibr" target="#b29">[30]</ref>, METEOR (M) <ref type="bibr" target="#b30">[31]</ref>, CIDEr (C) <ref type="bibr" target="#b31">[32]</ref>, ROUGE (R) <ref type="bibr" target="#b32">[33]</ref> and two diversity metrics i.e., 2-gram diversity (Div@2) <ref type="bibr" target="#b33">[34]</ref> and 4-gram repetition (R@4) <ref type="bibr" target="#b5">[6]</ref>.</p><p>Adam optimizer was used to train our VLCap with an initial learning rate of 1e-4, ? 1 = 0.9, ? 2 = 0.999, L 2 weight decay of 0.01, and learning rate warmup over the first 5 epochs. During the training, we use the label smoothing with a value of 0.1 and ? = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance and Comparison</head><p>Tables 2, 3 report the performance comparison on ActivityNet Captions corresponding to ae-val and ae-test sets whereas <ref type="table" target="#tab_4">Table 4</ref> shows the performance comparison on YouCookII validation set. In each table, we highlight the best and the second-best with bold and underline. On YouCookII, VL-Cap obtains the best performance on B@4, C and R metrics whereas it gains compatible on other metrics. On ActivityNet Captions, VLCap obtains the best performance with large gaps on both accuracy metrics and diversity metrics compared to the second-best score. Take ActivityNet Captions as an example, corresponding to ae-val and ae-test sets, our VLCap gains (2.2%/1.18%/5.31%/6.05%) and (2.53%/1.49%/3.10%/5.14%) higher on BLEU@4/METEOR/CIDEr/ROUGE metrics while improves (0.61%) and (1.0%) on Div@2 as well as reduces (0.31%) and (1.26%) on R@4 compare to the second-best achievement.</p><p>To evaluate the effectiveness of our proposed VL feature as well as VL loss, we conduct ablation studies as shown in <ref type="table" target="#tab_5">Table  5</ref>. The capability of the proposed VL loss (L V L ) is shown in comparisons between Exp.#1 v.s #2 and Exp.#3 v.s #4 where we compare between VL loss and MLE loss. The advantage of the proposed VL feature is shown in comparisons between Exp.#1 v.s #3 and Exp.#2 v.s #4 where we compare between vision feature (i.e. C3D) and VL feature. Both of our proposed VL feature and VL loss contribute in improving the accuracy and diversity metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this work, we present a novel VLCap network for video paragraph captioning. Our VLCap network is trained in an endto-end framework with a two-fold contribution: (i) VL feature, which extracts the global visual features of the entire scene and local linguistics feature of scene elements; and (ii) VL loss, which is trained by a constrative learning mechanism. In VLCap network, the intra-event coherence is learnt by a Transformer whereas the inter-event coherence is modeled by GRUlike memory. Comprehensive experiments and ablation studies on ActivityNet Captions and YouCookII datasets demonstrate the effectiveness of our VLCap, which outperforms the existing SOTA approaches on both accuracy (BLEU@4, METEOR, CIDEr, ROUGE) and diversity (Div@2, R@4) metrics. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overall network architecture of our proposed VLCap consisting of two modules i.e. (i) VLCap Encoder (left) takes a snippet S i as an input and returns VL feature f V L i as its output. VLCap Decoder (right) takes a list of VL features {f V L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Datasets information. 2 nd col.: number of training videos; 3 rd cols.: number of validation videos. 4 st col.: number of event segments for each video on average. This module aims to extract VL feature F V L given an event e, presented by a sequence of frames V e . Follow the standard setup<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13]</ref>, we divide V e into L snippets,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>#event</cell></row><row><cell>Dataset</cell><cell>train</cell><cell>val</cell><cell>/ video</cell></row><row><cell cols="4">ActivityNet Captions [21] 10,009 4,917 3.65</cell></row><row><cell>YouCookII [22]</cell><cell>1,333</cell><cell>457</cell><cell>7.7</cell></row><row><cell>2.1. VLCap Encoder</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Performance comparison of VLCap with other SOTA models on ActivityNet Captions ae-val. ? denotes results by us. Performance comparison of VLCap with other SOTA models on ActivityNet Captions ae-test. ? denotes results by us.</figDesc><table><row><cell>Methods</cell><cell>Year</cell><cell>Input</cell><cell cols="2">B@4 ?</cell><cell>M ?</cell><cell>C ?</cell><cell>R ?</cell><cell>Div@2 ? R@4 ?</cell></row><row><cell cols="3">Vanilla Transformer [4] CVPR2018 Res200 + Flow</cell><cell>9.75</cell><cell></cell><cell cols="3">15.64 22.16 28.90  ?</cell><cell>77.40  ?</cell><cell>7.79</cell></row><row><cell>AdvInf [11]</cell><cell cols="2">CVPR2019 C3D + Object</cell><cell cols="4">10.04 16.60 20.97</cell><cell>-</cell><cell>-</cell><cell>5.76</cell></row><row><cell>GVD [12]</cell><cell cols="6">CVPR2019 Res200 + Flow + Object 11.04 15.71 21.95</cell><cell>-</cell><cell>-</cell><cell>8.76</cell></row><row><cell>Transformer-XL [1]</cell><cell>ACL2019</cell><cell>Res200 + Flow</cell><cell cols="5">10.39 15.09 21.67 30.18  ?</cell><cell>75.96  ?</cell><cell>8.54</cell></row><row><cell cols="2">Transformer-XLRG [2] ACL2020</cell><cell>Res200 + Flow</cell><cell cols="4">10.17 14.77 20.40</cell><cell>-</cell><cell>-</cell><cell>8.85</cell></row><row><cell>MART [2]</cell><cell>ACL2020</cell><cell>Res200 + Flow</cell><cell cols="5">10.33 15.68 23.42 30.32  ?</cell><cell>75.71  ?</cell><cell>5.18</cell></row><row><cell>PDVC [13]</cell><cell cols="2">ICCV2021 C3D + Flow</cell><cell cols="4">11.80 15.93 27.27</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VLCap (ours)</cell><cell>-</cell><cell>C3D + Language</cell><cell cols="5">14.00 17.78 32.58 36.37</cell><cell>78.01</cell><cell>4.42</cell></row><row><cell>Methods</cell><cell>Year</cell><cell>Input</cell><cell>B@4 ?</cell><cell></cell><cell>M ?</cell><cell>C ?</cell><cell>R ?</cell><cell>Div@2 ? R@4 ?</cell></row><row><cell cols="3">Vanilla Transformer [4] CVPR2018 Res200 + Flow</cell><cell>9.31</cell><cell cols="4">15.54 21.33 28.98  ?</cell><cell>77.29  ?</cell><cell>7.45</cell></row><row><cell>Transformer-XL [1]</cell><cell>ACL2019</cell><cell>Res200 + Flow</cell><cell cols="5">10.25 14.91 21.71 30.25  ?</cell><cell>76.17  ?</cell><cell>8.79</cell></row><row><cell cols="2">Transformer-XLRG [2] ACL2020</cell><cell>Res200 + Flow</cell><cell cols="4">10.07 14.58 20.34</cell><cell>-</cell><cell>-</cell><cell>9.37</cell></row><row><cell>MART [2]</cell><cell>ACL2020</cell><cell>Res200 + Flow</cell><cell>9.78</cell><cell cols="4">15.57 22.16 30.85  ?</cell><cell>75.69  ?</cell><cell>5.44</cell></row><row><cell>MART w/ COOT [23]</cell><cell>NIPS2020</cell><cell>COOT</cell><cell cols="4">10.85 15.99 28.19</cell><cell>-</cell><cell>-</cell><cell>6.64</cell></row><row><cell>VLCap (ours)</cell><cell>-</cell><cell cols="6">C3D + Language 13.38 17.48 30.29 35.99</cell><cell>78.29</cell><cell>4.18</cell></row><row><cell>f w</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>i onto text projection matrix W t pre-trained by CLIP to obtain word embedding word i.e</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison of VLCap with other SOTA models on YouCookII validation set.</figDesc><table><row><cell>Methods</cell><cell>Year</cell><cell>Input</cell><cell>B@4 ?</cell><cell>M ?</cell><cell>C?</cell><cell>R ?</cell><cell cols="2">Div@2 ? R@4 ?</cell></row><row><cell cols="2">Vanilla Transformer [4] CVPR2018</cell><cell>Res200 + Flow</cell><cell>4.38</cell><cell cols="2">11.55 38.00</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GPaS [28]</cell><cell cols="2">IEEE-TM2020 Res200</cell><cell>1.64</cell><cell cols="3">12.20 41.44 27.98</cell><cell>-</cell><cell>-</cell></row><row><cell>MART [2]</cell><cell>ACL2020</cell><cell>Res200 + Flow</cell><cell>8.00</cell><cell cols="2">15.90 35.74</cell><cell>-</cell><cell>-</cell><cell>4.39</cell></row><row><cell>MART w/ COOT [23]</cell><cell>NIPS 2020</cell><cell>COOT</cell><cell>9.44</cell><cell cols="2">18.17 46.06</cell><cell>-</cell><cell>-</cell><cell>6.30</cell></row><row><cell>VLCap (ours)</cell><cell>-</cell><cell>C3D + Language</cell><cell>9.56</cell><cell cols="3">17.95 49.41 35.17</cell><cell>67.97</cell><cell>5.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on the contribution of the proposed VL feature and VL loss L V L on ActivityNet Captions dataset.</figDesc><table><row><cell>ae-test</cell><cell>ae-val</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MART: Memory-augmented recurrent transformer for coherent video paragraph captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">BMVC</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need,&quot; in NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Move forward and tell: A progressive generator of video descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning longterm dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effects of Language on Visual Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lupyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boroditsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial inference for multi-sentence video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grounded video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with parallel decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">COOT: cooperative hierarchical transformer for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ging</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Styleclip: Text-driven manipulation of stylegan imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">CLIP Meets Video Captioners: Attribute-Aware Representation Learning Promotes Accurate Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dense Video Captioning Using Graph-Based Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimed</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">UNITER: universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Speaking the same language: Matching machine to human captions by adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
