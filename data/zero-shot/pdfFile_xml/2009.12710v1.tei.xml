<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Heterogeneous Molecular Graph Neural Networks for Predicting Molecule Properties</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeren</forename><surname>Shui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Minnesota Minneapolis</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
							<email>karypis@umn.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Minnesota Minneapolis</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Heterogeneous Molecular Graph Neural Networks for Predicting Molecule Properties</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Heterogeneous molecular graphs</term>
					<term>many-body in- teractions</term>
					<term>graph neural networks</term>
					<term>molecular property prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As they carry great potential for modeling complex interactions, graph neural network (GNN)-based methods have been widely used to predict quantum mechanical properties of molecules. Most of the existing methods treat molecules as molecular graphs in which atoms are modeled as nodes. They characterize each atom's chemical environment by modeling its pairwise interactions with other atoms in the molecule. Although these methods achieve a great success, limited amount of works explicitly take many-body interactions, i.e., interactions between three and more atoms, into consideration. In this paper, we introduce a novel graph representation of molecules, heterogeneous molecular graph (HMG) in which nodes and edges are of various types, to model many-body interactions. HMGs have the potential to carry complex geometric information. To leverage the rich information stored in HMGs for chemical prediction problems, we build heterogeneous molecular graph neural networks (HMGNN) on the basis of a neural message passing scheme. HMGNN incorporates global molecule representations and an attention mechanism into the prediction process. The predictions of HMGNN are invariant to translation and rotation of atom coordinates, and permutation of atom indices. Our model achieves state-of-the-art performance in 9 out of 12 tasks on the QM9 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-As they carry great potential for modeling complex interactions, graph neural network (GNN)-based methods have been widely used to predict quantum mechanical properties of molecules. Most of the existing methods treat molecules as molecular graphs in which atoms are modeled as nodes. They characterize each atom's chemical environment by modeling its pairwise interactions with other atoms in the molecule. Although these methods achieve a great success, limited amount of works explicitly take many-body interactions, i.e., interactions between three and more atoms, into consideration. In this paper, we introduce a novel graph representation of molecules, heterogeneous molecular graph (HMG) in which nodes and edges are of various types, to model many-body interactions. HMGs have the potential to carry complex geometric information. To leverage the rich information stored in HMGs for chemical prediction problems, we build heterogeneous molecular graph neural networks (HMGNN) on the basis of a neural message passing scheme. HMGNN incorporates global molecule representations and an attention mechanism into the prediction process. The predictions of HMGNN are invariant to translation and rotation of atom coordinates, and permutation of atom indices. Our model achieves state-of-the-art performance in 9 out of 12 tasks on the QM9 dataset.</p><p>Index Terms-Heterogeneous molecular graphs, many-body interactions, graph neural networks, molecular property prediction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Predicting quantum mechanical properties of molecules based on their structures is important for molecule screening and drug design. We can compute exact molecular properties by solving the many-body Schr?dinger equation. However, closed form solution to this equation is only available for simple systems. Although researchers developed methods such as Density Functional Theory (DFT) <ref type="bibr" target="#b0">[1]</ref> to approximate the solution, the computational cost of these methods scales poorly and is worse than O(n 3 ) w.r.t. the number of electrons.</p><p>Recently, researchers have been developing machine learning methods that are orders of magnitude faster with a moderate compromise in prediction accuracy. Among the machine learning approaches, graph neural network (GNN)based methods attract a lot of research attention as their ability to model complex interactions among atoms. These methods treat molecules as molecular graphs (e.g., distance graphs <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>, chemical graphs <ref type="bibr" target="#b5">[6]</ref>, K-nearest neighbor graphs <ref type="bibr" target="#b6">[7]</ref>) in which atoms are modeled as nodes. They compute an atom's low-dimensional representation as a function of its feature and characteristics of its graph neighbors. The low-dimensional representations are then used to estimate the local contribution of the atoms to the desired property, or to compute a global representation of the molecule for downstream predictions.</p><p>The many-body expansion (MBE) <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> is an important scheme that computes the energy of an N -particle system as the sum of the contributions of many-body terms</p><formula xml:id="formula_0">E = i E i + i&lt;j E ij + i&lt;j&lt;k E ijk + ? ? ? + E 12???N , (1)</formula><p>where E i is the local energy contribution of a single atom, E ij is the energy contribution of a two-body (a group of two atoms), E ijk is the energy contribution of a three-body, and eventually E 12???N is the contribution of the body formed by all the atoms in the molecule. Note that, the local contribution to the total energy decreases fast with the number of atoms in the many-body. As most of the existing GNN-based methods are developed on molecular graphs, they focus mainly on modeling atom-based representations, interactions, and predictions which correspond to the first two terms of the series and do not have an explicit characterization of the higher order terms. This may compromise their accuracy in the chemical prediction problems.</p><p>In this paper, we introduce a novel graph representation of molecules, heterogeneous molecular graph (HMG), to explicitly model many-body interactions. A p-body (the value of p is called the order of the many-body) is a group of p atoms that functions as a whole entity. In HMGs, a p-body is modeled as a node of order p. Nodes connect to nodes of the same or different order via different types of edges. This heterogeneous structure allows us to explicitly model interactions, representations, and predictions associated with many-bodies. Moreover, edges between nodes of the same order carry the potential of incorporating complex geometric information (e.g., bond angles and dihedral angles) into node embeddings.</p><p>To leverage the rich information stored in HMG for tasks of molecular property predictions, we design heterogeneous molecular graph neural networks (HMGNN) by following a message passing framework. In the message passing framework <ref type="bibr" target="#b5">[6]</ref>, nodes send and receive messages from their neigh-bors and update their low-dimensional representations using the received messages. HMGNN is a multi-task learning <ref type="bibr" target="#b10">[11]</ref> model whose design is inspired by the MBE of energy surfaces. In HMGNN, each many-body order possesses its own set of parameters and shares computations with other orders. In the prediction phase, HMGNN computes one estimation for each many-body and aggregates them based on their orders. It uses an attention-based model that takes into account a global representation of the molecule to fuse the prediction of different orders, which correspond to different terms in Eq 1. We design a multi-task learning loss that enforces the prediction of each order and the fused prediction to be close to the true target. Experimental results show that the fused prediction is better than any of the standalone predictions. The fusing weight of the predictions are also consistent with the convergence assumption in the many-body expansion.</p><p>The main contribution of this work lies in two folds. First, we propose HMG which allows graph learning methods to explicitly model many-body representation, interaction, and prediction. Second, we develop a multi-task learning method HMGNN for the task of molecule property prediction. HMGNN explicitly incorporates many-body interaction and a global molecule representation into the prediction process and achieves state-of-the-art performance on the QM9 dataset <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The code of HMGNN is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. REVIEW OF RELEVANT PRIOR WORKS</head><p>Traditionally, prediction of many important molecular properties such as atomization energies relies on methods that approximate the solution of the many-body Schr?dinger equation such as density function theory (DFT) and its variants <ref type="bibr" target="#b13">[14]</ref>. This class of methods involves solving complex linear systems and has a computational complexity worse than O(n 3 ) where n is the number of atoms.</p><p>Recent years have seen a surge in data-driven methods that train machine learning models to learn patterns from molecule databases. The learned patterns are assumed to be general in chemical space and can be used to estimate properties of unknown compounds. These attempts started from <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> which feed hand-crafted molecule descriptors (e.g., Coulomb matrix, bag of bonds) into regression models such as linear regression and random forests. These methods rely heavily on the quality of the crafted descriptors and have limited representation power.</p><p>Recently, graph neural networks (GNN) have been achieving a great success in graph-related applications <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b19">[20]</ref>. In chemistry, researchers developed GNN-based method for learning tasks over graph represented molecules. The authors of <ref type="bibr" target="#b5">[6]</ref> introduced a generic framework over chemical graphs that models interactions between atoms in a message passing fashion. In <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b20">[21]</ref>, the authors designed neural network structures that have no dependency on hand-crafted features but learn molecule representations from only atom types and coordinates. Since GNNs possess a hierarchical structure, i.e., 1 https://github.com/shuix007/HMGNN they iteratively apply GNN layers on graphs to encode each node's multi-hop neighbors into its embedding, GNN-based methods <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b1">[2]</ref> further decompose atom-wise prediction to layer-wise atom prediction to fit in the MBE framework. Although these methods include many-body contributions into final predictions, they do not have an explicit modeling of many-body representations and interactions. Some recent works have incorporated many-body interactions and representations by updating edge embeddings along message passing <ref type="bibr" target="#b6">[7]</ref> or by passing messages on line graphs of the corresponding molecular graphs <ref type="bibr" target="#b22">[23]</ref>. However, these methods capture only partial many-body interactions and lack manybody predictions.</p><p>Equivariant neural network is another class of neural network methods that has been applied in chemical prediction problems. The notion of group equivariant neural network was first introduced by <ref type="bibr" target="#b23">[24]</ref> in the domain of image processing. Later, researchers developed neural network methods that are equivariant to continuous rotations for learning representations for 3D objects, including molecules <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>. These methods achieve rotation invariance by transforming objects from Euclidean space to Fourier space and conducting computations in Fourier space. In these methods, each many-body interacts only with itself but not other many-bodies. Thus, they are not optimal in predicting molecule properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NOTATIONS AND DEFINITIONS</head><p>We denote matrices by bold upper-case letters (e.g., W), and vectors by bold lower-case letters (e.g., x). We denote entries of a matrix/vector by lower-case letter with subscripts (e.g., x ij /x i ). We use superscripts to indicate variables at the t-th message passing layer (e.g., h (t) ). We denote molecular graphs by G = (V, E) where V and E represent the set of nodes (atoms) and edges, respectively. Two atoms are connected in a molecular graph when the Euclidean distance between them is less than a cutoff threshold c &gt; 0. Each edge in the graph is associated with a distance to store the geometric structure of the molecule. We define a p-body in a molecular graph G as a p-clique of the graph. We refer to the value of p as the order of the many-body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HETEROGENEOUS MOLECULAR GRAPH AND MANY-BODY INTERACTIONS</head><p>In this section, we illustrate the construction of heterogeneous molecular graphs (HMG) and how we leverage the heterogeneous structure of HMGs to model many-body representations and interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Heterogeneous Molecular Graph</head><p>An HMG is a graph in which nodes are many-bodies and edges are defined by various types of geometric and set relations. HMGs are constructed from molecular graphs. We denote an HMG of order N of a molecular graph G as</p><formula xml:id="formula_1">H N (G) = ({V p } , {E pq }) where 1 ? p ? q ? N , V p is the set of p-bodies in G (i.e., all p-cliques of G)</formula><p>, and E pq is the set of edges between V p and V q . We denote the order p of (a) A formaldehyde molecule. (b) A molecular graph of the formaldehyde molecule.</p><p>(c) A heterogeneous molecular graph of the molecular graph. <ref type="figure">Fig. 1</ref>: An example of heterogeneous molecular graph (HMG). <ref type="figure">Figure 1</ref>(a) is a spatial structure of a formaldehyde (CH 2 O) molecule. Each atom in the molecule is associated with a three-dimensional coordinates in the Euclidean space. <ref type="figure">Figure 1</ref></p><formula xml:id="formula_2">(b)</formula><p>is the molecular graph of the methanol molecule with a cutoff distance c = 2. We convert atom coordinates to pair-wise distances to guarantee translation and rotation invariance of the representation. We denote edges whose distances are less than c using black solid lines, and edges that are broke by the cutoff using black dotted lines. <ref type="figure">Figure 1</ref>(c) is a HMG of order two constructed from the molecular graph. There are two types of nodes (1-bodies and 2-bodies denoted by yellow and blue circles, respectively) and three types of edges (1-1 and 2-2 denoted by yellow and blue lines, respectively, 1-2 denoted by black dashed lines) in the HMG. Edges between nodes of the same order are associated with features that depict the geometric relation between the nodes (distance for 1-1 edges, angle for 2-2 edges).</p><p>p-bodies as the node type and p-q as the type of the edges that connect nodes of order p and nodes of order q. Given two nodes i ? V p and j ? V q , when they are of the same order, i.e., p = q, i and j are connected if they share p ? 1 atoms. A special case is when p = q = 1, instead of building a complete graph, we use the edge set E of the molecular graph to define connections. When the two nodes are of different orders, presumably p &lt; q, (i, j) ? E pq if i is a sub-graph of j. An example HMG is shown in <ref type="figure">Figure 1</ref>. With this formulation, we can explicitly model up to N -body representations by node embeddings and N + 1-body interactions by message passing.</p><p>In an HMG, each node i of order p is associated with a discrete feature Z p,i that indicates its atomic composition, and a continuous feature x p,i that describes aspects of its geometry. Note that, nodes of order 1 do not have continuous features since they are points in the Euclidean space and do not have geometric structure. Each edge (i, j) is associated with an edge feature e p,ij when i and j are of the same order p. The edge feature characterizes the geometric relation between the two nodes, e.g., distance between atoms, angles between bonds. In this paper, we use a hash function to map the set of atomic numbers of the atoms to Z p,i . Construction of continuous node features and edge features requires feature engineering especially when order of the many-bodies are high. We will illustrate how we convert geometric information to feature vectors up to the second order in Section VI-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Message Passing on Heterogeneous Molecular Graphs</head><p>The message passing framework consists of two phases, message passing and node update. On molecular graphs, each node (atom) i sends/receives messages to/from its neighbors and uses the received messages to update its embedding</p><formula xml:id="formula_3">m (t) i = j?N (i) f (t) h (t) i , h (t) j , e ij h (t+1) i = g (t) h (t) i , m (t) i . (2) In Eq-2, N (i) is the set of neighbor nodes of i, h (t) i is the node (atom) embedding of i, m (t) i</formula><p>is the aggregation of messages from i's neighbor nodes, e ij is the edge feature associated with the edge between i and j, f (?) is a message function that maps embeddings of the sender and the receiver and the corresponding edge feature to a message vector, g(?) is a node update function that combines the incoming message and the old embedding to be the new node embedding. Both f (?) and g(?) are learnable. Message passing on HMGs is different from that on molecular graphs due to the heterogeneous property of HMGs. Nodes in HMGs are of different orders and they pass messages through edges of different types. A message passing framework needs to learn edge type specific message functions and order specific node update functions to capture this heterogeneous structure. Moreover, the framework should allow inter-order message passing such that the node embeddings can capture information from other orders. For example, by passing messages from 2-bodies, 1-bodies can encode edge angle information into their embeddings. Let i ? V p be a node of order p in a HMG and h (t) p,i be its embedding at the t-th layer, we design the message passing framework as</p><formula xml:id="formula_4">m (t) q,i = j?Nq(i) f (t) qp h (t) p,i , h (t) q,j , e ij h (t+1) p,i = g (t) p h (t) p,i , m (t) 1,i , m (t) 2,i , ? ? ? , m (t) N,i<label>(3)</label></formula><p>where N q (i) the set of nodes of order q that are connected to i, m (t) q,i denotes the aggregated messages from nodes i's neighbor nodes of order q, e ij denotes the edge feature between i and j if they are of the same order, f pq (?) and g p (?) are learnable functions specific to edge type pq and node type (order) p, respectively. Compare to the message passing framework on molecular graphs which has two functions to learn, this framework possesses larger model capacity and is able to model many-body interactions explicitly.</p><p>V. HETEROGENEOUS MOLECULAR GRAPH NEURAL NETWORKS.</p><p>We present Heterogeneous Molecular Graph Neural Networks (HMGNN) for the purpose of predicting molecule properties. An HMGNN contains four types of modules, input module, interaction module, output module, and fusion module. All the modules except the fusion module are order specific. HMGNNs learn functions for message passing on heterogeneous molecular graphs to compute local node representations, and uses a readout function to combine the representations to form a global molecule representation. HMGNNs compute node-wise contributions to the target property and aggregates them based on their orders. The final prediction is a weighted combination of the predictions of all orders where the weights are computed by an attention mechanism from the global molecule representation. An HMGNN is learned by optimizing a loss function which forces predictions of each order and the fused prediction to be close to the true target. Since the construction of heterogeneous molecular graphs and associated features rely on atom pairwise distances and atomic numbers but not atom coordinates, HMGNNs are invariant under both translations and rotations. HMGNNs are also permutation invariant to atom indices as the message aggregation function in Eq-3 and the readout function are permutation invariant <ref type="bibr" target="#b27">[28]</ref>. <ref type="figure">Figure 2</ref> shows an overview of the architecture of HMGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Input Module</head><p>The input module of HMGNN converts raw features of nodes to latent embeddings. As we described in Section IV-A, each node i ? V p in a HMG is associated with a discrete feature Z p,i and a continuous feature x p,i . We use an embedding lookup table to map the discrete feature Z p,i to a real value vector e Zp,i and apply a fully connected layer to the concatenation of the latent vector e Zp,i and the continuous feature x p,i to get the initial node embedding</p><formula xml:id="formula_5">h (1) p,i = ? W in p e Zp,i x p,i + b in p<label>(4)</label></formula><p>where W in p and b in p are learnable parameters for nodes of order p (p-bodies), ?(?) is an element-wise activation function, denotes concatenation of vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Interaction Module</head><p>HMGNN stacks T interaction modules to encode information across far reaches of the heterogeneous molecular graph into node embeddings. Each interaction module takes the output embeddings of the previous module and update the embeddings. Note that, edges between nodes of the same orders have features while other edges do not. As a result, we paramatrize the message functions between nodes of the same order as</p><formula xml:id="formula_6">m (t) p,i = j?Np(i) G (t) p e p,ij ? W (t) pp h (t) p,j + b (t) pp<label>(5)</label></formula><p>and the message functions along edges without features as</p><formula xml:id="formula_7">m (t) q,i = j?Nq(i) ? W (t) qp h (t) q,j + b (t) qp .<label>(6)</label></formula><p>In Eq-5 and Eq-6, N p (i) and N q (i) denotes the set of neighbor nodes of order p and order q of node i, respectively, denotes the Hadamard product, G, W, and b are learnable parameters. A node embedding h (t) p,i is then updated as a function of its old embedding and the incoming messages,</p><formula xml:id="formula_8">h (t+1) p,i = h (t) p,i +? W (t) p h (t) p,i m (t) 1,i ? ? ? m (t) N,i + b (t) p ,<label>(7)</label></formula><p>where denotes concatenation of vectors. The interaction module then refines the node embeddings with two consecutive fully connected layers with residual connections <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Output Module</head><p>Each many-body order p possesses a specific output module that passes the output of its interaction module, final node embeddings h</p><formula xml:id="formula_9">(T +1) p,i</formula><p>, through a sequence of linear mappings and a aggregation process to compute the estimated value of the target property. First, we use a fully connected layer to convert the node embeddings to node predictions</p><formula xml:id="formula_10">y p,i = w out p h (T +1) p,i + b out p .<label>(8)</label></formula><p>where w out p and b out p are learnable parameters for nodes of order p. Then we follow <ref type="bibr" target="#b1">[2]</ref> and scale the predictions with scaling parameters that are specific to the discrete feature Z p,i of the nodes? p,i = s Zp,i?p,i + r Zp,i .</p><p>where s and r are learnable embedding lookup tables that map Z p,i to the corresponding scaling factors and shifts. The goal of the scaling layer is to adapt the magnitude of the predictions to different unit systems of the target property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fusion Module</head><p>The fusion module computes a global molecule representation out of the final node embeddings and uses the global representation to weigh the prediction of different orders. We sum the final node embeddings h </p><formula xml:id="formula_12">v = i?V1 h (T +1) 1,i i?V2 h (T +1) 2,i ? ? ? i?V N h (T +1) N,i .<label>(10)</label></formula><p>Since node embeddings of different orders are computed by different parameters and the number of nodes of the orders also HMGNN:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Residual:</p><p>Interaction: Output: Fusion: <ref type="figure">Fig. 2</ref>: Computation flow of heterogeneous molecular graph neural networks (HMGNN) for many-bodies up to order two. We use to represent the input to the function. The activation function is set to be the shifted softplus function, i.e., ?(x) = ln(0.5e x + 0.5). Each many-body order p owns its input module, interaction module, and output module. For each node i of order p, an input module converts the discrete and continuous feature of the node to an initial node embedding h (0) p,i . HMGNN passes the initial embeddings through a stack of T interaction modules to encode information from its neighbor nodes of different orders to the node embedding. The outputs of the last interaction module, the final node embedding h (T +1) p,i , are then fed into a fusion module and an output module to compute a weight vector ? and prediction? p,i , respectively. HMGNN sums the predictions per many-body order and computes the final prediction as a weighted sum of these summed predictions.</p><p>varies, the distributions of the order specific representations could be dramatically different from each other. In order to unify the distributions of the representations and to accelerate training, we apply batch normalization <ref type="bibr" target="#b29">[30]</ref> followed by a fully connected layer on the intermediate representation to obtain the global representation v = BatchNorm (?)</p><formula xml:id="formula_13">z = ? (Wv + b) .<label>(11)</label></formula><p>Then we pass the global representation through an attention layer to compute the weight ? p that measures the importance of the predictions of order p</p><formula xml:id="formula_14">? p = exp LeakyReLU z T a p N q=1 exp (LeakyReLU (z T a q ))<label>(12)</label></formula><p>where a are learnable vectors, and p ? p = 1. We can understand the global representation as a query to the knowledgebase distilled in a for assigning contributions to predictions of different orders. This gives the model better flexibility and explainability in dealing with different molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Final prediction</head><p>Inspired by the many-body expansion, we decompose the final prediction as a weighted sum of the prediction of different order?</p><formula xml:id="formula_15">y = ? 1 i?V1? 1,i + ? 2 i?V2? 2,i + ? 3 i?V3? 3,i + ? ? ?<label>(13)</label></formula><p>where the weights ? p are computed by the fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Model Training</head><p>Since all the modules in HMGNNs except for the fusion module are order specific, and the final prediction is a weighted average of the predictions per order, training HMGNNs by optimizing objective functions that only depend on the final prediction (the fused prediction) may cause gradient vanishing issues for parameters of some orders so that these parameters do not learn enough and lose their prediction utilities. To avoid this issue, we treat the computation of each order as a separate prediction task and propose a multitask objective function that forces the prediction of all orders together with the final prediction to be close to the true target</p><formula xml:id="formula_16">L = 1 N + 1 |? ? y| + N p=1 |? p ? y| + ? ? 2 2<label>(14)</label></formula><p>where? p = i?Vp? p,i is the node order specific prediction, ? denotes all trainable parameters of the model, ? ? 0 is a hyper-parameter that controls the strength of L 2 normalization to prevent the model overfits. This objective function preserves gradient flow for parameters of each order and gives higher training importance to orders that the fussing module assigning larger weights to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Complexity Analysis</head><p>The time and space complexity of HMGNN depends linearly on the number of nodes and edges in a HMG. The number of nodes determines the complexity of the input module and the output module while the number of edges determines the complexity of message passing.</p><p>Let G be a molecular graph with N atoms and H P (G) be its HMG that explicitly models up to P -bodies. We assume G is a complete graph for the worst case scenario. The number of nodes of order p in H P (G) is N p . Let i ? V p be a node of order p (i.e., a q-body), i is connected to nodes that are of order q where q ? {1, ? ? ? , P }. When q &lt; p, the number of q order neighbors of node i is p q as i is connected to all q-bodies who are sub-graphs of i; when q = p, the number of order p neighbors of i is p(N ? p) since i is connected to p-bodies who share p ? 1 atoms with i; When q &gt; p, the number of q-body neighbors of i is N ?p q?p . As a result, the complexity of message passing is</p><formula xml:id="formula_17">P p=1 N p p?1 q=1 p q + P q=p+1 N ? p q ? p + p(N ? p)<label>(15)</label></formula><p>and the complexity of the input/output module of HMGNN is P p=1 N p . In this paper, we experiment with HMGs and HMGNNs for up to 2-bodies, consequently, the time complexity and space complexity of our model are both O(N 3 ). Modern computing architectures such as graphics processing unit (GPU) and tensor processing unit (TPU) are optimized to accelerate this computation. Empirically, HMGNNs can generate property predictions for 10000 randomly drawn molecules from the QM9 dataset in 4 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>We conduct experiments to investigate three research problems in regards of many-body modeling and the HMGNN model ? How does HMGNN perform in the molecule property prediction tasks compared against the current state-ofthe-art methods? ? How does many-body representation, interaction, and prediction contribute to the prediction? ? What is the utility of the components of HMGNN?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We experiment with HMGs and HMGNNs for many-bodies up to order two. There are two types of nodes (1-bodies and 2-bodies), two types of edges with edge features (1-1 and 2-2 edges), and one type of edge without edge features (1-2 edges). Since 1-bodies are atoms, they only have discrete features. Each 2-body i is determined by its two end atoms and the distance between them d 2,i .</p><p>There are three types of geometries that we need to model, distance d ij ? (0, c) between 1-bodies i and j, length l 2,i ? (0, c) of 2-bodies, and angle ? ij ? [0, ?] between 2-bodies i and j. We use a set of K radial basis functions (RBF) to </p><formula xml:id="formula_18">x k = exp ?? k (exp (?x) ? ? k ) 2<label>(16)</label></formula><p>where ? k and ? k specify the center and width of x k . For distance d ij between 1-bodies, we multiply its feature vector by a continuous monotonic decreasing function ?(d ij ) that has ?(0) = 1 and ?(c) = 0. With this formulation, an 1body node will have less influence to/from its distant order 1 neighbors. We follow <ref type="bibr" target="#b1">[2]</ref> and set the value of ? k to be equally spaced between exp (?a) and exp (?b) while ? k = (2K ?1 (exp (?a) ? exp (?b))) ?2 . The goal of using RBFs is to decorrelate the scalar features to accelerate training <ref type="bibr" target="#b2">[3]</ref>. We apply three different sets of RBFs to convert the distance d ij , the length l 2,i , and the angle ? ij to the corresponding features e 1,ij , x 2,i , and e 2,ij , respectively. We set the latent dimension to be 128 and use 5 interaction modules for our experiments. We use the shifted softplus function as the activation function. For ZPVE, U , U 0 , H, G and C v , the cutoff distances c = 3 while for other targets c = 5. We initialize the weights of fully connected layers with random orthogonal matrices scaled by the glorot initialization scheme <ref type="bibr" target="#b30">[31]</ref> and the bias to zero. For learning the parameters of HMGNN, we run the AMSGrad algorithm <ref type="bibr" target="#b31">[32]</ref> with a batch size of 32 for up to 3000000 steps and set the L 2 regularizer ? to be 1 ? 10 ?6 . We initialize the learning rate to be 1 ? 10 ?3 and multiply it with 0.1 every 2000000 gradient steps. The training algorithm stops if the MAE on the validation set does not decrease for 1000000 steps. We implement HMGNN using the Deep Graph Library (DGL) <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setting</head><p>We evaluate the performance of the proposed model on the QM9 dataset <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. QM9 is a widely used benchmark for evaluating models that predict molecule properties. It consists of around 130K equilibrium molecules associated with 12 geometric, energetic, electronic, and thermodynamic properties. The properties are described in <ref type="table" target="#tab_0">Table I</ref>. These molecules contain up to nine heavy atoms (C, O, N, and F). We randomly select 110000 molecules for training, 10000 molecules for validation, and 10831 molecules as the test set. We conduct model selection for different targets on the validation set and report the mean absolute error (MAE) of the best performing models. For properties with atomic reference values (U 0 , U , H, G, C v ), we subtract the original value by the per-atom-type reference values to be the target. Since ? is defined as the gap between LUMO and HOMO , we predict it as ? = LUMO ? HOMO . In our experiments, we convert the units of HOMO , LUMO , ? , ZPVE, U 0 , U , H, G to eV.</p><p>We compare the performance of HMGNN with six stateof-the-art methods, enn-s2s <ref type="bibr" target="#b5">[6]</ref>, SchNet <ref type="bibr" target="#b2">[3]</ref>, neural message passing with edge updates (NMP-edge) <ref type="bibr" target="#b6">[7]</ref>, Cormorant <ref type="bibr" target="#b24">[25]</ref>, PhysNet <ref type="bibr" target="#b1">[2]</ref>, and directional message passing neural network (DimeNet) <ref type="bibr" target="#b22">[23]</ref>. Results of enn-s2s, SchNet, NMP-edge, Cormorant, and DimeNet are from the corresponding papers. We take the results of PhysNet from <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prediction Performance</head><p>We show the prediction performance of HMGNN and the competing methods on the 12 properties of QM9 in <ref type="table" target="#tab_0">Table  II</ref>. Our proposed method sets the new state-of-the-art on 9 out of the 12 target properties. HMGNN's performance aligns with the best results on the remaining targets with an exception of R 2 . We also present the performance of summing over predictions over 1-bodies (HMGNN-1) and 2bodies (HMGNN-2), respectively. Although the performance of HMGNN-2 is consistently worse than HMGNN-1, their weighted combination outperforms any of the standalone prediction. This demonstrates the effectiveness of the fusion module driven by the global molecule representations and the attention mechanism, and that explicitly modeling and computing predictions of many-bodies can be beneficial for chemical prediction tasks.</p><p>We analyze the effect of a critical hyper-parameter, the cutoff distance c, on prediction performances of four types of properties. We choose U 0 to represent properties related to atomization energies (U 0 , U , H, G), C v to represent thermodynamic properties (C v ), ZPVE to represent properties related to fundamental vibrations of the molecule (ZPVE), and ? to represent electronic properties (?, ?, HOMO , LUMO , ? , R 2 ) <ref type="bibr" target="#b5">[6]</ref>. We present the training and test mean absolute error (MAE) of HMGNNs on HMGs constructed with c ? {2, 3, 5} in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>When constructing molecular graphs as well as HMGs, the larger the cutoff distance we choose, the less geometric information about the molecules that we lose. However, a large cutoff value does not always lead to better performance. In <ref type="figure" target="#fig_2">Figure 3</ref>, despite the training error decreases across all the four targets as the cutoff value increases, the test error shows an increasing trend for three properties. This is a signal that the model over-fits the training set on the three properties. This is because of the large model capacity of HMGNNs as they have one set of parameters for each many-body order. An HMGNN of order N possesses N times the number of parameters of a normal GNN-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>In this section, we conduct ablation study on two targets (i.e., U 0 , C v ) to demonstrate the importance of the multitask learning loss, inter-order message passing, and explicit modeling of high-order bodies in improving the performance of molecular property prediction. We propose three variants of the HMGNN model and show their results in <ref type="table" target="#tab_0">Table III.</ref> 1) Remove MTL (Multi-Task Learning loss): This variant has the same specification with the default model. It differs with the default model in that it is trained by minimizing the naive loss |??y| instead of the multi-task learning loss that we proposed in Eq-14. As shown in <ref type="table" target="#tab_0">Table III</ref>, the 2-bodies of this variant lose their prediction power while the fusion module gives all attention weights to the 1-bodies, and as a result, the performance of this variant is worse than the default HMGNN. Furthermore, the prediction of 1-bodies (i.e., HMGNN-1) is also less accurate than the default model.</p><p>2) Remove IOMP (Inter-Order Message Passing): This variant removes edges/messages between 1-bodies and 2bodies, as a result, information of the two orders are not shared. We can see that the performance of HMGNN-1 and HMGNN drops in the prediction of both U 0 and C v . This demonstrates the importance of inter-order message passing. However, the prediction accuracy of HMGNN-2 on U 0 is better than models with inter-order message passing. This might because 2-bodies (both distance and angle) contain more geometric information than 1-bodies (only distance).</p><p>3) Remove HO (High-Order modeling): This variant removes high-order related modeling (2-body interaction, representation, and prediction) and is similar to existing GNN-based prediction methods (i.e., PhysNet). As shown in <ref type="table" target="#tab_0">Table III</ref>, this method performs worse than HMGNN-1 of the variant that removes multi-task learning loss. This shows another evidence of the effectiveness of inter-order message passing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization of Attention weights</head><p>In <ref type="figure" target="#fig_4">Figure 4</ref>, we show the attention scores of the 1-body predictions generated by the fusion module for predicting U 0 , C v , ?, and ZPVE on the test set. Since we only experiment with many-bodies up to the second order, the attention weights of the 2-bodies is one minus that of the 1-bodies. On the four types of chemical properties, 1-body contribution dominates the prediction of most of the molecules. However, 2-body predictions also take a considerable amount of attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We propose a novel heterogeneous graph based molecule representation, heterogeneous molecular graph (HMG), to model many-body representations and interactions. Inspired by the many-body expansion of energy surfaces, we design a heterogeneous molecular graph neural network (HMGNN) to leverage the rich information stored in HMGs for molecular prediction tasks. HMGNN follows a message passing paradigm and leverages global molecule representations using an attention mechanism. We propose to train HMGNNs by    optimizing a multi-task learning loss. HMGNN achieves stateof-the-art performance on 9 out of 12 properties on the QM9 dataset. Experiments also show that the multi-task learning loss improves the generalization of the model. In this paper, we only model many-bodies up to the second order, future works should aim to model many-bodies of higher than third orders and also to enable HMGNNs for another important chemical prediction tasks, molecular dynamics simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ACKNOWLEDGEMENT</head><p>This work was supported in part by NSF (1447788, 1704074, 1757916, 1834251), Army Research Office (W911NF1810344), Intel Corp, and the Digital Technology Center at the University of Minnesota. Access to research and computing facilities was provided by the Digital Technology Center and the Minnesota Supercomputing Institute. We are grateful to Mingjian Wen for his fruitful comments, corrections and inspiration.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>of each p-body to form an order specific representation and concatenate them to be an intermediate representatio?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Effect of the cutoff distance c on prediction performance on four target properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Attention weights generate by the fusion module for predicting the four properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Target properties in the QM9 dataset.</figDesc><table><row><cell>Target</cell><cell>Description</cell></row><row><cell>?</cell><cell>Dipole moment</cell></row><row><cell>?</cell><cell>Isotropic polarizability</cell></row><row><cell>HOMO</cell><cell>Energy of Highest occupied molecular orbital (HOMO)</cell></row><row><cell>LUMO</cell><cell>Energy of Lowest occupied molecular orbital (LUMO)</cell></row><row><cell>?</cell><cell>Gap, difference between LUMO and HOMO</cell></row><row><cell>R 2</cell><cell>Electronic spatial extent</cell></row><row><cell>ZPVE</cell><cell>Zero point vibrational energy</cell></row><row><cell>U 0</cell><cell>Internal energy at 0 K</cell></row><row><cell>U</cell><cell>Internal energy at 298.15 K</cell></row><row><cell>H</cell><cell>Enthalpy at 298.15 K</cell></row><row><cell>G</cell><cell>Free energy at 298.15 K</cell></row><row><cell>Cv</cell><cell>Heat capacity at 298.15 K</cell></row><row><cell cols="2">convert the scalar geometries to real valued vector features.</cell></row><row><cell cols="2">Let x ? [a, b] be a scalar input and x ? R K be the real</cell></row><row><cell cols="2">valued output of the RBFs, the k-th entry of x is computed as</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Mean absolute error on QM9 with 110K training molecules. In each row, we use boldface for the best performance method. Column HMGNN-1 and HMGNN-2 correspond to the performance of summing over predictions of 1-bodies and 2-bodies, respectively.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Target</cell><cell>Unit</cell><cell cols="11">enn-s2s SchNet NMP-edge Cormorant PhysNet DimeNet HMGNN-1</cell><cell>HMGNN-2 HMGNN</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>D</cell><cell>0.030</cell><cell></cell><cell cols="2">0.033</cell><cell>0.029</cell><cell>0.038</cell><cell></cell><cell cols="2">0.0529</cell><cell>0.0286</cell><cell>0.0276</cell><cell>0.0283</cell><cell>0.0272</cell></row><row><cell></cell><cell></cell><cell cols="2">? HOMO</cell><cell>a 3 0 meV</cell><cell>0.092 43</cell><cell></cell><cell cols="2">0.235 41</cell><cell>0.077 36.7</cell><cell>0.085 34</cell><cell></cell><cell cols="2">0.0615 32.9</cell><cell>0.0469 27.8</cell><cell>0.0571 24.94</cell><cell>0.0647 26.31</cell><cell>0.0561 24.78</cell></row><row><cell></cell><cell></cell><cell cols="2">LUMO</cell><cell>meV</cell><cell>37</cell><cell></cell><cell></cell><cell>34</cell><cell>30.8</cell><cell>38</cell><cell></cell><cell cols="2">27.4</cell><cell>19.7</cell><cell>20.72</cell><cell>21.42</cell><cell>20.61</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>meV</cell><cell>69</cell><cell></cell><cell></cell><cell>63</cell><cell>58.0</cell><cell>61</cell><cell></cell><cell cols="2">42.5</cell><cell>34.8</cell><cell>33.44</cell><cell>35.02</cell><cell>33.31</cell></row><row><cell></cell><cell></cell><cell cols="2">R 2 ZPVE</cell><cell>a 2 0 meV</cell><cell>0.180 1.5</cell><cell></cell><cell cols="2">0.073 1.7</cell><cell>0.072 1.49</cell><cell>0.961 2.03</cell><cell></cell><cell cols="2">0.765 1.39</cell><cell>0.331 1.29</cell><cell>0.43 1.24</cell><cell>0.6 1.34</cell><cell>0.416 1.18</cell></row><row><cell></cell><cell></cell><cell>U 0</cell><cell></cell><cell>meV</cell><cell>19</cell><cell></cell><cell></cell><cell>14</cell><cell>10.5</cell><cell>22</cell><cell></cell><cell cols="2">8.15</cell><cell>8.02</cell><cell>6.19</cell><cell>9.06</cell><cell>5.92</cell></row><row><cell></cell><cell></cell><cell>U</cell><cell></cell><cell>meV</cell><cell>19</cell><cell></cell><cell></cell><cell>19</cell><cell>10.6</cell><cell>21</cell><cell></cell><cell cols="2">8.34</cell><cell>7.89</cell><cell>7.22</cell><cell>11</cell><cell>6.85</cell></row><row><cell></cell><cell></cell><cell>H</cell><cell></cell><cell>meV</cell><cell>17</cell><cell></cell><cell></cell><cell>14</cell><cell>11.3</cell><cell>21</cell><cell></cell><cell cols="2">8.42</cell><cell>8.11</cell><cell>6.35</cell><cell>8.37</cell><cell>6.08</cell></row><row><cell></cell><cell></cell><cell>G</cell><cell></cell><cell>meV</cell><cell>19</cell><cell></cell><cell></cell><cell>14</cell><cell>12.2</cell><cell>20</cell><cell></cell><cell cols="2">9.40</cell><cell>8.98</cell><cell>7.95</cell><cell>11.06</cell><cell>7.61</cell></row><row><cell></cell><cell></cell><cell>cv</cell><cell></cell><cell>cal mol K</cell><cell>0.040</cell><cell></cell><cell cols="2">0.033</cell><cell>0.032</cell><cell>0.026</cell><cell></cell><cell cols="2">0.0280</cell><cell>0.0249</cell><cell>0.0241</cell><cell>0.025</cell><cell>0.0233</cell></row><row><cell>Test MAE / meV</cell><cell>2 4 6 8 10 12</cell><cell>2.80 9.68</cell><cell></cell><cell>1.61 5.92</cell><cell>1.54 7.92 Train Test</cell><cell>Test MAE / cal/mol K</cell><cell>0.01 0.02 0.03</cell><cell>0.0031 0.0317</cell><cell>0.0027 0.0233</cell><cell>0.0027 0.0237 Train Test</cell><cell>Test MAE / meV</cell><cell>0.75 1.00 1.25 1.50 1.75</cell><cell>1.02 1.59</cell><cell>0.76 1.18</cell><cell>0.81 1.29 Train Test</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell cols="2">3 c / Angstrom</cell><cell>5</cell><cell></cell><cell></cell><cell>2</cell><cell>3 c / Angstrom</cell><cell>5</cell><cell></cell><cell></cell><cell>2</cell><cell>3 c / Angstrom</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) U 0 .</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Cv.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) ZPVE.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Ablation study on U 0 and C v .</figDesc><table><row><cell cols="2">Target Architecture</cell><cell cols="3">HMGNN-1 HMGNN-2 HMGNN</cell></row><row><cell></cell><cell>Default</cell><cell>6.19</cell><cell>9.06</cell><cell>5.92</cell></row><row><cell>U 0</cell><cell>Remove MTL Remove IOMP</cell><cell>8.22 10.26</cell><cell>9716.95 8.18</cell><cell>8.22 7.88</cell></row><row><cell></cell><cell>Remove HO</cell><cell>10.08</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Default</cell><cell>0.0241</cell><cell>0.0250</cell><cell>0.0233</cell></row><row><cell>Cv</cell><cell>Remove MTL Remove IOMP</cell><cell>0.0247 0.0297</cell><cell>1.4022 0.0275</cell><cell>0.0247 0.0244</cell></row><row><cell></cell><cell>Remove HO</cell><cell>0.0289</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inhomogeneous electron gas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hohenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="issue">3B</biblScope>
			<biblScope unit="page">864</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Physnet: a neural network for predicting energies, forces, dipole moments, and partial charges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">T</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meuwly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3678" to="3693" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Schnet-a deep learning architecture for molecules and materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">241722</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Molecular property prediction: A multilevel quantum interactions modeling perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1052" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural message passing with edge updates for predicting properties of molecules and materials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>J?rgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03146</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computer simulation of local order in condensed phases of silicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Stillinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review B</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5262</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Many-body effects in intermolecular forces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Elrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Saykally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical reviews</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1975" to="1997" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The many-body expansion combined with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Parkhill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14106</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Von Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Density functional theory of atoms and molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Horizons of Quantum Chemistry</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1980" />
			<biblScope unit="page" from="5" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Lilienfeld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05532</idno>
		<title level="m">Machine learning prediction errors better than dft accuracy</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Machine learning unifies the modeling of materials and molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Bart?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poelking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kermode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cs?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ceriotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1701816</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velikovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical modeling of molecular energies using a deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lubbers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">241715</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gnnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1eWbxStPH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cormorant: Covariant molecular neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="510" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Covariant compositional networks for learning graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">in International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryQu7f-RZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning graph neural networks with deep graph library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the Web Conference 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="305" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
