<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Monocular 3D Human Pose Estimation via Cascaded Dimension-Lifting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changgong</forename><surname>Zhang</surname></persName>
							<email>changgong.zcg@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
							<email>fnzhan@ntu.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Chang</surname></persName>
							<email>changyuan@bupt.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Monocular 3D Human Pose Estimation via Cascaded Dimension-Lifting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 3D pose estimation from a single image is a challenging problem due to depth ambiguity. One type of the previous methods lifts 2D joints, obtained by resorting to external 2D pose detectors, to the 3D space. However, this type of approaches discards the contextual information of images which are strong cues for 3D pose estimation. Meanwhile, some other methods predict the joints directly from monocular images but adopt a 2.5D output representation P 2.5D = (u, v, z r ) where both u and v are in the image space but z r in root-relative 3D space. Thus, the ground-truth information (e.g., the depth of root joint from the camera) is normally utilized to transform the 2.5D output to the 3D space, which limits the applicability in practice. In this work, we propose a novel end-to-end framework that not only exploits the contextual information but also produces the output directly in the 3D space via cascaded dimension-lifting. Specifically, we decompose the task of lifting pose from 2D image space to 3D spatial space into several sequential sub-tasks, 1) kinematic skeletons &amp; individual joints estimation in 2D space, 2) rootrelative depth estimation, and 3) lifting to the 3D space, each of which employs direct supervisions and contextual image features to guide the learning process. Extensive experiments show that the proposed framework achieves state-of-the-art performance on two widely used 3D human pose datasets (Human3.6M, MuPoTS-3D).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The goal of 3D human pose estimation is to localize semantic keypoints of single or multiple human bodies in 3D space. It is an essential technique for human behavior understanding and human-computer interaction, yet it is challenging due to the lack of depth information and large variations in human poses, appearances, and camera settings. Traditional approaches often use specialized devices under highly controlled environments, such as multi-view capture <ref type="bibr">(Amin et al. 2013)</ref>, marker systems <ref type="bibr" target="#b16">(Mandery et al. 2015)</ref> and multi-modal sensing <ref type="bibr" target="#b23">(Palmero et al. 2016)</ref>, which require a laborious setup process and thus limits the practical applicability. <ref type="figure">Figure 1</ref>: Estimated 3D pose by our method on the Hu-man3.6M dataset. Our approach produces visually correct results even on challenging poses.</p><p>Recently, many learning based methods <ref type="bibr" target="#b17">(Martinez et al. 2017;</ref><ref type="bibr" target="#b25">Pavlakos et al. 2017;</ref><ref type="bibr" target="#b31">Sun et al. 2017</ref><ref type="bibr" target="#b32">Sun et al. , 2018</ref><ref type="bibr" target="#b35">Yang et al. 2018</ref>) have achieved noticeable performance improvement. As their models take a single cropped image, estimating the absolute camera-centered coordinate of each keypoint is difficult. To handle this issue, many methods estimate the relative 3D pose to a reference point in the body, e.g., the center joint (i.e., pelvis) of a human, called root. Our work also adopts the same strategy as above.</p><p>Several previous researches (Moreno-Noguer 2017; <ref type="bibr" target="#b17">Martinez et al. 2017;</ref><ref type="bibr" target="#b33">Wang et al. 2019;</ref><ref type="bibr" target="#b30">Sharma et al. 2019</ref>) take detected 2D keypoints as input and predict corresponding 3D joint locations. This motivation is straightforward but extending them to 3D cases is nontrivial, which has promising results but relies on a high-precision 2D keypoints detector. Still some works <ref type="bibr" target="#b32">Sun et al. 2018;</ref><ref type="bibr" target="#b49">Zhou et al. 2019</ref>) adopt a 2.5D pose representation P 2.5D = (u j , v j , z r j ) j?J where u j and v j are the 2D projection of the body joint j and z r j = z root ? z j represents its metric depth with respect to the root joint. This decomposition of 3D joint locations is superior as additional supervision from in-the-wild images with only 2D pose annotations which can be used for better generalization. While the two dimensions of the output are still in pixel coordi-nates, the final 3D pose can only be obtained by projection transformation with the help of ground-truth information.</p><p>Our work combines the advantages of the above two pipelines and presents a framework that enables direct estimation of 3D pose from monocular RGB images via dimension-lifting. Generally, the network performs lifting followed by a list from 2D image to 2.5D pose, and then from 2.5D representation to final pose in the physical space.</p><p>Firstly, we propose a novel branch called Step-by-Step Heatmap Transformation (S 3 HT ). In this branch, we first adopt a structure regularization methodology for 3D pose estimation, which utilizes skeleton heatmaps to guide keypoints structure learning. The synthetic heatmaps are derived directly from keypoint positions without any additional data annotations. The use of bone information can constrain the search space of keypoints and explore the skeletal relationship. Therefore, the network can predict more accurate 2D keypoint heatmaps. Then we continue to transform the obtained 2D keypoint heatmaps to generate supervision signal in the depth dimension to reduce depth ambiguity. By mapping the image space to a cube, the depth of each point is normalized to the range of the cube's side length. Then we insert the corresponding heatmap into the channel of the cube according to the depth value of each keypoint which plays the role of index. By continuing to learn the volume constructed as described above, the network not only knows the global position relationship but also learns the specific relative depth of each keypoint based on the offset of the index. Some previous methods intend to introduce order ranking for the joint in the depth dimension. However, they only encode three states (&gt;, &lt;, =) for a pair of joints, which destroy the spatial information and cannot reflect different distances in the same state. For example, the states of +100mm and +1000mm are consistent which is not conducive to the subsequent depth prediction. In contrast, our method provides accurate depth distance and more global context which can better alleviate the problem of depth ambiguity in monocular estimation. In a word, we use a stepby-step approach to encode important supervision information in the process of dimension-lifting, from bone heatmap to keypoint heatmap, from keypoint heatmap to the depth order in the cube.</p><p>Then we merge the above feature maps and image context information, use soft-argmax operation to get a set of solid <ref type="bibr">(u, v, z)</ref> coordinates where u, v are pixel coordinates. Previous work has shown that 3D pose can be regressed from discrete 2D coordinates and the difficult part is the estimation of z coordinates in 3D space. Compared with the previous methods, the proposed framework can adopts MLP with residual connection to complete the lifting and easily get a high-precision 3D pose with the aid of the accurate predicted depth z. Furthermore, the network not only exploits the semantic information but also gets rid of dependence on external 2D keypoint detectors. Figure 1 displays qualitative results even on challenging poses. We show that our approach outperforms previous 3D pose estimation methods on several publicly available datasets for 3D single-person and multi-person pose estimation. respectively</p><p>Overall, our contributions can be summarized as follows.</p><p>? We present an end-to-end pipeline directly predicting the 3D pose from monocular images via sequential dimension-lifting, which takes the advantages of 2.5D representation &amp; 3D lifting paradigms and efficiently guides the learning process. ? We propose a novel branch (S 3 HT ), which serially combines multiple heatmap representations to supervise the task in various dimensions. We constrain the search space and increase robust human prior information through 2D bone heatmaps. A new depth representation which effectively reflects the relative depth and differences is also introduced to make the final 3D lifting more accurate. ? Our method can be applied not only to 3d singleperson pose estimation but also 3D multi-person pose estimation with a top-down pipeline. We show that our method significantly outperforms previous methods on both single and multi-person publicly available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Compared to 2D human pose estimation, 3D human pose estimation is more challenging since it needs to predict the depth information of body joints. In addition, the training data for 3D human pose estimation are not easy to obtain as 2D human pose estimation. Most existing datasets are obtained under constrained environments with limited generalize ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D single-person pose estimation</head><p>Vision-based estimation is one of the most fundamental and challenging problems in computer vision <ref type="bibr" target="#b32">(Sun et al. 2018;</ref><ref type="bibr" target="#b7">Habibie et al. 2019;</ref><ref type="bibr" target="#b11">Li and Chan 2014;</ref><ref type="bibr" target="#b25">Pavlakos et al. 2017;</ref><ref type="bibr" target="#b24">Pavlakos, Zhou, and Daniilidis 2018;</ref><ref type="bibr" target="#b45">Zhan et al. 2020b</ref><ref type="bibr" target="#b43">Zhan et al. , 2021b</ref><ref type="bibr" target="#b44">Zhan and Zhang 2020;</ref><ref type="bibr" target="#b39">Zhan, Lu, and Xue 2018;</ref><ref type="bibr" target="#b46">Zhan, Zhu, and Lu 2019a;</ref><ref type="bibr" target="#b41">Zhan, Xue, and Lu 2019;</ref><ref type="bibr" target="#b40">Zhan et al. 2020a;</ref><ref type="bibr" target="#b47">Zhan, Zhu, and Lu 2019b;</ref><ref type="bibr" target="#b42">Zhan et al. 2021a)</ref>. Specially, there are several approaches developed for the estimation of single-person pose. 1) directly map an image to 3D pose, and 2) lift 2D pose to 3D pose. <ref type="bibr" target="#b49">(Zhou et al. 2019;</ref><ref type="bibr" target="#b21">Moon, Chang, and Lee 2019;</ref><ref type="bibr">Li et al. 2020b,a)</ref> are based on the first approach. (Li and Chan 2014) employed a shallow network to regress 3D joint coordinates directly with synchronous task of body part detection with sliding windows. <ref type="bibr" target="#b25">(Pavlakos et al. 2017</ref>) proposed a volumetric representation for 3D human pose and employed a coarse-to fine prediction scheme to refine predictions with a multi-stage structure.  trained the network with additional ordinal depths of human joints as constraints. <ref type="bibr" target="#b32">(Sun et al. 2018</ref>) used soft-argmax operation to obtain the 3D coordinates of body joints in a differentiable manner. In addition, <ref type="bibr" target="#b17">(Martinez et al. 2017;</ref><ref type="bibr" target="#b51">Zhou et al. 2017;</ref><ref type="bibr" target="#b6">Fang et al. 2017;</ref><ref type="bibr" target="#b35">Yang et al. 2018;</ref><ref type="bibr" target="#b30">Sharma et al. 2019;</ref><ref type="bibr" target="#b26">Pavllo et al. 2019)</ref> are based on the second approach. This approach utilizes the high accuracy of 2D human pose estimation network and images from 2D human datasets. They initially localize body </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D multi-person pose estimation</head><p>The achievements of monocular 3D multi-person pose estimation are based on 3D single person pose estimation and other deep learning methods. <ref type="bibr" target="#b20">(Mehta et al. 2018</ref>) proposed a bottom-up method by using 2D pose and part affinity fields to infer person instances. An occlusion-robust pose-maps (ORPM) was proposed to provide multi-style occlusion information regardless of the number of people. <ref type="bibr" target="#b27">(Rogez, Weinzaepfel, and Schmid 2017)</ref> proposed a Localization-Classification-Regression Network (LCR-Net) following three stage processing. The localization part detected a human from an input image, and the classification part classified the detected human into several anchor poses. <ref type="bibr" target="#b37">(Zanfir, Marinoiu, and Sminchisescu 2018)</ref> proposed a framework relying on detailed semantic representations at both model and image level to guide a combined optimization with feed forward and feed backward stages for 3D multi-person pose and shape estimation. <ref type="bibr" target="#b19">(Mehta et al. 2020</ref>) operated in subsequent stages and contributed a new architecture called SelecSLS Net. The first stage estimated 2D and 3D pose features along with identity assignments for all visible joints. Then the second stage turned the possibly partial pose features into a complete 3D pose. The third stage applied space-time skeletal model fitting and enforce temporal coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ordinal relations</head><p>In the field of computer vision, ordinal relations have been applied to estimate depth <ref type="bibr" target="#b53">(Zoran et al. 2015;</ref><ref type="bibr" target="#b4">Chen et al. 2016</ref>) and reflectance <ref type="bibr" target="#b50">(Zhou, Krahenbuhl, and Efros 2015)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Approach</head><p>Network Design <ref type="figure" target="#fig_0">Figure 2</ref> depicts the overall architecture of our neural network. It takes a cropped single image as the input and outputs the estimated 3D pose for the target image. A ResNet-50 <ref type="bibr" target="#b8">(He et al. 2016</ref>) backbone is adopted for basic feature extraction for a fair comparison. The framework involves two types of processing modules: original image context branch (indicated by the blue rectangle) and step-by-step heatmap transformation branch (S 3 HT , indicated by the red rectangle). The S 3 HT branch can be further categorized as Bone-Joint heatmap block and Depth-Centric block.</p><p>Firstly, the Bone-Joint heatmap block uses deconvolution to upsample the featuremaps extracted by the backbone and then outputs the 2D skeleton heatmaps through the 2D convolution operation. After fusing with the original image context branch and several 2D convolution operations, highprecision 2D keypoint heatmaps can be predicted. Then the Depth-Centric block takes in joint heatmaps, combined with the original image semantic information which using dilated convolution expanding its receptive field, learning the depth order and distance in the space. Finally the S 3 HT branch and original image context branch are joined to predict a three-dimensional featuremap for each joint.</p><p>We perform a soft-argmax operation <ref type="bibr" target="#b32">(Sun et al. 2018</ref>) to aggregate information to obtain the 2.5D joint estimation where (u, v) are pixel coordinates. Then the subsequent layer takes the concatenated coordinates and applies a fully connected layer with 1024 output channels. Then it is followed by five blocks that are surrounded by residual connections. For each block, two fully connected layers (1024 channels) followed by Batch Normalization, rectified linear units, and dropout, are stacked for efficiently mapping the pose to high-level features. Finally, the features extracted by the last residual block are fed into an extra linear layer (N ?3 channels) to output 3D poses.</p><p>By decomposing the task step by step, we guide the network to perform dimension-lifting from 2D image space to 2.5D representation to 3D spatial space and achieve outstanding performance in the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bone-Joint Heatmap Block</head><p>Bone-Joint heatmap block learns the skeleton's structure and keypoint heatmaps in a serial flow. This allows the network to utilize the composite structure representation to learn robust pose information. The same as many pose estimation methods, we adopt heatmap representation to predict accurate 2D coordinates. A ground-truth heatmap consist of a 2D gaussian (with standard deviation of 2 px) centered on the joint location. In order to further introduce the prior information of bones' length and direction, Gaussian is performed on their connected straight lines to generate a bone heatmap according to the parent-children joint pairs. The Bones definition and connection relationship is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Each pixel belongs to H Bone with a Gaussian-alike confidence value as defined bellow.</p><formula xml:id="formula_0">H Bone (p | H) = exp ? Dist(p, p i p j ) 2? 2<label>(1)</label></formula><p>where Dist indicates the distance from point to the line of bone.</p><p>We adopt the unified representation form of heatmap and can retain large amounts of spatial information. The Bone-Joint heatmap block not only constrains the search space of joints but also promotes the interrelationship between joints by learning the bone structure. Since the learning process is always from coarse to fine, we dynamically reduce the Gaussian kernel size of the bone heatmaps during the training according to the epoch to accelerate the network convergence. It is worth mentioning that the synthetic heatmaps are For joint and bone heatmaps, we use mean squared error as the loss functions, where N denotes the number of joints.</p><formula xml:id="formula_1">L 2D = N n=1 H gt n ?? n 2 2</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth-Centric Block</head><p>To further bridge the gap between the 2D image and the target 3D human pose, it is necessary to employ immediate supervision for depth. Previous work (Pavlakos, Zhou, and Daniilidis 2018) adopted the joint ranking loss to encourage the correct pairwise depth ordering in order to alleviate pose ambiguities. However, it only encodes three ordinal states as below rather than the quantitative depth differences. Furthermore, it also tends to lose some important features encoded in the spatial domain because of discrete expression, which would be worse in the pipeline based on the heatmap.</p><p>? +1, if joint i is closer than j,</p><p>? -1, if joint j is closer than i,</p><p>? 0, if their depths are roughly the same Therefore we propose the Depth-Centric block which still employs the heatmap representation but can provide depth distances and more global position information. Each joint is mapped to a specific location based on its corresponding root-relative depth value and we then arrange the 2D heatmaps of all the joints into a 3D heatmap. Specifically, we normalize the image space to a cube, of which the side length is denoted as C. The depth of each joint relative to the root joint is scaled to the range of [? C 2 , C 2 ], and we can get the discrete depth value in the range of [0, C] after the depth discretization. We construct 3D featuremap groundtruth equivalent to the cube size and insert the corresponding heatmap into the channel according to the discrete depth value of each keypoint which plays the role of index. In our network, C is set to 64. Note that when several joints have exactly the same discretize values, we also allow multiple joints in the same channel.</p><p>By continuing to learn the volume constructed as described above shown in <ref type="figure" target="#fig_0">Figure 2(c)</ref>, the network not only knows the global position relationship but also learns the depth value of each keypoint. We effectively use the expression passed by the previous module without destroying the information of the heatmaps in the spatial domain, and generate a more effective representation for the subsequent learning. It is worth mentioning that this block efficiently uses the semantic information of the original image. A dilated convolution block is employed to extract features from the original image context branch to obtain multiple large receptive fields, which can provide long-range dependencies and capture more global features.</p><p>We train the block by minimizing the L2 distance between the estimated and constructed ground-truth. The loss function L 3D is defined as follows:</p><formula xml:id="formula_2">L 3D = C c=1 H gt c ?? c 2 2<label>(3)</label></formula><p>But the mean squared error may be the same when the joints are in the wrong channels. For example, if the true label for a given joint is located at the 12th channel, it produces the same error no mater the predicted joint heatmap is located in the 14th or 30th channel, thus it fails to reflect the distance from the ground-truth. This may affects the network convergence speed. Thus we add a conv layer with kernel size of 1 ? 1 in parallel to use a new depth loss for assisting. After obtaining the discrete depth values, it is straightforward to turn the problem into a multi-class classification task and adopts ordinal softmax regression loss. The L depth formula is as follows and d ( w, h) indicates the depth of the pixel. Minimizing L depth ensures that predictions farther from the true label incur a greater penalty than those closer to the true label.</p><formula xml:id="formula_3">L depth = W w H h d (w,h) c=1 log P c (w,h) + C c=d (w,h) +1 log 1 ? P c (w,h) (4) P c (w,h) = P d (w,h) &gt; c (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Lifting block</head><p>In this section, our goal is to estimate body joint locations in camera space. Formally, our input is a series of points x ? 2.5D, and our output is a series of points y in 3D space. We aim to learn a function that minimizes the prediction error over a dataset of N poses.</p><formula xml:id="formula_4">f * = min f 1 N N i=1 L (f (x i ) ? y i )<label>(6)</label></formula><p>There have been some previous works showing that "lifting" 2D joint locations to 3D space that can be solved with a low error rate. But the mapping from 2D coordinates to 3D joints is highly-nonlinear and the discrete joints are not enough to support the high precision depth returned to the physical space. Thus some approaches constrain the solution space by dealing with temporal information. In contrast, we get accurate depth value in the previous network, which can result in a lower-dimensional solution space. So we can get remarkable low error by regression through MLP layers from a single frame.</p><p>To avoid quantization errors and allow end-to-end learning, we follow Sun et al. using differentiable soft-argmax regression instead of argmax to get the 2.5D coordinates, the outputs are given as:</p><formula xml:id="formula_5">J n = p?? p ?H n (p)<label>(7)</label></formula><p>H n (p) = e Hn(p) q?? e Hn(q)</p><p>And in the final 3D lifting block, mean absolute error is used for minimizing the distance between the estimated and ground-truth 3D pose.</p><formula xml:id="formula_7">L pose = 1 N N n=1 J gt n ?J n 1<label>(9)</label></formula><p>We observe that a child and an adult have the same size in the cropped image. However, the child is closer to the camera than the adult. We use a standard bone length to normalize the joints to a uniform height. Specifically, we define the distance from neck to knee as geodesic distance, which accumulates the length of each passing bone. We calculate the average geodesic distance (1077 mm) of the Human3.6M training set and then calculate the scale for each sample and multiply the keypoints by this scale for normalization.</p><formula xml:id="formula_8">J i 3D =J i 3D ? 1077 Geo(J i 3D )<label>(10)</label></formula><p>whereJ 3D means the original 3D joints ground-truth and Geo(J i 3D ) indicates the geodesic distance of the sample i. We add an extra branch to regress the target scale for inference. Although the overall performance can be slightly improved, we later find that only several values of the geodesic distance in the Human3.6M training set because of actors, and the geodesic distances in the test set are very close to the training average. Therefore, the gain may be caused by overfitting, and we finally give up the above strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset and Evaluation Metric</head><p>Human3.6M dataset. Human3.6M <ref type="bibr" target="#b9">(Ionescu et al. 2013)</ref> is the largest 3D human pose estimation benchmark with accurate 3D labels. It contains 3.6 million RGB images captured by a MoCap System in an indoor environment, in which 7 professional actors were performing 15 activities from 4 camera viewpoints such as walking, eating, sitting, making a phone call and engaging in a discussion, etc. 2D joint locations and 3D ground truth positions are available,   as well as projection (camera) parameters and body proportions for all the actors. Two evaluation metrics are widely used. The first one computes the mean Euclidean distance for all the joints after aligning the root joints (i.e. pelvis) between the predicted and ground-truth poses, referred as MPJPE. The second one is MPJPE after applying an additional rigid transformation (i.e., Procrustes analysis (PA) ) to the predicted pose as an enhancement. This metric is called PA MPJPE.</p><p>MuCo-3DHP and MuPoTS-3D datasets. These are the 3D multi-person pose estimation datasets proposed by Mehta et al. The training set, MuCo-3DHP, is generated by compositing the existing MPI-INF-3DHP 3D single-person pose estimation dataset <ref type="bibr" target="#b18">(Mehta et al. 2017)</ref>. The test set, MuPoTS-3D dataset, was captured at outdoors and it includes 20 real-world scenes with ground-truth 3D poses for up to three subjects. The ground-truth is obtained with a multi-view marker-less motion capture system. For evaluation, a 3D percentage of correct keypoints (3DPCK) is used after root alignment with ground-truth. It treats a joint's prediction as correct if it lies within a 15cm from the groundtruth joint location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Protocol</head><p>Human3.6M dataset. Two experimental protocols are widely used. Protocol 1 uses five subjects (S1, S5, S6, S7, S8) in training and two subjects (S9, S11) in testing. MPJPE is used as an evaluation metric. Protocol 2 uses six subjects (S1, S5, S6, S7, S8, S9) in training and S11 in testing following <ref type="bibr" target="#b28">Rogez, Weinzaepfel, and Schmid 2019;</ref><ref type="bibr" target="#b21">Moon, Chang, and Lee 2019)</ref>. PA MPJPE is used as an evaluation metric. We use every 5th and 64th frames in videos for training and testing following <ref type="bibr" target="#b32">(Sun et al. , 2018</ref>. When training, besides the Human3.6M dataset, we used additional MPII 2D human pose estimation dataset <ref type="bibr" target="#b2">(Andriluka et al. 2014)</ref> following <ref type="bibr" target="#b25">(Pavlakos et al. 2017;</ref><ref type="bibr" target="#b31">Sun et al. 2017</ref><ref type="bibr" target="#b32">Sun et al. , 2018</ref><ref type="bibr" target="#b21">Moon, Chang, and Lee 2019</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We implement our method in PyTorch. The backbone is initialized with the publicly released ResNet-50 pre-trained on the ImageNet dataset. In order to use both images with 3D annotations (e.g., Human3.6M) and 2D annotations (e.g., MPII), the training procedure contains two stages. Because 3D lifting block can only use 3D ground-truth supervision, we train previous network and 3D lifting block sequentially.</p><p>For previous network, we use the Adam as the optimizer and train the network for 20 epochs with a batch size of 64 and the initial learning rate is set to 0.001 and reduced by a factor of 10 at the 17th epoch. We use 256 ? 256 as the size of the input image and perform data augmentation including rotation (?30 ? ), horizontal flip, synthetic occlusion in training. Then for the 3D lifting block we still follow the above strategy. The only difference is that we just use 3D dataset (e.g., Human3.6M) for fine-tuning. The training is performed on two NVIDIA P100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with State-of-the-art Methods</head><p>Human3.6M dataset. We compare our approach with state-of-the-art techniques under two protocols on the Hu-man3.6M dataset <ref type="bibr" target="#b9">(Ionescu et al. 2013)</ref> as shown in Tables 1 and 2. As can be seen, our method outperforms all competing single-frame methods and obtains average errors of 48.7mm and 31.8mm under two evaluation protocols. In addition, some approaches such as <ref type="bibr" target="#b32">(Sun et al. 2018;</ref><ref type="bibr" target="#b49">Zhou et al. 2019</ref>) use the camera intrinsics and the ground-truth distance of the subject from the camera to convert their (u, v) predictions to (x, y). Therefore, their reported results are not representative of the 3D pose performance. Our method achieves comparable performance despite not using any ground-truth information in inference time.  <ref type="table">Table 3</ref>: Comparison of our approach on the MuPoTS-3D benchmark dataset. The metric used is 3D percentage of correct keypoints (3DPCK), so higher is better.</p><p>MuCo-3DHP and MuPoTS-3D datasets. Our approach can complete 3D multi-person pose estimation with a topdown pipeline. We quantitatively evaluate our method's performance on the MuPoTS 3D monocular multi-person benchmark dataset from Mehtaet al. We report the 3DPCK (percentage of joint prediction within a 15cm ball centred on ground-truth) per sequence, averaged over the subjects for which ground truth is available. Results are reported in <ref type="table">Table 3</ref>. We establish a new state-of-the-art performance of 83.3% compared to 81.8% which also belongs to the topdown pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>To verify the impact and performance of each component in the network, we conduct ablation experiments on the Human3.6M dataset under Protocol #1. We examine the effectiveness of using different supervision we proposed. We evaluate the model trained without any intermediate supervision (Baseline), with keypoint heatmap supervision only, with Bone-Joint heatmap block supervision, and with Depth-Centric block supervision (Full). All of these design variants are evaluated with the same experimental setting (including training data, network architecture). Our baseline is based on the combination of the PoseNet in <ref type="bibr" target="#b21">(Moon, Chang, and Lee 2019)</ref> and the 3D Lifting block.</p><p>The detailed results are presented in <ref type="table" target="#tab_6">Table 4</ref>. Using keypoint heatmaps supervision for training, the prediction error is reduced by 2.1mm compared to the baseline. The Bone-Joint heatmaps supervision provide 1.2mm lower mean error compared to the keypoint heatmaps supervision. By combining all the component, our method achieves significant performance improvements, and the MPJPE decreases from 54.1(mm) to 48.7 (mm). It is worth mentioning that our network only adds a few parameters compared with the previous methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>This paper presents a novel framework for monocular 3D pose estimation, consisting of a clean CNN architecture and effective intermediate supervisions for network optimization. Our work takes the advantages of 2.5D representation and 2D-to-3D lifting methods, decomposing the task into several sequential sub-tasks, helping network completing dimension-lifting step by step. The proposed system is very helpful to bridge the 2D-3D domain gap in the learning procedure and achieves outstanding performance without using any ground-truth information during inference. In the future, we intend to explore the extension to temporal domain, which can be easily plugged into our framework. Moreover, we will improve the joints depth representation and investigate more prior constraints for the network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The network architecture of our proposed approach. Blue: Original image context branch; Red: Step-by-step heatmap transformation branch (S 3 HT ). (a, b) Bone-Joint heatmap block for skeletal structure learning. (c) The novel Depth-Centric block we exploit. (d) 3D lifting block for the final output. keypoints in a 2D space and focus on lifting them to a 3D space. (Yang et al. 2018) utilized adversarial loss to handle the 3D human pose estimation in the wild. Fang et al. (Fang et al. 2017) proposed a pose grammar and considered prior knowledge to incorporate high-level dependencies and relations of human body. (Sharma et al. 2019) employed a Deep Conditional Variational Autoencoder (CVAE for short) that synthesizes diverse anatomically plausible 3D-pose samples conditioned on the estimated 2D-pose. (Pavllo et al. 2019) effectively estimated the 3D pose with a fully convolutional model based on dilated temporal convolutions over 2D keypoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc><ref type="bibr" target="#b53">Zoran et al. 2015)</ref> proposed a framework that infers midlevel visual properties of an image by learning about ordinal relationships.<ref type="bibr" target="#b4">(Chen et al. 2016</ref>) employed a new algorithm for learning to estimate metric depth using only annotations of relative depth. used a weaker supervision signal provided by the ordinal depths of human joints for 3D human pose.<ref type="bibr" target="#b29">(Ronchi et al. 2018</ref>) proposed a 3D human pose estimation algorithm that only required relative estimates of depth at training time.) designed a Pairwise Ranking Convolutional Neural Network (PRCNN) to extract depth rankings of human joints from images. proposed deep conditional variational autoencoder based model and derived joint-ordinal depth relations from an RGB image and employed them to score and weight-average the candidate 3D-poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Bones definition and connection relationship. derived directly from keypoint positions requiring no extra data annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Methods Dir. Dis. Eat Gre. Phon. Pose Pur. Sit SitD. Smo. Phot. Wait Walk WalkD. WalkP. Avg Chen et al. CVPR'17 89.9 97.6 90.0 107.9 107.3 93.6 136.1 133.1 240.1 106.7 139.2 106.2 87.0 114.1 90.6 114.2 Tome et al. CVPR'17 65.0 73.5 76.8 86.4 86.3 68.9 74.8 110.2 173.9 85.0 110.7 85.8 71.4 86.3 73.1 88.4 Mehta et al. 3DV'17 57.5 68.6 59.6 67.3 78.1 56.9 69.1 98.0 117.5 69.5 82.4 68.0 55.3 76.5 61.4 72.9 Martinez et al. ICCV'17 51.8 56.2 58.1 59.0 69.5 55.2 58.1 74.0 94.6 62.3 78.4 59.1 49.5 65.1 52.4 62.9 Zhou et al. TPAMI'18 68.7 74.8 67.8 76.4 76.3 84.0 70.2 88.0 113.8 78.0 98.4 90.1 62.6 75.1 73.6 79.9 Fang et al. AAAI'18 50.1 54.3 57.0 57.1 66.6 53.4 55.7 72.8 88.6 60.3 73.3 57.7 47.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5 62.7</cell><cell cols="2">50.6 60.4</cell></row><row><cell>Zhao et al. CVPR'19</cell><cell cols="14">47.3 60.7 51.4 60.5 61.1 47.3 68.1 86.2 55.0 67.8 49.9 61.0 60.6 42.1</cell><cell cols="2">45.3 57.6</cell></row><row><cell>Li and Lee CVPR'19</cell><cell cols="14">43.8 48.6 49.1 49.8 57.6 45.9 48.3 62.0 73.4 54.8 61.5 50.6 43.4 56.0</cell><cell cols="2">45.5 52.7</cell></row><row><cell cols="15">Sharma et al. ICCV'19 48.6 54.5 54.2 55.7 62.6 50.5 54.3 70.0 78.3 58.1 72.0 55.4 45.2 61.4</cell><cell cols="2">49.7 58.0</cell></row><row><cell>Moon et al. ICCV'19</cell><cell cols="14">51.5 56.8 51.2 52.2 55.2 47.7 50.9 63.3 69.9 54.2 57.4 50.4 42.5 57.5</cell><cell cols="2">47.7 54.4</cell></row><row><cell>Mehta et al. TOG'20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.6</cell></row><row><cell>Liu et al. IJCAI'20</cell><cell cols="14">46.5 55.0 54.7 60.2 63.3 50.2 64.2 54.0 76.0 63.2 48.9 55.5 59.3 47.6</cell><cell cols="2">44.0 56.2</cell></row><row><cell>Li et al. CVPR'20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>50.9</cell></row><row><cell>Ours</cell><cell cols="14">43.2 50.5 42.7 47.1 50.7 41.1 45.2 58.9 61.6 48.8 60.1 46.3 35.5 51.4</cell><cell cols="2">41.0 48.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparisons of the mean per-joint position error (MPJPE) on Human3.6M under Protocol #1.</figDesc><table><row><cell>Methods</cell><cell cols="7">Dir. Dis. Eat Gre. Phon. Pose Pur.</cell><cell cols="9">Sit SitD. Smo. Phot. Wait Walk WalkD. WalkP. Avg</cell></row><row><cell>Yasin et al. CVPR'16</cell><cell cols="16">88.4 72.5 108.5 110.2 97.1 81.6 107.2 119.0 170.8 108.2 142.5 86.9 92.1 165.7 102.0 108.3</cell></row><row><cell>Chen et al. CVPR'17</cell><cell cols="14">71.6 66.6 74.7 79.1 70.1 67.6 89.3 90.7 195.6 83.5 93.3 71.2 55.7 85.9</cell><cell cols="2">62.5 82.7</cell></row><row><cell cols="15">Moreno et al. CVPR'17 67.4 63.8 87.2 73.9 71.5 69.9 65.1 71.7 98.6 81.3 93.3 74.6 76.5 77.7</cell><cell cols="2">74.6 76.5</cell></row><row><cell cols="15">Martinez et al. ICCV'17 39.5 43.2 46.4 47.0 51.0 41.4 40.6 56.5 69.4 49.2 56.0 45.0 38.0 49.5</cell><cell cols="2">43.1 47.7</cell></row><row><cell>Sun et al. ICCV'17</cell><cell cols="14">42.1 44.3 45.0 45.4 51.5 43.2 41.3 59.3 73.3 51.0 53.0 44.0 38.3 48.0</cell><cell cols="2">44.8 48.3</cell></row><row><cell>Zhou et al. TPAMI'18</cell><cell cols="14">47.9 48.8 52.7 55.0 56.8 49.0 45.5 60.8 81.1 53.7 65.5 51.6 50.4 54.8</cell><cell cols="2">55.9 55.3</cell></row><row><cell>Fang et al. AAAI'18</cell><cell cols="14">38.2 41.7 43.7 44.9 48.5 40.2 38.2 54.5 64.4 47.2 55.3 44.3 36.7 47.3</cell><cell cols="2">41.7 45.7</cell></row><row><cell>Sun et al. ECCV'18</cell><cell cols="14">36.9 36.2 40.6 40.4 41.9 34.9 35.7 50.1 59.4 40.4 44.9 39.0 30.8 39.8</cell><cell cols="2">36.7 40.6</cell></row><row><cell>Rogez et al. TPAMI'19</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.7</cell></row><row><cell>Moon et al. ICCV'19</cell><cell cols="14">32.5 31.5 41.5 36.7 36.3 31.9 33.2 36.5 44.4 36.7 38.7 31.2 25.6 37.1</cell><cell cols="2">30.5 35.2</cell></row><row><cell>Ours</cell><cell cols="14">27.3 29.2 37.2 31.5 33.1 29.2 30.7 32.5 42.0 34.6 36.9 28.0 23.3 34.7</cell><cell cols="2">27.5 31.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>PA MPJPE comparison with state-of-the-art methods on the Human3.6M dataset using Protocol #2. The results of all approaches are obtained from the<ref type="bibr" target="#b21">(Moon, Chang, and Lee 2019)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablative study on the effects of the intermediate supervision we proposed. The evaluation is performed on Human3.6M under Protocol #1 with MPJPE(mm). JH: joint heatmap, BH: bone heatmap, BJ:bone-joint block, DC: depth-centric block.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-view pictorial structures for 3d human pose estimation</title>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PandaNet: Anchor-Based Single-Shot Multi-Person 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benzine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luvison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Achard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6856" to="6865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-person 3d human pose estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="405" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06513</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">In the wild human pose estimation using explicit 2d features and intermediate 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10905" to="10914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3d human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9887" to="9895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cascaded Deep Monocular 3D Human Pose Estimation With Evolutionary Training Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6173" to="6183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometry-Driven Self-Supervised Method for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11442" to="11449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-Dynamic Hypergraph Neural Network for 3D Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The KIT whole-body human motion database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mandery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Terlemez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vahrenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Advanced Robotics (ICAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">XNect: real-time multi-person 3D motion capture with a single RGB camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="82" to="83" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Single-shot multiperson 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2823" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-modal rgbdepth-thermal human body segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Palmero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clap?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bahnsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>M?gelmose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="239" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3433" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1146" to="1161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">It&apos;s all Relative: Monocular 3D Human Pose Estimation from Weakly Supervised Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06880</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2325" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Not All Parts Are Created Equal: 3D Pose Estimation by Modeling Bi-directional Dependencies of Body Parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7771" to="7780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08973</idno>
		<title level="m">Drpose3d: Depth ranking in 3d human pose estimation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4948" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Esir: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2059" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Verisimilar image synthesis for accurate detection and recognition of texts in scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="249" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarial Image Composition with Auxiliary Illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">GA-DAN: Geometryaware domain adaptation network for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9105" to="9115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unbalanced Feature Transport for Exemplar-based Image Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10244</idno>
		<title level="m">GMLight: Lighting Estimation via Geometric Distribution Approximation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatial-Aware GAN for Unsupervised Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11116</idno>
		<title level="m">EMLight: Lighting Estimation via Spherical Distribution Approximation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Scene text synthesis for efficient and effective deep network training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09193</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatial fusion gan for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3653" to="3662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3D human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">HEMlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2344" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning data-driven reflectance priors for intrinsic image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3469" to="3477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weaklysupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="901" to="914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning ordinal relationships for mid-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="388" to="396" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
