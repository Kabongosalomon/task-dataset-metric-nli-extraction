<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LaserMix for Semi-Supervised LiDAR Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingdong</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
							<email>jiawei011@e.ntu.edu.sgliang.pan</email>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
							<email>ziwei.liu@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LaserMix for Semi-Supervised LiDAR Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Densely annotating LiDAR point clouds is costly, which restrains the scalability of fully-supervised learning methods. In this work, we study the underexplored semi-supervised learning (SSL) in LiDAR segmentation. Our core idea is to leverage the strong spatial cues of LiDAR point clouds to better exploit unlabeled data. We propose LaserMix to mix laser beams from different LiDAR scans, and then encourage the model to make consistent and confident predictions before and after mixing. Our framework has three appealing properties: 1) Generic: LaserMix is agnostic to LiDAR representations (e.g., range view and voxel), and hence our SSL framework can be universally applied. 2) Statistically grounded: We provide a detailed analysis to theoretically explain the applicability of the proposed framework. 3) Effective: Comprehensive experimental analysis on popular LiDAR segmentation datasets (nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness and superiority. Notably, we achieve competitive results over fully-supervised counterparts with 2? to 5? fewer labels and improve the supervised-only baseline significantly by 10.8% on average. We hope this concise yet high-performing framework could facilitate future research in semi-supervised LiDAR segmentation. Code will be publicly available 2 . * Equal contribution. 2 https://github.com/ldkong1205/LaserMix. Preprint. Under review. arXiv:2207.00026v1 [cs.CV] 30 Jun 2022 veg LiDAR Representation Option 1: Range View Option 2: Voxel LaserMix LaserMix 62 64 66 68 70 72 74 RangeNet++ [IROS'19] FIDNet [IROS'21] PolarNet [CVPR'20] Cylinder3D [CVPR'21] 10% SalsaNext [ISVC'20] mIoU (%)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>LiDAR segmentation is a fundamental task in autonomous driving <ref type="bibr" target="#b1">[1]</ref>. It enables autonomous vehicles to semantically perceive the dense 3D structure of the surrounding scenes <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. However, densely annotating LiDAR point clouds is inevitably expensive and labor-intensive <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>, which restrains the scalability of fully-supervised LiDAR segmentation methods. Semi-supervised learning (SSL) that directly leverages the easy-to-acquire unlabeled data is hence a viable and promising solution to achieve scalable LiDAR segmentation <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7]</ref>.</p><p>Yet, semi-supervised LiDAR segmentation is still underexplored. Modern SSL frameworks are mainly designed for image recognition <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10]</ref> and semantic segmentation <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b13">13]</ref> tasks, which only yield sub-par performance on LiDAR data due to the large modality gap between 2D and 3D. Recent research <ref type="bibr" target="#b14">[14]</ref> starts to consider semi-supervised point cloud semantic segmentation as a fresh task, and it designs a novel point contrastive-learning framework. However, their solutions do not differentiate indoor and outdoor scenes and therefore overlook the intrinsic and important properties that only exist in LiDAR point clouds.</p><p>In this work, we explore the use of spatial prior for semi-supervised LiDAR segmentation. Unlike the general 2D/3D semantic segmentation tasks, the spatial cues are especially significant in LiDAR  <ref type="figure">Figure 1</ref>: a) LiDAR scans contain strong spatial prior. Objects and backgrounds around the egovehicle have a patterned distribution on different (lower, middle, upper) laser beams. b) Following the scene structure, the proposed LaserMix blends beams from different scans, which is compatible with various popular LiDAR representations. c) We achieve superior results over SoTA methods in both low-data (10%, 20%, and 50% labels) and high-data (full labels) regimes on nuScenes <ref type="bibr" target="#b15">[15]</ref>.</p><p>data. In fact, LiDAR point clouds serve as a perfect reflection of the real-world distribution, which is highly dependent on the spatial areas in the LiDAR-centered 3D coordinates. To effectively leverage this strong spatial prior, we propose LaserMix to mix laser beams from different LiDAR scans, and then encourage the segmentation model to make consistent and confident predictions before and after mixing. Our SSL framework is statistically grounded, which consists of:</p><p>1. Partitioning LiDAR scans into low-variation areas. We observe a strong distribution pattern on laser beams (see <ref type="figure">Fig. 1(a)</ref>) and thus propose the laser partition.</p><p>2. Efficiently mixing every area in the scan with foreign data and obtaining model predictions.</p><p>We propose LaserMix, which mixes the laser-grouped areas from two LiDAR scans in an intertwining way (see <ref type="figure">Fig. 1(b)</ref>), and serves as an efficient LiDAR mixing strategy.</p><p>3. Encouraging models to make confident and consistent predictions on the same area in different mixing. We, therefore, propose a mixing-based teacher-student training pipeline.</p><p>Despite the simplicity of our overall pipeline, it achieves competitive results over the fully supervised counterpart using 2? to 5? fewer labels (see <ref type="figure">Fig. 1(c)</ref>), and significantly outperforms all prevailing semi-supervised segmentation methods on nuScenes <ref type="bibr" target="#b15">[15]</ref> (up to +5.7% mIoU) and Se-manticKITTI <ref type="bibr" target="#b16">[16]</ref> (up to +3.5% mIoU). Moreover, LaserMix directly operates on point clouds so as to be agnostic to different LiDAR representations, e.g., range view <ref type="bibr" target="#b17">[17]</ref> and voxel <ref type="bibr" target="#b18">[18]</ref>. Therefore, our pipeline can be compatible with existing state-of-the-art (SoTA) LiDAR segmentation methods under various representations <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21]</ref>. Besides, our pipeline achieves competitive performance using very limited annotations on weak supervision dataset <ref type="bibr" target="#b4">[4]</ref>: it achieves 54.4% mIoU on SemanticKITTI <ref type="bibr" target="#b16">[16]</ref> using only 0.8% labels, which is on-par with PolarNet <ref type="bibr" target="#b19">[19]</ref> (54.3%), RandLA-Net <ref type="bibr" target="#b22">[22]</ref> (53.9%), and RangeNet++ <ref type="bibr" target="#b17">[17]</ref> (52.2%) using 100% labels. Spatial prior is proven to play a pivotal role in the success of our framework through comprehensive empirical analysis.</p><p>To summarize, this work has the following key contributions:</p><p>? We present a statistically grounded SSL framework that effectively leverages the spatial cues in LiDAR data to facilitate learning with semi-supervisions.</p><p>? We propose LaserMix, a novel and representation-agnostic mixing technique that strives to maximize the "strength" of the spatial cues in our SSL framework.</p><p>? Our overall pipeline significantly outperforms previous SoTA methods in both low-and high-data regimes. We hope this work could lay a solid foundation for semi-supervised LiDAR segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Technical Approach</head><p>In this section, we first introduce our SSL framework that leverages the spatial prior of LiDAR data by encouraging confidence and consistency in predictions (Sec. 2.1). We then present LaserMix that strives to maximize the "strength" of the spatial prior and mixes LiDAR scans in an efficient manner (Sec. 2.2). Finally, we elaborate the overall pipeline (Sec. 2.3) and present the pseudo-code (Alg. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Leveraging the Spatial Prior for SSL</head><p>Spatial prior formulation. The distribution of real-world objects/backgrounds has a strong correlation to their spatial positions in LiDAR scans. Objects and backgrounds inside a specified spatial area of a LiDAR point cloud follow similar patterns, e.g., the close-range area is most likely road while the long-range area consists of building, vegetation, etc. In another word, there exists a spatial area a ? A where LiDAR points and semantic labels inside the area (denoted as X in and Y in , respectively) will have relatively low variations. Formally, the conditional entropy H(X in , Y in |A) is smaller. Therefore, when estimating the parameter ? of the segmentation network G ? , we would expect:</p><formula xml:id="formula_0">E ? [H(X in , Y in |A)] = c ,<label>(1)</label></formula><p>where c is a small constant. Similar to the classic entropy minimization <ref type="bibr" target="#b23">[23]</ref>, the constraint in Eq. 1 can be converted to a prior on the model parameter ? using the principle of entropy maximization:</p><formula xml:id="formula_1">P (?) ? exp(??H(X in , Y in |A)) ? exp(??H(Y in |X in , A)) ,<label>(2)</label></formula><p>where ? &gt; 0 is the Lagrange multiplier corresponding to constant c; H(X in |A) has been ignored for being independent of the model parameter ?. We consider Eq. 2 as the formal formulation of the spatial prior and discuss how to empirically compute it in the following sections.</p><p>Marginalization. To utilize the spatial prior defined in Eq. 2, we empirically compute the entropy H(Y in |X in , A) as follows:</p><formula xml:id="formula_2">H(Y in |X in , A) =? Xin,Yin,A [P (Y in |X in , A) log P (Y in |X in , A)] ,<label>(3)</label></formula><p>where. denotes the empirical estimation. The end-to-end segmentation model G ? usually takes full-sized data as inputs during inference. Therefore, to compute P (Y in |X in , A) in Eq. 3, we first pad the data outside the area to obtain the full-sized data. Here we denote the data outside the area as X out ; we then let the model infer P (Y in |X in , X out , A), and finally marginalize X out as follows:</p><formula xml:id="formula_3">P (Y in |X in , A) =? Xout [P (Y in |X in , X out , A)] .<label>(4)</label></formula><p>The generative distribution of the padding P (X out ) can be directly obtained from the dataset.</p><p>Training. Finally, we train the segmentation model G ? using the standard maximum-a-posteriori (MAP) estimation. We maximize the posterior that can be computed by Eq. 2, Eq. 3 and Eq. 4:</p><formula xml:id="formula_4">C(?) = L(?) ? ??(Y in |X in , A) = L(?) ?? Xin,Yin,A [P (Y in |X in , A) log P (Y in |X in , A)], where P (Y in |X in , A) =? Xout [P (Y in |X in , X out , A)] .<label>(5)</label></formula><p>L(?) is the likelihood function computed using labeled data, i.e., the conventional supervised learning. Minimizing?(Y in |X in , A) requires the marginal probability P (Y in |X in , A) to be confident, which further requires P (Y in |X in , X out , A) to be both confident and consistent to different X out .</p><p>In summary, our proposed SSL framework in Eq. 5 encourages the segmentation model to make confident and consistent predictions at a predefined area, regardless of the data outside the area. The predefined area set A determines the "strength" of the prior. When setting A to the full area (i.e., the whole point cloud), our framework degrades to the classic entropy minimization framework <ref type="bibr" target="#b23">[23]</ref>.</p><p>Implementation. There are three key steps for implementing our framework:</p><p>? Step 1): Select a partition set A with strong spatial prior; ? Step 2): Efficiently compute the marginal probability P (Y in |X in , A);</p><formula xml:id="formula_5">? Step 3): Minimize the marginal entropy?(Y in |X in , A).</formula><p>We propose a simple yet effective implementation following these steps in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LaserMix</head><p>Partition. LiDAR sensors have a fixed number (e.g., 32, 64, and 128) of laser beams which are emitted isotropically around the ego-vehicle with predefined inclination angles (see <ref type="figure">Fig. 2</ref>). To obtain a proper set of spatial areas A, we propose to partition the LiDAR point cloud based on laser beams. Specifically, points captured by the same laser beam have a unified inclination angle to the sensor plane. For point i, its inclination ? i is defined as follows:</p><formula xml:id="formula_6">? i = arctan( p z i (p x i ) 2 + (p y i ) 2 ) ,<label>(6)</label></formula><p>where (p x , p y , p z ) is the Cartesian coordinate of the LiDAR point. Given two LiDAR scans x 1 and x 2 , we first group all points on each scan by their inclination angles. Concretely, to form m non-overlapping areas, a set of m + 1 inclination angles ? = {? 0 , ? 1 , ? 2 , ..., ? m } will be evenly sampled within the range of the minimum and maximum inlincation angles in the dataset, and the area set A = {a 1 , a 2 , ..., a m } can be formed by bounding area a i in the inclination range</p><formula xml:id="formula_7">[? i?1 , ? i ).</formula><p>Role in our framework: Laser partition effectively "excites" a strong spatial prior in the LiDAR data, as described by Step 1 in our framework. As shown in <ref type="figure">Fig. 1</ref>(a), we observe an overt pattern in semantic classes detected by each laser beam. Despite being an empirical choice, we will show in later sections that laser partition significantly outperforms other partition choices, including: random points (MixUp-like partition <ref type="bibr" target="#b24">[24]</ref>), random areas (CutMix-like partition <ref type="bibr" target="#b25">[25]</ref>), and other heuristics like azimuth ? (sensor horizontal direction) or radius r (sensor range directon) partitions.  <ref type="figure">Figure 2</ref>: Laser partition example. We group points whose inclinations ? are within the same inclination range into the same area a.</p><p>Mixing. To this end, we propose LaserMix, a simple yet effective LiDAR mixing strategy that can better control the "strength" of the spatial prior. LaserMix mixes the aforementioned laser partitioned areas A from two scans in an intertwining way, i.e., one takes from odd-indexed areas A 1 = {a 1 , a 3 , ...} and the other takes from even-indexed areas A 2 = {a 2 , a 4 , ...}, so that each area's neighbor will be from the other scan:</p><formula xml:id="formula_8">x 1 ,x 2 = LaserMix(x 1 , x 2 ) , x 1 = {x a1 1 , x a2 2 , x a3 1 , ...} , x 2 = {x a1 2 , x a2 1 , x a3 2 , ...} ,<label>(7)</label></formula><p>where x aj i is the data crop of x i in the area a j . The semantic labels are mixed in the same way.</p><p>LaserMix is directly applied to the point clouds and is thus agnostic to the various LiDAR representations <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b18">18]</ref>. We show LaserMix's instantiations on range view and voxel representations (see <ref type="figure">Fig. 1</ref>(b)) since they are currently the most efficient and the best-performing options, respectively.</p><p>Role in our framework: LaserMix helps to efficiently compute the marginal probability P (Y in |X in , A), as described by Step 2 in our framework. The cost for directly computing the marginal probability in Eq. 4 on real-world LiDAR data is prohibitive; we need to iterate through all areas in A and all outside data in X out , which requires |A| ? |X out | predictions in total. To reduce the training overhead, we take advantage of the fact that a prediction in an area will be largely affected by its neighboring areas and let X out fill only the neighbors instead of all the remaining areas. LaserMix mixes two scans by intertwining the areas so that the neighbors of each area are filled with foreign data. As a result, we obtain the prediction on all areas A of two scans from only two predictions, which on average reduces the cost from |A| to 1. The scan before and after mixing counts as two data fillings, therefore |X out | = 2. Overall, the training overhead is reduced from |A| ? |X out | to 2: only one prediction on original data and one additional prediction on mixed data are required for each LiDAR scan. During training, the memory consumption for a batch will be 2? compared to a standard SSL framework, and the training speed will not be affected.  <ref type="figure">Figure 3</ref>: Framework overview. Labeled scan x l is fed into the Student net to compute the supervised loss L sup (w/ ground-truth y l ). Unlabeled scan x u and the generated pseudo-label y u are mixed with (x l , y l ) via LaserMix (Sec. 2.2) to produce mixed data (x mix , y mix ), which is then fed into the Student net to compute the mix loss L mix . Additionally, we adopt the EMA update in <ref type="bibr" target="#b26">[26]</ref> for the Teacher net and compute the mean teacher loss L mt over Student net's and Teacher net's predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Overall Pipeline</head><p>We show the overall framework in <ref type="figure">Fig. 3</ref> and the pseudo-code in Alg. 1. There are two branches in our pipeline, one Student net G s ? and one Teacher net G t ? . During training, a batch is composed of half labeled data and half unlabeled data. We collect the predictions from both G s ? and G t ? , and produce pseudo-labels from Teacher net's prediction with a predefined confidence threshold T . For labeled data, we compute the cross-entropy loss between the Student net's prediction and the ground-truth as L sup . For unlabeled data, LaserMix blends every scan with a random labeled scan, together with their pseudo-label or ground-truth. Then, we let G s ? predict on the mixed data and compute the cross-entropy loss L mix (w/ mixed labels). The point-wise cross-entropy loss for a scan x and its corresponding ground-truth/pseudo-label y on the segmentation net G ? is defined as:</p><formula xml:id="formula_9">L ce = 1 |x| |x| i=1 CrossEntropy(y (i) , G (i) ? (x)) ,<label>(8)</label></formula><p>where (i) denotes the i-th point. Moreover, we adopt the mean teacher idea in <ref type="bibr" target="#b26">[26]</ref> and use Exponential Moving Average (EMA) to update the weights of G t ? from G s ? , and compute the L2 loss between their predictions as L mt :</p><formula xml:id="formula_10">L mt = ||G s ? (x) ? G t ? (x)|| 2 2 ,<label>(9)</label></formula><p>where || ? || denotes the L2 norm. The overall loss function will be:</p><formula xml:id="formula_11">L = L sup + ? mix L mix + ? mt L mt ,<label>(10)</label></formula><p>where ? mix and ? mt are loss weights. We use the Teacher net during inference as it empirically gives more stable results. There will be no extra inference overhead in our framework.</p><p>Role in our framework: Our overall pipeline minimizes the marginal entropy, as described by Step 3 in our framework. Since the objective for minimizing the entropy has a hard optimization landscape, pseudo-labeling is a common resort in practice <ref type="bibr" target="#b27">[27]</ref>. Unlike conventional pseudo-label optimization in SSL that only aims to encourage the predictions to be confident, minimizing the marginal entropy requires all predictions to be both confident and consistent. Therefore, we use the ground-truth and pseudo-label as an anchor and encourage the model's predictions to be confident and consistent with these supervision signals.</p><p>Algorithm 1 Pseudo-code for one training iteration in our SSL framework.</p><formula xml:id="formula_12">1: Input: Shuffled labeled batch (X l , Y l ) = {(x (b) l , y (b) l ); b ? (1, . . . , B)}, shuffled unlabeled batch Xu = {x (b) u ; b ? (1, .</formula><p>. . , B)}, threshold T , loss weights ?mix and ?mt, Student and Teacher nets.</p><formula xml:id="formula_13">2: for b = 1 to B do 3: x (2b?1) mix , x (2b) mix = LaserMix(x (b) l , x (b) u ) // LaserMix data 4: end for 5: Xmix = {x (i) mix ; i ? (1, . . . , 2B)} 6: S l , Su, Smix = Student Concat(X l , Xu, Xmix)</formula><p>// Student net prediction scores 7:? l ,?u = Teacher Concat(X l , Xu) // Teacher net prediction scores 8: Yu = PseudoLabel(?u, T ) // Produce pseudo-label from scores larger than threshold 9: for b = 1 to B do 10: y 3 Experiments</p><formula xml:id="formula_14">(2b?1) mix , y (2b) mix = LaserMix(y (b) l , y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Protocol. We follow the Realistic Evaluation Protocol <ref type="bibr" target="#b28">[28]</ref> when building the benchmark. Specifically, all experiments share the same backbones and are within the same codebase. All configurations and data augmentations are unified to ensure a fair comparison among different SSL algorithms.</p><p>Data. We build the LiDAR SSL benchmark upon nuScenes <ref type="bibr" target="#b15">[15]</ref>, SemanticKITTI <ref type="bibr" target="#b16">[16]</ref>, and Scrib-bleKITTI <ref type="bibr" target="#b4">[4]</ref>. nuScenes <ref type="bibr" target="#b15">[15]</ref> and SemanticKITTI <ref type="bibr" target="#b16">[16]</ref> are the two most popular LiDAR segmentation datasets, with 29130 and 19130 training scans and 6019 and 4071 validation scans, respectively. ScribbleKITTI <ref type="bibr" target="#b4">[4]</ref> is a recent variant of SemanticKITTI <ref type="bibr" target="#b16">[16]</ref>, which contains the same number of scans but is annotated with scribbles (approximately 8% valid semantic labels) rather than dense annotations. For all three sets, we uniformly sample 1%, 10%, 20%, and 50% labeled training scans and assume the remaining ones as unlabeled. Implementation details. We use compact versions of FIDNet <ref type="bibr" target="#b21">[21]</ref> and Cylinder3D <ref type="bibr" target="#b18">[18]</ref> for the range view option and the voxel option, respectively. The input resolution of the range images is set as 64 ? 2048 for SemanticKITTI <ref type="bibr" target="#b16">[16]</ref> and ScribbleKITTI <ref type="bibr" target="#b4">[4]</ref>, and 32 ? 1920 for nuScenes <ref type="bibr" target="#b15">[15]</ref>. The voxel resolution is fixed as <ref type="bibr">[240,</ref><ref type="bibr">180,</ref><ref type="bibr" target="#b20">20]</ref> for all three sets. The number of spatial areas m in LaserMix is uniformly sampled from 1 to 6. We denote the supervised-only baseline as sup.-only. Due to the lack of LiDAR SSL works <ref type="bibr" target="#b14">[14]</ref>, we also compare SoTA consistency regularization <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b29">29]</ref> and entropy minimization <ref type="bibr" target="#b30">[30]</ref> methods from semi-supervised image segmentation. We report the intersection-over-union (IoU) scores over each semantic class and the mean IoU (mIoU) scores over all classes in our experiments. All experiments are implemented using PyTorch on NVIDIA Tesla V100 GPUs with 32GB RAM. Please refer to our Appendix for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparative Studies</head><p>Improvements over baseline. Tab. 1 benchmarks results on nuScenes <ref type="bibr" target="#b15">[15]</ref>, SemanticKITTI <ref type="bibr" target="#b16">[16]</ref>, and ScribbleKITTI <ref type="bibr" target="#b4">[4]</ref>. For all three sets under different data splits, we observe significant improvements of our approaches over the sup.-only baseline. Such gains are especially evident in range view, which reach up to 11.2% mIoU. We also observe constant improvements for the voxel option, which provide on average 4.1% mIoU gains over all splits across all sets. The results verify the effectiveness of our framework and further highlight the importance of leveraging unlabeled data in LiDAR segmentation.</p><p>Comparison with SoTA. We compare GPC <ref type="bibr" target="#b14">[14]</ref>, the SoTA 3D SSL method tested on SemanticKITTI <ref type="bibr" target="#b16">[16]</ref>. The results in Tab. 2 show that our method exhibits much better results than GPC <ref type="bibr" target="#b14">[14]</ref>, Repr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>nuScenes <ref type="bibr" target="#b15">[15]</ref> SemanticKITTI <ref type="bibr" target="#b16">[16]</ref> ScribbleKITTI <ref type="bibr">[</ref>   especially in scenarios where very few annotations are available. We also reimplement popular SSL algorithms from the image segmentation domain and show their results in Tab. 1. We find that these methods, albeit competitive in 2D, only yield sub-par performance in the LiDAR SSL benchmark, highlighting the importance of exploiting the LiDAR data structure. The comparison between prevailing methods in <ref type="figure">Fig. 1</ref>(c) shows that our approaches achieve competitive scores over the fully-supervised counterparts <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32]</ref> while with 2? to 5? fewer annotations. The strong augmentation and regularization ability of LaserMix further yields better results in high-data regime.</p><p>Qualitative examination. <ref type="figure" target="#fig_6">Fig. 5</ref> visualizes the scene segmentation results for different SSL algorithms. We find that previous methods <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b13">13]</ref> can only improve predictions in limited regions, while our approaches can holistically eliminate false predictions in almost every direction around the ego-vehicle. The consistency enlightened by LaserMix has yielded better segmentation results compared to the baseline and prior arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies</head><p>Framework. The component analysis in Tab. 3 shows that L mix contributes significantly to the overall improvement. Meanwhile, using Teacher net instead of Student net to generate pseudo-labels leads to better results, as the formal is temporally ensembled and encourages consistency in-between mixed and original data, which is crucial besides enforcing confident predictions.</p><p>Mixing strategies. <ref type="figure" target="#fig_5">Fig. 4</ref>(a) compares LaserMix with other mixing methods <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b25">25]</ref>. MixUp and CutMix can be considered as setting A to random points and random areas, respectively. We observe that MixUp has no improvements over the baseline on average since there is no distribution pattern in random points. CutMix has a considerable improvement over the baseline, as there is always a structure prior in scene segmentation, i.e., the same semantic class points tend to cluster, which reduces the entropy in any continuous area. This prior is often used in image semantic segmentation SSL <ref type="bibr" target="#b29">[29]</ref>. However, our spatial prior is much stronger, where not only the area structure but also the area's spatial position has been considered. LaserMix outperforms CutMix by a large margin (up to 3.3% mIoU) on all sets. CutOut can be considered as setting X out to a dummy filling instead of sampling from datasets, and it leads to a considerable performance drop from CutMix. w/ CutOut <ref type="bibr" target="#b33">[33]</ref> w/ MixUp <ref type="bibr" target="#b24">[24]</ref> Baseline <ref type="bibr" target="#b26">[26]</ref>   Orderless mix. We revert the area ordering (i.e., put the topmost laser beam at the bottom, and vice versa) in LaserMix, and the performance drops from 68.2% to 64.4% (?3.8% mIoU). When we shuffle the ordering, the performance drops to 63.8% (?4.4% mIoU), which becomes comparable with CutMix. In such cases, the spatial consistency is impaired, which highlights the importance of enforcing structural proper cues in LiDAR data. The results once again confirm the superiority of using spatial prior in LiDAR SSL.</p><p>Other heuristics. Besides our proposed inclination partition, the LiDAR scans can also be split based on azimuth (sensor horizontal direction). Results in Tab. 4 reveal that in contrast to laser partitioning, pure azimuth splitting (the first column) does not improve the performance, which is attributed to the fact that the semantic distribution has a weak correlation in the azimuth direction. We also observe that the results tend to improve as the mixing granularity increases (row direction in Tab. 4). The scores start to drop when the granularity is beyond a certain limit (e.g., the last two columns). We conjecture that over-fine-grained partition and mixing tend to hurt semantic coherence. A trade-off between these two factors should be taken into consideration in order to maximize the gains.  Mix unlabeled data only. To verify that our method is more than trivially augmenting seen data and label pairs, we apply LaserMix only on unlabeled data. Instead of mixing an unlabeled scan with a labeled scan as described in Sec. 2.3, we mix two unlabeled scans with their pseudo-labels. The score drops from 68.2% to 66.9% (?1.3% mIoU) but still outperforms all existing methods by a large margin.</p><formula xml:id="formula_15">Baseline (1?, 2?) (1?, 3?) (1?, 4?) (1?, 5?) (1?, 6?) 60.4 63.5 (+3.1) 65.2 (+4.8) 66.5 (+6.1) 66.2 (+5.8) 65.4 (+5.0) (2?, 1?) (2?, 2?) (2?, 3?) (2?, 4?) (2?, 5?) (2?, 6?) 61.5 (+1.1) 63.3 (+2.9) 65.9 (+5.5) 66.1 (+5.7) 66.7 (+6.3) 65.3 (+4.9) (3?, 1?) (3?, 2?) (3?, 3?) (3?, 4?) (3?, 5?) (3?, 6?) 60.9 (+0.6) 64.2 (+3.8) 65.9 (+5.5) 66.3 (+5.9) 66.0 (+5.6) 65.2 (+4.8) (4?, 1?) (4?, 2?) (4?, 3?) (4?, 4?) (4?, 5?) (4?,</formula><p>EMA. <ref type="figure" target="#fig_5">Fig. 4</ref>(b) provides results with different EMA decay rates, Typically, a rate between 0.9 and 0.99 yields the best possible results. Large rates like 0.999 tend to hurt the consistency between two networks. The results also verify that mean teacher <ref type="bibr" target="#b26">[26]</ref> has good synergy with our proposed SSL framework. Thanks to our simplicity, more modern SSL techniques can be easily introduced in future works.</p><p>Confidence threshold. As pseudo-labels play an important role in our framework, we further analyze the impact of the threshold parameter T used in pseudo-label generation and show results in <ref type="figure" target="#fig_5">Fig. 4(c)</ref>. When T is too low, a forced consistency to low-quality pseudo-labels tends to deteriorate the performance. When T is too high, the benefits from mixing might diminish. Generally, T is a dataset-dependent parameter and we find that a value around 0.9 leads to the best possible results on the three tested LiDAR segmentation sets in our benchmark.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>LiDAR segmentation. Various approaches from different aspects have been proposed for LiDAR scene segmentation, i.e., range view <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b21">21]</ref>, bird's eye view <ref type="bibr" target="#b19">[19]</ref>, voxel <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b36">36]</ref>, and multi-view <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref> methods. Although appealing results have been achieved, these fully-supervised methods rely on large-scale annotated LiDAR datasets and their performance would degrade severely in the low-data regime <ref type="bibr" target="#b6">[6]</ref>. Recent works seek weak <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b39">39]</ref>, scribble <ref type="bibr" target="#b4">[4]</ref>, and box <ref type="bibr" target="#b40">[40]</ref> supervisions or activate learning <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref> to ease the annotation cost. We tackle this problem from the perspective of semi-supervised learning (SSL), aiming at directly leveraging the easy-to-acquire unlabeled data to boost LiDAR segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSL in 2D.</head><p>Well-known SSL algorithms are prevailing in handling image recognition problems <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b9">9]</ref>. In the context of semantic segmentation, CutMix-Seg <ref type="bibr" target="#b29">[29]</ref> and PseudoSeg <ref type="bibr" target="#b44">[44]</ref> apply perturbations on inputs and hope the decision boundary lies in the low-density region. CPS <ref type="bibr" target="#b13">[13]</ref> and GCT <ref type="bibr" target="#b12">[12]</ref> enforce consistency between two perturbed networks <ref type="bibr" target="#b45">[45]</ref>. These perturbations <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b48">48]</ref>, however, are either inapplicable or only yield sub-par results in 3D. Another line of research is entropy minimization. Methods like CBST <ref type="bibr" target="#b30">[30]</ref> and ST++ <ref type="bibr" target="#b49">[49]</ref> generate pseudo-labels <ref type="bibr" target="#b27">[27]</ref> offline per round during self-training. The extra storage needed might become costly for largescale LiDAR datasets <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b50">50]</ref>. Our framework encourages both consistency regularization and entropy minimization and does not require extra overhead, which better maintains scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSL in 3D.</head><p>Most works focus on developing SSL for object-centric point clouds <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref> or indoor scenes <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b55">55]</ref>, whose scale and diversity are much lower than the outdoor LiDAR point clouds <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. Some other works <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b58">58]</ref> try to utilize SSL for 3D object detection on LiDAR data. A recent work <ref type="bibr" target="#b14">[14]</ref> tackles semi-supervised point cloud semantic segmentation using contrastive learning, but it still focuses on indoor scenes and does not distinguish between the uniformly distributed indoor point clouds and the spatially structured LiDAR point clouds. We are one of the first works to explore SSL for LiDAR segmentation. Our work also establishes a comprehensive benchmark upon popular autonomous driving databases <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b4">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we exploit the unique spatial prior in LiDAR point clouds for semi-supervised LiDAR semantic segmentation. We proposed a statistically-principled and effective SSL pipeline, including LaserMix, a novel LiDAR mixing technique that intertwines laser beams from different LiDAR scans. Through comprehensive empirical analysis, we show the importance of spatial prior and the superiority of our method on three popular LiDAR semantic segmentation datasets. The effectiveness and simplicity of our framework have shed light on the scalable deployment of the LiDAR semantic mapping system. Our future work seeks to enhance more fine-grained spatial partitions and introduce more modern SSL techniques via our proposed framework.</p><p>Broader Impact: This work studies the important SSL problem for LiDAR semantic segmentation, which might be helpful to reduce both the time cost and human labor needed for annotating LiDAR point clouds in practice.</p><p>Limitation: Although we improve the LiDAR semantic segmentation performance in general, label bias and out-of-domain data are not addressed in this method, which could be safety-critical issues when deploying in the real-world autonomous driving applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgment</head><p>We thank Fangzhou Hong for the insightful discussions and feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this appendix, we provide the following content to support the findings and experimental results in the main body of this paper:</p><p>? Sec. 7 provides the technical details for the LiDAR range view and voxel representations.</p><p>? Sec. 8 gives a concrete case study on the strong spatial prior in the outdoor LiDAR data.</p><p>? Sec. 9 elaborates the implementation details for different SSL algorithms in our experiments.</p><p>? Sec. 10 provides additional experimental results.</p><p>? Sec. 11 acknowledges the public resources used during the course of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">LiDAR Representation</head><p>The LiDAR data has an unique and structural format. Various representations have been proposed to better capture the internal information in and between LiDAR points, including: raw points <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b20">20]</ref>, range view (RV) <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b31">31]</ref>, bird's eye view <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b32">32]</ref>, and voxel <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b35">35]</ref> representations. This section reviews the technical details for RV projection and cylindrical voxel partition, which are currently the most efficient and the best-performing LiDAR representations, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Range View Projection</head><p>Given a LiDAR sensor with a fixed number (typically 32, 64, and 128) of laser beams and T times measurement in one scan cycle, we project LiDAR point (p x , p y , p z ) within this scan into a matrix x rv (u, v) (i.e., range image) of size h ? w via a mapping ? : R 3 ? R 2 , where h and w are the height and width, respectively. More concretely, this can be formulated as follows:</p><formula xml:id="formula_16">u v = 1 2 [1 ? arctan(p y , p x )? ?1 ] w [1 ? (arcsin(p z , r ?1 ) + ? up )? ?1 ] h ,<label>(11)</label></formula><p>where (u, v) denotes the matrix grid coordinates of x rv ; r = (p x ) 2 + (p y ) 2 + (p z ) 2 is the range between the point and the LiDAR sensor; ? = |? up | + |? low | denotes the inclination range (also known as field-of-view or FOV) of the sensor; ? up and ? low are the inclinations at the upward direction and the downward direction, respectively.</p><p>Note that h is set based on the number of laser beams of the LiDAR sensor, and w is determined by its horizontal angular resolution. The projected range image x rv (u, v) serves as the input for RV-based LiDAR segmentation networks <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b21">21]</ref>. The semantic labels are projected in the same way as x rv (u, v).</p><p>For range view representation, training losses are calculated on the range view predictions of size [k, h, w], where k denotes the number of semantic classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Cylindrical Partition</head><p>The cylinder voxels used in <ref type="bibr" target="#b18">[18]</ref> exhibits better segmentation performance than the conventional cubic voxels on the LiDAR data. This is because the outdoor LiDAR point clouds have a varying density, which decreases as the range increases. More formally, the cylinderical partition transforms points in the Cartesian coordinate (p x , p y , p z ) into cylinder coordinate (?, ?, p z ), where ? is the distance to the origin in X-Y plane and ? is the azimuth in the sensor horizontal direction. The transformation can be formulated as follows:</p><formula xml:id="formula_17">? = (p x ) 2 + (p y ) 2 , ? = arctan( p y p x ).<label>(12)</label></formula><p>Given a predefined voxel resolution [n ? , n ? , n z ], points in the cylinder coordinate can be partitioned into the corresponding voxel cells. The semantic labels are split into the partitioned cylinder voxels, where all points within the same voxel are assigned a unified label via majority voting.</p><p>For cylindircal representation, training losses are calculated on the voxel predictions of size [k, n ? , n ? , n z ], where k denotes the number of semantic classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Case Study: Spatial Prior in LiDAR Data</head><p>As mentioned in the main body of this paper, the LiDAR point clouds collected by the LiDAR sensor on top of the autonomous vehicle contain inherent spatial cues, which lead to strong patterns in laser beam partition. In this section, we conduct a case study on SemanticKITTI <ref type="bibr" target="#b16">[16]</ref> to verify our findings (see Tab. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Laser Partition</head><p>The LiDAR scans in the SemanticKITTI <ref type="bibr" target="#b16">[16]</ref> dataset are collected by the Velodyne-HDLE64 sensor, which contains 64 laser beams emitted isotropically around the ego-vehicle with predefined inclination angles. In this study, we split each LiDAR point cloud into eight non-overlapping areas, i.e., A = {a 1 , a 2 , ..., a 8 }. Each area a i contains points captured from the consecutive 8 laser beams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Spatial Prior</head><p>As can be seen from the fourth column in Tab. 5, different semantic classes have their own behaviors on these predefined areas. Specifically, the road class occupies mostly the first four areas (close to the ego-vehicle) while hardly appearing in the last two areas (far from the ego-vehicle). The vegetation class and the building class behavior conversely to road and appear at the long-distance areas (e.g., a 6 , a 7 , a 8 ). The dynamic classes, including car, bicyclist, motorcyclist, and person, tend to appear in the middle-distance areas (e.g., a 4 , a 5 , a 6 ). Similarly, from the heatmaps shown in the fifth column in Tab. 5, we can see that these semantic classes tend to appear (lighter colors) in only certain areas. For example, the traffic-sign class has a high likelihood to appear in the long-distance regions from the ego-vehicle (upper areas in the corresponding heatmap).</p><p>These unique distributions reflect the spatial layout of street scenes in real-world. In this work, we propose to leverage these strong spatial cues to construct our SSL framework. The experimental results verify that the spatial prior can better encourage consistency regularization in LiDAR segmentation under annotation scracity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Implementation Details</head><p>In this section, we first compare the configuration details for the three LiDAR segmentation datasets (nuScenes <ref type="bibr" target="#b15">[15]</ref>, SemanticKITTI <ref type="bibr" target="#b16">[16]</ref>, and ScribbleKITTI <ref type="bibr" target="#b4">[4]</ref>) used in this work (see Tab. 6). We then provide more detailed information on different SSL algorithms implemented in our semi-supervised LiDAR segmentation benchmark.    <ref type="bibr" target="#b50">[50]</ref> and <ref type="bibr" target="#b4">[4]</ref>. Images in the last three rows are generated from the corresponding datasets.</p><p>SemanticKITTI. Derived from the famous KITTI Vision Odometry Benchmark, Se-manticKITTI <ref type="bibr" target="#b16">[16]</ref> is another large-scale LiDAR segmentation dataset widely adopted in academia. It consists of 22 driving sequences, which are splitted into a train set (Seq. 00 to 10, where 08 is used for validation) and a test set (Seq. 11 to 21). The LiDAR point clouds are captured from Karlsruhe, Germany, by a 64-beam LiDAR sensor. The inclination range is [3 ? , ?25 ? ]. We follow the official label mapping and use 19 semantic classes in our experiments.</p><p>ScribbleKITTI. Efficiently annotating LiDAR point clouds is a viable solution for scaling up LiDAR segmentation. ScribbleKITTI <ref type="bibr" target="#b4">[4]</ref> adopts scribbles to annotate SemanticKITTI <ref type="bibr" target="#b16">[16]</ref>, resulting in around 8.06% semantic labels compared to the dense annotations. The other configurations are the same as SemanticKITTI <ref type="bibr" target="#b16">[16]</ref>. We use the densely annotated set (Seq. 08 in SemanticKITTI <ref type="bibr" target="#b16">[16]</ref>) as the validation set.</p><p>In summary, we choose datasets with different number of laser beams (i.e., 32 for nuScenes <ref type="bibr" target="#b15">[15]</ref> and 64 for SemanticKITTI <ref type="bibr" target="#b16">[16]</ref> and ScribbleKITTI <ref type="bibr" target="#b4">[4]</ref>), different inclination ranges (i.e., [10 ? , ?30 ? ] for nuScenes <ref type="bibr" target="#b15">[15]</ref> and [3 ? , ?25 ? ] for SemanticKITTI <ref type="bibr" target="#b16">[16]</ref> and ScribbleKITTI <ref type="bibr" target="#b4">[4]</ref>), and different annotation proportions (i.e., 100% for nuScenes <ref type="bibr" target="#b15">[15]</ref> and SemanticKITTI <ref type="bibr" target="#b16">[16]</ref> and 8.06% for Scrib-bleKITTI <ref type="bibr" target="#b4">[4]</ref>). Our proposed SSL framework exhibit constant and evident improvements on all three datasets, which further verifies the scalability of our approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Model Configurations</head><p>FIDNet. We use the ResNet34-point variant in FIDNet <ref type="bibr" target="#b21">[21]</ref> as our range view segmentation backbone. It contains fewer parameters (6.05M) than the one used in the original paper (19.64M) while still maintaining good segmentation performance: 58.8% mIoU (compared to 59.5% mIoU) on the val set of SemanticKITTI <ref type="bibr" target="#b16">[16]</ref>, and 71.6% mIoU (compared to 72.3% mIoU) on the val set of nuScenes <ref type="bibr" target="#b15">[15]</ref>. We refer to the FIDNet <ref type="bibr" target="#b21">[21]</ref> paper for more details on the model architecture.</p><p>Cylinder3D. We use a more compact version of Cylinder3D <ref type="bibr" target="#b18">[18]</ref> as the voxel segmentation backbone in our experiments, which has 28.13M parameters (compared to 56.26M for the one used in the original paper). We also use a smaller voxel resolution <ref type="bibr">([240, 180, 20]</ref>) compared to the original configuration <ref type="bibr">([480, 360, 32]</ref>). This saves around 4? memory consumption and further helps to speed up training. We found that with the smaller resolution (larger voxel size), the performance drops from 76.1% mIoU to 74.1% mIoU on the val set of nuScenes <ref type="bibr" target="#b15">[15]</ref>. We refer to the Cylinder3D <ref type="bibr" target="#b18">[18]</ref> paper for more details on the model architecture.</p><p>Training configurations. All SSL algorithms implemented are sharing the same LiDAR segmentation backbones, i.e., FIDNet <ref type="bibr" target="#b21">[21]</ref> for the LiDAR range view representation and Cylinder3D <ref type="bibr" target="#b18">[18]</ref> for the LiDAR voxel representation. For both FIDNet and Cylinder3D, we adopt AdamW <ref type="bibr" target="#b60">[60]</ref> as the optimizer and use the OneCycle learning rate scheduler <ref type="bibr" target="#b61">[61]</ref>. The maximum learning rate is 0.0025 for FIDNet and 0.001 for Cylinder3D. The batch size for the LiDAR range view representation is 10 for nuScenes and 4 for SemanticKITTI and ScribbleKITTI. The batch size for the LiDAR voxel representation is 8 for nuScenes and 4 for SemanticKITTI and ScribbleKITTI. All experiments can be conducted using a single NVIDIA Tesla V100 GPU with 32GB RAM.</p><p>Data augmentations. The data augmentations used for the range view inputs for all SSL algorithms include: random jittering, scaling, flipping (for nuScenes), and shifting (for SemanticKITTI and ScribbleKITTI). The data augmentations used for the voxel inputs for all SSL algorithms include: random rotation and flipping (for nuScenes, SemanticKITTI and ScribbleKITTI).</p><p>Other configurations. For LaserMix, the number of spatial areas is uniformly sampled from 1 to 6. The weight ? mix is set as 1 for all three datasets. The weight ? mt is set as 1e3 for nuScenes and 2e3 for SemanticKITTI and ScribbleKITTI. For CPS <ref type="bibr" target="#b13">[13]</ref>, the weight ? cps is set as 1 for all three datasets. We tried 2 and 6 and found 1 yielded the best results. For MeanTeacher <ref type="bibr" target="#b26">[26]</ref>, the weight ? mt is set as 1e3 for nuScenes and 2e3 for SemanticKITTI and ScribbleKITTI. For CutMix-Seg <ref type="bibr" target="#b29">[29]</ref>, the weight ? cons is set as 1 which is the same as the original paper. For CBST <ref type="bibr" target="#b30">[30]</ref>, we use the sup.-only checkpoints to generate the pseudo-labels and then train the segmentation network from scratch with the pseudo-labels. We refer to the original papers for the aforementioned algorithms <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref> for other details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Additional Experimental Results</head><p>Method  <ref type="table">Table 7</ref>: Benchmarking results on the val set of Cityscapes <ref type="bibr" target="#b62">[62]</ref>. All mIoU scores are given in percentage (%).</p><p>In this section, we provide the class-wise IoU results for our compartive studies and ablation studies in the main body of this paper. Since our proposed SSL framework is a generic design, we also include the benchmarking results on Cityscapes <ref type="bibr" target="#b62">[62]</ref> to further verify our generalizability on structural RGB data.</p><p>Comparative studies. Tab. 8, Tab. 9, and Tab. 10 provide the class-wise IoU scores for different SSL algorithms on the val set of nuScenes <ref type="bibr" target="#b15">[15]</ref>, Se-manticKITTI <ref type="bibr" target="#b16">[16]</ref>, and ScribbleKITTI <ref type="bibr" target="#b4">[4]</ref>, respectively. For almost all semantic classes, we observe overt improvements from LaserMix. This can be credited to the strong consistency regularization encouraged by our SSL framework.</p><p>Ablation studies. Tab. 11 and Tab. 12 provide the class-wise IoU scores for the granularity studies of the LiDAR range view and voxel representations, respectively. Among different LaserMix strategies, we find that increasing the granularity along inclination tend to yield better segmentation performance. In our benchmarking experiments, we combine different strategies together by uniformly sampling the number of spatial areas. This simple ensembling further increases diversity and provides higher segmentation scores.</p><p>Extension to RGB data. To further attest the scalability of our proposed spatial-prior SSL framework, we conduct experiments on Cityscapes <ref type="bibr" target="#b62">[62]</ref>, which contains structural RGB images collected from street scenes. We follow the data split from recent work <ref type="bibr" target="#b13">[13]</ref> and show the results in Tab. 7. Since the images from this dataset also contain strong spatial cues, the mixing strategy used here is similar to that for the LiDAR range view representation, i.e., partitioning areas along the image vertical direction. We combine our proposed L mix with MeanTeacher <ref type="bibr" target="#b26">[26]</ref> (L mt ) and CPS <ref type="bibr" target="#b13">[13]</ref> (L cps ). The results verify that our SSL framework can also encourage consistency for image data. For all four splits, our approaches constantly improve the segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Public Resources Used</head><p>We acknowledge the use of the following public resources, during the course of this work:      <ref type="table" target="#tab_0">Table 12</ref>: Class-wise IoU scores for granurality studies on the voxel representation (under 10% split on the val set of nuScenes <ref type="bibr" target="#b15">[15]</ref>). All scores are given in percentage. The best score for each semantic class is highlighted in bold.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>12 :</head><label>12</label><figDesc>Ymix = {y (i) mix ; i ? (1, . . . , 2B)} 13: Lsup = CrossEntropy(S l , Y l ) // Supervised loss 14: Lmix = CrossEntropy(Smix, Ymix) // Mix loss 15: Lmt = L2 Concat(S l , Su), Concat(? l ,?u) // Mean Teacher loss 16: L = Lsup + ?mixLmix + ?mtLmt 17: Backward(L), Update(Student), UpdateEMA(Teacher)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Ablation studies. a) Mixing techniques; b) EMA decay rate; c) Confidence threshold T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results from LiDAR top view and range view. The correct and incorrect predictions are painted in green and red to highlight the difference. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>? nuScenes 4 .</head><label>4</label><figDesc>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . CC BY-NC-SA 4.0 ? nuScenes-devkit 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 ? SemanticKITTI 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . CC BY-NC-SA 4.0 ? SemanticKITTI-API 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License ? ScribbleKITTI 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown ? FIDNet 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Unknown ? Cylinder3D 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Apache License 2.0 ? TorchSemiSeg 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License ? MixUp 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Attribution-NonCommercial 4.0 International ? CutMix 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .MIT License ? CutMix-Seg 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MIT License ? CBST 15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Attribution-NonCommercial 4.0 International ? MeanTeacher 16 . . . . . . . . . . . . . . . . . . . . . . . . . Attribution-NonCommercial 4.0 International ? Cityscapes 17 . . . . . . . . . . Custom License by Daimler AG, MPI Informatics, TU Darmstadt Split Repr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Sup.-only 38.3 57.5 62.7 67.6 36.2 52.2 55.9 57.2 33.1 47.7 49.9 52.5 MeanTeacher [26] 42.1 60.4 65.4 69.4 37.5 53.1 56.1 57.4 34.2 49.8 51.6 53.3 CBST [30] 40.9 60.5 64.3 69.3 39.9 53.4 56.1 56.9 35.7 50.7 52.7 54.6 CutMix-Seg [29] 43.8 63.9 64.8 69.8 37.4 54.3 56.6 57.6 36.7 50.7 52.9 54.3 CPS [13] 40.7 60.8 64.9 68.0 36.5 52.3 56.3 57.4 33.7 50.0 52.8 54.6 LaserMix (Ours) 49.5 68.2 70.6 73.0 43.4 58.8 59.4 61.4 38.3 54.4 55.6 58.7 ? ? +11.2 +10.7 +7.9 +5.4 +7.2 +6.6 +3.5 +4.2 +5.2 +6.7 +5.7 +6.2 Voxel Sup.-only 50.9 65.9 66.6 71.2 45.4 56.1 57.8 58.7 39.2 48.0 52.1 53.8 MeanTeacher [26] 51.6 66.0 67.1 71.7 45.4 57.1 59.2 60.0 41.0 50.1 52.8 53.9 CBST [30] 53.0 66.5 69.6 71.6 48.8 58.3 59.4 59.7 41.5 50.6 53.3 54.5 CPS [13] 52.9 66.3 70.0 72.5 46.7 58.7 59.6 60.5 41.4 51.8 53.9 54.8 LaserMix (Ours) 55.3 69.9 71.8 73.2 50.6 60.0 61.9 62.3 44.2 53.7 55.1 56.8 ? ? +4.4 +4.0 +5.2 +2.0 +5.2 +3.9 +4.1 +3.6 +5.0 +5.7 +3.0 +3.0 Benchmarking results for methods with the LiDAR range view and voxel representations. All mIoU scores are given in percentage (%). The best result for each split is highlighted in bold.</figDesc><table><row><cell>4]</cell></row></table><note>+14.9 +10.1 +3.1 +1.7 +1.4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>20% 50%</cell></row></table><note>Comparison to state-of-the-art 3D SSL method on the val set of SemanticKITTI [16]. All mIoU scores are given in percentage.# L mt L mix SS TS 1% 10%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Ablation results (mIoU in percentage) on nuScenes [15] val set. (1) Baseline [26]; (2) Student supervision (SS); (3) Teacher supervision (TS).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>6?) 60.9 (+0.6) 64.7 (+4.3) 65.3 (+4.9) 65.6 (+5.2) 65.7 (+5.3) 65.2 (+4.8)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation on LiDAR partitions (horizontal: inclination ?; vertical: azimuth ?). (i-?, j-?) denotes that there are i azimuth and j inclination partitions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>A case study of the strong spatial prior in the LiDAR data (SemanticKITTI<ref type="bibr" target="#b16">[16]</ref> in this example). For each semantic class, we show its type (static or dynamic), occupation (in percentage), distribution among eight areas (A = {a 1 , a 2 , ..., a 8 }), and heatmap in range view (lighter colors correspond to areas that have higher likelihood to appear and vice versa). As a comprehensive autonomous driving dataset, nuScenes<ref type="bibr" target="#b3">3</ref> <ref type="bibr" target="#b15">[15]</ref> provides 1000 driving scenes of 20s duration each collected by a 32-beam LiDAR sensor from Boston and Singapore. We follow the official train and val sample splittings. The total number of LiDAR scans is 40000. The training and validation sets contain 28130 and 6019 scans, respectively. The semantic labels are annotated within the ranges: p</figDesc><table><row><cell>9.1 Data</cell></row><row><cell>nuScenes.</cell></row></table><note>x ? [50m, ?50m], p y ? [50m, ?50m], and p z ? [3m, ?5m]. Points outside the range are labeled as ignored. The inclination range is [10 ? , ?30 ? ]. We use the official label mapping which contains 16 semantic classes in total.nuScenes</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Configuration details for the three LiDAR segmentation datasets (nuScenes [15], Se- manticKITTI [16], and ScribbleKITTI [4]) used in this work. Rows from top to bottom: visualization examples, number of semantic classes, number of training scans, number of validation scans, reso- lution for range view inputs, resolution for voxel inputs, number of laser beams, inclination angle range, x-axis range, y-axis range, z-axis range, the proportion of semantic labels, sensor intensity examples, range examples, and semantic label examples. Images in the second row are adopted from</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Class-wise IoU scores for different SSL algorithms on the val set of nuScenes<ref type="bibr" target="#b15">[15]</ref>. All IoU scores are given in percentage. The sup.-only and the best scores for each semantic class within each split are highlighted in red and blue, respectively.</figDesc><table><row><cell>Split Repr.</cell><cell>Method</cell><cell>mIoU</cell><cell>car</cell><cell cols="4">bicy moto truck bus</cell><cell cols="5">ped b.cyc m.cyc road park walk o.gro build fence veg trunk terr pole sign</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>36.2</cell><cell cols="2">86.8 0.6</cell><cell>0.0</cell><cell>13.0</cell><cell cols="2">5.7 12.1</cell><cell>6.6</cell><cell>0.0</cell><cell>87.9 13.4 71.3</cell><cell>0.1</cell><cell>80.4</cell><cell>42.3 78.7 38.1 62.8 52.5 35.7</cell></row><row><cell>Range View</cell><cell>MeanTeacher [26] CBST [30] CutMix-Seg [29] CPS [13]</cell><cell>37.5 39.9 37.4 36.5</cell><cell cols="2">88.0 0.1 89.4 1.9 86.6 0.2 88.9 0.0</cell><cell>0.1 0.0 0.0 0.0</cell><cell>12.4 4.6 3.2 3.1</cell><cell cols="3">3.6 13.0 12.6 5.8 27.3 3.4 1.5 18.6 6.4 0.4 5.7 2.7</cell><cell>0.0 0.0 0.0 0.0</cell><cell>89.2 19.6 73.0 91.3 25.9 76.5 90.8 24.2 74.9 90.8 13.7 76.7</cell><cell>0.0 0.0 0.0 0.0</cell><cell>81.6 83.9 81.5 83.4</cell><cell>44.8 80.2 41.8 64.4 54.0 33.3 49.1 82.7 56.4 68.1 57.5 33.6 45.5 81.3 50.0 65.7 52.9 34.6 52.2 79.9 40.8 63.8 55.9 32.3</cell></row><row><cell>1%</cell><cell>LaserMix</cell><cell>43.4</cell><cell cols="2">88.8 37.1</cell><cell>0.2</cell><cell>2.1</cell><cell cols="3">4.1 10.7 40.7</cell><cell>0.2</cell><cell>91.9 32.3 77.0</cell><cell>0.0</cell><cell>83.9</cell><cell>48.8 81.4 55.9 69.4 59.0 41.7</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>45.4</cell><cell cols="2">90.9 24.5</cell><cell>2.8</cell><cell cols="4">35.1 20.4 31.7 49.5</cell><cell>0.0</cell><cell>85.5 23.4 67.5</cell><cell>1.3</cell><cell>85.0</cell><cell>46.0 84.1 49.1 70.3 55.0 40.6</cell></row><row><cell>Voxel</cell><cell>MeanTeacher [26] CBST [30] CPS [13]</cell><cell>45.4 48.8 46.7</cell><cell cols="2">91.2 13.2 92.4 16.3 92.0 13.5</cell><cell>5.4 6.4 7.1</cell><cell cols="4">47.3 14.5 29.0 37.3 61.9 27.0 35.7 49.4 37.8 12.7 33.0 54.5</cell><cell>0.0 0.0 0.0</cell><cell>86.8 22.6 70.3 88.9 29.4 73.2 89.8 25.0 73.8</cell><cell>1.2 0.7 0.0</cell><cell>86.7 89.1 88.8</cell><cell>45.4 84.7 59.4 70.9 55.8 40.8 49.5 83.9 51.4 68.1 59.8 44.0 50.1 83.6 57.4 67.8 58.2 42.1</cell></row><row><cell></cell><cell>LaserMix</cell><cell>50.6</cell><cell cols="7">91.8 35.7 19.8 37.5 25.6 53.6 45.7</cell><cell>2.5</cell><cell>87.8 33.5 71.3</cell><cell>0.7</cell><cell>87.3</cell><cell>43.8 84.6 62.7 69.3 59.8 47.6</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>52.2</cell><cell cols="7">90.4 34.2 22.6 48.2 24.5 59.7 60.9</cell><cell>0.0</cell><cell>92.2 31.8 78.1</cell><cell>0.5</cell><cell>85.7</cell><cell>47.9 83.9 59.3 69.3 59.0 44.2</cell></row><row><cell>Range View</cell><cell>MeanTeacher [26] CBST [30] CutMix-Seg [29] CPS [13]</cell><cell>53.1 53.4 54.3 52.3</cell><cell cols="7">91.1 30.8 23.1 58.9 27.5 60.1 57.9 91.7 33.7 28.9 62.0 29.7 57.9 55.2 90.9 34.9 37.2 57.4 31.7 56.1 63.9 90.2 32.8 19.7 54.0 23.8 56.8 50.5</cell><cell>0.0 0.0 0.0 0.0</cell><cell>92.9 34.7 78.7 92.9 32.5 78.7 92.9 34.5 78.6 92.7 36.3 79.5</cell><cell>0.9 0.8 0.5 0.4</cell><cell>87.3 87.1 87.0 87.6</cell><cell>53.5 83.3 59.6 66.9 57.0 44.1 53.7 83.5 59.4 68.1 56.7 42.4 52.3 83.6 58.8 68.8 55.2 44.7 52.0 85.7 59.4 69.2 58.6 45.1</cell></row><row><cell>10%</cell><cell>LaserMix</cell><cell>58.8</cell><cell cols="7">92.0 43.5 50.4 76.1 37.1 69.9 74.3</cell><cell>0.0</cell><cell>93.4 38.8 80.1</cell><cell>0.6</cell><cell>87.1</cell><cell>53.3 84.2 63.2 68.3 58.8 45.3</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>56.1</cell><cell cols="7">93.4 38.4 47.7 65.7 31.0 61.9 64.9</cell><cell>0.0</cell><cell>90.7 37.7 75.3</cell><cell>0.9</cell><cell>89.2</cell><cell>50.5 86.4 56.0 73.9 56.2 46.0</cell></row><row><cell>Voxel</cell><cell>MeanTeacher [26] CBST [30] CPS [13]</cell><cell>57.1 58.3 58.7</cell><cell cols="7">94.1 40.5 58.4 56.0 38.0 66.5 75.6 93.6 40.3 43.5 80.4 33.8 57.6 78.1 94.0 38.7 51.0 60.3 39.8 65.7 80.0</cell><cell>0.0 0.0 0.0</cell><cell>88.4 22.7 72.0 91.6 36.3 76.6 91.4 33.2 76.4</cell><cell>1.5 5.1 2.9</cell><cell>87.9 89.2 89.8</cell><cell>49.3 86.7 66.1 74.2 58.0 49.2 51.1 86.3 61.9 71.2 61.3 49.7 53.8 87.2 65.7 74.6 61.5 50.0</cell></row><row><cell></cell><cell>LaserMix</cell><cell>60.0</cell><cell cols="7">93.8 44.9 58.4 65.6 39.4 65.8 80.9</cell><cell>0.2</cell><cell>92.0 44.2 77.1</cell><cell>3.9</cell><cell>89.1</cell><cell>49.0 86.2 66.8 72.3 58.4 51.2</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>55.9</cell><cell cols="7">92.2 38.4 34.9 68.8 35.1 63.1 69.4</cell><cell>0.0</cell><cell>93.1 33.8 79.0</cell><cell>1.1</cell><cell>86.6</cell><cell>50.4 84.1 60.9 69.2 56.9 45.3</cell></row><row><cell>Range View</cell><cell>MeanTeacher [26] CBST [30] CutMix-Seg [29] CPS [13]</cell><cell>56.1 56.1 56.6 56.3</cell><cell cols="7">93.2 33.1 36.3 67.3 39.1 64.9 66.8 92.8 33.2 33.9 64.9 38.9 66.6 69.1 91.5 42.8 39.8 60.6 32.9 64.3 71.6 90.8 44.0 40.7 67.9 30.7 65.5 58.0</cell><cell>0.0 0.0 0.0 0.0</cell><cell>93.3 36.7 79.8 93.2 36.9 79.7 93.1 39.8 79.3 93.3 39.1 79.5</cell><cell>1.0 1.7 0.6 1.1</cell><cell>87.6 87.3 87.1 87.5</cell><cell>54.0 83.9 60.7 67.7 56.5 43.7 53.6 84.5 60.7 69.1 55.1 44.9 53.8 85.0 61.6 71.0 56.1 45.4 55.6 83.8 60.4 67.9 56.8 46.7</cell></row><row><cell>20%</cell><cell>LaserMix</cell><cell>59.4</cell><cell cols="7">92.5 43.3 51.5 73.1 45.8 69.4 74.7</cell><cell>0.0</cell><cell>94.0 40.4 80.4</cell><cell>5.0</cell><cell>87.3</cell><cell>53.7 83.8 64.1 66.7 58.0 44.6</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>57.8</cell><cell cols="7">94.0 31.6 47.3 89.5 38.3 57.9 79.1</cell><cell>0.0</cell><cell>91.6 29.6 76.1</cell><cell>0.9</cell><cell>87.8</cell><cell>43.6 86.6 63.7 72.5 61.8 47.5</cell></row><row><cell>Voxel</cell><cell>MeanTeacher [26] CBST [30] CPS [13]</cell><cell>59.2 59.4 59.6</cell><cell cols="7">94.4 38.7 52.5 81.2 45.8 64.2 78.0 94.2 41.8 51.4 77.7 39.8 65.4 79.8 94.2 41.8 52.9 78.2 39.6 66.1 80.6</cell><cell>0.0 0.0 0.0</cell><cell>90.9 35.2 75.7 91.7 29.8 76.3 91.9 30.2 76.4</cell><cell>1.8 3.5 3.7</cell><cell>89.2 89.2 89.2</cell><cell>49.8 86.3 65.6 72.6 56.0 47.6 49.7 87.1 66.1 74.2 60.1 51.3 50.0 87.0 66.6 73.7 60.0 51.1</cell></row><row><cell></cell><cell>LaserMix</cell><cell>61.9</cell><cell cols="7">94.4 46.0 68.0 74.3 47.6 68.1 83.7</cell><cell>0.2</cell><cell>92.6 42.7 78.0</cell><cell>1.9</cell><cell>89.7</cell><cell>52.9 86.0 69.3 70.6 59.2 51.7</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>57.2</cell><cell cols="7">91.3 41.1 47.7 70.2 41.2 66.0 74.4</cell><cell>0.0</cell><cell>93.0 39.2 79.2</cell><cell>2.0</cell><cell>86.0</cell><cell>44.2 83.4 59.3 68.6 55.5 45.1</cell></row><row><cell>Range View</cell><cell>MeanTeacher [26] CBST [30] CutMix-Seg [29] CPS [13]</cell><cell>57.4 56.9 57.6 57.4</cell><cell cols="7">93.1 38.6 42.4 61.0 45.0 65.7 73.9 91.5 40.0 42.9 66.1 41.7 64.8 74.2 92.0 43.3 48.9 44.6 40.7 67.4 78.5 92.1 38.5 44.3 69.6 45.2 66.5 71.0</cell><cell>0.0 0.0 0.0 0.0</cell><cell>93.1 38.1 79.4 93.0 34.9 79.2 93.3 39.1 79.7 93.5 36.6 80.1</cell><cell>2.1 1.2 3.0 1.7</cell><cell>87.5 87.0 87.2 87.0</cell><cell>53.8 85.0 60.3 71.5 53.6 47.2 48.7 83.7 59.6 68.9 55.3 47.1 54.2 86.0 61.6 74.8 55.1 44.8 48.0 83.9 62.3 68.0 58.0 43.7</cell></row><row><cell>50%</cell><cell>LaserMix</cell><cell>61.4</cell><cell cols="7">92.5 45.6 58.8 73.0 53.2 71.2 82.4</cell><cell>0.0</cell><cell>93.7 43.2 80.7</cell><cell>5.5</cell><cell>87.5</cell><cell>52.6 85.4 64.0 71.9 57.9 47.9</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>58.7</cell><cell cols="7">93.9 40.4 48.0 81.4 33.7 65.7 79.7</cell><cell>0.0</cell><cell>91.9 32.6 76.7</cell><cell>1.3</cell><cell>89.0</cell><cell>51.8 87.2 61.4 72.5 58.7 48.7</cell></row><row><cell>Voxel</cell><cell>MeanTeacher [26] CBST [30] CPS [13]</cell><cell>60.0 59.7 60.5</cell><cell cols="7">94.1 41.3 57.7 64.6 39.5 65.3 86.8 94.9 40.9 54.4 75.3 43.8 67.3 86.8 94.6 43.3 55.3 80.5 42.5 67.9 84.6</cell><cell>0.0 0.0 0.0</cell><cell>91.3 32.8 75.2 91.5 33.3 75.7 92.0 34.3 76.9</cell><cell>3.5 2.6 2.2</cell><cell>89.7 89.3 89.8</cell><cell>48.6 85.4 65.9 70.6 58.7 49.1 50.7 86.7 63.9 72.4 56.4 48.8 52.3 86.0 67.4 71.1 59.5 49.4</cell></row><row><cell></cell><cell>LaserMix</cell><cell>62.3</cell><cell cols="7">94.7 48.4 64.7 65.2 44.5 71.0 88.3</cell><cell>2.1</cell><cell>92.7 43.0 78.4</cell><cell>2.0</cell><cell>90.3</cell><cell>54.9 88.1 68.1 75.3 66.6 51.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Class-wise IoU scores for different SSL algorithms on the val set of SemanticKITTI<ref type="bibr" target="#b16">[16]</ref>. All IoU scores are given in percentage. The sup.-only and the best scores for each semantic class within each split are highlighted in red and blue, respectively.</figDesc><table><row><cell>Split Repr.</cell><cell>Method</cell><cell>mIoU</cell><cell>car</cell><cell cols="4">bicy moto truck bus</cell><cell cols="6">ped b.cyc m.cyc road park walk o.gro build fence veg trunk terr pole sign</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>33.1</cell><cell cols="2">81.3 2.6</cell><cell>0.4</cell><cell>11.7</cell><cell cols="2">8.3 11.5</cell><cell>8.7</cell><cell>0.0</cell><cell>76.7 10.9 61.8</cell><cell>0.1</cell><cell>75.8</cell><cell>26.3 73.8 40.7 56.1 48.9 32.5</cell></row><row><cell>Range View</cell><cell>MeanTeacher [26] CBST [30] CutMix-Seg [29] CPS [13]</cell><cell>34.2 35.7 36.7 33.7</cell><cell cols="2">82.3 1.7 84.8 1.6 84.7 0.9 82.7 0.1</cell><cell>0.1 0.4 0.0 0.0</cell><cell cols="3">10.4 11.7 10.6 14.9 6.7 6.1 5.5 0.9 18.7 0.9 0.1 2.9</cell><cell>4.7 8.0 1.9 4.1</cell><cell>0.0 0.0 0.0 0.0</cell><cell>78.6 13.4 67.8 83.6 13.4 68.1 89.3 25.1 74.6 85.9 8.9 70.8</cell><cell>0.1 0.1 0.1 0.0</cell><cell>80.7 79.5 82.6 81.2</cell><cell>31.3 76.1 43.1 60.0 53.3 32.8 32.4 77.1 44.6 60.5 53.0 34.4 27.0 77.7 52.1 65.0 54.7 35.8 47.3 78.1 36.0 61.2 51.9 27.5</cell></row><row><cell>1%</cell><cell>LaserMix</cell><cell>38.3</cell><cell cols="2">86.5 1.9</cell><cell>0.9</cell><cell>12.8</cell><cell cols="2">2.9 25.9</cell><cell>2.6</cell><cell>0.0</cell><cell>90.8 25.0 75.8</cell><cell>1.0</cell><cell>83.9</cell><cell>26.4 77.8 55.5 63.9 56.7 38.2</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>39.2</cell><cell cols="2">83.2 13.8</cell><cell>3.4</cell><cell cols="4">26.3 11.8 28.0 25.2</cell><cell>0.0</cell><cell>72.5 13.0 59.5</cell><cell>0.2</cell><cell>86.6</cell><cell>33.7 78.7 55.7 58.4 54.0 40.3</cell></row><row><cell>Voxel</cell><cell>MeanTeacher [26] CBST [30] CPS [13]</cell><cell>41.0 41.5 41.4</cell><cell cols="7">82.3 15.8 83.7 22.1 82.8 18.2 11.4 20.9 15.1 22.5 35.5 7.1 32.0 15.4 23.7 36.3 5.9 28.3 13.4 27.1 34.7</cell><cell>0.0 0.0 0.0</cell><cell>75.0 12.6 61.4 74.0 14.4 61.7 74.7 15.7 61.6</cell><cell>0.9 0.2 0.4</cell><cell>85.3 88.1 86.0</cell><cell>30.0 80.1 57.0 67.0 56.1 41.3 36.6 80.3 58.7 60.4 57.1 41.4 34.2 82.2 58.4 69.9 56.7 40.0</cell></row><row><cell></cell><cell>LaserMix</cell><cell>44.2</cell><cell cols="7">82.6 25.5 18.8 29.0 19.8 41.1 47.2</cell><cell>0.6</cell><cell>71.5 10.5 64.2</cell><cell>2.2</cell><cell>85.1</cell><cell>33.5 82.0 59.9 65.8 54.5 45.2</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>47.7</cell><cell cols="7">85.1 30.2 20.4 40.4 20.9 54.4 55.9</cell><cell>0.0</cell><cell>82.8 21.6 68.4</cell><cell>0.5</cell><cell>84.2</cell><cell>40.5 80.3 58.6 61.9 56.0 45.2</cell></row><row><cell>Range View</cell><cell>MeanTeacher [26] CBST [30] CutMix-Seg [29] CPS [13]</cell><cell>49.8 50.7 50.7 50.0</cell><cell cols="7">83.5 30.0 22.7 62.2 31.1 59.1 52.4 90.3 27.2 18.1 53.1 24.6 60.3 56.5 87.4 28.1 25.9 60.5 24.5 58.4 57.7 85.8 26.7 17.4 54.5 20.5 54.4 53.7</cell><cell>0.0 0.0 0.0 0.0</cell><cell>77.9 17.6 70.5 90.3 32.4 76.0 85.5 27.5 72.1 88.9 29.2 74.4</cell><cell>2.0 0.7 1.3 0.6</cell><cell>86.8 86.1 84.7 86.5</cell><cell>42.6 82.0 61.2 62.6 58.4 46.5 46.0 81.1 58.7 64.5 51.7 45.0 39.4 82.4 58.8 68.3 56.4 44.4 48.6 82.4 58.6 65.2 57.3 45.1</cell></row><row><cell>10%</cell><cell>LaserMix</cell><cell>54.4</cell><cell cols="7">87.1 35.4 44.4 62.5 36.4 66.9 72.6</cell><cell>0.0</cell><cell>80.8 27.8 73.7</cell><cell>0.6</cell><cell>85.2</cell><cell>35.2 83.9 60.6 70.0 59.3 51.6</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>48.0</cell><cell cols="7">85.7 25.6 21.3 52.8 29.9 46.5 47.2</cell><cell>0.1</cell><cell>79.5 15.4 63.8</cell><cell>0.3</cell><cell>85.4</cell><cell>39.6 84.8 59.7 71.5 57.7 45.8</cell></row><row><cell>Voxel</cell><cell>MeanTeacher [26] CBST [30] CPS [13]</cell><cell>50.1 50.6 51.8</cell><cell cols="7">83.7 32.6 45.1 41.0 34.7 56.0 59.2 85.8 31.4 30.5 58.5 24.4 55.1 58.8 84.6 34.9 47.1 37.5 29.5 60.1 69.1</cell><cell>0.0 0.0 0.0</cell><cell>75.9 14.0 64.0 82.6 15.3 67.8 79.8 16.5 67.3</cell><cell>0.7 0.5 2.7</cell><cell>85.6 87.7 88.0</cell><cell>37.9 83.3 62.6 68.2 59.7 47.0 40.0 82.8 62.5 65.0 62.0 50.8 39.2 84.5 64.5 71.0 60.4 47.9</cell></row><row><cell></cell><cell>LaserMix</cell><cell>53.7</cell><cell cols="7">85.8 34.7 45.6 54.9 35.8 63.2 73.6</cell><cell>1.3</cell><cell>79.8 25.0 68.2</cell><cell>1.8</cell><cell>87.7</cell><cell>35.4 84.0 65.8 70.8 59.4 48.2</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>49.9</cell><cell cols="7">86.3 32.2 23.8 49.5 30.3 60.5 58.4</cell><cell>0.0</cell><cell>83.6 22.4 69.5</cell><cell>1.1</cell><cell>85.1</cell><cell>40.6 80.9 59.9 62.3 55.9 46.4</cell></row><row><cell>Range View</cell><cell>MeanTeacher [26] CBST [30] CutMix-Seg [29] CPS [13]</cell><cell>51.6 52.7 52.9 52.8</cell><cell cols="7">82.9 27.7 43.1 59.5 32.8 59.5 60.7 90.0 33.1 30.2 53.6 33.8 60.0 60.4 86.9 30.0 35.6 64.8 35.7 60.9 63.6 86.3 35.4 28.1 67.1 27.7 59.5 59.2</cell><cell>0.0 0.0 0.0 0.0</cell><cell>80.8 25.7 70.3 89.3 30.3 75.8 88.3 29.0 74.7 89.0 28.0 75.0</cell><cell>0.7 0.6 0.9 0.8</cell><cell>85.3 85.6 85.2 86.7</cell><cell>41.6 82.1 60.5 66.0 55.6 45.7 44.8 83.5 58.6 70.3 54.7 47.1 40.3 82.0 59.4 65.2 56.5 45.5 47.3 83.1 61.0 66.9 58.1 44.6</cell></row><row><cell>20%</cell><cell>LaserMix</cell><cell>55.6</cell><cell cols="7">87.3 36.0 34.3 69.5 40.6 66.3 70.6</cell><cell>0.0</cell><cell>84.2 27.2 72.3</cell><cell>2.4</cell><cell>86.4</cell><cell>44.6 84.1 62.8 69.8 59.4 57.7</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>52.1</cell><cell cols="7">86.9 38.0 39.5 67.3 29.7 56.5 69.9</cell><cell>0.0</cell><cell>79.0 16.0 66.0</cell><cell>0.3</cell><cell>87.0</cell><cell>38.6 84.3 60.6 66.2 58.8 45.2</cell></row><row><cell>Voxel</cell><cell>MeanTeacher [26] CBST [30] CPS [13]</cell><cell>52.8 53.3 53.9</cell><cell cols="7">85.9 27.9 41.5 55.5 33.0 64.1 72.0 86.6 36.8 40.9 72.9 28.3 58.0 69.5 85.4 37.2 44.7 58.9 32.9 63.5 71.0</cell><cell>1.2 0.0 0.0</cell><cell>81.0 22.5 67.8 81.1 18.3 68.2 81.6 23.1 69.2</cell><cell>1.2 0.7 1.9</cell><cell>89.1 88.7 88.4</cell><cell>39.9 82.9 63.7 66.9 60.5 46.7 44.3 83.6 63.3 64.4 60.3 47.5 38.2 83.8 65.7 69.2 60.2 48.9</cell></row><row><cell></cell><cell>LaserMix</cell><cell>55.1</cell><cell cols="7">88.0 38.8 51.3 54.8 36.6 60.2 73.9</cell><cell>0.0</cell><cell>78.8 22.7 71.9</cell><cell>1.5</cell><cell>90.3</cell><cell>43.3 85.3 66.5 70.9 60.3 51.6</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>52.5</cell><cell cols="7">86.7 35.9 40.2 55.9 30.1 63.2 62.9</cell><cell>0.1</cell><cell>83.6 25.3 70.8</cell><cell>1.1</cell><cell>85.1</cell><cell>40.0 82.9 60.4 69.0 56.3 48.3</cell></row><row><cell>Range View</cell><cell>MeanTeacher [26] CBST [30] CutMix-Seg [29] CPS [13]</cell><cell>53.3 54.6 54.3 54.6</cell><cell cols="7">86.9 31.9 37.5 58.6 36.3 63.3 62.0 90.1 36.0 36.6 64.7 41.6 61.2 66.7 88.1 35.3 40.0 68.8 39.3 62.4 69.8 87.1 35.0 41.0 66.1 40.8 63.2 65.5</cell><cell>0.0 0.0 0.0 0.0</cell><cell>87.6 29.5 74.1 90.4 33.8 76.8 88.0 32.0 74.3 87.9 30.0 74.6</cell><cell>1.0 3.8 0.9 1.4</cell><cell>86.4 84.5 85.1 86.1</cell><cell>40.7 82.6 61.3 68.9 58.0 47.0 44.3 83.7 57.6 70.2 48.4 47.8 38.4 82.4 59.3 67.4 56.3 44.5 42.4 82.7 60.9 67.9 57.5 48.2</cell></row><row><cell>50%</cell><cell>LaserMix</cell><cell>58.7</cell><cell cols="7">88.2 37.1 56.0 80.9 51.8 70.8 75.0</cell><cell>0.0</cell><cell>87.0 31.8 74.7</cell><cell>0.8</cell><cell>86.6</cell><cell>41.3 84.6 62.1 72.9 59.8 53.7</cell></row><row><cell></cell><cell>Sup.-only</cell><cell>53.8</cell><cell cols="7">87.5 37.2 41.3 71.4 29.6 58.8 80.4</cell><cell>0.0</cell><cell>81.1 16.7 67.5</cell><cell>0.4</cell><cell>88.4</cell><cell>39.4 83.1 64.4 65.5 61.8 47.5</cell></row><row><cell>Voxel</cell><cell>MeanTeacher [26] CBST [30] CPS [13]</cell><cell>53.9 54.5 54.8</cell><cell cols="7">86.9 33.6 46.2 48.9 33.2 62.8 77.7 87.6 39.5 36.7 65.9 35.7 62.8 78.1 85.1 35.2 45.2 68.6 32.0 65.7 77.9</cell><cell>0.0 0.0 0.2</cell><cell>82.7 22.8 68.6 82.4 20.4 69.6 81.2 21.7 69.0</cell><cell>3.2 0.1 1.6</cell><cell>89.2 88.8 89.2</cell><cell>38.6 83.8 66.4 68.0 62.3 48.5 42.3 84.2 64.0 67.4 60.1 50.1 40.2 84.5 65.1 70.1 60.9 48.5</cell></row><row><cell></cell><cell>LaserMix</cell><cell>56.8</cell><cell cols="7">88.0 40.8 51.6 63.1 38.4 61.7 79.9</cell><cell>2.0</cell><cell>83.1 26.1 71.2</cell><cell>2.8</cell><cell>90.1</cell><cell>41.7 85.9 69.5 70.5 63.0 51.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Class-wise IoU scores for different SSL algorithms on the val set of ScribbleKITTI<ref type="bibr" target="#b4">[4]</ref> (the same as SemanticKITTI<ref type="bibr" target="#b16">[16]</ref>). All IoU scores are given in percentage. The sup.-only and the best scores for each semantic class within each split are highlighted in red and blue, respectively. 12.5 67.0 83.<ref type="bibr" target="#b6">6</ref> 27.2 22.0 63.7 55.0 40.4 58.8 95.0 63.8 67.2 71.3 73.1 20.5 69.6 87.5 27.1 67.8 71.1 61.0 43.4 64.6 95.6 69.1 70.1 74.1</figDesc><table><row><cell>Method</cell><cell>Illustr.</cell><cell cols="2">mIoU barr bicy</cell><cell>bus</cell><cell>car</cell><cell cols="2">const moto ped</cell><cell>cone trail truck driv</cell><cell>othe walk</cell><cell>terr</cell><cell>manm veg</cell></row><row><cell>Baseline</cell><cell></cell><cell>60.4</cell><cell cols="8">69.0 85.6</cell><cell>84.6</cell></row><row><cell>(1?, 2?)</cell><cell></cell><cell>63.5</cell><cell cols="4">70.8 17.8 65.3 84.9 26.9</cell><cell cols="4">44.7 65.8 59.2 46.6 62.2 95.5 64.3 69.2 72.5</cell><cell>86.1</cell><cell>84.9</cell></row><row><cell>(1?, 3?)</cell><cell></cell><cell>65.2</cell><cell cols="4">72.3 21.5 67.1 85.1 26.2</cell><cell cols="4">57.1 70.4 59.3 45.8 60.7 95.6 65.4 69.3 73.7</cell><cell>87.0</cell><cell>85.9</cell></row><row><cell>(1?, 4?)</cell><cell></cell><cell>66.5</cell><cell cols="4">73.7 22.4 72.9 87.0 26.3</cell><cell cols="4">59.4 70.2 60.3 44.7 64.7 95.8 67.8 70.9 74.2</cell><cell>87.0</cell><cell>85.9</cell></row><row><cell>(1?, 5?)</cell><cell></cell><cell>66.2</cell><cell cols="8">72.8 24.1 74.0 85.7 36.3 47.8 71.5 60.8 45.8 64.5 95.7 64.8 69.9 73.3</cell><cell>87.1</cell><cell>85.9</cell></row><row><cell>(1?, 6?)</cell><cell></cell><cell>65.4</cell><cell cols="4">72.6 25.2 69.8 84.6 33.8</cell><cell cols="4">48.3 70.1 60.5 44.8 61.9 95.4 65.3 68.9 73.3</cell><cell>86.8</cell><cell>85.5</cell></row><row><cell>(2?, 1?)</cell><cell></cell><cell>61.5</cell><cell cols="4">68.4 19.1 67.1 83.5 28.1</cell><cell cols="4">26.1 64.8 57.6 41.5 59.0 95.1 64.4 68.1 72.0</cell><cell>85.4</cell><cell>84.3</cell></row><row><cell>(2?, 2?)</cell><cell></cell><cell>63.3</cell><cell cols="4">70.8 11.9 64.8 84.4 27.3</cell><cell cols="4">51.4 69.2 58.3 41.8 59.5 95.6 63.8 69.8 73.2</cell><cell>86.2</cell><cell>85.3</cell></row><row><cell>(2?, 3?)</cell><cell></cell><cell>65.9</cell><cell cols="4">71.9 24.2 69.1 86.3 28.2</cell><cell cols="4">58.5 71.5 60.1 44.7 63.4 95.7 65.3 70.1 73.4</cell><cell>86.9</cell><cell>85.9</cell></row><row><cell>(2?, 4?)</cell><cell></cell><cell>66.1</cell><cell cols="4">72.9 27.8 70.4 86.4 34.2</cell><cell cols="4">54.1 71.4 61.5 42.9 61.3 95.4 64.6 69.0 73.3</cell><cell>86.9</cell><cell>85.8</cell></row><row><cell>(2?, 5?)</cell><cell></cell><cell cols="9">66.7 87.0</cell><cell>85.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Refer to the lidarseg set in nuScenes, details at: https://www.nuscenes.org/lidar-segmentation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.nuscenes.org/nuscenes 5 https://github.com/nutonomy/nuscenes-devkit</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Class-wise IoU scores for granurality studies on the range view representation (under 10% split on the val set of nuScenes [15]). All scores are given in percentage. The best score for each semantic class is highlighted in bold</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automotive lidar technology: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Roriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Liability, ethics, and culture-aware behavior specification using rulebooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Slutsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tichakorn</forename><surname>Wongpiromsarn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Yershov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8536" to="8542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">You Hong Eng, Daniela Rus, and Marcelo H Ang. Perception, planning, control, and coordination for autonomous vehicles. Machines, 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Scott Drew Pendleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malika</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meghjani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scribble-supervised lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Unal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sqn: Weakly-supervised semantic segmentation of large-scale 3d point clouds with 1000x fewer labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangchi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04891</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we hungry for 3d lidar data for semantic segmentation? a survey of datasets and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijing</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on deep domain adaptation for lidar perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariella</forename><surname>Larissa T Triess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">B</forename><surname>Dreissig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Marius</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Z?llner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium Workshops (IVW)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="350" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09785</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with crossconsistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guided collaborative training for pixel-wise semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson Wh</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Guided point contrastive learning for semi-supervised point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rangenet++: Fast and accurate lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Polarnet: An improved grid representation for online lidar point clouds semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fidnet: Lidar point cloud semantic segmentation with fully interpolation decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki Trigoni Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Workshops</title>
		<imprint>
			<publisher>ICMLW</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation needs strong, high-dimensional perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B V K Vijaya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><forename type="middle">Erdal</forename><surname>Aksoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03653</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Polarstream: Streaming lidar object detection and segmentation with polar pillars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Point-to-voxel knowledge distillation for lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="8479" to="8488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Amvnet: Assertion-based multi-view fusion network for lidar semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi Ngoc Tho</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Widjaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjai</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang Jie</forename><surname>Chong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rpvnet: A deep and efficient range-point-voxel fusion network for lidar point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16024" to="16033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Perturbed selfdistillation: Weakly supervised large-scale point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyun</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuihua</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15520" to="15528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Box2seg: Learning semantics of 3d point clouds with box-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02963</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic segmentation of 3d lidar data in dynamic scene using semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijing</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2496" to="2509" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Less: Labelefficient semantic segmentation for lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno>2017. 9</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pseudoseg: Designing pseudo labels for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>Jia-Bin Huang, and Tomas Pfister</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Perturbed and strict mean teachers for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengbei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12903</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A simple baseline for semi-supervised semantic segmentation with strong data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8229" to="8238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation via strong-weak dual-branch network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">St++: Make self-training work better for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Panoptic nuscenes: A large-scale benchmark for lidar panoptic segmentation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Whye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juana</forename><forename type="middle">Valeria</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantic implicit neural scene representations with semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit Pal Singh</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="423" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Semi-supervised 3d shape segmentation with multilevel consistency and part substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Qi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Xiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08824</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Superpoint-guided semi-supervised semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiulei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sspc-net: Semi-supervised semantic 3d point cloud segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1140" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Mix3D: Out-of-Context Data Augmentation for 3D Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Engelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semi-supervised 3d object detection via temporal graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiming</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddarth</forename><surname>Ancha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Detmatch: Two teachers are better than one for joint 2d and 3d semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09510</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Offboard 3d object detection from point cloud sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khoa</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6134" to="6144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Topin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07120</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
