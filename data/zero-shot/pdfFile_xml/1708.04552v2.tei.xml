<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Regularization of Convolutional Neural Networks with Cutout</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Canadian Institute for Advanced Research and Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Regularization of Convolutional Neural Networks with Cutout</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well.</p><p>In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-ofthe-art results of 2.56%, 15.20%, and 1.30% test error respectively. Code available at https://github.com/ uoguelph-mlrg/Cutout.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years deep learning has contributed to considerable advances in the field of computer vision, resulting in state-of-the-art performance in many challenging vision tasks such as object recognition <ref type="bibr" target="#b7">[8]</ref>, semantic segmentation <ref type="bibr" target="#b10">[11]</ref>, image captioning <ref type="bibr" target="#b18">[19]</ref>, and human pose estimation <ref type="bibr" target="#b16">[17]</ref>. Much of these improvements can be attributed to the use of convolutional neural networks (CNNs) <ref type="bibr" target="#b8">[9]</ref>, which are capable of learning complex hierarchical feature representations of images. As the complexity of the task to be solved increases, the resource utilization of such models increases as well: memory footprint, parameters, operations count, inference time and power consumption <ref type="bibr" target="#b1">[2]</ref>. Modern networks commonly contain on the order of tens to hundreds of millions of learned parameters which provide the necessary representational power for such tasks, but with the increased representational power also comes increased probability of overfitting, leading to poor generalization.</p><p>In order to combat the potential for overfitting, several different regularization techniques can be applied, such as data augmentation or the judicious addition of noise to activations, parameters, or data. In the domain of computer vision, data augmentation is almost ubiquitous due to its ease of implementation and effectiveness. Simple image transforms such as mirroring or cropping can be applied to create new training data which can be used to improve model robustness and increase accuracy <ref type="bibr" target="#b8">[9]</ref>. Large models can also be regularized by adding noise during the training process, whether it be added to the input, weights, or gradients. One of the most common uses of noise for improving model accuracy is dropout <ref type="bibr" target="#b5">[6]</ref>, which stochastically drops neuron activations during training and as a result discourages the co-adaptation of feature detectors.</p><p>In this work we consider applying noise in a similar fashion to dropout, but with two important distinctions. The first difference is that units are dropped out only at the input layer of a CNN, rather than in the intermediate feature layers. The second difference is that we drop out contiguous sections of inputs rather than individual pixels, as demon-strated in <ref type="figure" target="#fig_0">Figure 1</ref>. In this fashion, dropped out regions are propagated through all subsequent feature maps, producing a final representation of the image which contains no trace of the removed input, other than what can be recovered by its context. This technique encourages the network to better utilize the full context of the image, rather than relying on the presence of a small set of specific visual features. This method, which we call cutout, can be interpreted as applying a spatial prior to dropout in input space, much in the same way that convolutional neural networks leverage information about spatial structure in order to improve performance over that of feed-forward networks.</p><p>In the remainder of this paper, we introduce cutout and demonstrate that masking out contiguous sections of the input to convolutional neural networks can improve model robustness and ultimately yield better model performance. We show that this simple method works in conjunction with other current state-of-the-art techniques such as residual networks and batch normalization, and can also be combined with most regularization techniques, including standard dropout and data augmentation. Additionally, cutout can be applied during data loading in parallel with the main training task, making it effectively computationally free. To evaluate this technique we conduct tests on several popular image recognition datasets, achieving state-of-the-art results on CIFAR-10, CIFAR-100, and SVHN. We also achieve competitive results on STL-10, demonstrating the usefulness of cutout for low data and higher resolution problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is most closely related to two common regularization techniques: data augmentation and dropout. Here we examine the use of both methods in the setting of training convolutional neural networks. We also discuss denoising auto-encoders and context encoders, which share some similarities with our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data Augmentation for Images</head><p>Data augmentation has long been used in practice when training convolutional neural networks. When training LeNet5 <ref type="bibr" target="#b8">[9]</ref> for optical character recognition, LeCun et al. apply various affine transforms, including horizontal and vertical translation, scaling, squeezing, and horizontal shearing to improve their model's accuracy and robustness.</p><p>In <ref type="bibr" target="#b0">[1]</ref>, Bengio et al. demonstrate that deep architectures benefit much more from data augmentation than shallow architectures. They apply a large variety of transformations to their handwritten character dataset, including local elastic deformation, motion blur, Gaussian smoothing, Gaussian noise, salt and pepper noise, pixel permutation, and adding fake scratches and other occlusions to the images, in addition to affine transformations.</p><p>To improve the performance of AlexNet <ref type="bibr" target="#b7">[8]</ref>   <ref type="bibr" target="#b20">[21]</ref> on the Ima-geNet dataset. In addition to flipping and cropping they apply a wide range of colour casting, vignetting, rotation, and lens distortion (pin cushion and barrel distortion), as well as horizontal and vertical stretching.</p><p>Lemley et al. tackle the issue of data augmentation with a learned end-to-end approach called Smart Augmentation <ref type="bibr" target="#b9">[10]</ref> instead of relying on hard-coded transformations. In this method, a neural network is trained to intelligently combine existing samples in order to generate additional data that is useful for the training process.</p><p>Of these techniques ours is closest to the occlusions applied in <ref type="bibr" target="#b0">[1]</ref>, however their occlusions generally take the form of scratches, dots, or scribbles that overlay the target character, while we use zero-masking to completely obstruct an entire region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dropout in Convolutional Neural Networks</head><p>Another common regularization technique is dropout <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>, which was first introduced by Hinton et al. Dropout is implemented by setting hidden unit activations to zero with some fixed probability during training. All activations are kept when evaluating the network, but the resulting output is scaled according to the dropout probability. This technique has the effect of approximately averaging over an exponential number of smaller sub-networks, and works well as a robust type of bagging, which discourages the co-adaptation of feature detectors within the network.</p><p>While dropout was found to be very effective at regularizing fully-connected layers, it appears to be less powerful when used with convolutional layers <ref type="bibr" target="#b15">[16]</ref>. This reduction in potency can largely be attributed to two factors. The first is that convolutional layers already have much fewer parameters than fully-connected layers, and therefore require less regularization. The second factor is that neighbouring pixels in images share much of the same information. If any of them are dropped out then the information they contain will likely still be passed on from the neighbouring pixels that are still active. For these reasons, dropout in convolutional layers simply acts to increase robustness to noisy inputs, rather than having the same model averaging effect that is observed in fully-connected layers.</p><p>In an attempt to increase the effectiveness of dropout in convolutional layers, several variations on the original dropout formula have been proposed. Tompson et al. introduce SpatialDropout <ref type="bibr" target="#b15">[16]</ref>, which randomly discards en-tire feature maps rather than individual pixels, effectively bypassing the issue of neighbouring pixels passing similar information.</p><p>Wu and Gu propose probabilistic weighted pooling <ref type="bibr" target="#b19">[20]</ref>, wherein activations in each pooling region are dropped with some probability. This approach is similar to applying dropout before each pooling layer, except that instead of scaling the output with respect to the dropout probability at test time, the output of each pooling function is selected to be the sum of the activations weighted by the dropout probability. The authors claim that this approach approximates averaging over an exponential number of sub-networks as dropout does.</p><p>In a more targeted approach, Park and Kwak introduce max-drop <ref type="bibr" target="#b12">[13]</ref>, which drops the maximal activation across feature maps or channels with some probability. While this regularization method performed better than conventional dropout on convolutional layers in some cases, they found that when used in CNNs that utilized batch normalization, both max-drop and SpatialDropout performed worse than standard dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Denoising Auto-encoders &amp; Context Encoders</head><p>Denosing auto-encoders <ref type="bibr" target="#b17">[18]</ref> and context encoders <ref type="bibr" target="#b13">[14]</ref> both rely on self-supervised learning to elicit useful feature representations of images. These models work by corrupting input images and requiring the network to reconstruct them using the remaining pixels as context to determine how best to fill in the blanks. Specifically, denoising autoencoders that apply Bernoulli noise randomly erase individual pixels in the input image, while context encoders erase larger spatial regions. In order to properly fill in the missing information, the auto-encoders are forced to learn how to extract useful features from the images, rather than simply learning an identity function. As context encoders are required to fill in a larger region of the image they are required to have a better understanding of the global content of the image, and therefore they learn higher-level features compared to denoising auto-encoders <ref type="bibr" target="#b13">[14]</ref>. These feature representations have been demonstrated to be useful for pre-training classification, detection, and semantic segmentation models.</p><p>While removing contiguous sections of the input has previously been used as an image corruption technique, like in context encoders, to our knowledge it has not previously been applied directly to the training of supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cutout</head><p>Cutout is a simple regularization technique for convolutional neural networks that involves removing contiguous sections of input images, effectively augmenting the dataset with partially occluded versions of existing samples. This technique can be interpreted as an extension of dropout in input space, but with a spatial prior applied, much in the same way that CNNs apply a spatial prior to achieve improved performance over feed-forward networks on image data.</p><p>From the comparison between dropout and cutout, we can also draw parallels to denoising autoencoders and context encoders. While both models have the same goal, context encoders are more effective at representation learning, as they force the model to understand the content of the image in a global sense, rather than a local sense as denoising auto-encoders do. In the same way, cutout forces models to take more of the full image context into consideration, rather than focusing on a few key visual features, which may not always be present.</p><p>One of the major differences between cutout and other dropout variants is that units are dropped at the input stage of the network rather than in the intermediate layers. This approach has the effect that visual features, including objects that are removed from the input image, are correspondingly removed from all subsequent feature maps. Other dropout variants generally consider each feature map individually, and as a result, features that are randomly removed from one feature map may still be present in others. These inconsistencies produce a noisy representation of the input image, thereby forcing the network to become more robust to noisy inputs. In this sense, cutout is much closer to data augmentation than dropout, as it is not creating noise, but instead generating images that appear novel to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>The main motivation for cutout comes from the problem of object occlusion, which is commonly encountered in many computer vision tasks, such as object recognition, tracking, or human pose estimation. By generating new images which simulate occluded examples, we not only better prepare the model for encounters with occlusions in the real world, but the model also learns to take more of the image context into consideration when making decisions.</p><p>We initially developed cutout as a targeted approach that specifically removed important visual features from the input of the image. This approach was similar to maxdrop <ref type="bibr" target="#b12">[13]</ref>, in that we aimed to remove maximally activated features in order to encourage the network to consider less prominent features. To accomplish this goal, we extracted and stored the maximally activated feature map for each image in the dataset at each epoch. During the next epoch we then upsampled the saved feature maps back to the input resolution, and thresholded them at the mean feature map value to obtain a binary mask, which was finally overlaid on the original image before being passed through the CNN. <ref type="figure" target="#fig_1">Figure 2</ref> demonstrates this early version of cutout.</p><p>While this targeted cutout method performed well, we found that randomly removing regions of a fixed size per- formed just as well as the targeted approach, without requiring any manipulation of the feature maps. Due to the inherent simplicity of this alternative approach, we focus on removing fixed-size regions for all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>To implement cutout, we simply apply a fixed-size zeromask to a random location of each input image during each epoch of training, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Unlike dropout and its variants, we do not apply any rescaling of weights at test time. For best performance, the dataset should be normalized about zero so that modified images will not have a large effect on the expected batch statistics.</p><p>In general, we found that the size of the cutout region is a more important hyperparameter than the shape, so for simplicity, we conduct all of our experiments using a square patch as the cutout region. When cutout is applied to an image, we randomly select a pixel coordinate within the image as a center point and then place the cutout mask around that location. This method allows for the possibility that not all parts of the cutout mask are contained within the image. Interestingly, we found that allowing portions of the patches to lay outside the borders of the image (rather than constraining the entire patch to be within the image) was critical to achieving good performance. Our explanation for this phenomenon is that it is important for the model to receive some examples where a large portion of the image is visible during training. An alternative approach that achieves similar performance is to randomly apply cutout constrained within the image region, but with 50% probability so that the network sometimes receives unmodified images.</p><p>The cutout operation can easily be applied on the CPU along with any other data augmentation steps during data loading. By implementing this operation on the CPU in parallel with the main GPU training task, we can hide the computation and obtain performance improvements for virtually free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the performance of cutout, we apply it to a variety of natural image recognition datasets: CIFAR-10, CIFAR-100, SVHN, and STL-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CIFAR-10 and CIFAR-100</head><p>Both of the CIFAR datasets <ref type="bibr" target="#b6">[7]</ref> consist of 60,000 colour images of size 32 ? 32 pixels. CIFAR-10 has 10 distinct classes, such as cat, dog, car, and boat. CIFAR-100 contains 100 classes, but requires much more fine-grained recognition compared to CIFAR-10 as some classes are very visually similar. For example, it contains five different classes of trees: maple, oak, palm, pine, and willow. Each dataset is split into a training set with 50,000 images and a test set with 10,000 images.</p><p>Both datasets were normalized using per-channel mean and standard deviation. When required, we apply the standard data augmentation scheme for these datasets <ref type="bibr" target="#b4">[5]</ref>. Images are first zero-padded with 4 pixels on each side to obtain a 40 ? 40 pixel image, then a 32 ? 32 crop is randomly extracted. Images are also randomly mirrored horizontally with 50% probability.</p><p>To evaluate cutout on the CIFAR datasets, we train models using two modern architectures: a deep residual network <ref type="bibr" target="#b4">[5]</ref> with a depth of 18 (ResNet18), and a wide residual network <ref type="bibr" target="#b21">[22]</ref> with a depth of 28, a widening factor of 10, and dropout with a drop probability of p = 0.3 in the convolutional layers (WRN-28-10). For both of these experiments, we use the same training procedure as specified in <ref type="bibr" target="#b21">[22]</ref>. That is, we train for 200 epochs with batches of 128 images using SGD, Nesterov momentum of 0.9, and weight decay of 5e-4. The learning rate is initially set to 0.1, but is scheduled to decrease by a factor of 5x after each of the 60th, 120th, and 160th epochs. We also apply cutout to shake-shake regularization models <ref type="bibr" target="#b3">[4]</ref> that currently achieve state-of-the-art performance on the CIFAR datasets, specifically a 26 2 ? 96d "Shake-Shake-Image" ResNet for CIFAR-10 and a 29 2 ? 4 ? 64d "Shake-Even-Image" ResNeXt for CIFAR-100. For our tests, we use the original code and training settings provided by the author of <ref type="bibr" target="#b3">[4]</ref>, with the only change being the addition of cutout.</p><p>To find the best parameters for cutout we isolate 10% of the training set to use as a validation set and train on the remaining images. As our cutout shape is square, we perform a grid search over the side length parameter to find the optimal size. We find that model accuracy follows a parabolic trend, increasing proportionally to the cutout size until an optimal point, after which accuracy again decreases and eventually drops below that of the baseline model. This behaviour can be observed in <ref type="figure">Figure 3a</ref> and 3b, which depict the grid searches conducted on CIFAR-10 and CIFAR-100 respectively. Based on these validation results we select a cutout size of 16 ? 16 pixels to use on CIFAR-10 and a   <ref type="table">Table 1</ref>: Test error rates (%) on CIFAR (C10, C100) and SVHN datasets. "+" indicates standard data augmentation (mirror + crop). Results averaged over five runs, with the exception of shake-shake regularization which only had three runs each. Baseline shake-shake regularization results taken from <ref type="bibr" target="#b3">[4]</ref>.</p><p>cutout size of 8 ? 8 pixels for CIFAR-100 when training on the full datasets. Interestingly, it appears that as the number of classes increases, the optimal cutout size decreases. This makes sense, as when more fine-grained detection is required then the context of the image will be less useful for identifying the category. Instead, smaller and more nuanced details are important. <ref type="table">Table 1</ref>, the addition of cutout to the ResNet18 and WRN-28-10 models increased their accuracy on CIFAR-10 and CIFAR-100 by between 0.4 to 2.0 percentage points. We draw attention to the fact that cutout yields these performance improvements even when applied to complex models that already utilize batch normalization, dropout, and data augmentation. Adding cutout to the current state-of-the-art shake-shake regularization models improves performance by 0.3 and 0.6 percentage points on CIFAR-10 and CIFAR-100 respectively, yielding new stateof-the-art results of 2.56% and 15.20% test error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">SVHN</head><p>The Street View House Numbers (SVHN) dataset <ref type="bibr" target="#b11">[12]</ref> contains a total of 630,420 colour images with a resolution of 32 ? 32 pixels. Each image is centered about a number from one to ten, which needs to be identified. The official dataset split contains 73,257 training images and 26,032 test images, but there are also 531,131 additional training images available. Following standard procedure for this dataset <ref type="bibr" target="#b21">[22]</ref>, we use both available training sets when training our models, and do not apply any data augmentation. All images are normalized using per-channel mean and standard deviation.</p><p>To evalute cutout on the SVHN dataset we apply it to a WideResNet with a depth of 16, a widening factor of 8, and dropout on the convolutional layers with a dropout rate of p = 0.4 (WRN- . This particular configuration currently holds state-of-the-art performance on the SVHN dataset with a test error of 1.54% <ref type="bibr" target="#b21">[22]</ref>. We repeat the same training procedure as specified in <ref type="bibr" target="#b21">[22]</ref> by training for 160 epochs with batches of 128 images. The network is optimized using SGD with Nesterov momentum of 0.9 and weight decay of 5e-4. The learning rate is initially set to 0.01, but is reduced by a factor of 10x after the 80th and 120th epochs. The one change we do make to the original training procedure (for both baseline and cutout) is to normalize the data so that it is compatible with cutout (see ? 3.2). The original implementation scales data to lie between 0 and 1.</p><p>To find the optimal size for the cutout region we conduct a grid search using 10% of the training set for validation and ultimately select a cutout size of 20 ? 20 pixels. While this may seem like a large portion of the image to remove, it is important to remember that the cutout patches are not constrained to lie fully within the bounds of the image.</p><p>Using these settings we train the WRN-16-8 and observe an average reduction in test error of 0.3 percentage points, resulting in a new state-of-the-art performance of 1.30% test error, as shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">STL-10</head><p>The STL-10 dataset <ref type="bibr" target="#b2">[3]</ref> consists of a total of 113,000 colour images with a resolution of 96 ? 96 pixels. The training set only contains 5,000 images while the test set consists of 8,000 images. All training and test set images belong to one of ten classes, such as airplane, bird, or horse. The remainder of the dataset is composed of 100,000 unlabeled images belonging to the target ten classes, plus additional but visually similar classes. While the main purpose of the STL-10 dataset is to test semi-supervised learning algorithms, we use it to observe how cutout performs when applied to higher resolution images in a low data setting. For this reason, we discard the unlabeled portion of the dataset and only use the labeled training set.</p><p>The dataset was normalized by subtracting the perchannel mean and dividing by the per-channel standard deviation. Simple data augmentation was also applied in a similar fashion to the CIFAR datasets. Specifically, images were zero-padded with 12 pixels on each side and then a 96 ? 96 crop was randomly extracted. Mirroring horizontally was also applied with 50% probability.</p><p>To evaluate the performance of cutout on the STL-10 dataset we use a WideResNet with a depth of 16, a widening factor of 8, and dropout with a drop rate of p = 0.3 in the convolutional layers. We train the model for 1000 epochs with batches of 128 images using SGD with Nesterov momentum of 0.9 and weight decay of 5e-4. The learning rate is initially set to 0.1 but is reduced by a factor of 5x after the 300th, 400th, 600th, and 800th epochs.</p><p>We perform a grid search over the cutout size parameter using 10% of the training images as a validation set and select a square size of 24 ? 24 pixels for the no dataaugmentation case and 32 ? 32 pixels for training STL-10 with data augmentation. Training the model using these values yields a reduction in test error of 2.7 percentage points in the no data augmentation case, and 1.5 percentage points when also using data augmentation, as shown in <ref type="table">Table 2</ref>.</p><formula xml:id="formula_0">Model STL10 STL10+ WideResNet</formula><p>23.48 ? 0.68 14.21 ? 0.29 WideResNet + cutout 20.77 ? 0.38 12.74 ? 0.23 <ref type="table">Table 2</ref>: Test error rates on STL-10 dataset. "+" indicates standard data augmentation (mirror + crop). Results averaged over five runs on full training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis of Cutout's Effect on Activations</head><p>In order to better understand the effect of cutout, we compare the average magnitude of feature activations in a ResNet18 when trained with and without cutout on CIFAR-10. The models were trained with data augmentation using the same settings as defined in Section 4.1, achieving scores of 3.89% and 4.94% test error respectively.</p><p>In <ref type="figure" target="#fig_2">Figure 4</ref>, we sort the activations within each layer by ascending magnitude, averaged over all samples in the test set. We observe that the shallow layers of the network experience a general increase in activation strength, while in deeper layers, we see more activations in the tail end of the distribution. The latter observation illustrates that cutout is indeed encouraging the network to take into account a wider variety of features when making predictions, rather than relying on the presence of a smaller number of features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Cutout was originally conceived as a targeted method for removing visual features with high activations in later layers of a CNN. Our motivation was to encourage the network to focus more on complimentary and less prominent features, in order to generalize to situations like occlusion. However, we discovered that the conceptually and computationally simpler approach of randomly masking square sections of the image performed equivalently in the experiments we conducted. Importantly, this simple regularizer proved to be complementary to existing forms of data augmentation and regularization. Applied to modern architectures, such as wide residual networks or shake-shake regularization models, it achieves state-of-the-art performance on the CIFAR-10, CIFAR-100, and SVHN vision benchmarks. So why hasn't it been reported or analyzed to date? One reason could be the fact that using a combination of corrupted and clean images appears to be important for its success. Future work will return to our original investigation of visual feature removal informed by activations. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Cutout applied to images from the CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An early version of cutout applied to images from the CIFAR-10 dataset. This targeted approach often occludes part-level features of the image, such as heads, legs, or wheels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 5 demonstrates similar observations for individual samples, where the effects of cutout are more pronounced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Magnitude of feature activations, sorted by descending value, and averaged over all test samples. A standard ResNet18 is compared with a ResNet18 trained with cutout at three different depths. Magnitude of feature activations, sorted by descending value. Each row represents a different test sample. A standard ResNet18 is compared with a ResNet18 trained with cutout at three different depths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>for the 2012 ImageNet Large Scale Visual Recognition Competition, Krizhevsky et al. apply image mirroring, cropping, as well as randomly adjusting colour and intensity values based on ranges determined using principal component analysis on the dataset. Wu et al. take a more aggressive approach with image augmentation when training Deep Image</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? 0.08 3.08 ? 0.16 23.94 ? 0.15 18.41 ? 0.27 1.30 ? 0.03</figDesc><table><row><cell></cell><cell>97.0 97.2</cell><cell></cell><cell></cell><cell>Cutout Baseline</cell><cell></cell><cell>81.5</cell><cell></cell><cell></cell><cell></cell><cell>Cutout Baseline</cell></row><row><cell></cell><cell>96.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>81.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Validation Accuracy (%)</cell><cell>96.0 96.2 96.4 96.6</cell><cell></cell><cell></cell><cell></cell><cell>Validation Accuracy (%)</cell><cell>79.5 80.0 80.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>95.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>95.6</cell><cell>8</cell><cell>12 Patch Length (pixels) 16</cell><cell>20</cell><cell>24</cell><cell>79.0</cell><cell>4</cell><cell>8</cell><cell>12 Patch Length (pixels)</cell><cell>16</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a) CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) CIFAR-100</cell><cell></cell></row><row><cell cols="12">Figure 3: Cutout patch length with respect to validation accuracy with 95% confidence intervals (average of five runs). Tests</cell></row><row><cell cols="12">run on CIFAR-10 and CIFAR-100 datasets using WRN-28-10 and standard data augmentation. Baseline indicates a model</cell></row><row><cell cols="3">trained with no cutout.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Method</cell><cell></cell><cell cols="2">C10</cell><cell cols="2">C10+</cell><cell></cell><cell>C100</cell><cell>C100+</cell><cell>SVHN</cell></row><row><cell cols="2">ResNet18 [5]</cell><cell></cell><cell cols="5">10.63 ? 0.26 4.72 ? 0.21</cell><cell>36.68 ? 0.57</cell><cell>22.46 ? 0.31</cell><cell>-</cell></row><row><cell cols="3">ResNet18 + cutout</cell><cell cols="2">9.31 ? 0.18</cell><cell cols="3">3.99 ? 0.13</cell><cell>34.98 ? 0.29</cell><cell>21.96 ? 0.24</cell><cell>-</cell></row><row><cell cols="3">WideResNet [22]</cell><cell cols="2">6.97 ? 0.22</cell><cell cols="3">3.87 ? 0.08</cell><cell>26.06 ? 0.22</cell><cell>18.8 ? 0.08</cell><cell cols="2">1.60 ? 0.05</cell></row><row><cell cols="4">WideResNet + cutout 5.54 Shake-shake regularization [4]</cell><cell>-</cell><cell>2.86</cell><cell></cell><cell></cell><cell>-</cell><cell>15.85</cell><cell>-</cell></row><row><cell cols="4">Shake-shake regularization + cutout</cell><cell>-</cell><cell cols="3">2.56 ? 0.07</cell><cell>-</cell><cell>15.20 ? 0.21</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Daniel Jiwoong Im for feedback on the paper and for suggesting the analysis in ? 4.4. The authors also thank NVIDIA for the donation of a Titan X GPU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learners benefit more from out-of-distribution examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chherawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="164" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An analysis of deep neural network models for practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits &amp; Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Smart augmentation-learning an optimal data augmentation strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lemley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bazrafkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Corcoran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysis on the dropout effect in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="189" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards dropout training for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02876</idno>
		<title level="m">Deep image: Scaling up image recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
