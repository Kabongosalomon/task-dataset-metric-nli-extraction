<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InverseForm: A Loss Function for Structured Boundary-Aware Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhankar</forename><surname>Borse</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
							<email>yingwang0022@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><forename type="middle">Porikli</forename><surname>Qualcomm</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">InverseForm: A Loss Function for Structured Boundary-Aware Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel boundary-aware loss term for semantic segmentation using an inverse-transformation network, which efficiently learns the degree of parametric transformations between estimated and target boundaries. This plug-in loss term complements the cross-entropy loss in capturing boundary transformations and allows consistent and significant performance improvement on segmentation backbone models without increasing their size and computational complexity. We analyze the quantitative and qualitative effects of our loss function on three indoor and outdoor segmentation benchmarks, including Cityscapes, NYU-Depth-v2, and PASCAL, integrating it into the training phase of several backbone networks in both single-task and multi-task settings. Our extensive experiments show that the proposed method consistently outperforms baselines, and even sets the new state-of-the-art on two datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a fundamental computer vision task with numerous real-world applications such as autonomous driving, medical image analysis, 3D reconstruction, AR/VR and visual surveillance. It aims to perform pixel-level labeling with a given set of target categories.</p><p>There have been notable advancements in semantic segmentation thanks to recent solutions based on deep learning models, such as the end-to-end fully convolutional networks (FCN) <ref type="bibr" target="#b29">[30]</ref> that lead to significant performance gains in popular benchmarks. Extensive studies have been conducted to improve semantic segmentation. One prevailing direction is to integrate multi-resolution and hierarchical feature maps <ref type="bibr" target="#b39">[40]</ref> <ref type="bibr" target="#b37">[38]</ref>. Another ambitious objective is to exploit boundary information to enhance segmentation <ref type="bibr" target="#b49">[50]</ref> <ref type="bibr" target="#b38">[39]</ref> further, driven by the observations that segmentation prediction errors are more likely to occur near the boundaries <ref type="bibr" target="#b38">[39]</ref>  <ref type="bibr" target="#b21">[22]</ref>. In parallel, multi-task learning (MTL) * Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Baseline Baseline w/ ours <ref type="figure">Figure 1</ref>: Left: Images from Cityscapes val benchmark <ref type="bibr" target="#b8">[9]</ref>. Middle: Segmented prediction for an HRNet-48-OCR <ref type="bibr" target="#b44">[45]</ref> [49] baseline. Right: Same backbone trained using our In-verseForm boundary loss. Our model achieves clear improvements, e.g. the curbside boundary in the top figure is aligned better with the structure of boundary, and the curbside in the bottom is correctly detected.</p><p>frameworks <ref type="bibr" target="#b13">[14]</ref>[31] explored joint optimization of semantic segmentation and supplemental tasks such as boundary detection <ref type="bibr" target="#b13">[14]</ref> [44] <ref type="bibr" target="#b30">[31]</ref> or depth estimation <ref type="bibr" target="#b47">[48]</ref>. Our approach is aligned with making the best use of boundary exploration. One of the main differences is that most of the previous works use a weighted cross-entropy loss as their loss function for boundary detection, which we show in <ref type="figure">Figure 2</ref>, is sub-optimal for measuring the boundary shifts. This is also partially observed in <ref type="bibr" target="#b22">[23]</ref>. The crossentropy loss mainly builds on estimated and ground-truth pixel label changes but ignores the spatial distance of the pixels from the target boundaries. It cannot effectively measure localized spatial variations such as translation, rotation, or scaling between predicted and target boundaries.</p><p>Motivated to address this limitation, we introduce a boundary distance-based measure called InverseForm, into the popular segmentation loss functions. We design an inverse transformation network to model the distance between boundary maps, which can efficiently learn the degree of parametric transformations between local spatial regions. This measure allows us to achieve a significant and consistent improvement in semantic segmentation using any backbone model without increasing the inference size and computational complexity of the network.</p><p>Specifically, we propose a boundary-aware segmentation scheme during the training phase, by integrating our spatial-distance loss, InverseForm, into the existing pixelbased loss. Our distance-based loss complements the pixelbased loss in capturing boundary transformations. We utilize our inverse transformation network for distance measurement from boundaries and jointly optimize pixel-label accuracy and boundary distance. We can integrate our proposed scheme into any segmentation model; for instance, we adopt the latest HRNet <ref type="bibr" target="#b44">[45]</ref> architecture as one of the backbones since it maintains high resolution feature maps.</p><p>We also adopt various MTL frameworks <ref type="bibr" target="#b30">[31]</ref> [44] to leverage on their boundary detection task towards further segmentation performance improvement, at no added computational and memory cost. In this variant, we show a consistent performance improvement across all tasks as a plus.</p><p>We conduct comprehensive experiments on large benchmark datasets including NYU-Depth-v2 <ref type="bibr" target="#b36">[37]</ref>, Cityscapes <ref type="bibr" target="#b8">[9]</ref> and PASCAL-Context <ref type="bibr" target="#b14">[15]</ref>. For NYU-Depth-v2, we show that InverseForm based segmentation outperforms the state-of-art in terms of mean intersectionover-union (mIoU). We also show that we outperform state-of-the-art Multi-task learning methods on PAS-CAL in terms of their multi-task performance. This includes superior performance in predicted edge quality on odsF-score <ref type="bibr" target="#b31">[32]</ref> and improvement in other tasks, such as human-parts estimation and saliency estimation, on mIoU. Then, we perform rigorous experiments on Cityscapes to compare our method with contemporary works such as SegFix <ref type="bibr" target="#b49">[50]</ref> and Gated-SCNN <ref type="bibr" target="#b38">[39]</ref>.</p><p>The contributions of our work include the following:</p><p>? We propose a boundary distance-based measure, In-verseForm, to improve semantic segmentation. We show that our specifically tailored measure is significantly more capable of capturing the spatial boundary transforms than cross-entropy based measures, thus resulting in more accurate segmentation results.</p><p>? Our scheme is agnostic to the backbone architecture choice and is very flexible to be plugged into any existing segmentation model, with no additional inference cost. It does not impact the main structure of the network due to its plug-and-play property. It is flexible and can fit into multi-task learning frameworks.</p><p>? We show through extensive experiments that our boundary-aware-segmentation method consistently outperforms its baselines, and also outperforms the state-of-the-art methods in both single-task (on NYU-Depth-v2) and multi-task settings (on PASCAL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic segmentation: The introduction of fully convolutional networks (FCNs) <ref type="bibr" target="#b29">[30]</ref> led to significant progress in semantic segmentation. Many works build on FCNs to improve segmentation performance. The work in <ref type="bibr" target="#b25">[26]</ref> proposed an approximate inference algorithm for conditional random fields (CRFs) models to improve segmentation efficiency. Various architectures have been proposed to improve inference speed, such as DeepLab <ref type="bibr" target="#b4">[5]</ref>, PSPNet <ref type="bibr" target="#b53">[54]</ref>, and HRNet <ref type="bibr" target="#b44">[45]</ref>. Numerous recent works utilize HRNet by integrating diverse contextual models <ref type="bibr" target="#b48">[49]</ref> [50] <ref type="bibr" target="#b38">[39]</ref>.</p><p>Boundary-aware segmentation: Boundary-aware segmentation has been studied in quite a few recent efforts. There are several ways of modeling this problem. In <ref type="bibr" target="#b0">[1]</ref>, a global energy model called boundary neural field (BNF) is introduced to predict boundary cues to enhance semantic segmentation. The authors show that the energy decomposes semantic segmentation into a set of binary problems which can be relaxed and solved by global optimization. In <ref type="bibr" target="#b9">[10]</ref> a boundary-aware feature propagation module (BFP) is proposed to propagate local features isolated by the boundaries learned in the UAG-structured image. The work in <ref type="bibr" target="#b28">[29]</ref> learns semantically-aware affinity through a spatial propagation network to refine image segmentation boundaries. Similarly, <ref type="bibr" target="#b2">[3]</ref> adopts a fast domain transform filtering on the output and generates edge maps to refine segmentation output. Gated-SCNN <ref type="bibr" target="#b38">[39]</ref> injects the learned boundary into intermediate layers to improve segmentation. This is done using gating to allow interaction between the main segmentation stream and the shape stream. However, these boundaries are learnt using a pixel-based loss. We show in Section 4 how this loss benefits from using our proposed spatial loss. Other related works include <ref type="bibr" target="#b21">[22]</ref> [11] <ref type="bibr" target="#b11">[12]</ref> [28] [7] <ref type="bibr" target="#b33">[34]</ref>, and <ref type="bibr" target="#b22">[23]</ref>.</p><p>One of the main differences between our work and all the previous works is that most of the previous works use weighted cross-entropy as their loss function for boundary detection, which is sub-optimal for measuring the boundary changes. The cross-entropy loss mainly considers the pixel label but ignores the distance between the boundaries. It cannot effectively measure the shifts, scales, rotations, and other spatial changes between two image regions. In our work, we introduce a distance-based metric into this loss function. Specifically, we employ an inverse-transformation network to model the spatial distance between boundary <ref type="figure">Figure 2</ref>: Cross-entropy(XE) based distance fails for spatial transformations of boundaries. In the above example, x 1 and x 2 are slices sampled from a boundary map. Only a mild shift is applied to x 2 to generate a third slice t(x 2 ). maps, which is shown to significantly improve the capability in capturing such boundary transforms. An interesting recent work which makes use of spatial relations is Seg-Fix <ref type="bibr" target="#b49">[50]</ref>. It learns discrete distance-transformation maps to incorporate spatial relations. Instead of learning these offset maps, our work regresses over homography parameters over which it uses a derived distance computation. We show both qualitatively and quantitatively that this is more effective in Section 4. Another major difference between our work and all previous mentioned works is that our proposed method requires no extra cost during inference.</p><p>Multi-task learning: Multi-task learning (MTL) <ref type="bibr" target="#b23">[24]</ref> learns shared representations from multi-task supervisory signals under a unified framework. It is effective at exploring interactions among multiple tasks while saving memory and computation. <ref type="bibr" target="#b43">[44]</ref> shows the superior performance of multi-task learning for semantic segmentation and depth estimation for NYU-Depth-v2. <ref type="bibr" target="#b42">[43]</ref> provides a comprehensive overview of multi-task learning techniques. In [31] the authors address the task interference issue by performing one task at a time using the multi-task framework. They allow the network to adapt its behavior through task attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Scheme: InverseForm</head><p>Below, we explain our method for boundary-aware segmentation using a spatial distance-based loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation for distance-based metrics</head><p>Boundary-aware segmentation approaches use pixelwise cross-entropy or balanced cross-entropy losses. Such loss functions take into account the pixel-wise features (intensity, etc.) but not spatial distance between object boundaries and ground-truth boundaries. Hence, they are insufficient for imposing boundary alignment for segmentation.</p><p>This drawback is illustrated in <ref type="figure">Figure 2</ref>. We pick an image from NYU-Depth-v2 and convert the ground-truth segmentation into a boundary map. We then sample two regions x 1 and x 2 from the map. We apply a spatial translation by 3 pixels in both dimensions to x 2 . The cross-entropy(XE) between pairs of these images is also shown. Notice that the actual difference between the region x 2 and its transformed version t(x 2 ) is not high, as the boundary maps are shifted by only a few pixels. However, crossentropy between this pair of images is much higher than the cross-entropy between x 1 and x 2 , which have no correlation. Ideally, if x 2 is the ground truth map; a network should be penalized more when it generates x 1 as compared to when it generates t(x 2 ). This is clearly not the case. Pixel-based losses are thus, not enough to measure 'distance' between such inputs. To counter the effect of sensitivity to small displacements, boundary detection networks trained with pixel-based losses produce thicker and distorted boundaries. Hence, there is an existing need to measure a distance function between two boundary maps which models possible spatial transformations between them.</p><p>Some works use Hausdorff distance <ref type="bibr" target="#b35">[36]</ref> to model this measure between boundaries for instance-level segmentation, but this loss cannot be efficiently applied in a semantic segmentation setting. The correlation operation could also be considered between two images, but this would only model translations between image boundaries and cannot model other transformations. Thus, we need a loss function that accurately models the distance between two images of object boundaries and can be computed efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inverse transformation network</head><p>To model spatial relations between two boundary maps, we assume that they are related to each other through a homography transformation. Thus, we create a network that inputs two boundary maps and predicts the "homography change" as its output. Theoretically, this network performs the inverse operation of a spatial transformer network (STN) <ref type="bibr" target="#b20">[21]</ref>. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>(a), STN takes an image x as its input, generates a controlled transformation matrix ?, and produces a realistic transformed boundary image t ? (x). <ref type="figure" target="#fig_1">Figure 3</ref>(b) shows our proposed network. Inputs to our network are two boundary maps. The network regresses on the transformation parameters between these maps as its output. Thus, we call it an inverse transformation network. Some recent works <ref type="bibr" target="#b26">[27]</ref> [52] attempted to train a similar network to use its encoder for an unsupervised learning task. Our architecture and application are completely different; we use the network to model a spatial distance measure, and our model has only dense layers. We compare our inverse transformation network's performance with <ref type="bibr" target="#b26">[27]</ref> in Section 5, where we discuss the translation-invariance of convolutions and why we selected a dense architecture.</p><p>The outputs of the inverse-transformation network are the coefficients of the homography matrix. There are numerous methods to formulate a distance metric from these values. Below, we present two distance metric choices.</p><p>We note that one may also attempt to directly regress on  the distance instead of estimating the transformation coefficients. However, such an approach would not allow optimization of the boundary-aware segmentation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Measuring distances from homography</head><p>Assuming two boundary maps are related to each other with a homography transformation, our inversetransformation net inputs two boundary maps and estimates the transformation matrix parameters. Once our inversetransformation network is trained, we freeze its weights to generate our proposed InverseForm loss. If there is a perfect match between input boundary maps, the network should estimate an identity matrix. Thus, a measure of spatial distance between the boundary maps can be calculated by comparing the network outputs to an identity matrix.</p><p>Here are two distance measures we propose to relate the two matrices: Euclidean distance and Geodesic distance.</p><p>Euclidean distance: The Euclidean distance between two homography matrices is a measure of spatial distance between the two input boundary maps. We observed that Euclidean distance could model shift and scale relations well. However, it fails to reflect rotations and other perspective transformations. Considering the output of the network as?, the 3 ? 3 identity matrix is I 3 and ? F is the Frobenius norm, the InverseForm distance is calculated as</p><formula xml:id="formula_0">d if (x, t ? (x)) = ? ? I 3 F .<label>(1)</label></formula><p>We train the inverse-transformation network by reducing the Euclidean distance between predicted and ground-truth parameters, and use Equation (1) at inference time.</p><p>Geodesic distance: Homography transformations reside on an analytical manifold instead of a flat Euclidean space. The geodesic distance can capture these transformations, as explained in the recent works <ref type="bibr" target="#b26">[27]</ref>  <ref type="bibr" target="#b41">[42]</ref>. Considering the ground-truth parameters as ?, we use the formulation of geodesic distance from both these works, as follows</p><formula xml:id="formula_1">d if (x, t ? (x)) = Log(? ?1? ) Log(I 3 ) F .<label>(2)</label></formula><p>To train the network using this formulation of geodesic distance, we are required to calculate gradient errors over the Riemannian logarithm, which does not have a closed-form solution. Thus, we use the method in <ref type="bibr" target="#b26">[27]</ref> to project the homography Lie group onto a subgroup SO(3) <ref type="bibr" target="#b40">[41]</ref> where the calculation of geodesic distance does not need the Riemannian logarithm. The formulation is given by</p><formula xml:id="formula_2">d if (x, t ? (x)) = arccos Tr (P ) ? 1 2 + ? Tr (R T ? R ? ),<label>(3)</label></formula><p>where ? is a weighting parameter set at 0.1 and the projection P onto the rotation group SO(3) is given as P = U diag{1, 1, det(U V ) T }V T and the projection residual R ? is calculated as</p><formula xml:id="formula_3">R ? = ? ?1? ? P .<label>(4)</label></formula><p>This formulation of geodesic distance can be used to train the inverse-transformation network. During inference, we insert ? = I 3 in Equation <ref type="formula" target="#formula_3">(4)</ref> to compute the distance between the two inputs to the inverse-transformation network. For using geodesic distance, there are 8 degrees of freedom in the 3 ? 3 homography matrix. The inversetransformation network predicts 8 values and we set the 9th value to 1. On the other hand, the matrix has 6 degrees of freedom if we assume only 2D affine transformations. Hence, the network must predict 6 values if we use the Euclidean distance measure. We have compared the effect of using these distance measures in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Using InverseForm as a loss function</head><p>We first train the inverse-transformation network using boundary maps of images sampled from the target dataset. We apply the STN <ref type="bibr" target="#b20">[21]</ref> to generate the transformed versions of boundary images. We could also do this by randomly sampling transformation parameters using the method described in <ref type="bibr" target="#b26">[27]</ref>, however this leads to lesser realistic transformations. Then, we use these images and their affine transformations as inputs to the inverse-transformation network. Before feeding boundary maps to the network, we split the images into smaller tiles. Tiling is done in a way that the local context can be preserved along with the global context. Ideally, the best tiling dimension should provide a balance between local and global contexts. We discuss the effect of tiling dimension in the Appendix. Training details and hyperparameters used can be found in the Appendix.</p><p>Once we train the inverse-transformation network, we freeze its weights and use it as a loss function in a boundaryaware segmentation setting. Using networks as loss functions is a common practice in generative adversarial network literature <ref type="bibr" target="#b15">[16]</ref>. However, the distance computed based on inverse-transformation network outputs doesn't fall in the category of discriminator or adversarial losses, according to the definition given by <ref type="bibr" target="#b12">[13]</ref>.</p><p>To model the loss function, we assume the predicted boundary b pred is a transformed version of the ground truth boundary label b gt . i.e. b pred = t ? (b gt ). Hence, we can formulate our InverseForm loss function in terms of the spatial distance calculated in Section 3.5. This function first splits the input boundaries b pred and b gt into N smaller tiles b pred,j and b gt,j . Next, it passes the inputs through the inverse-transformer network and calculates spatial distance in one of the ways described in Section 3.2. The formulation of our InverseForm loss function L if is given by</p><formula xml:id="formula_4">L if (b pred , b gt ) = N j=1 d if (b pred,j , b gt,j ) .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Boundary-aware segmentation setup</head><p>We train various architectures using our loss function, with both single-task and multi-task settings in Section 4. For multi-task settings, we directly use the architecture used by MTI-Nets <ref type="bibr" target="#b43">[44]</ref> and ASTMT <ref type="bibr" target="#b30">[31]</ref> and add our Inverse-Form loss term to the boundary loss.</p><p>To train single-task architectures using InverseForm loss, we use a simple boundary-aware segmentation setup, as dis-played in <ref type="figure" target="#fig_2">Figure 4</ref>. This setup could be used over any backbone. Consider x is the input image, y gt is the groundtruth segmentation map and y pred is the network prediction. The network produces intermediate features f pred , which are fed into a segmentation backbone to produce the output segmentation. We pass the features f pred into an auxiliary head to produce a boundary map b pred . Optionally, this boundary map could be concatenated with the features f pred and then fed to the segmentation head to produce the output y pred . We don't add this supervision in our experiments to ensure zero computational overhead.</p><p>Pixel-wise cross-entropy loss L xe is used on the segmentation output. The ground-truth y gt is passed through a Sobel filter <ref type="bibr" target="#b1">[2]</ref> to produce a binary boundary map. We integrate the InverseForm loss and weighted cross-entropy loss L bxe over the predicted boundary and ground-truth maps into the overall loss function. Specifically, we define the overall loss function as</p><formula xml:id="formula_5">L total = L xe (y pred , y gt ) + ?L bxe (b pred , b gt ) + ?L if (b pred , b gt )<label>(6)</label></formula><p>where L xe is the cross-entropy loss, L bxe is weighted crossentropy loss, and L if is the InverseForm loss defined in <ref type="bibr" target="#b4">(5)</ref>. ?, ? are the scaling parameters for the weighted crossentropy loss and InverseForm loss terms, respectively. They are treated as hyperparameters and we discuss the effects of these terms in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>Datasets: We present performance scores for various tasks on three datasets, NYU-Depth-v2 <ref type="bibr" target="#b36">[37]</ref>, PASCAL <ref type="bibr" target="#b14">[15]</ref> and Cityscapes <ref type="bibr" target="#b8">[9]</ref>. We use the original 795 training and 654 testing images for NYU-Depth-v2. For PASCAL, we use a split from PASCAL-Context, which has annotations for segmentation, edge-detection, and human parts. We also use la-  <ref type="table">Table 1</ref>: Comparing baselines for NYU-Depth-v2 using HRNet-w18 and HRNet-w48 backbones in both single task and multi-task learning using our loss. In multi-task settings, S:Semantic segmentation, D:Depth, E:Edge detection, N:Surface normal estimation. Consistent improvement in segmentation mIoU and boundary mBA is visible.</p><p>bels for surface normals and saliency from <ref type="bibr" target="#b30">[31]</ref>. This split for PASCAL is used by ASTMT <ref type="bibr" target="#b30">[31]</ref> and MTI-Nets <ref type="bibr" target="#b43">[44]</ref> for a multi-task framework. For Cityscapes, we use their 2975/500/1525 train/val/test splits to report performance. Models reporting on test set are trained using train+val set.</p><p>Evaluation metrics: For single-task learning settings, we measure semantic segmentation performance. This is evaluated using mean intersection-over-union (mIoU) and pixelaccuracy. To show improvement on boundary regions, we use our implementation of multi-class Mean Boundary Accuracy(mBA) from <ref type="bibr" target="#b7">[8]</ref>. This is computed over segmentation outputs. For multi-task learning settings on PASCAL, the model is trained for five tasks: semantic segmentation, boundary detection, saliency estimation, human parts and surface normal estimation. We measure mIoU for semantic segmentation, human parts, and saliency estimation. The mean error in predicted angles is calculated for surface normal, and the optimal dataset F-measure is used to evaluate boundary detection. We measure depth and semantic segmentation for NYU-Depth-v2. Depth estimation is measured using root mean square error (rmse). Finally, the multi-task performance of model m, ? m [31] is given by the relative increase wrt. single-task performance for all tasks:</p><formula xml:id="formula_6">? m = 1 T T i=1 (?1) li (Mm,i?M b,i ) M b,i</formula><p>. Here, M m,i and M b,i are metrics for the multi-task model and single-task model for task i. l i is 1 when the metric is positive v/s 0 when the metric is negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on NYU-Depth-v2</head><p>For NYU-Depth-v2, we report scores using three backbone architectures, i.e. HRNet-w18, HRNet-w48 (adapted from <ref type="bibr" target="#b45">[46]</ref>) and ResNet-101 <ref type="bibr" target="#b16">[17]</ref>. For the vanilla HRNet-w18 and HRNet-w48 backbones, we train with Inverse-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Backbone mIoU Pixel-acc TD-Net <ref type="bibr" target="#b18">[19]</ref> PSPNet-50 43.5 55.2 PAD-Net <ref type="bibr" target="#b47">[48]</ref> ResNet-50 50.2 75.2 PAP-Net <ref type="bibr" target="#b52">[53]</ref> ResNet-50 50.4 76.2 MTI-Net <ref type="bibr" target="#b43">[44]</ref> HRNet-w48 49.0 -RSP <ref type="bibr" target="#b24">[25]</ref> ResNet-152 46.5 73.6 ACNet <ref type="bibr" target="#b19">[20]</ref> ResNet-50 48.3 -Malleable 2.5D <ref type="bibr" target="#b46">[47]</ref> ResNet-101 50.9 76.9 SA-Gate <ref type="bibr" target="#b34">[35]</ref> ResNet-101 52.4 77.9 Ours</p><p>ResNet-101 53.1 78.1  Form loss using the method described in <ref type="figure" target="#fig_2">Figure 4</ref>. We also add the InverseForm loss term to the boundary loss function within available multi-task baselines as implemented by MTI-Net <ref type="bibr" target="#b43">[44]</ref> over HRNet-18. We used their implementation of PAD-Net <ref type="bibr" target="#b47">[48]</ref> in multi-task settings. These results are given in <ref type="table">Table 1</ref>. For computational analysis, we plot the no. of parameters v/s mIoU and GFLOPs v/s mIoU curves in <ref type="figure" target="#fig_3">Figure 5</ref>. We remove the boundary detection head before running inference. Hence, our method increases the mIoU score without any extra computations during inference.</p><p>The best scores reported on NYU-Depth-v2 are achieved by the multi-scale inference on RGBD inputs using SA-Gates <ref type="bibr" target="#b34">[35]</ref> over ResNet-101 backbone and DeepLab-V3+ <ref type="bibr" target="#b4">[5]</ref> as the decoder. We use their implementation and train this model with our InverseForm loss and the boundary-aware framework shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Our model outperforms the SA-Gates baseline by 0.7% mIoU and other state-of-the-art models by a large margin on NYU-Depth-v2, as shown in <ref type="table" target="#tab_1">Table 2</ref>. We provide the hyperparameters for our experiments and the multi-task results for this experiment in the Appendix.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on PASCAL</head><p>For the PASCAL dataset, we report scores comparing with the state-of-the-art multi-task learning results from ASTMT <ref type="bibr" target="#b30">[31]</ref> and MTI-Net <ref type="bibr" target="#b43">[44]</ref>. As implemented by ASTMT, we use a base architecture of Deeplab-V3+ <ref type="bibr" target="#b5">[6]</ref> with a ResNet encoder and squeeze-and-excite (SE) <ref type="bibr" target="#b17">[18]</ref> blocks. We train this network both with and without the In-verseForm loss function over the boundary detection task. We also add our loss term over boundary detection task to train the HRNet-w18 backbone from MTI-Net. On this benchmark, we observe consistent improvement in performance by training with our InverseForm loss. These results can be found in <ref type="table" target="#tab_3">Table 3</ref>. We calculate the multi-task metric relative to the single-task scores reported by the respective papers. The hyperparameters used for our training algorithm are shown in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Cityscapes</head><p>We report results on the Cityscapes val and test splits. As a baseline, we use an HRNet-48+OCR <ref type="bibr" target="#b48">[49]</ref> model. We train the model by adding InverseForm both with and without hierarchical multi-scale attention (HMS) <ref type="bibr" target="#b39">[40]</ref>. We also use their auto-labeled coarse annotations. We train with our boundary-aware segmentation scheme from Section 3.5. We only use the backbone during inference, as illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>; the network does not require any extra compute compared to the baseline.</p><p>We compare our approach to state-of-the-art boundaryaware segmentation frameworks such as Gated-SCNN <ref type="bibr" target="#b38">[39]</ref> and SegFix <ref type="bibr" target="#b49">[50]</ref>, quantitatively in <ref type="table" target="#tab_6">Table 4</ref> and qualitatively in <ref type="figure" target="#fig_4">Figure 6</ref>. We outperform SegFix using an HRNet-OCR backbone by 0.3% mIoU, and using a GSCNN model with WideResNet-38 <ref type="bibr" target="#b50">[51]</ref> backbone by 1.1% mIoU. For GSCNN, we show that our method is complimentary to their work. We train their network by adding the Inverse-  Form loss term to their existing pixel-based boundary loss, outperforming their baseline by 1.6% mIoU. In <ref type="table" target="#tab_6">Table 4</ref>, all networks are pretrained on Mapiliary Vistas <ref type="bibr" target="#b32">[33]</ref>. They use different training sets provided by Cityscapes. These contain coarse and finely-annotated training samples, which are available in the dataset. In <ref type="figure" target="#fig_4">Figure 6</ref>, we use the images provided by authors of <ref type="bibr" target="#b49">[50]</ref> and <ref type="bibr" target="#b38">[39]</ref>, and use the state-of-the-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Split F C mIoU Naive-student <ref type="bibr">[</ref>   art model provided by <ref type="bibr" target="#b39">[40]</ref> to generate baseline predictions. We then use the same baselines trained with InverseForm from <ref type="table" target="#tab_6">Table 4</ref> to generate our predictions. As seen in <ref type="table" target="#tab_6">Table  4</ref> and <ref type="figure" target="#fig_4">Figure 6</ref>, all models trained using InverseForm loss consistently improve compared to their baselines. We are rank 3 on the public Cityscapes leaderboard at the time of submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Studies</head><p>Searching for the best inverse-transformation: We analyze how our current dense inverse-transformation architecture performs compared to the convolutional architecture used in AET <ref type="bibr" target="#b51">[52]</ref>. To reproduce their result, we use their pretrained model and fine-tune with tiled NYU-Depth-v2 boundary images, using their training strategy. We also conduct experiments with both formulations of distance given in Section 3.3 on NYU-Depth-v2 using vanilla HRNet-w48 and HRNet-w18 as backbones, and the architecture defined in <ref type="figure" target="#fig_2">Figure 4</ref>. The loss functions used are Inverse-Form using geodesic distance measure L if,geo and Inverse-Form using the Euclidean measure L if,euc . Our experiments show a clear improvement in performance for the inverse-transformer architecture v/s the one used by AET. We hypothesize that the reason for better performance is that convolutions are translation-invariant. While the framework used by AET stacks two convolutional embeddings, the local resolution takes a hit compared to global context.  <ref type="table">Table 5</ref>: Searching for the optimal InverseForm loss Distance function: Our results for using the geodesic distance v/s Euclidean distance however, do not show a clear winner. This is partly due to the reason that the formulation in Section 3.3 for geodesic distance can lead to exploding gradients easily. This severely limits the search-space for hyperparameters. Euclidean distance on the other hand might not model perspective homography best, but using this gives us a wider search-space and hence a more consistent improvement. We recognize the need to explore the benefits of using geodesic distance, and hence continue our research on the optimal InverseForm network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose a distance-based boundary-aware segmentation method that consistently improves any semantic segmentation backbone it is plugged in. During training, the boundary detection and segmentation are jointly optimized in a multi-task learning framework. At inference time, it does not require any additional computational load. The distance-based measure outperforms cross-entropy based measures while efficiently capturing the boundary transformation. We show empirically that our scheme achieves superior segmentation accuracy and better structured segmentation outputs. We continue to look for the optimal architecture and distance measure for this method, as we show some room for improvement in our ablation experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Spatial transformer network (b) Inverse transformation network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Spatial transformer (a) and our inversetransformation network (b). Our inverse-transformation network architecture inputs two images x and t ? (x), and predicts the transformation parameters?. This performs the inverse operation as that of the spatial transformer network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Overall framework for our proposed boundary-aware segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of mIoU v/s FLOPs and mIou v/s # params for different schemes on NYU-Depth-v2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Cityscapes results showing visual effect of training different baselines with InverseForm loss. The structure of predicted outputs is improved in highlighted regions due to boundary-aware segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>NYU-Depth-v2 results showing visual effect of training SA-Gates<ref type="bibr" target="#b34">[35]</ref> baseline with InverseForm loss. Clear improvement is visible in the structure of predicted outputs due to boundary-aware segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the art methods on NYU-Depth-v2. Training with InverseForm consistently improves results, and outperforms state-of-the-art methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Training state-of-the-art multi-task learning methods on PASCAL by adding InverseForm loss over boundary detection. HRNet-18 and SE-Resnet backbones are used in a multi-task setting and mIoU scores for segmentation, saliency, human parts and surface normal tasks as well as F-scores for boundary detection are compared with the original results. InverseForm loss consistently improves results barring a few cases.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Our</figDesc><table><row><cell></cell><cell cols="3">method compared to various state-of-the-art</cell></row><row><cell cols="4">algorithms on Cityscapes. The models reporting on test split</cell></row><row><cell cols="4">are trained using training+validation data. F:Fine annota-</cell></row><row><cell cols="2">tions, C:Coarse annotations</cell><cell></cell><cell></cell></row><row><cell>Input</cell><cell>GT</cell><cell>w/o Ours</cell><cell>w/ Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>model Loss net. L if,geo L if,euc mIoU</figDesc><table><row><cell></cell><cell>AET</cell><cell>47.19</cell></row><row><cell></cell><cell>Ours</cell><cell>47.28</cell></row><row><cell>HRNet-w48</cell><cell>AET</cell><cell>47.03</cell></row><row><cell></cell><cell>Ours</cell><cell>47.42</cell></row><row><cell></cell><cell>AET</cell><cell>33.82</cell></row><row><cell></cell><cell>Ours</cell><cell>34.84</cell></row><row><cell>HRNet-w18</cell><cell>AET</cell><cell>33.97</cell></row><row><cell></cell><cell>Ours</cell><cell>34.79</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic segmentation with boundary neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3602" to="3610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4545" to="4554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Naive-student: Leveraging semi-supervised learning in video sequences for urban scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fusionnet: Edge aware deep convolutional networks for semantic segmentation of remote sensing harbor images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongcai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhong</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5769" to="5783" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascadepsp: Toward class-agnostic and very highresolution segmentation via global and local refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Ho Kei Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8890" to="8899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Magnenat Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6819" to="6829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic correlation promoted shape-variant context for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8885" to="8894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multiscale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2393" to="2402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards a deeper understanding of adversarial losses under a discriminative adversarial network setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Blitznet: A real-time deep network for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4154" to="4162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Temporally distributed networks for fast video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8818" to="8827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointrend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016. International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6129" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="956" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Aetv2: Autoencoding transformations for selfsupervised representation learning by minimizing geodesic distances in lie groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning affinity via spatial propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Shalini De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attentive single-tasking of multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Kevis-Kokitsi Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5000" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bi-directional cross-modality feature propagation with separation-andaggregation gate for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Weighted hausdorff distance: A loss function for object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Ribera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>G?era</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Delp</surname></persName>
		</author>
		<idno>abs/1806.07564</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04514</idno>
		<title level="m">Wenyu Liu, and Jingdong Wang. High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gated-scnn: Gated shape cnns for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towaki</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5229" to="5238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Minimization on the lie group so (3) and related manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Camillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
		<respStmt>
			<orgName>Yale University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning on lie groups for invariant detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mti-net: Multi-scale task interaction networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Malleable 2.5d convolution: Learning receptive fields along the depth-axis for rgb-d scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Segfix: Model-agnostic boundary refinement for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="489" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference 2016. British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aet vs. aed: Unsupervised representation learning by autoencoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2547" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
