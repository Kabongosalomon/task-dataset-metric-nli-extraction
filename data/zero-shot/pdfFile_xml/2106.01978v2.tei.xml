<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Computer System Engineering Research Institute of China</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingwei</forename><surname>Wei</surname></persName>
							<email>weilingwei18@mails.ucas.edu.cnhuaixy@sina.com</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Huai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Computer System Engineering Research Institute of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion Recognition in Conversations (ERC) has gained increasing attention for developing empathetic machines. Recently, many approaches have been devoted to perceiving conversational context by deep learning models. However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective. Inspired by the Cognitive Theory of Emotion, we design multiturn reasoning modules to extract and integrate emotional clues. The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking. Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotion recognition in conversation (ERC) aims to detect emotions expressed by the speakers in each utterance of the conversation. The task is an important topic for developing empathetic machines  in a variety of areas including social opinion mining <ref type="bibr" target="#b20">(Kumar et al., 2015)</ref>, intelligent assistant <ref type="bibr" target="#b19">(K?nig et al., 2016)</ref>, health care <ref type="bibr" target="#b31">(Pujol et al., 2019)</ref>, and so on.</p><p>A conversation often contains contextual clues ) that trigger the current utterance's emotion, such as the cause or situation. Recent context-based works <ref type="bibr" target="#b29">(Poria et al., 2017;</ref><ref type="bibr" target="#b10">Hazarika et al., 2018b;</ref> on ERC have been devoted to perceiving situation-level or speaker-level context by deep learning models. However, these methods are insufficient in understanding the context that usually contains rich emotional clues. We argue they mainly suffer from the following challenges. 1) The extraction of emotional clues. Most approaches <ref type="bibr">(Hazarika et al., 2018a,b;</ref><ref type="bibr" target="#b15">Jiao et al., 2020b)</ref> generally retrieve the relevant context from a static memory, which limits the ability to capture richer emotional clues. 2) The integration of emotional clues. Many works <ref type="bibr" target="#b8">Ghosal et al., 2019;</ref><ref type="bibr" target="#b23">Lu et al., 2020)</ref> usually use the attention mechanism to integrate encoded emotional clues, ignoring their intrinsic semantic order. It would lose logical relationships between clues, making it difficult to capture key factors that trigger emotions.</p><p>The Cognitive Theory of Emotion <ref type="bibr" target="#b32">(Schachter and Singer, 1962;</ref><ref type="bibr" target="#b33">Scherer et al., 2001)</ref> suggests that cognitive factors are potently determined for the formation of emotional states. These cognitive factors can be captured by iteratively performing the intuitive retrieving process and conscious reasoning process in our brains <ref type="bibr" target="#b5">(Evans, 1984</ref><ref type="bibr" target="#b6">(Evans, , 2003</ref><ref type="bibr" target="#b7">(Evans, , 2008</ref><ref type="bibr" target="#b35">Sloman, 1996)</ref>. Motivated by them, this paper attempts to model both critical processes to reason emotional clues and sufficiently understand the conversational context. By following the mechanism of working memory <ref type="bibr" target="#b0">(Baddeley, 1992)</ref> in the cognitive phase, we can iteratively perform both cognitive processes to guide the extraction and integration of emotional clues, which imitates human unique cognitive thinking.</p><p>In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to recognize the utterance's emotion by sufficiently understanding the conversational context. The model introduces a cognitive phase to extract and integrate emotional clues from the context retrieved by the perceive phase. Firstly, in the perceptive phase, we leverage Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks to capture situation-level and speaker-level context. Based on the above context, global memories can be obtained to storage different contextual information. Sec-ondly, in the cognitive phase, we design multi-turn reasoning modules to iteratively extract and integrate the emotional clues. The reasoning module performs two processes, i.e., an intuitive retrieving process and a conscious reasoning process. The former utilizes the attention mechanism to match relevant contextual clues by retrieving static global memories, which imitates the intuitive retrieving process. The latter adopts LSTM networks to learn intrinsic logical order and integrate contextual clues by retaining and updating dynamic working memory, which imitates the conscious reasoning process. It is slower but with human-unique rationality <ref type="bibr" target="#b0">(Baddeley, 1992)</ref>. Finally, according to the above contextual clues at situation-level and speaker-level, an emotion classifier is used to predict the emotion label of the utterance.</p><p>To evaluate the performance of the proposed model, we conduct extensive experiments on three public benchmark datasets, i.e., IEMOCAP, SE-MAINE and MELD datasets. Results consistently demonstrate that our proposed model significantly outperforms comparison methods. Moreover, understanding emotional clues from a cognitive perspective can boost the performance of emotion recognition.</p><p>The main contributions of this work are summarized as follows:</p><p>? We propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective. To the best of our knowledge, this is the first attempt to explore cognitive factors for emotion recognition in conversations.</p><p>? We design multi-turn reasoning modules to extract and integrate emotional clues by iteratively performing the intuitive retrieving process and conscious reasoning process, which imitates human unique cognitive thinking.</p><p>? We conduct extensive experiments on three public benchmark datasets. The results consistently demonstrate the effectiveness and superiority of the proposed model 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Statement</head><p>Formally, let U = [u 1 , u 2 , ..., u N ] be a conversation, where N is the number of utterances. And there are M speakers/parties p 1 , p 2 , ..., p M (M ? 2). Each utterance u i is spoken by the speaker p ?(u i ) , where ? maps the index of the utterance into that of the corresponding speaker. Moreover, for each ? ? [1, M ], we define U ? to represent the set of utterances spoken by the speaker p ? , i.e.,</p><formula xml:id="formula_0">U ? = {u i | u i ? U and u i spoken by p ? , ?i ? [1, N ]}.</formula><p>The task of emotion recognition in conversations (ERC) aims to predict the emotion label y i for each utterance u i from the pre-defined emotions Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Textual Features</head><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b16">(Kim, 2014)</ref> are capable of capturing n-grams information from an utterance. Following previous works <ref type="bibr" target="#b10">(Hazarika et al., 2018b;</ref><ref type="bibr" target="#b8">Ghosal et al., 2019)</ref>, we leverage a CNN layer with max-pooling to exact context-free textual features from the transcript of each utterance. Concretely, the input is the 300 dimensional pre-trained 840B GloVe vectors <ref type="bibr" target="#b28">(Pennington et al., 2014)</ref>. We employ three filters of size 3, 4 and 5 with 50 feature maps each. These feature maps are further processed by max-pooling and ReLU activation <ref type="bibr" target="#b26">(Nair and Hinton, 2010)</ref>. Then, these activation features are concatenated and finally projected onto a dense layer with dimension d u = 100, whose output forms the representation of an utterance. We denote {u i } N i=1 , u i ? R du as the representation for N utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model</head><p>Then, we propose Contextual Reasoning Networks (DialogueCRN) for emotion recognition in conversations. DialogueCRN is comprised of three integral components, i.e., the perception phase (Section 2.3.1), the cognition phase (Section 2.3.2), and an emotion classifier (Section 2.3.3). The overall architecture is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Perception Phase</head><p>In the perceptive phase, based on the input textual features, we first generate the representation of conversational context at situation-level and speakerlevel. Then, global memories are obtained to storage different contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversational</head><p>Context Representation. Long Short-Term Memory (LSTM) <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> introduces the gating mechanism into recurrent neural networks to capture long-term dependencies from the input sequences. In this part, two bi-directional LSTM networks are leveraged to capture situationlevel and speaker-level context dependencies, respectively. For learning the context representation at the situation level, we apply a bi-directional LSTM network to capture sequential dependencies between adjacent utterances in a conversational situation. The input is each utterance's textual features u i ? R du . The situation-level context representation c s i ? R 2du can be computed as:</p><formula xml:id="formula_1">c s i , h s i = ? ??? ? LST M s (u i , h s i?1 ),<label>(1)</label></formula><p>where h s i ? R du is the i-th hidden state of the situation-level LSTM.</p><p>For learning the context representation at the speaker level, we also employ another bi-directional LSTM network to capture selfdependencies between adjacent utterances of the same speaker. Given textual features u i of each utterance, the speaker-level context representation c v i ? R 2du is computed as:</p><formula xml:id="formula_2">c v i , h v ?,j = ? ??? ? LST M v (u i , h v ?,j?1 ), j ? [1, |U ? |],<label>(2)</label></formula><p>where ? = ?(u i ). U ? refers to all utterances of the speaker p ? . h v ?,j ? R du is the j-th hidden state of speaker-level LSTM for the speaker p ? .</p><p>Global Memory Representation. Based on the above conversational context representation, global memories can be obtained to storage different contextual information via a linear layer. That is, global memory representation of situation-level context G s = [g s 1 , g s 2 , ..., g s N ] and that of speaker-</p><formula xml:id="formula_3">level context G v = [g v 1 , g v 2 , ..., g v N ]</formula><p>can be computed as:</p><formula xml:id="formula_4">g s i = W s g c s i + b s g ,<label>(3)</label></formula><formula xml:id="formula_5">g v i = W v g c v i + b v g ,<label>(4)</label></formula><formula xml:id="formula_6">where W s g , W v g ? R 2du?2du , b s g , b v g ? R 2du are learnable parameters.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Cognition Phase</head><p>Inspired by the Cognitive Theory of Emotion <ref type="bibr" target="#b32">(Schachter and Singer, 1962;</ref><ref type="bibr" target="#b33">Scherer et al., 2001)</ref>, cognitive factors are potently determined for the formation of emotional states. Therefore, in the cognitive phase, we design multi-turn reasoning modules to iteratively extract and integrate the emotional clues. The architecture of a reasoning module is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The reasoning module performs two processes, the intuitive retrieving process, and the conscious reasoning process. In the t-th turn, for the reasoning process, we adopt the LSTM network to learn intrinsic logical order and integrate contextual clues in the working memory, which is slower but with human-unique rationality <ref type="bibr" target="#b0">(Baddeley, 1992)</ref>. That is,</p><formula xml:id="formula_7">q (t?1) i , h (t) i = ????? LST M (q (t?1) i , h (t?1) i ),<label>(5)</label></formula><formula xml:id="formula_8">whereq (t?1) i ? R 2du is the output vector. q (t) i ? R 4du is initialized by the context representation c i of the current utterance, i.e., q (0) i = W q c i + b q , where W q ? R 4du?2du and b q ? R 4du are learn- able parameters. h (t)</formula><p>i ? R 2du refers to the working memory, which can not only storage and update the previous memory h (t?1) i , but also guide the extraction of clues in the next turn. During sequential flowing of the working memory, we can learn implicit logical order among clues, which resembles the conscious thinking process of humans. h (t) i is initialized with zero. t is the index that indicates how many "processing steps" are being carried to compute the final state.</p><p>For the retrieving process, we utilize an attention mechanism to match relevant contextual clues from the global memory. The detailed calculations are as follows:</p><formula xml:id="formula_9">e (t?1) ij = f (g j ,q (t?1) i ),<label>(6)</label></formula><formula xml:id="formula_10">? (t?1) ij = exp(e (t?1) ij ) N j=1 exp(e (t?1) ij ) ,<label>(7)</label></formula><formula xml:id="formula_11">r (t?1) i = N j=1 ? (t?1) ij g j ,<label>(8)</label></formula><p>where f is a function that computes a single scalar from g j andq (t?1) i (e.g., a dot product). Then, we concatenate the output of reasoning processq</p><formula xml:id="formula_12">(t?1) i with the resulting attention readout r (t?1) i to form the next-turn query q (t) i . That is, q (t) i = [q (t?1) i ; r (t?1) i ].<label>(9)</label></formula><p>The query q (t) i will be updated under the guidance of working memory h (t) i , and more contextual clues can be retrieved from the global memory.</p><p>To sum up, given context representation c i of the utterance u i , global memory representation G, and the number of turns T , the whole cognitive phase (Eq.5-9) can be denoted as, q i = Cognition(c i , G; T ). In this work, we design two individual cognition phases to explore contextual clues at situation-level and speaker-level, respectively. The outputs are defined as:</p><formula xml:id="formula_13">q s i = Cognition s (c s i , G s ; T s ), (10) q v i = Cognition v (c v i , G v ; T v ),<label>(11)</label></formula><p>where T s and T v are the number of turns in situation-level and speaker-level cognitive phases, respectively. Based on the above output vectors, the final representation o can be defined as a concatenation of both vectors, i.e.,</p><formula xml:id="formula_14">o i = [q s i ; q v i ].<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Emotion Classifier</head><p>Finally, according to the above contextual clues, an emotion classifier is used to predict the emotion label of the utterance.</p><formula xml:id="formula_15">y i = sof tmax(W o o i + b o ),<label>(13)</label></formula><p>where</p><formula xml:id="formula_16">W o ? R 8du?|Y| and b o ? R |Y| are trainable parameters.</formula><p>|Y| is the number of emotion labels. Cross entropy loss is used to train the model. The loss function is defined as:</p><formula xml:id="formula_17">L = ? 1 L l=1 ? (l) L i=1 ? (i) k=1 y l i,k log(? l i,k ),<label>(14)</label></formula><p>where L is the total number of conversations/samples in the training set. ? (i) is the number of utterances in the sample i. y l i,k and? l i,k denote the one-hot vector and probability vector for emotion class k of utterance i of sample l, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setups</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate our proposed model on following benchmark datasets, IEMOCAP <ref type="bibr" target="#b2">(Busso et al., 2008)</ref>, <ref type="bibr">SEMAINE (McKeown et al., 2012)</ref>, and MELD  datasets. The statistics are reported in <ref type="table" target="#tab_1">Table 1</ref>. The above datasets are multimodal datasets with textual, visual, and acoustic features. In this paper, we focus on emotion recognition in textual conversations. Multimodal emotion recognition in conversations is left as future work.</p><p>IEMOCAP 2 : The dataset <ref type="bibr" target="#b2">(Busso et al., 2008</ref>) contains videos of two-way conversations of ten  unique speakers, where only the first eight speakers from session one to four belong to the training set. The utterances are annotated with one of six emotion labels, namely happy, sad, neutral, angry, excited, and frustrated. Following previous works <ref type="bibr" target="#b9">(Hazarika et al., 2018a;</ref><ref type="bibr" target="#b8">Ghosal et al., 2019;</ref><ref type="bibr" target="#b15">Jiao et al., 2020b)</ref>, the validation set is extracted from the randomly shuffled training set with the ratio of 80:20 since no pre-defined train/val split is provided in the IEMOCAP dataset.  <ref type="bibr" target="#b27">(Nicolle et al., 2012)</ref>. Following <ref type="bibr" target="#b9">(Hazarika et al., 2018a;</ref><ref type="bibr" target="#b8">Ghosal et al., 2019)</ref>, the attributes are averaged over the span of an utterance to obtain utterance-level annotations. We utilize the standard both training and testing splits provided in the sub-challenge.</p><p>MELD 4 : Multimodal Emotion Lines Dataset (MELD) , a extension of the EmotionLines <ref type="bibr" target="#b12">(Hsu et al., 2018)</ref>, is collected from TV-series Friends containing more than 1400 multiparty conversations and 13000 utterances. Each utterance is annotated with one of seven emotion labels (i.e., happy/joy, anger, fear, disgust, sadness, surprise, and neutral). We use the pre-defined train/val split provided in the MELD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparisons Methods</head><p>We compare the proposed model against the following baseline methods. TextCNN <ref type="bibr" target="#b16">(Kim, 2014</ref>) is a convolutional neural network trained on contextindependent utterances. Memnet <ref type="bibr" target="#b36">(Sukhbaatar et al., 2015)</ref> is an end-to-end memory network and update memories in a multi-hop fashion. bc-LSTM+Att <ref type="bibr" target="#b29">(Poria et al., 2017</ref>) adopts a bidirectional LSTM network to capture the contextual content from the surrounding utterances. Additionally, an attention mechanism is adopted to re-weight features and provide a more informative output. CMN <ref type="bibr" target="#b10">(Hazarika et al., 2018b</ref>) encodes conversational context from dialogue history by two distinct GRUs for two speakers. ICON <ref type="bibr" target="#b9">(Hazarika et al., 2018a)</ref> extends CMN by connecting outputs of individual speaker GRUs using another GRU for perceiving inter-speaker modeling. DialogueRNN ) is a recurrent network that consists of two GRUs to track speaker states and context during the conversation. DialogueGCN <ref type="bibr" target="#b8">(Ghosal et al., 2019)</ref> a graph-based model where nodes represent utterances and edges represent the dependency between the speakers of the utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Metrics</head><p>Following previous works <ref type="bibr" target="#b9">(Hazarika et al., 2018a;</ref><ref type="bibr" target="#b15">Jiao et al., 2020b)</ref>, for IEMOCAP and MELD datasets, we choose the accuracy score (Acc.) to measure the overall performance. We also report the Weighted-average F1 score (Weighted-F 1) and Macro-averaged F1 score (Macro-F 1) to evaluate the model performance on both majority and minority classes, respectively. For the SEMAINE dataset, we report Mean Absolute Error (MAE) for each attribute. The lower MAE, the better the detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>We use the validation set to tune hyperparameters. In the perceptive phase, we employ two-layer bidirectional LSTM on IEMOCAP and SEMAINE datasets and single-layer bi-directional LSTM on the MELD dataset. In the cognitive phase, singlelayer LSTM is used on all datasets. The batch size is set to 32. We adopt Adam (Kingma and Ba, 2015) as the optimizer with an initial learning rate of {0.0001, 0.001, 0.001} and L2 weight decay of {0.0002, 0.0005, 0.0005} for IEMOCAP, SE-MAINE, MELD datasets, respectively. The dropout rate is set to 0.2. We train all models for a maximum of 100 epochs and stop training if the validation loss does not decrease for 20 consecutive epochs.</p><p>For results of DialogueGCN and DialogueRNN, we implement them according to the public code 5 provided by ; <ref type="bibr" target="#b8">Ghosal et al. (2019)</ref>       <ref type="table" target="#tab_1">Table 1</ref>, the number of speakers of each conversation in the MELD dataset is large (up to 9), and the average length of conversations is 10. The shorter conversation length of the MELD dataset indicates it contains less contextual information. From the result in <ref type="table" target="#tab_6">Table 4</ref>, interestingly, TextCNN ignoring conversational context achieves better results than most baselines. It indicates that it is difficult to learn useful features from perceiving a limited and missing context. Besides, Dia-logueGCN leverages graph structure to perceive the interaction of multiple speakers, which is sufficient to perceive the speaker-level context. Thereby, the performance is slightly improved. Compared with baselines, DialogueCRN enables to perform sequential thinking of context and understand emotional clues from a cognitive perspective. Therefore, it achieves the best recognition results, e.g., 2.9% improvements on Weighted-F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MELD. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>To better understand the contribution of different modules in DialogueCRN to the performance, we conduct several ablation studies on both IEMOCAP and SEMAINE datasets. Different modules that model the situation-level and speaker-level context in both perceptive and cognitive phases are removed separately. The results are shown in <ref type="table" target="#tab_8">Table 5</ref>. When cognition and perception modules are removed successively, the performance is greatly  declined. It indicates the importance of both the perception and cognition phases for ERC.</p><p>Effect of Cognitive Phase. When only removing cognition phase, as shown in the third block of <ref type="table" target="#tab_8">Table 5</ref>, the performance on the IEMOCAP dataset decreases 4.3%, 4.3% and 6.5% in terms of Acc., Weighted-F1, and Macro-F1, respectively. And on the SEMAINE dataset, the MAE scores of Valence, Arousal, and Expectancy attributes are increased by 2.3%, 12.5% and 2.9%, respectively. These results indicate the efficacy of the cognitive phase, which can reason based on the perceived contextual information consciously and sequentially. Besides, if removing the cognitive phase for either speaker-level or situation-level context, as shown in the second block, the results decreased on both datasets. The fact reflects both situational factors and speaker factors are critical in the cognitive phase.</p><p>Effect of Perceptive Phase. As shown in the last row, when removing the perception module, the performance is dropped sharply. The inferior results reveal the necessity of the perceptive phase to unconsciously match relevant context based on the current utterance.</p><p>Effect of Different Context. When removing either situation-level or speaker-level context in both cognitive and perceptive phases, respectively, the performance has a certain degree of decline. The phenomenon shows both situation-level and speaker-level context play an effective role in the perceptive and cognitive phases. Besides, the margin of dropped performance is different on both datasets. This suggests speaker-level context plays a greater role in the perception phase while more complex situation-level context works well in the cognitive phase. The explanation is that it is limited to learn informative features from context by intuitive matching perception, but conscious cognitive reasoning can boost better understanding. <ref type="figure">Figure 3</ref>: Results against the number of turns. We report the Weighted-F1 score on the IEMOCAP dataset and MAE of Arousal attribute on the SEMAINE dataset. The lighter the color, the better the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Analysis</head><p>We investigate how our model performs w.r.t the number of turns in the cognitive phase. <ref type="figure">From Figure 3</ref>, the best {T s , T v } is {2, 2} and {1, 3} on IEMOCAP and SEMAINE datasets, which obtain 66.20% Weighted-F1 and 0.1522 MAE of Arousal attribute, respectively. Note that the SEMAINE dataset needs more turns for the speaker-level cognitive phase. It implies speaker-level contextual clues may be more vital in arousal emotion, espe-All you're going to do is just give me fifty dollars and say go have fun on your vacation without any of your stuff?</p><p>We'd be willing to give you fifty dollars to reimburse you for the bag.</p><p>[NEUTRAL]</p><p>[ANGRY]</p><p>[ANGRY]</p><p>[ANGRY]</p><p>[NEUTRAL]</p><p>[NEUTRAL]</p><p>[NEUTRAL]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[FRUSTRATED]</head><p>There are some shops here in the airport. I realize this. But I have to ask you to move so I can help the next person.</p><p>What am I gonna do without anything for three weeks?</p><p>But fifty dollars isn't going to get me anything.</p><p>It sounds like it's no big deal whatever. It's a big deal to me. cially empathetic clues that require complex reasoning.</p><p>Besides, if we solely consider either situationlevel or speaker-level context in the cognitive phase, results on the two datasets are significantly improved within a certain number of turns. The fact indicates the effectiveness of using multi-turn reasoning modules to understand contextual clues. <ref type="figure" target="#fig_3">Figure 4</ref> shows a conversation sampled from the IEMOCAP dataset. The goal is to predict the emotion label of utterance 8. Methods such as Dia-logueRNN and DialogueGCN lack the ability to consciously understand emotional clues, e.g., the cause of the emotion (failed expectation). They are easy to mistakenly identify the emotion as angry or neutral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>Our model DialogueCRN can understand the conversational context from a cognitive perspective. In the cognitive phase, the following two processes are performed iteratively: the intuitive retrieving process of 8-7-2-1 (blue arrows) and the conscious reasoning process of a-b-c (red arrows), to extract and integrate emotional clues. We can obtain that utterance 8 implied that more compensation expected by female was not achieved. The failed compensation leads to more negative of his emotion and thus correctly identified as depression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Emotion Recognition</head><p>Emotion recognition (ER) has been drawing increasing attention to natural language processing (NLP) and artificial intelligence (AI). Existing works generally regard the ER task as a classification task based on context-free blocks of data, such as individual reviews or documents. They can roughly divided into two parts, i.e., featureengineering based <ref type="bibr" target="#b4">(Devillers and Vidrascu, 2006)</ref>, and deep-learning based methods <ref type="bibr" target="#b37">(Tang et al., 2016;</ref><ref type="bibr" target="#b41">Wei et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Emotion Recognition in Conversations</head><p>Recently, the task of Emotion Recognition in Conversations (ERC) has received attention from researchers. Different traditional emotion recognition, both situation-level and speaker-level context plays a significant role in identifying the emotion of an utterance in conversations . The neglect of them would lead to quite limited performance <ref type="bibr" target="#b1">(Bertero et al., 2016)</ref>. Existing works generally capture contextual characteristics for the ERC task by deep learning methods, which can be divided into sequence-based and graph-based methods.</p><p>Sequence-based Methods. Many works capture contextual information in utterance sequences. <ref type="bibr" target="#b29">Poria et al. (2017)</ref> employed LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> to capture conversational context features. <ref type="bibr">Hazarika et al. (2018a,b)</ref> used end-to-end memory networks <ref type="bibr" target="#b36">(Sukhbaatar et al., 2015)</ref> to capture contextual features that distinguish different speakers. <ref type="bibr" target="#b43">Zhong et al. (2019)</ref>; <ref type="bibr" target="#b44">Li et al. (2020)</ref> utilized the transformer <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> to capture richer contextual features based on the attention mechanism.  introduced a speaker state and global state for each conversation based on GRUs <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>. Moreover, <ref type="bibr" target="#b14">Jiao et al. (2020a)</ref> introduced a conversation completion task to learn from unsupervised conversation data. <ref type="bibr" target="#b15">Jiao et al. (2020b)</ref> proposed a hierarchical memory network for real-time emotion recognition without future context.  modeled ERC as sequence tagging to learn the emotional consistency. <ref type="bibr" target="#b23">Lu et al. (2020)</ref> proposed an iterative emotion interaction network to explicitly model the emotion interaction.</p><p>Graph-based Methods. Some works <ref type="bibr" target="#b42">(Zhang et al., 2019;</ref><ref type="bibr" target="#b8">Ghosal et al., 2019;</ref><ref type="bibr" target="#b13">Ishiwatari et al., 2020;</ref><ref type="bibr" target="#b22">Lian et al., 2020)</ref> model the conversational context by designing a specific graphical structure. They utilize graph neural networks <ref type="bibr" target="#b18">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b39">Velickovic et al., 2017)</ref> to capture multiple dependencies in the conversation, which have achieved appreciable performance.</p><p>Different from previous works, inspired by the Cognitive Theory of Emotion <ref type="bibr" target="#b32">(Schachter and Singer, 1962;</ref><ref type="bibr" target="#b33">Scherer et al., 2001)</ref>, this paper makes the first attempt to explore cognitive factors for emotion recognition in conversations. To sufficiently understand the conversational context, we propose a novel DialogueCRN to extract and then integrate rich emotional clues in a cognitive manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper has investigated cognitive factors for the task of emotion recognition in conversations (ERC). We propose novel contextual reasoning networks (DialogueCRN) to sufficiently understand both situation-level and speaker-level context. Di-alogueCRN introduces the cognitive phase to extract and integrate emotional clues from context retrieved by the perceptive phase. In the cognitive phase, we design multi-turn reasoning modules to iteratively perform the intuitive retrieving process and conscious reasoning process, which imitates human unique cognitive thinking. Finally, emotional clues that trigger the current emotion are successfully obtained and used for better classification. Experiments on three benchmark datasets have proved the effectiveness and superiority of the proposed model. The case study shows that considering cognitive factors can better understand emotional clues and boost the performance of ERC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of the proposed model DialogueCRN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The detailed structure of reasoning module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>SEMAINE 3 : The dataset (McKeown et al., 2012) is a video database of human-agent interactions. It is available at AVEC 2012's fully continuous sub-challenge (Schuller et al., 2012) that requires predictions of four continuous affective attributes: Arousal, Expectancy, Power, and Valence. The gold annotations are available for every 0:2 seconds in each video</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The case study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>refers to the number of real valued attributes.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># Dialogues train val test train # Utterances val test</cell><cell>Avg. Length</cell><cell># Classes</cell></row><row><cell>IEMOCAP</cell><cell>120</cell><cell>31</cell><cell>5,810</cell><cell>1,623</cell><cell>50</cell><cell>6</cell></row><row><cell>SEMAINE</cell><cell>63</cell><cell>32</cell><cell>4,368</cell><cell>1,430</cell><cell>72</cell><cell>4  *</cell></row><row><cell>MELD</cell><cell cols="4">1,039 114 280 9,989 1,109 2,610</cell><cell>10</cell><cell>7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The statistics of three datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>under the same environment.</figDesc><table><row><cell></cell><cell cols="2">IEMOCAP</cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">Acc. Weighted-F1 Macro-F1</cell></row><row><cell>TextCNN</cell><cell>49.35</cell><cell>49.21</cell><cell>48.13</cell></row><row><cell>Memnet</cell><cell>55.70</cell><cell>53.10</cell><cell>55.40</cell></row><row><cell cols="2">bc-LSTM+Att 56.32</cell><cell>56.19</cell><cell>54.84</cell></row><row><cell>CMN</cell><cell>56.56</cell><cell>56.13</cell><cell>54.30</cell></row><row><cell>ICON</cell><cell>59.09</cell><cell>58.54</cell><cell>56.52</cell></row><row><cell cols="2">DialogueRNN 63.03</cell><cell>62.50</cell><cell>60.66</cell></row><row><cell cols="2">DialogueGCN 64.02</cell><cell>63.65</cell><cell>63.43</cell></row><row><cell cols="2">DialogueCRN 66.05</cell><cell>66.20</cell><cell>66.38</cell></row><row><cell>Improve</cell><cell>3.2%</cell><cell>4.0%</cell><cell>4.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5 https://github.com/declare-lab/</cell></row><row><cell></cell><cell></cell><cell></cell><cell>conv-emotion</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on the IEMOCAP dataset.</figDesc><table><row><cell></cell><cell cols="2">SEMAINE</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4">MAE Valence Arousal Expectancy Power</cell></row><row><cell>TextCNN</cell><cell>0.545</cell><cell>0.542</cell><cell>0.605</cell><cell>8.71</cell></row><row><cell>Memnet</cell><cell>0.202</cell><cell>0.211</cell><cell>0.216</cell><cell>8.97</cell></row><row><cell>bc-LSTM+Att</cell><cell>0.189</cell><cell>0.213</cell><cell>0.190</cell><cell>8.67</cell></row><row><cell>CMN</cell><cell>0.192</cell><cell>0.213</cell><cell>0.195</cell><cell>8.74</cell></row><row><cell>ICON</cell><cell>0.180</cell><cell>0.190</cell><cell>0.180</cell><cell>8.45</cell></row><row><cell>DialogueRNN</cell><cell>0.175</cell><cell>0.171</cell><cell>0.181</cell><cell>8.66</cell></row><row><cell>DialogueGCN</cell><cell>0.176</cell><cell>0.210</cell><cell>0.193</cell><cell>8.65</cell></row><row><cell>DialogueCRN</cell><cell>0.173</cell><cell>0.152</cell><cell>0.175</cell><cell>8.20</cell></row><row><cell>Improve</cell><cell>1.1%</cell><cell>11.1%</cell><cell>2.8%</cell><cell>2.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on the SEMAINE dataset.</figDesc><table><row><cell>4 Results and Analysis</cell></row><row><cell>4.1 Experimental Results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>IEMOCAP and SEMAINE. Both IEMOCAP and SEMAINE datasets have long conversation lengths and the average length is not less than 50. The fact implies that the two datasets contain richer contextual information. TextCNN ignoring conversational context obtains the worst performance. Memnet and bc-LSTM+Att perceive the situation-level context of the current utterance. CMN perceives the speaker-level context. Thereby, Memnet, bc-LSTM+Att and CMN slightly outperforms TextCNN. ICON, DialogueRNN, and DialogueGCN consider both situation-level and speaker-level context to model the perceptive phase of context. They achieve better performance than the above methods. Compared with baseline methods, DialogueCRN can extract and integrate rich</figDesc><table><row><cell>, 3 and 4 show the comparison results</cell></row><row><cell>for emotion recognition in textual conversations.</cell></row><row><cell>DialogueCRN consistently achieves better perfor-</cell></row><row><cell>mance than the comparison methods on all datasets,</cell></row><row><cell>while also being statistically significant under the</cell></row><row><cell>paired t-test (p&lt;0.05).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on the MELD dataset.</figDesc><table><row><cell>emotional clues by exploring cognitive factors. Ac-</cell></row><row><cell>cordingly, our model obtains more effective per-</cell></row><row><cell>formance. That is, as shown in Table 2 and 3, for</cell></row><row><cell>the IEMOCAP dataset, DialogueCRN gains 3.2%,</cell></row><row><cell>4.0%, 4.7% relative improvements over the previ-</cell></row><row><cell>ous best baselines in terms of Acc., Weighted-F1,</cell></row></table><note>and Macro-F1, respectively. For the SEMAINE dataset, DialogueCRN achieves a large margin of 11.1% MAE for the Arousal attribute.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Experimental results of ablation studies on IEMOCAP and SEMAINE datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code is available at https://github. com/zerohd4869/DialogueCRN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://sail.usc.edu/iemocap/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://semaine-db.eu 4 https://github.com/SenticNet/MELD</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Baddeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">255</biblScope>
			<biblScope unit="issue">5044</biblScope>
			<biblScope unit="page" from="556" to="559" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time speech emotion and sentiment recognition for interactive dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Farhad Bin Siddique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Ho Yin Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1042" to="1047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">IEMOCAP: interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Resour. Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Aglar G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio. The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reallife emotions detection with lexical and paralinguistic cues on human-human call center dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Vidrascu</surname></persName>
		</author>
		<editor>IN-TERSPEECH. ISCA</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Heuristic and analytic processes in reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="468" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">In two minds: dualprocess accounts of reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="454" to="459" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual-processing accounts of reasoning, judgment, and social cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="255" to="278" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyati</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="154" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ICON: interactive conversational memory network for multimodal emotion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
	<note>NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Emotionlines: An emotion corpus of multi-party conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Chun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yeh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ting-Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC. European Language Resources Association (ELRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taichi</forename><surname>Ishiwatari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Miyazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7360" to="7370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting unsupervised data for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiang</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (Findings)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4839" to="4846" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time emotion recognition via attention gated hierarchical memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiang</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8002" to="8009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Defining affective identities in elderly nursing home residents for the design of an emotionally intelligent cognitive assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>K?nig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarti</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Hoey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Per-vasiveHealth</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="206" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Emotion analysis of twitter using opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Dogra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikrant</forename><surname>Dabas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">HiTrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4190" to="4200" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conversational emotion recognition using self-attention mechanisms and graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="2347" to="2351" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An iterative emotion interaction network for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Committee on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4078" to="4088" />
		</imprint>
	</monogr>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dialoguernn: An attentive RNN for emotion detection in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6818" to="6825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The SEMAINE database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><forename type="middle">Fran?ois</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schr?der</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust continuous prediction of human emotions using multiscale dynamic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?mie</forename><surname>Nicolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chetouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MELD: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Emotion recognition to improve e-healthcare systems in smart cities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">A</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Higinio</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Mart?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RIIFORUM</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cognitive, social and physiological determinants of emotional state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Schachter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="378" to="399" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Appraisal processes in emotion: Theory, methods, research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnstone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">AVEC 2012: the continuous audio/visual emotion challenge -an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bj?rn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><forename type="middle">Fran?ois</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="361" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The empirical case for two systems of reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sloman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contextualized emotion recognition in conversation as sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
	<note>SIGdial</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hierarchical interaction networks with rethinking mechanism for document-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangqing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5415" to="5421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Knowledge-enriched transformer for emotion detection in textual conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixiang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The design and implementation of xiaoice, an empathetic social chatbot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="93" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
