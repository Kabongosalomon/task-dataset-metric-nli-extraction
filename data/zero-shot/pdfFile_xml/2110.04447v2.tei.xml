<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Cardiac Measurement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G</orgName>
								<orgName type="institution">Allen School of Computer Science &amp; Engineering University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hill</surname></persName>
							<email>brian.l.hill@cs.ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
							<email>ziheng@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Paul G</orgName>
								<orgName type="institution">Allen School of Computer Science &amp; Engineering University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shwetak</forename><surname>Patel</surname></persName>
							<email>shwetak@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Paul G</orgName>
								<orgName type="institution">Allen School of Computer Science &amp; Engineering University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
							<email>damcduff@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Cardiac Measurement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Camera-based physiological measurement is a growing field with neural models providing state-of-the-art performance. Prior research has explored various "end-to-end" models; however these methods still require several preprocessing steps and are not able to run directly on mobile and edge devices. These additional operations are often non-trivial to implement, making replication and deployment difficult and can even have a higher computational budget than the "core" network itself. In this paper, we propose two novel and efficient neural models for camera-based physiological measurement called EfficientPhys that remove the need for face detection, segmentation, normalization, color space transformation or any other preprocessing steps. Using an input of raw video frames, our models achieve strong accuracy on three public datasets. We show that this is the case whether using a transformer or convolutional backbone. We further evaluate the latency of the proposed networks and show that our most lightweight network also achieves a 33% improvement in efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Camera-based physiological measurement is a non-contact approach for capturing cardiac signals via light reflected from the body <ref type="bibr" target="#b19">[20]</ref>. The most common such signal is the blood volume pulse (BVP) measured via the photoplethysmogram (PPG). From this, heart rate <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>, respiration rate <ref type="bibr" target="#b24">[25]</ref> and pulse transit times <ref type="bibr" target="#b27">[28]</ref> can be derived. Furthermore, there is promising evidence that the PPG signals be be used to measure signs of arterial disease <ref type="bibr" target="#b32">[33]</ref>. Neural models are the current state-of-the-art in this domain <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. These networks can learn strong feature representations and effectively disentangle the subtle changes in pixels due to underlying physiological processes from those due to body motions, lighting changes and other sources of "noise".</p><p>While prior research has framed architectures as "end-to-end" methods, those that achieve state-of-the-art performance actually require several preprocessing steps before data is input into the network. For example, <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b10">[11]</ref> use handcrafted normalized difference frames and normalized appearance frames as input to their convolutional attention network. <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b18">[19]</ref> use a complex schema to create feature maps called "MSTmap". This process includes facial landmark detection, extractions of several regions of interest (ROI) using these landmarks, and then averaging pixel values in both the RGB and YUV color spaces.</p><p>These preprocessing steps have several drawbacks: 1) They make assumptions about optimal normalization or representation without allowing the network to learn these features in a data-driven manner. 2) They are computationally costly and in many cases add a significant number of operations to the video processing pipeline. There are several reasons why we would prefer to run camera-based physiological sensing on-device: preserving privacy, analyzing raw video (i.e., not compressed) and saving data costs and bandwidth. Therefore, any additional computation needs to be justified by improving model accuracy, otherwise it is considerably disadvantageous. Moreover, since camera-based physiological sensing is a privacy-sensitive application, it is preferable to store the data on local devices instead of streaming the video and physiological data to the cloud. The overhead from processing is not affordable if we aim to make the system accessible to low-end mobile devices. 3) Many of these steps are non-trivial to implement and optimize in and of themselves. This makes it harder to deploy real-time systems and to replicate the implementation on different platforms. For instance, implementing existing methods on Android, iOS, or in JavaScript requires a significant amount of effort. Some libraries, such as facial landmark detection, are not even available on every platform. Thus, the last mile engineering using the existing methods becomes especially challenging.</p><p>Ideally, a video-based physiological measurement method would be able to run at a high frame rate even on mobile devices, be simple to implement across different platforms, and achieve state-of-the-art performance. Addressing the aforementioned challenges would help achieve these properties. We propose a truly end-to-end network, EfficientPhys, for which the input is unprocessed video frames without requiring accurate face cropping (see <ref type="figure" target="#fig_1">Fig. 1</ref>). Due to recent advancements in visual transformers, we propose both a convolutional and visual transformer architecture and compare and contrast the performance of these two.</p><p>In summary, our key contributions are to: 1) propose two novel one-stop neural architectures, a visual transformer and a convolutional network, which do not require any preprocessing steps, 2) evaluate the proposed methods on three popular benchmark datasets, 3) evaluate on-device latency across both stateof-the-art machine learning-based approaches as well as signal processing-based techniques. To the best of our knowledge, this is the first paper that explores the visual transformer in camera-based physiological measurement and its comparison with convolutional networks. This is also the first paper exploring a completely end-to-end on-device neural architecture for mobile devices. We include our code and a video in the supplementary materials. Please see supplementary materials for more results.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Camera-based Vital Measurement. There is a growing community studying the use of cameras to sense physiological vitals signs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b34">35]</ref>. Prior work established the fundamentals of how RGB images could be used to extract the pulse signal using signal source separation techniques (e.g., ICA) <ref type="bibr" target="#b24">[25]</ref>. Other methods derived these parameters from physically-based models to achieve elegant and fast demixing (e.g., Plane Orthogonal-to-Skin (POS)) <ref type="bibr" target="#b35">[36]</ref>. By calculating a projection plane orthogonal to the skin-tone based on optical and physiological principles, the authors were able to achieve a stronger BVP signal-to-noise ratio (SNR).</p><p>Since the underlying relationship between the pulse and skin pixels is complex, deep convolutional neural networks have shown superior performance over the traditional source separation algorithms. DeepPhys <ref type="bibr" target="#b5">[6]</ref> was the first paper that demonstrated that a deep neural network outperforms all the traditional signal processing approaches. Liu et al. have also proposed an on-device efficient neural architecture called MTTS-CAN for camera-based physiological sensing, which leverages a tensor-shift module and 2D-convolutional operations to perform efficient spatial-temporal modeling <ref type="bibr" target="#b11">[12]</ref>. More recently, an adversarial learning approach, called Dual-GAN, has also been studied to learn noise-resistant mappings from video frames to pulse waveform and noise distributions <ref type="bibr" target="#b18">[19]</ref>. With two generative-adversarial networks, they can promote each adversarial network's representation and further improve the feature disentanglement between pulse and various noise sources.</p><p>However, DeepPhys and MTTS-CAN both require a few preprocessing steps including calculating difference frames and performing image normalization. Dual-GAN has a even more complex preprocessing module called MSTMaps proposed by <ref type="bibr" target="#b22">[23]</ref>. The MSTMaps is a multi-scale spatial temporal map by 1) fine-grained facial cropping, 2) landmarker extraction, 3) performing average pooling for every color channel and every ROI combination for each frame, 4) generating ROI combinations using all the detected ROI regions and landmarkers, 5) multiplications of each item in all ROI combinations with six channels, respectively. The final size of MSTMap is (2 n ? 1) ? T ? 6 where T is the number of frames and n is the number of ROI regions. Such a preprocessing module not only consumes large amounts of memory but also introduces a large computational burden to the entire pipeline. Moreover, stacking all of these extra procedures makes development and deployment much more difficult. Unlike these previous works, the goal of our proposed method EfficientPhys is to create a preprocessing-free neural architecture that is simple to use and deploy, efficient on mobile devices, and accurate on settings with various types of noise.</p><p>Visual Transformers. Although convolutional neural networks have been widely studied and used in many computer vision applications, the vision transformer started showing its superior performance on the task image classification task. By training on larger datasets, vision transformer (ViT) attains excellent performance and can be used in downstream fine-tuning with fewer amounts of data <ref type="bibr" target="#b7">[8]</ref>. More recently, the state-of-the-art vision transformer, called Swin Transformer, was proposed to construct hierarchical feature maps and improve computational efficiency by using a hierarchical representation and limiting self-attention computation to non-overlapping local windows while allowing for cross-window connection <ref type="bibr" target="#b13">[14]</ref>. However, transformer architectures have been barely studied in the field of camera-based vital measurement. The closest work is using transformers to detect remote photoplethysmography (rPPG) for attack/spoofing detection <ref type="bibr" target="#b37">[38]</ref>. However, this paper did not evaluate the proposed vision transformer in the task of heart rate estimation using any public datasets, which is considered as the gold-standard benchmark for the field of camera-based vital measurement. To our best knowledge, our proposed vision transformer is the first architecture in camera-based heart rate measurement with detailed evaluation on various public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolution based EfficientPhys</head><p>To enable simple, fast and accurate real-time on-device camera-based vitals measurement, we propose a one-stop-solution architecture that takes raw video frames as the input to the network and outputs a first-derivative PPG signal. The convolution-based EfficientPhys is a one-branch network that contains a custom normalization layer, self-attention module, tensor-shift module and 2D convolution operation to perform efficient and accurate spatial-temporal modeling while making it simple to deploy.</p><p>Normalization Module. Existing neural methods all require different levels of preprocessing before providing the visual representation to the network to learn the underlying relationship between skin pixels and cardiac pulse signal. For instance, The state-of-the-art networks Dual-GAN <ref type="bibr" target="#b18">[19]</ref> and CVD <ref type="bibr" target="#b22">[23]</ref> proposed a hand-crafted spatial-temporal representations called STMaps. These preprocessed representations are generated for each video frame and includes steps of detecting 81 facial landmark points, extracting a set of region of interest (ROI) combinations (2 n ?1 where n is the number of ROIs, n=6) using these landmarks, and averaging pixel values in both the RGB and YUV color spaces, multiplying the 63 ROI combinations with the six channels. These modules not only add significant computational burden ( <ref type="table" target="#tab_5">Table 4</ref> shows that Dual-GAN's preprocessing module takes 275ms per frame) but also make the system more challenging to implement and deploy on real-world computing systems such as mobile devices.</p><p>One of the goals of EfficientPhys is to remove these preprocessing modules completely and provide a one-stop solution. To achieve such simplicity and deployability, we propose a custom normalization module, which can perform motion modeling between every two consecutive frames and normalization to reduce the lighting and motion noise. More specifically, the proposed normalization module includes a difference layer and a batchnorm layer. The difference layer (e.g., torch.diff) computes the first forward difference along the temporal axis of the raw video frames, by subtracting every two adjacent frames. To provide optical basis in our work, equation 1 illustrates the optical grounding of difference frame where D D D k (t) of every two consecutive frames where I(t) is the luminance intensity which is modulated by the specular reflection v v v s (t) and the diffuse reflection v v v d (t) as well as optical sensor's quantization noise v v v n (t).</p><formula xml:id="formula_0">D D D k (t) = (I(t) ? (v v vs(t) +v v v d (t)) +v v vn(t)) ? (I(t ? 1) ? (v v vs(t ? 1) +v v v d (t ? 1)) +v v vn(t ? 1)) (1)</formula><p>However, difference frames could be dramatically different in scale and make it hard for the network to learn meaningful feature representations, especially when the signal of interest is hidden in subtle pixel changes along the temporal axis and noise artifacts can cause significantly larger relative changes. To address this, we add a batch-normalization (batchnorm) layer following the difference layer. Adding a batchnorm layer provides two benefits: 1) it normalizes the difference frames to the same scale within the batch during training, 2) unlike</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Frame</head><p>Output of Diff</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output of BatchNorm Layer</head><p>HandCrafted Normalized Frame vs <ref type="figure">Fig. 3</ref>. Outputs of Diff and batchnorm layers and comparison with normalized frames generated via the hand-crafted process in prior work <ref type="bibr" target="#b5">[6]</ref>. The output from the diff layer is almost black because the difference in skin pixels of consecutive frames is very subtle.</p><p>fixed normalization in previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>, batchnorm provides two learnable parameters ? and ? for scaling (to a different variance) and shifting (to a different mean) and two constant parameters which are the mean ? and the standard deviation ?. Through the learning process, the batchnorm layer can learn the best parameters for removing noise as Equation 2 shows. Without a batchnorm layer, directly applying a difference layer means the frames appear "black"; because the subtle changes of skin pixels in every two consecutive frames are relatively very small. On the other hand, adding a follow-up batchnorm layer will help it learn the normalization function to magnify the subtle changes of skin pixels substantially. The result is not simply a magnification of values but a normalization and magnification. Moreover, we also compare the output batchnorm layer to the hand-crafted normalized frame as shown in <ref type="figure">Fig.3</ref>. The output of batchnorm layer contains more information and qualitative analysis suggests it should be a better tool for skin segmentation after the learning process.</p><formula xml:id="formula_1">N N N k (t) = (? t * D D D k (t) + ? t ) ? ? D D D k ? D D D k<label>(2)</label></formula><p>Self-Attention-Shifted Network. To efficiently capture the rich spatialtemporal information, we propose a self-attention-shifted network (SASN). SASN is built on top of the previous state-of-the-art method for on-device spatialtemporal modeling in optical cardiac measurement -tensor-shift convolutional attention network (TS-CAN) <ref type="bibr" target="#b11">[12]</ref>. TS-CAN has two convolutional branches, one of which takes a preprocessed difference frame representation and one of which takes a normalized appearance frame. The motion branch performs the main spatial-temporal modeling and estimation, and the appearance branch provides attention masks to guide the motion branch to better isolate the pixels of interest (e.g., skin pixels). However, we argue that the attention masks do not have to be obtained through a separate appearance branch and they can be also learned with a single branch end-to-end network. As <ref type="figure">Fig. 2</ref> illustrates, our proposed selfattention-shifted network starts with the custom normalization module discussed in the previous section then continues with two tensor-shifted convolutional operations. After the second and fourth tensor-shifted 2D convolutional layers, we add a self-attention module respectively to help the network minimize the negative effects introduced by tensor shifting as well as motion and lighting noises. The selfattention layers are softmax attention layers with 1D convolutions followed by a sigmoid activation function. Then, normalization is applied to remove the outlying values in the attention mask, and the final normalized attention mask is elementwise multiplied with the output from the tensor-shifted convolution. Equation 3 summarize how our self-attention mechanism works where ts(.) denotes tensor shift operation, ? t c denotes the 2D convolutional kernel followed by the tensor shift module, and ? t a is the 1 ? 1 convolutional kernel for self attention.</p><formula xml:id="formula_2">(? t c ts(N N N k (t)) + b t c ) ? H t W t ? ?(? t a X t ? + b t a ) 2 ? ?(? t a X t ? + b t a ) ? 1<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer based EfficientPhys</head><p>Efficient Spatial-Temporal Video Transformer. Due to the recent success of visual transformers for image and video understanding and the importance of attention mechanisms for this task <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12]</ref>, we also present a visual transformer version of EfficientPhys. For this task, we need a visual transformer to learn both spatial and temporal representations. Several existing video-based visual transformers are based on 3D-embedding tokens and input all the frames into 3D encoder and spatial-temporal attention modules <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>. However, the computational complexity makes that unfavourable for real-time efficient modeling on mobile devices. In the convolutional version we used tensor-shifted 2D convolutions which have been shown to achieve comparable performance as 3D convolutions <ref type="bibr" target="#b11">[12]</ref>. Inspired by this, our proposed transformer based EfficientPhys is based on a 2D visual transformer, Swin transformer <ref type="bibr" target="#b14">[15]</ref>, but with added components that we will describe below.</p><p>Since the 2D Swin transformer is only able to learn spatial features that map raw RGB values to latent representations between a single frame and the target signal (pulse) and does not have ability to model temporal relationships beyond consecutive frames. One of the main contributions of the Swin transformer is the shifted window module which has linear computation complexity and allows crosswindow connection by shifting the window partition and limiting self-attention computation to non-overlapping local windows. Inspired by the idea of shifting of spatial window partitions, we propose to add a tensor-shift module (TSM) <ref type="bibr" target="#b9">[10]</ref> before every Swin transformer block to facilitate information exchange across the temporal axis. The TSM first splits the input tensor into three chunks, shifts the first chunk to the left by one place (advancing time by one frame) and shifts the second chunk to right by one place (delaying time by one frame). All the shifting operations are along temporal axis and performed before the tensor is fed into each transformer block as shown in <ref type="figure">Fig. 2</ref>. By adding the TSM module to the Swin transformer, the new transformer architecture now has the ability to perform efficient spatial-temporal modeling and attention by combining shifting window partitions spatially and shifting frames temporally. It is worth noting that TSM does not introduce any learnable parameters thus the proposed transformer architecture has the same number of parameters as the original Swin transformer. Finally, to enable truly end-to-end inference and learning, we also add the same normalization module proposed in the convolution EfficientPhys to this architecture.</p><p>In summary, the transformer-based EfficientPhys is the first end-to-end transformer architecture for camera-based cardiac pulse measurement that leverages tensor-shift modules and window-partition shift modules to perform efficient spatial-temporal modeling and attention to learn the underlying physiological signal from skin pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Training Data. To help create a robust and generalizable model for cross-dataset evaluation we use two datasets. The first is AFRL <ref type="bibr" target="#b8">[9]</ref>, which includes 300 videos from 25 subjects (17 males and 8 females). For each video, the raw resolution is 658x492 and the sampling rate of the synchronized pulse measurement is 30Hz. The dataset includes videos with a range of head motions. Every participant was instructed to maintain stationary for the first two tasks, and then to perform head motions with increasing rotational velocity in the next four tasks (turning from left to right). Along with AFRL dataset, we also leverage a synthetic avatar video dataset introduced by <ref type="bibr" target="#b20">[21]</ref> where each synthetic video is parameterized and generated with a custom pulse signal, background, facial appearance, and motion. More specifically, the input pulse signal is used to augment skin color and the subsurface radius of skin pixels to mimic the effect of the blood volume pulse on the skin's appearance. Synthetic data such as this introduces greater diversity into the training set and has been shown to effectively help reduce disparities in performance by skin type.</p><p>Testing Data. We use three popular benchmark datasets to evaluate the accuracy of the proposed EfficientPhys. UBFC <ref type="bibr" target="#b2">[3]</ref> is a dataset of 42 videos from 42 subjects, and the raw resolution of each video is 640x480 in a uncompressed 8-bit RGB format. The sampling for synchronized pulse signal is 30 Hz. All of the tasks collected in UBFC are stationary. MMSE <ref type="bibr" target="#b39">[40]</ref> is a dataset including 102 videos from 40 subjects, and the raw resolution of each video is at 1040x1392. The ground-truth waveform for MMSE is blood pressure signal instead of bloodvolume pulse signal, and the sampling rate is 25hz. It is worth noting that MMSE contains a diverse distribution of skin types in Fitzpatrick scale (II=8, III=11, IV=17, V+VI=4). PURE <ref type="bibr" target="#b30">[31]</ref> is a containing 60 videos from 10 subjects. The raw resolution of each video is 640x480, and the sampling rate of ground-truth pulse signal is 60 Hz. PURE includes a diverse set of motion tasks such as steady, talking, slow/fast translation between head movements and the camera plane, small/medium head rotation.</p><p>Implementation &amp; Experiment Details. We implemented both convolution based and transformer based EfficientPhys in PyTorch <ref type="bibr" target="#b23">[24]</ref>. We used an AdamW optimizer to train both networks instead of Adam by introducing additional regularization to reduce the effects of over-fitting through weight decay <ref type="bibr" target="#b16">[17]</ref>. The learning rate we used for Convolutional model was 0.001 while the one for transformer model was 0.0001. Based on empirical studies, we used mean squared error(MSE) loss for training the transformer models and negative pearson loss <ref type="bibr" target="#b33">[34]</ref> for the convolutional model. We trained both models for five epochs with a fixed random seed. We implemented TS-CAN based on the open-sourced code <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>. To calculate the performance metrics, we first applied a band-pass filter to the signal with a cutoff frequency of 0.75 and 2.5Hz (45 beats/minute to 150 beats/minute). We then followed Dual-GAN's evaluation scheme to run peak detection and FFT to get estimated heart rate on each video of UBFC and PURE datasets <ref type="bibr" target="#b17">[18]</ref> and MetaPhys's evaluation schema on MMSE <ref type="bibr" target="#b12">[13]</ref>. We conducted video-level evaluation where we calculated an averaged heart rate for each single video. We calculated three standard metrics for each video: mean absolute error (MAE), root mean squared error (RMSE) and Pearson correlation (?) in heart rate estimations and the corresponding ground-truth heart rates from the blood volume pulse collected via contact oximeter sensor. To explore the efficiency of different architectures on mobile devices, we also conducted experiments on a quad-core Cortex-A72 Raspberry Pi 4B to evaluate the model's performance on an edge device. We performed inference 10 times to get a reliable averaged on-device inference latency for EfficientPhys and TS-CAN. Due to the lack of open-source implementation from Dual-GAN, we were only able to find the implementation of STMaps which is the preprocessing module of Dual-GAN. Thus, we only evaluated the on-device latency for the preprocessing module in Dual-GAN. We also evaluated the latency of POS, CHROM, and ICA as they are traditional signal processing methods and don't have a separate preprocessing module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>EfficientPhys vs. State-of-the-Art. In <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref>, we present results from our proposed EfficientPhys models and the current state-of-the-art neural and signal processing methods. The learning models are all trained on the    <ref type="figure">Fig. 2</ref>. T2 has a much lighter architecture to enable real-time on-device inference which has a depth of <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. EfficientPhys-C denotes the Convolution-based EfficientPhys as shown in the <ref type="figure">Fig.2</ref>. For UBFC and PURE, as <ref type="table" target="#tab_2">Table 1</ref> illustrates, EfficientPhys-C and EfficientPhys-T1 outperform all the existing methods. As <ref type="table" target="#tab_3">Table 2</ref> demonstrates, all the neural methods outperform the signal processing methods. EfficientPhys-T1 achieved and TS-CAN achieved slightly better results than EfficientPhys-C and EfficientPhys-T2. Unfortunately, due to the lack of open source implementation or released models (e.g., Dual-GAN <ref type="bibr" target="#b18">[19]</ref>), we could not successfully replicate their complicated model architecture and conduct cross-dataset evaluation on this comparison.</p><p>To conduct a fair comparison with the current state-of-the-art methods, we followed Dual-GAN <ref type="bibr" target="#b18">[19]</ref> to train our models in PURE only and to test on UBFC as <ref type="table" target="#tab_4">Table 3</ref> shows. Although Dual-GAN outperforms all of other methods, we argue that the margin is relatively small as both Dual-GAN and EfficientPhys-C achieve a Pearson correlation of 0.99. Moreover, according to American National Standards Institute (ANSI) and Consumer Technology Association's standard <ref type="bibr" target="#b1">[2]</ref>, MAPE of ?5 is an acceptable error rate. Various studies have also used this standard to validate FDA approved sensors and systems <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4]</ref>. All the methods in <ref type="table" target="#tab_4">Table 3</ref> have met this recommended bar. Computational Cost and On-Device Latency. <ref type="figure" target="#fig_2">Fig. 4</ref> and the Table 4 summarize the computational cost of the existing neural methods. Again, due to the lack of open source implementation and complex algorithm design, we were not able to replicate every architecture to benchmark its on-device latency. The results show that EfficientPhys-C only takes 40ms to process a single frame and it does not take any extra computational time to perform preprocessing. On the other hand, due to the complexity model architecture and extra time for calculating hand-crafted normalized raw and difference frames, TS-CAN takes 63ms per frame. As mentioned earlier, Dual-GAN has a complicated preprocessing procedure for facial landmark detection, segmentation, color transformation and augmentation. We implemented this and benchmark the preprocessing module on our platform, and it takes 275ms per frame, which is already 7x than the entire computational time of EfficientPhys-C. The estimation network in Dual-GAN also includes 12 2D convolution operations, numerous 1D convolution operations. Thus, we believe it would add a significant amount of computational time on top of the 275ms preprocessing time per frame. The default Transformer-based EfficientPhys (T1) has a unfavorable inference due to its deep architecture design and takes 300ms to process every single frame. After reducing the depth to EfficientPhys-T2, it can achieve same inference time as the EfficientPhys-C. However, EfficientPhys-T2 has a much worse performance on all three benchmark datasets. Convolution vs. Transformer in Camera-Based Vitals Measurement. Although visual transformers have begun to achieve state-of-the-art performance on some vision tasks, it is not the case in the task of video based vitals measurement. Based on the results shown in <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref>, Efficient-C outperforms both Efficient-T1 by 45% of MAE in UBFC and similar performance in MMSE and PURE, while Efficient-C is more than 7x faster in terms of latency. When we shrink the Transformer-based EfficientPhys to a similar complexity as Convolution-based EfficientPhys, the accuracy performance is significantly diminished. The errors from lightweight Transformer-based EfficientPhys-T2 increased 48% of MAE in UBFC, 141% of MAE in PURE and 15% of MAE in MMSE. These results indicate a shallow transformer architecture struggles to model subtle changes of skin pixels in the video. These finding suggest two potential insights. First, further optimizations will be necessary for transformers to outperform, even relatively shallow, convolutional models in this domain, this is possibly especially true when there is not a large amount of high-quality data available. As previous studies have shown <ref type="bibr" target="#b7">[8]</ref>, Transformers usually require more pre-training samples to obtain state-of-the-art accuracy. Unfortunately, currently the amount of data in the field of camera-based vital measurement is limited compared to other visual tasks. Our experiments in <ref type="table" target="#tab_4">Table 3</ref> also support this hypothesis where EfficientPhys-C surpasses both EfficientPhys-T1 and T2 with training only on PURE. We believe synthetic data is one way to help address this issue. Second, the good accuracy-efficiency trade-off for visual transformer might not be scaled to on-device architectures without further work. Since many ondevice neural networks require significantly less amount of computing resources to perform real-time operations, scaling the Transformer architecture down is not ideal as our experimental results of EfficientPhys-T2 have shown.</p><p>Ablation Study. We provide ablation studies on various parameters in EfficientPhys-C and EfficientPhys-T1 in <ref type="table" target="#tab_6">Table 5</ref>. Without self-attention module, MAE of EfficientPhys-C is increased by 21%. Without Normalization Module, in both EfficientPhys-C and EfficientPhys-T1, the MAEs are also increased by 787% and 420%. As <ref type="figure">Figure 3</ref> illustrates, the output of difference layer contains almost black pixels and these results indicate that neural methods are sensitive to the magnitude of pixel values and whether they are zero centered. Finally, without tensor shift module (TSM) in transformer based models, the error is also increased by 300% which indicates TSM plays an important role in exchanging temporal information and dynamics.</p><p>Simplifying Last-Mile ML Deployment. Numerous real-world applications are driven by novel machine learning algorithms. However, deploying these algorithms on different computing platforms has been extremely challenging for various reasons. One of these is researchers sometimes only pay attention to the accuracy of the model and ignore the complexity of the last-mile engineering efforts. In this paper, we address this important issue through our one-stop architecture which takes the unprocessed raw frames and directly outputs the desired signal. This elegant and simple design will not only reduce the burden of engineering required for cross-platform implementations, but also will help the research community to replicate and reproduce results.</p><p>Extensible to Other Signal. Finally, as another potential upside of our end-to-end design and the low latency, we envision EfficientPhys being applied to various other video-based applications. Since the input of our model is raw frames, we believe EfficientPhys can be easily extended to other physiological signals and formats of data such as video-based blood pressure measurement and video understanding &amp; recognition etc. On the other hand, most of the baseline methods we compared with (e.g., Dual-GAN, PulseGAN) require many custom preprocessing operations for video-based measurement which are less useful in other applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Broader Impacts and Ethics Statement</head><p>Recent Information and Communications Technologies for Development (ICT4D) work reconceptualize technology development to empower the people it serves. Machine learning based applications such as health interventions also tend to focus more on the development of the technology itself rather than the people and problems they address. Although many large-scale deep neural network models are trained on the data created by the public, they are often not freely available to the public. In this work we only used data that was collected under informed consent for the purposes of physiological analysis. We also make the trained models accessible and available to more diverse communities as described below.</p><p>During the development of EfficientPhys, we intended that the innovations do not create larger disparities between different populations. By achieving the state-of-the-art accuracy and efficiency as well as our simple and elegant design, we believe EfficientPhys will help make camera-based vitals measurement more widely available to the medical research community and broader community in computing. We also believe that this technology can have a particular impact in low-resource settings where there are greater barriers to access healthcare. We envision our proposed method could, with the appropriate clinical validation and regulatory approval, eventually be used in healthcare applications (e.g., real-time vitals measurement in telehealth appointments). During the COVID-19 pandemic the need for such technology has been clearly highlighted. We contextualize our contributions within the scope of democratizing technology for social good and helping to reduce health disparities with advanced AI technology. However, we are aware that machine learning systems are biased and can propagate inequalities. Before technology such as that presented in this paper is ready for deployment we need to make sure that that is not the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we present a novel method called EfficientPhys to enable simple, fast, accurate camera-based contactless vitals measuremnt. We achieved strong performance with using significant less computational power. With the simple and elegant one-stop design, EfficientPhys also help address the issue of last-time machine learning deployment and reduce health disparity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Video</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>High-level comparison between EfficientPhys and existing deep learning approaches for camera-based vitals measurement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Accuracy-Latency Trade-off in eight different methods. Y-axis denotes the MAE error, and X-axis denotes the latency. The methods in the left-top corner have the best accuracy-latency Trade-off.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Cross-dataset heart rate evaluation on UBFC and PURE (beats per minute). Method MAE? MAPE? RMSE? ? ? MAE? MAPE? RMSE? ? ?</figDesc><table><row><cell></cell><cell cols="2">UBFC [3]</cell><cell cols="2">PURE [31]</cell></row><row><cell>EfficientPhys-C 1.14</cell><cell>1.16%</cell><cell>1.81 0.99 1.33</cell><cell>1.71%</cell><cell>5.99 0.97</cell></row><row><cell>EfficientPhys-T1 2.08</cell><cell>2.53%</cell><cell>4.91 0.96 1.11</cell><cell>1.30%</cell><cell>5.94 0.97</cell></row><row><cell>EfficientPhys-T2 3.07</cell><cell>3.41%</cell><cell>4.78 0.96 2.67</cell><cell>3.22%</cell><cell>9.08 0.92</cell></row><row><cell>TS-CAN[12] 1.70</cell><cell>1.99%</cell><cell>2.72 0.99 2.23</cell><cell>2.25%</cell><cell>3.71 0.98</cell></row><row><cell>POS[36] 3.52</cell><cell>3.36%</cell><cell>8.38 0.90 1.68</cell><cell>1.56%</cell><cell>9.60 0.92</cell></row><row><cell>CHROM[7] 3.10</cell><cell>3.83%</cell><cell cols="3">6.84 0.93 6.23 10.04% 17.18 0.71</cell></row><row><cell>ICA[25] 4.39</cell><cell>4.30%</cell><cell>11.60 0.82 5.70</cell><cell>5.69%</cell><cell>18.10 0.70</cell></row></table><note>MAE = Mean Absolute Error in HR estimation, MAPE = Mean Absolute Error Percentage in HR estimation, RMSE = Root Mean Square Error in HR estimation, ? = Pearson Correlation in HR estimation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Cross-dataset heart rate evaluation on MMSE (beats per minute).</figDesc><table><row><cell></cell><cell cols="2">MMSE [40]</cell></row><row><cell cols="3">Method MAE? MAPE? RMSE? ? ?</cell></row><row><cell>EfficientPhys-C 3.48</cell><cell>4.02%</cell><cell>7.21 0.86</cell></row><row><cell>EfficientPhys-T1 3.04</cell><cell>3.91%</cell><cell>5.91 0.92</cell></row><row><cell>EfficientPhys-T2 3.51</cell><cell>3.96%</cell><cell>6.98 0.88</cell></row><row><cell>TS-CAN[12] 3.04</cell><cell>3.41%</cell><cell>6.55 0.89</cell></row><row><cell>POS[36] 3.79</cell><cell>4.28%</cell><cell>8.47 0.82</cell></row><row><cell>CHROM[7] 3.61</cell><cell>4.50%</cell><cell>7.43 0.85</cell></row><row><cell>ICA[25] 7.96</cell><cell>9.20%</cell><cell>14.02 0.51</cell></row><row><cell cols="3">MAE = Mean Absolute Error in HR estimation, MAPE = Mean Absolute Error Percentage in HR estimation,</cell></row><row><cell cols="3">RMSE = Root Mean Square Error in HR estimation, ? = Pearson Correlation in HR estimation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Cross dataset evaluation with models trained on PURE only and tested on UBFC (beats per minute).</figDesc><table><row><cell cols="3">Method MAE? MAPE? RMSE? ? ?</cell></row><row><cell>EfficientPhys-C 2.04</cell><cell>2.25%</cell><cell>3.06 0.99</cell></row><row><cell>EfficientPhys-T1 3.83</cell><cell>4.32%</cell><cell>5.62 0.87</cell></row><row><cell>EfficientPhys-T2 3.97</cell><cell>4.35%</cell><cell>5.91 0.94</cell></row><row><cell>TS-CAN[12] 1.47</cell><cell>1.56%</cell><cell>2.31 0.99</cell></row><row><cell>Dual-GAN[19] 0.74</cell><cell>0.73%</cell><cell>1.02 0.99</cell></row><row><cell>PulseGAN[30] 2.09</cell><cell>2.23%</cell><cell>4.42 0.99</cell></row></table><note>MAE = Mean Absolute Error in HR estimation, MAPE = Mean Absolute Error Percentage in HR estimation, RMSE = Root Mean Square Error in HR estimation, ? = Pearson Correlation in HR estimation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>On-Device data preprocessing latency and model inference latency per frame (ms). Preprocessing and model latency on Raspberry Pi 4B per frame. same datasets (AFRL + Synthetic) and tested on three dataset (UBFC, PURE and MMSE) to test if the model can be generalize to videos with a different facial appearance, background and lighting. To investigate how the depth of the network impacts the Transformer architecture, we created two version of Transformer-based EfficientPhys: T1 and T2. T1 uses the same depth as the Swin Transformer reported in [15] ([2, 2, 6, 2]). Each number indicates the number of Swin Transformer blocks as illustrated in</figDesc><table><row><cell></cell><cell cols="3">Preprocessing Model Total</cell></row><row><cell>Method</cell><cell>(ms) ?</cell><cell cols="2">(ms) ? (ms) ?</cell></row><row><cell>EfficientPhys-C</cell><cell>0</cell><cell>40</cell><cell>40</cell></row><row><cell>EfficientPhys-T1</cell><cell>0</cell><cell>300</cell><cell>300</cell></row><row><cell>EfficientPhys-T2</cell><cell>0</cell><cell>40</cell><cell>40</cell></row><row><cell>TS-CAN[12]</cell><cell>3</cell><cell>60</cell><cell>63</cell></row><row><cell>Dual-GAN[19]</cell><cell>275</cell><cell cols="2">N/A &gt; 275</cell></row><row><cell>POS[36]</cell><cell>0</cell><cell>27</cell><cell>27</cell></row><row><cell>CHROM[7]</cell><cell>0</cell><cell>28</cell><cell>28</cell></row><row><cell>ICA[25]</cell><cell>0</cell><cell>31</cell><cell>31</cell></row><row><cell>ms =</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on EfficientPhys-C (Left) an EfficientPhys-T1 (Right). Models are trained on PURE only and tested on UBFC.</figDesc><table><row><cell cols="4">Self-Attention Diff BatchNorm MAE</cell><cell cols="3">TSM Normal. Module MAE</cell></row><row><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>2.04 2.43 16.06 16.06</cell><cell>? ? ?</cell><cell>? ? ?</cell><cell>3.83 16.10 11.52</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<idno>arXiv: 2103.15691</idno>
		<ptr target="http://arxiv.org/abs/2103.15691" />
		<title level="m">ViViT: A Video Vision Transformer</title>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Physical activity monitoring for heart rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Association</surname></persName>
		</author>
		<idno>ansi/cta-2065</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised skin tissue segmentation for remote photoplethysmography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bobbia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Macwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Validity and reliability of physiological data in applied settings measured by wearable technology: A rapid systematic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Barrios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Jolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Navalta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technologies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepphys: Video-based physiological measurement using convolutional attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="349" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1805.07888</idno>
		<idno>arXiv: 1805.07888</idno>
		<ptr target="http://arxiv.org/abs/1805.07888" />
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust pulse rate from chrominance-based rppg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jeanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2878" to="2886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recovering pulse rate during motion artifact with a multi-imager array for non-contact imaging photoplethysmography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Estepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Blackford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Meier</surname></persName>
		</author>
		<idno type="DOI">10.1109/SMC.2014.6974121</idno>
		<ptr target="https://doi.org/10.1109/SMC.2014.6974121" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Systems, Man, and Cybernetics (SMC)</title>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1062" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fromm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03790</idno>
		<title level="m">Multi-task temporal shift attention networks for on-device contactless vitals measurement</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals Measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fromm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fromm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01773</idno>
		<idno>arXiv: 2010.01773</idno>
		<ptr target="http://arxiv.org/abs/2010.01773" />
		<title level="m">MetaPhys: Few-Shot Adaptation for Non-Contact Physiological Measurement</title>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<idno>arXiv: 2103.14030</idno>
		<ptr target="http://arxiv.org/abs/2103.14030" />
		<title level="m">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Dual-GAN: Joint BVP and Noise Modeling for Remote Physiological Measurement p</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual-gan: Joint bvp and noise modeling for remote physiological measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12404" to="12413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11547</idno>
		<title level="m">Camera measurement of physiological vital signs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12949</idno>
		<idno>arXiv: 2010.12949</idno>
		<ptr target="http://arxiv.org/abs/2010.12949" />
		<title level="m">Advancing Non-Contact Vital Sign Measurement using Synthetic Avatars</title>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accuracy of consumer wearable heart rate measurement during an ecologically valid 24-hour period: intraindividual validation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR mHealth and uHealth</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10828</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video-based remote physiological measurement via cross-verified feature disentangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="295" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Advancements in noncontact, multiparameter physiological measurements using a webcam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="11" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeprhythm: Exposing deepfakes with attentional visual heartbeat rhythms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4318" to="4327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Assessing heart rate using consumer technology association standards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Reece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Navalta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technologies</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Noncontact monitoring breathing pattern, exhalation flow rate and pulse transit time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tsow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2760" to="2767" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accuracy in wrist-worn, sensor-based measurements of heart rate and energy expenditure in a diverse cohort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shcherbina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Mattsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Waggott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salisbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Christle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Ashley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personalized medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pulsegan: Learning to generate realistic pulse waveforms in remote photoplethysmography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1373" to="1384" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-contact video-based pulse rate measurement on a mobile service robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 23rd IEEE International Symposium on Robot and Human Interactive Communication</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1056" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Heart rate measurement based on a time-lapse image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical engineering &amp; physics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="853" to="857" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Assessment of Vasoactive Agents and Vascular Aging by the Second Derivative of Photoplethysmogram Waveform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ibukiyama</surname></persName>
		</author>
		<idno type="DOI">https:/www.ahajournals.org/doi/10.1161/01.HYP.32.2.365</idno>
		<ptr target="https://www.ahajournals.org/doi/10.1161/01.HYP.32.2.365" />
	</analytic>
	<monogr>
		<title level="j">Hypertension</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="370" />
			<date type="published" when="1998-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Siamese-rppg network: Remote photoplethysmography signal estimation from face videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Tsou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th annual ACM symposium on applied computing</title>
		<meeting>the 35th annual ACM symposium on applied computing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Remote plethysmographic imaging using ambient light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Verkruysse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Svaasand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="21434" to="21445" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Algorithmic Principles of Remote PPG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Den Brinker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stuijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBME.2016.2609282</idno>
		<ptr target="https://doi.org/10.1109/TBME.2016.2609282" />
	</analytic>
	<monogr>
		<title level="m">conference Name: IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1479" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Photoplethysmography imaging: a new noninvasive and noncontact method for mapping of the dermal perfusion changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blazek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optical Techniques and Instrumentation for the Measurement of Blood Composition, Structure, and Dynamics</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">4163</biblScope>
			<biblScope unit="page" from="62" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transrppg: Remote photoplethysmography transformer for 3d mask face presentation attack detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02419</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multimodal spontaneous emotion corpus for human behavior analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
