<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianlong</forename><surname>Gu</surname></persName>
							<email>nianlong@ini.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Neuroinformatics</orgName>
								<orgName type="institution">University of Zurich and ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><surname>Ash</surname></persName>
							<email>ashe@ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Humanities, Social and Political Sciences</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">H R</forename><surname>Hahnloser</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Neuroinformatics</orgName>
								<orgName type="institution" key="instit1">University of Zurich</orgName>
								<orgName type="institution" key="instit2">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture, MemSum obtains state-of-the-art testset performance (ROUGE) in summarizing long documents taken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the importance of local, global, and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from MemSum's awareness of extraction history.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic text summarization is the task of automatically summarizing a long document into a relatively short text while preserving most of the information <ref type="bibr" target="#b26">(Tas and Kiyani, 2007)</ref>. Text summarization methods can be categorized into abstractive and extractive summarization <ref type="bibr" target="#b3">(Gambhir and Gupta, 2017;</ref><ref type="bibr" target="#b20">Nenkova and McKeown, 2012)</ref>. Given a document d consisting of an ordered list of N sentences, extractive summarization aims to pick up M (M N ) sentences as the summary of the document. The extracted summaries tend to be both grammatically and semantically more reliable than abstractive summaries <ref type="bibr" target="#b13">(Liu* et al., 2018;</ref><ref type="bibr" target="#b14">Liu and Lapata, 2019a;</ref><ref type="bibr" target="#b16">Luo et al., 2019;</ref><ref type="bibr" target="#b11">Liao et al., 2020)</ref>, as they are directly selected from the source text.</p><p>Extractive summarization is usually modeled as two sequential phases <ref type="bibr">(Zhou et al., 2018)</ref>: 1) sentence scoring and 2) sentence selection. In the  sentence scoring phase, an affinity score is computed for each sentence by neural networks such as bidirectional RNNs <ref type="bibr" target="#b1">(Dong et al., 2018;</ref><ref type="bibr" target="#b19">Narayan et al., 2018;</ref><ref type="bibr" target="#b16">Luo et al., 2019;</ref><ref type="bibr">Xiao and Carenini, 2019)</ref> or <ref type="bibr">BERT (Zhang et al., 2019;</ref><ref type="bibr" target="#b15">Liu and Lapata, 2019b)</ref>. In the sentence selection phase, sentences are selected by either i) predicting a label (1 or 0) for each sentence based on its score, and selecting sentences with label 1 <ref type="bibr">(Zhang et al., 2019;</ref><ref type="bibr" target="#b15">Liu and Lapata, 2019b;</ref><ref type="bibr">Xiao and Carenini, 2019)</ref>, or ii) ranking sentences based on their scores and selecting the top K sentences as the summary <ref type="bibr" target="#b19">(Narayan et al., 2018)</ref>, or iii) sequentially sampling sentences without replacement, where the normalized scores of the remaining sentences are used as sampling likelihoods <ref type="bibr" target="#b1">(Dong et al., 2018;</ref><ref type="bibr" target="#b16">Luo et al., 2019)</ref>.</p><p>In these approaches, sentence scores are generally not updated based on the current partial summary of previously selected sentences, indicating a lack of knowledge of extraction history. We deem extractive summarizers that are not aware of the extraction history to be susceptible to redundancy in a document, because they will repeatedly add sentences with high scores to a summary, regardless of whether similar sentences have been selected before. And, redundancy leads to performance decreases evaluated by ROUGE F1.</p><p>In this paper, we propose to model extractive summarization as a multi-step episodic Markov Decision Process (MDP). As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, at each time step in an episode, we define a sentence state composed of three sub-states: 1) the local content of the sentence, 2) the global context of the sentence within the document, and 3) information on the extraction history, including the previously selected set of unordered sentences and the remaining sentences. At each time step, the policy network (agent) takes the current sentence state as input and produces scores used to select an action of either stopping the extraction process or selecting one of the remaining sentences into the candidate summary. Unlike one-step episodic MDP-based models <ref type="bibr" target="#b19">(Narayan et al., 2018;</ref><ref type="bibr" target="#b1">Dong et al., 2018;</ref><ref type="bibr" target="#b16">Luo et al., 2019)</ref> that encode the state information only once at the beginning of the episode, in our multi-step policy, the agent updates at each time step the extraction history before selecting an action. Such a step-wise state-updating strategy enables the agent to consider the content of the partial summary when selecting a sentence.</p><p>To efficiently encode local and global sentence states, we design an extraction agent based on LSTM networks <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997)</ref>. To encode the extraction history and to select actions, we use a reduced number of attention layers <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> of relatively low dimensionality. These choices enable our model to be easily trainable and to summarize long documents such as scientific papers <ref type="bibr" target="#b0">(Cohan et al., 2018;</ref><ref type="bibr" target="#b7">Huang et al., 2021)</ref> or reports <ref type="bibr" target="#b7">(Huang et al., 2021)</ref>.</p><p>The contributions of our work are as follows: 1) We propose to treat extractive summarization as a multi-step episodic MDP that is aware of the extraction history. 2) We show that extraction-history awareness allows our model to extract more compact summaries than models without history awareness and behave more robustly to redundancies in documents. 3) Our model outperforms both extractive and abstractive summarization models on PubMed, arXiv <ref type="bibr" target="#b0">(Cohan et al., 2018)</ref>, and GovReport <ref type="bibr" target="#b7">(Huang et al., 2021)</ref> datasets. 4) Finally, human evaluators rate the MemSum summaries to be of higher quality than those from a competitive approach, especially by virtue of lower redundancy 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Extraction history awareness was previously considered in NeuSum <ref type="bibr">(Zhou et al., 2018)</ref>, where a GRU encoded previously selected sentences into a hidden vector that then was used to update the scores of the remaining sentences to bias the next selection. NeuSum contains no stopping mechanism and therefore it can only extract a fixed number of sentences, which likely is sub-optimal. Also, the potential benefits of extraction history have not been quantified and so the idea remains unexplored to a large extent.</p><p>Recently, BERT-based extractors such as Match-Sum <ref type="bibr">(Zhong et al., 2020)</ref> achieved SOTA performance in extractive summarization of relatively short documents from the CNN/DM <ref type="bibr" target="#b5">(Hermann et al., 2015)</ref> dataset. However, the quadratic computational and memory complexities <ref type="bibr" target="#b7">(Huang et al., 2021)</ref> of such models limit their scalability for summarizing long documents with thousands of tokens, which is common for scientific papers and government reports. Although large pre-trained transformers with efficient attention <ref type="bibr" target="#b7">(Huang et al., 2021)</ref> have been adapted for abstractive summarization of long documents, we believe that extractive summarization is more faithful in general, which is why we chose an extractive approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>This section outlines the multi-step episodic MDP policy for extractive summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Policy Gradient Methods</head><p>In an episodic task with a terminal state (i.e. end of summary), policy gradient methods aim to maximize the objective function J(?) = E ? ? [R 0 ], where the return R t = T k=t+1 r k is the cumulative reward from time t + 1 until the end of the episode when the summary is complete. In applications of RL to extractive summarization, the instantaneous reward r t is zero except at the end of the episode when the final reward r is computed according to Equation (1), so R t ? R 0 = r. The reward r is usually expressed as <ref type="bibr" target="#b1">(Dong et al., 2018)</ref>:</p><formula xml:id="formula_0">r = 1 3 (ROUGE-1 f + ROUGE-2 f + ROUGE-L f )</formula><p>(1) According to the REINFORCE algorithm <ref type="bibr" target="#b28">(Williams, 1992)</ref>, the policy gradient is defined as:</p><formula xml:id="formula_1">?J(?) = E ? [R t ? log ?(A t |S t , ?)],<label>(2)</label></formula><p>where ?(A t |S t , ?) denotes the likelihood that at time step t the policy ? ? selects action A t given the  <ref type="bibr" target="#b25">(Sutton and Barto, 2018)</ref>:</p><formula xml:id="formula_2">? t+1 ? ? t + ?R t ? log ?(A t |S t , ? t ), (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-step Episodic MDP Policy</head><p>Different from one-step episodic MDP policies <ref type="bibr" target="#b19">(Narayan et al., 2018;</ref><ref type="bibr" target="#b1">Dong et al., 2018;</ref><ref type="bibr" target="#b16">Luo et al., 2019)</ref> that extract the entire summary via a single action, we define an episode, i.e., the generation of a summary, consisting of multiple time steps. At each time step t, corresponding to extracting sentence number t, the action A t is either to stop extraction or to select a sentence s at from the remaining sentences. The agent's policy is:</p><formula xml:id="formula_3">?(A t |S t , ? t ) = p(stop|S t , ? t )p(a t |stop, S t , ? t ) p(a t |stop, S t , ? t ) = ? ? ? ua t (St,?t) j?I t u j (St,?t) if stop = false 1 |It| if stop = true,<label>(4)</label></formula><p>where I t denotes the index set of remaining sentences at time step t. If the agent does not stop, it first computes a score u j for each remaining sentence and samples a sentence s at according to the probability distribution of normalized scores. When the agent stops the extraction, no sentence is selected and the conditional likelihood p(a t |stop=false, S t , ? t ) is set to 1 |It| (where |I t | represents the number of remaining sentences at time t), which is independent of the policy parameters to prohibit the gradient from being passed to the policy parameters via the conditional likelihood. After calculating the reward according to Equation (1), the policy parameters are updated according to Equation (3) (for all time steps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Policy Network</head><p>The state S t in Equation <ref type="formula" target="#formula_3">(4)</ref> is designed to be informative on: 1) the local content of the sentence, 2) the global context of the sentence within the document, and 3) the current extraction history. To encode these three properties in the state, we use a local sentence encoder, a global context encoder, and an extraction history encoder, respectively. Subsequently, the state is mapped by an extractor to an output score for each of the remaining sentences and the extraction stop signal. The overall framework of our model is depicted in <ref type="figure">Figure 2</ref>.</p><p>In the Local Sentence Encoder (LSE), ordered words (w 1 , w 2 , . . . w M ) in a sentence s i are first mapped onto word embeddings using a word embedding matrix. Subsequently, a N l -layer bidirectional LSTM <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997)</ref> transforms the word embeddings and maps them onto sentence embeddings l s i via a multihead pooling layer (MHP) <ref type="bibr" target="#b14">(Liu and Lapata, 2019a)</ref>.</p><p>The Global Context Encoder (GCE) consists of a N g -layer bi-LSTM that takes the L local sentence embeddings (l s 1 , l s 2 , . . . l s L ) as inputs and produces for each sentence s i an embedding g s i that encodes global contextual information such as the sentence's position in the document and information on neighboring sentences.</p><p>The Extraction History Encoder (EHE) encodes the extraction history information and produces the extraction history embedding h s r i for each remaining sentence s r i . The EHE is composed of a stack of N h identical layers. Within one layer, there are two multi-head attention sublayers, as contained in the transformer decoder in <ref type="bibr" target="#b27">Vaswani et al. (2017)</ref>. One sublayer is used to perform multi-head self-attention (MHA) among the local embeddings of the remaining sentences, so that each remaining sentence can capture the context provided by other remaining sentences. The other attention sublayer is used to perform multi-head attention over the embeddings of extracted sentences to enable each remaining sentence to attend to all the extracted sentences. The output of the two attention sublayers, one for each remaining sentence, captures the contextual information of both extracted and remaining sentences. The final output of the N th h layer of the EHE constitutes the extraction history embedding, one for each remaining sentence.</p><p>There is no positional encoding and the EHE produces the extraction history embeddings nonautoregressively by attending to both precedent and subsequent positions. Consequently, the extraction history embeddings h s r i for the remaining sentences are invariant to the order of the previously selected sentences. We believe that the sequential information of previously selected sentences is not crucial for reducing redundancy and for deciding whether to stop extraction or not.</p><p>The Extractor computes the score of each remaining sentence and outputs an extraction stop signal. As input to the extractor, we form for each of the remaining sentences s r i an aggregated embedding by concatenating the local sentence embedding l s r i , the global context embedding g s r i , and the extraction history embedding h s r i . As shown in <ref type="figure">Figure 2</ref>, to produce the score u s r i , the concatenated embedding of remaining sentence s r i is passed to fully connected layers with ReLU activation and then projected to a scalar by a Linear-1 layer followed by a sigmoid function. Note that the same fully connected layers are applied identically to all remaining sentences. We deem that the extractor can learn to stop extraction based on the remaining sentences' states. Therefore, we apply an MHP to the last hidden vectors of all remaining sentences to output a single vector. This vector is then passed to a linear layer with a sigmoid function, producing a stopping probability p stop .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>We train the parameterized policy network according to the update rule in Equation (3). At each training iteration, an episode is sampled to compute the final return r and the action probabilities ?(A t |S t , ? t ) for all time steps t. An example episode with T extracted sentences looks like: (S 0 , s a 0 , . . . , S T ?1 , s a T ?1 , S T , A stop , r), where S t represents the concatenated state information introduced in Section 3.3, s at represents the selection of sentence a t , A stop represents the extraction stops at the final time step T , and r is the reward as defined in Equation <ref type="formula">(1)</ref>. To encourage the agent to select compact summaries, we multiply the final reward r by a length penalty term 1/(T + 1) <ref type="bibr" target="#b16">(Luo et al., 2019)</ref>. Consequently, the return R t ? r T +1 .</p><p>Algorithm 1 The training algorithm. Parameters: learning rate ? Compute the action probability ?(A t |S t , ?) according to Equation <ref type="formula" target="#formula_3">(4)</ref> 12:</p><formula xml:id="formula_4">1: for each document-summary pair (D i , G i ) do 2: LSE outputs local sent. embed l s 1 ,. . . ,l s L 3: GCE outputs global context embed g s 1 ,. . . ,g s L 4: Sample an episode S 0 ,s a 0 ,. . . ,S T ?1 ,s a T ?1 , S T ,A stop ,r from the high-ROUGE episodes set E p of document D i 5:</formula><formula xml:id="formula_5">? ? ? + ? r T +1 ? log ?(A t |S t , ?)</formula><p>Algorithm 1 summarizes the training procedure of MemSum. We initialize the extraction history embeddings to 0, because at t = 0 no sentences have been extracted. E t represents the number of sentences that have been extracted into the summary up to time step t. Following the strategy in <ref type="bibr" target="#b19">Narayan et al. (2018)</ref> and <ref type="bibr" target="#b17">Mohsen et al. (2020)</ref>, instead of sampling an episode following the current policy ?(?|?, ? t ), we sample an episode from a set E p of episodes with high ROUGE scores, which enables the agent to quickly learn from optimal policies and to rapidly converge. Details on creating a set of high-ROUGE episodes for training are described in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we report implementation details of our model and describe the datasets used for training and for evaluation. Datasets. The documents to be summarized in the PubMed and arXiv datasets <ref type="bibr" target="#b0">(Cohan et al., 2018)</ref>    <ref type="bibr">(Gidiotis and Tsoumakas, 2020)</ref>, and Hepos <ref type="bibr" target="#b7">(Huang et al., 2021</ref>) that achieved the state-of-the-art in long document summarization using a large-scale pretrained BART model <ref type="bibr" target="#b10">(Lewis et al., 2020)</ref> with memory-efficient attention encoding schemes including Locality Sensitive Hashing <ref type="bibr" target="#b9">(Kitaev et al., 2020</ref>) (Hepos-LSH) and Sinkhorn attention (Hepos-Sinkhorn). We also present the performance of the oracle extraction model based on the greedy approach <ref type="bibr" target="#b18">(Nallapati et al., 2017)</ref> which sequentially selects from the document the sentence that maximally improves the average of R-1 and R-2 of selected sentences. Implementation Details. We computed local sentence embeddings using pretrained Glove word embeddings <ref type="bibr" target="#b22">(Pennington et al., 2014)</ref> of dimension d = 200, keeping the word embeddings fixed during training. For the LSE, we used N l = 2 bi-LSTM layers and for the GCE N g = 2. For the EHE, we used N h = 3 attention layers, and we set the number of attention heads to 8 and the dimension of the feed-forward hidden layer to 1024; during training we set the dropout rate to 0.1. The extractor consisted of 2 fully-connected hidden layers with output dimensions 2d and d, respectively. We trained our model using the Adam optimizer with ? 1 = 0.9, ? 2 = 0.999 (Kingma and Ba, 2015), fixed learning rate ? = 1e ?4 , and weight decay 1e ?6 . The training was stopped when the validation performance started to degrade. During validating and testing, the agent extracted sentences in a deterministic way: after computing the scores u s r i for the remaining sentences and the stop likelihood p stop , the agent stopped the extraction if p stop ? p thres or if the maximum admissible number N max of extracted sentences was reached; otherwise, the agent selected the sentence with the largest score. The model was trained on eight RTX 2080 Ti GPUs.</p><p>On the validating datasets we selected the best checkpoint of each model and determined the optimal N max and stopping criterion p * thres . For Pubmed, arXiv, Pubmed trunc , and GovReport, N max was set to 7, 5, 7, and 22, and p * thres was set to 0.6, 0.5, 0.8, and 0.6, respectively. For the detailed selection procedure of the optimal stopping threshold, see Appendix D. Information on reproducibility is available in Appendix I. Evaluation. We evaluated the performance of our model using F 1 ROUGE <ref type="bibr" target="#b12">(Lin, 2004)</ref>, including ROUGE-1,2, and L for measuring unigram, bigram, and longest common subsequence. We also conducted human evaluation in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Here we present the results on various extractive summarization tasks and analyze the contribution of different modules via ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results Comparison</head><p>By comparing with extractive baselines on the PubMed and arXiv datasets, we observed that models utilizing extraction history, such as NeuSum and our MemSum, perform significantly better than other models, revealing the effectiveness of the extraction history. MemSum also significantly outperformed NeuSum, suggesting a better utilization of extraction history, which we ascribed to the following factors: 1) In MemSum, we treat stopping extraction also as an action and train the policy network to output a stopping probability. There-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PubMed arXiv  <ref type="table">Table 2</ref>: Results on the PubMed and arXiv test sets. "*" indicates that they are statistically significant in comparison to the closest baseline with a 95% bootstrap confidence interval estimated by the ROUGE script 2 .</p><formula xml:id="formula_6">R-1 R-2 R-L R-1 R-2 R-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>PubMed trunc GovReport  fore, MemSum is able to automatically stop extracting at an optimal time step based on extraction history, while NeuSum can only extract a predefined number of sentences; 2) With the policy gradient method REINFORCE we can train MemSum to maximize the ROUGE score directly, while in NeuSum the loss was set to the KL-divergence between the model-computed sentence scores and the ROUGE score gains at each step, which is less intuitive. We further compare MemSum with NeuSum via human evaluation in Section 5.4.</p><formula xml:id="formula_7">R-1 R-2 R-L R-1 R-2 R-L<label>ORACLE</label></formula><p>We observed that the ROUGE performance on the PubMed trunc dataset is significantly lower than that on the PubMed dataset, with a 16.87 drop in R-1 for the extractive oracle and a 6.23 drop in R-1 for MemSum, indicating that the introduction section is not sufficient to generate summaries close to the ground truth (abstracts). Even so, our model still significantly outperformed MatchSum on PubMed trunc , and we attribute this improvement 2 https://pypi.org/project/rouge-score/   <ref type="table" target="#tab_7">Table 4</ref>: Comparison of the summary extracted by MemSum and the summary abstractively generated by Hepos-Sinkhorn <ref type="bibr" target="#b7">(Huang et al., 2021)</ref>. Compared with the abstractive summary, the MemSum-extracted summary has higher overlap with the human-written summary.</p><p>to the fact that MatchSum truncates the introduction section further to 512 tokens because it needs to compute document embeddings using Bert. Consequently, MatchSum extracts sentences mainly from the first 15 sentences of the document, while our MemSum produces a similar distribution of extracted sentence positions as the extractive oracle, <ref type="figure" target="#fig_3">Figure 3</ref>. Thus, summarizing long documents is a non-trivial task, and models that work well on summarizing short documents (e.g., CNN/DM) may fail to generalize to long documents. MemSum also significantly outperformed the state-of-the-art abstractive summarization model Hepos as measured by ROUGE scores, especially on the GovReport dataset. A comparison of an exemplary MemSum-extracted summary and the corresponding Hepos-Sinkhorn-generated summary from the GovReport dataset (  the MemSum-extracted summary is more accurate than the Hepos-Sinkhorn-generated summary and has higher overlap with the gold summary. We deem that this particularly good extraction performance on the GovReport dataset results from the higher "extractiveness" of the gold summaries in the GovReport dataset compared to other datasets, which may be due in part to technical language being difficult to abstractively summarize without a change in meaning. This is evidenced by the fact that the ROUGE scores of the extractive oracle on the GovReport dataset <ref type="table" target="#tab_5">(Table 3)</ref> are higher than those of the PubMed and arXiv datasets <ref type="table">(Table 2)</ref>. Therefore, extractive summarization may be more proper than abstractive summarization due to the requirement of stringent faithfulness of government report summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Test</head><p>We conduct ablation studies by comparing the full MemSum model with the following variations in structures: 1) MemSum w/o LSE, where we obtain local sentence embeddings by replacing the bi-LSTM based LSE by simple averages of word embeddings; 2) MemSum w/o GCE where we remove the GCE; 3) MemSum w/o EHE where we remove EHE, compute the scores for all sentences in one step, and samples sentences following the BanditSum policy <ref type="bibr" target="#b1">(Dong et al., 2018)</ref>; 4) MemSum with GRU-EHE where we use a GRU to encode previously extracted sentences at each time step, and uses the last hidden state as the extraction history embedding for all remaining sentences, following Zhou et al. <ref type="bibr">(2018)</ref>. Meanwhile, we also tested two variations that adopted different stopping mechanisms: 1) Mem-Sum w/o auto-stop that does not stop extraction automatically based on p stop , but that extracts a fixed number of sentences; 2) MemSum with "STOP" that inserts a special stop sentence (e.g. "STOP") into the document, and stops extraction once the agent selects this sentence.  Contribution of Modules. Removing GCE has a greater impact on performance than removing LSE ( <ref type="table" target="#tab_8">Table 5</ref>), suggesting that modeling global contextual information is more critical than modeling local sentence information in our MemSum framework, which contrasts with the result that modeling local sentence information is more important in the Atten-Cont (Xiao and Carenini, 2019) framework. Furthermore, we observed a significant performance degradation when removing EHE, but no significant difference between MemSum and MemSum with GRU-EHE, indicating that EHE is necessary, but our MemSum policy is not strongly dependent on the specific structure of this module (e.g., attention-based or RNN-based).</p><p>Influence of Stopping Mechanisms. MemSum w/o auto-stop achieves lower ROUGE scores than MemSum, revealing the necessity of auto stopping in our MemSum architecture. Meanwhile, Mem-Sum with "STOP" produced summaries with fewer extracted sentences (3.9 vs. 6.0 sentences on average) and significantly lower ROUGE scores. We attribute this reduction to the predictable positive reward obtained from selecting the special stop sentence that ends an episode, which leads to a preference for this final action and increases the likelihood of taking this action prematurely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">History Awareness Avoids Redundancy</head><p>We hypothesized that the extraction history allows MemSum to avoid sentences that are similar to existing sentences in the current partial summary, intuitively mimicking what humans do when extractively summarizing documents. To verify this, we created a redundant PubMed dataset in which we repeated each sentence in the document, with the replicated sentences immediately following the originals. On this dataset, we trained and tested MemSum and MemSum w/o EHE (no history awareness), and we compared different models in terms of ROUGE scores and average duplicate percentage that is defined as the average percent-10 0.00 0.05 0.10 0.15 0.20</p><p>Step 0: pstop = 0.00 &lt; p * thres , selecting the sentence No. 10 49 0.00 0.05 Sentence score</p><p>Step 1: pstop = 0.04 &lt; p * thres , selecting the sentence No. 49 28 0.00 0.05</p><p>Step 2: pstop = 0.36 &lt; p * thres , selecting the sentence No. 28 0 10 20 30 40 Sentence positions 0.00 0.05</p><p>Step 3: pstop = 0.60 p * thres . stopping extraction. <ref type="figure">Figure 4</ref>: The sentence scores of 50 sentences computed by MemSum at extraction steps 0 to 3. In the document, there is artificial redundancy in that the (2n) th and the (2n + 1) th sentences are identical (n = 0, 1, ..., 24).</p><p>age of the duplicated sentences among all extracted sentences in a summary.</p><p>As reported in <ref type="table" target="#tab_10">Table 6</ref>, for MemSum w/o EHE, on average 41% of sentences in the extracted summaries were duplicated. Along with the high duplicate ratio came a significant decrease in ROUGE score. By contrast, the performance of the full MemSum model with history awareness was only slighted affected when comparing the results of the MemSum on the PubMed dataset <ref type="table">(Table 2)</ref> and on the redundant PubMed dataset <ref type="table" target="#tab_10">(Table 6)</ref>.</p><p>Meanwhile, using the Trigram Blocking method that skips a sentence if it has a trigram that overlaps with the current summary <ref type="bibr" target="#b15">(Liu and Lapata, 2019b)</ref> is also successful in avoiding repetitive sentences. However, the ROUGE scores associated with Trigram Blocking were significantly lower than those of the MemSum with awareness of extraction history. In summary, the history-aware MemSum model spontaneously learns an optimized strategy to avoid redundant sentences without explicit human guidance or crude rules, and thus shows better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study: How does MemSum Avoid Redundancy?</head><p>We let MemSum summarize a document sampled from the test set of the redundant PubMed dataset and monitored the sentence scores produced by the Extractor during each extraction step. The results are shown in <ref type="figure">Figure 4</ref>. At time step 0, the 10 th sentence obtained the maximum score and was thus selected into the summary. At time step 1, we noticed that the 11 th sentence, which is a replica of   <ref type="bibr" target="#b29">(Woolson, 2008)</ref>.</p><p>the 10 th sentence, had a score close to zero. The same was also true for the other selected sentences and their following sentences, revealing competent repetition avoidance of the Extractor. Because the EHE is insensitive to the extraction order and to sentence position information, as described in Section 3.3, we can conclude that the full MemSum avoids redundancy by evaluating the similarity between selected and remaining sentences, rather than by "remembering" selected sentences' positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Human Evaluation</head><p>We conducted human evaluation following Wu and Hu (2018); <ref type="bibr" target="#b1">Dong et al. (2018)</ref>; <ref type="bibr" target="#b16">Luo et al. (2019)</ref>. For each document sampled from the test set of the PubMed dataset, we provide a reference summary, and volunteers are asked to rank a pair of randomly ordered summaries produced by two models according to three criteria: non-redundancy, coverage, and overall quality. The better model will be ranked #1 while the other is ranked #2, and if both models extract the same summary, then they will both get the #1 rank. In experiment 1, we compared NeuSum, which always extracts 7 sentences, and MemSum, which extracts a flexible number of sentences thanks to automatic stopping. In experiment 2, we discounted for differences in the number of extracted sentences by making MemSum w/o autostop to also extract 7 sentences. A user-friendly interactive web interface was implemented to assist the evaluation process, with details in Appendix G. <ref type="table" target="#tab_12">Table 7</ref> reports the human evaluation results for both experiments. Both MemSum and MemSum w/o auto-stop ranked significantly higher (p&lt;0.005) than NeuSum in terms of non-redundancy and achieved a better average overall quality. In terms of word count, MemSum produces shorter summaries than NeuSum in both experiments, even though both models extract the same number of sentences in experiment 2. These results show that redundancy avoidance of MemSum is particularly good, even without the auto-stop mechanism. The slightly better performance of NeuSum in terms of coverage needs to be weighed against it extracting significantly longer summaries. Note that neither NeuSum nor our model is trained to optimize the order of the extracted sentences. Therefore, we did not use fluency, which depends on sentence order, as a metric for human evaluation. Improving the fluency of the extracted summaries will be the subject of our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Extractive summarization can be achieved effectively with a multi-step episodic Markov decision process with history awareness. Using encoders of local sentence, global context, and extraction history, MemSum is given information that is intuitively also used by humans when they summarize a document. Awareness of the extraction history helps MemSum to produce compact summaries and to be robust against redundancy in the document. As a lightweight model (Appendix C), Mem-Sum outperforms both extractive and abstractive baselines on diverse long document summarization tasks. Because MemSum achieves SOTA performance on these tasks, MDP approaches will be promising design choices for further research. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Computing Hardware</head><p>We trained our MEMSUM model and its variations on 8 NVIDIA GeForce RTX 2080 Ti 11GB GPUs. During testing, we used a single NVIDIA TITAN X Pascal 12GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Comparison of Validating and Testing Performance</head><p>We compare the validating and testing performance of the MemSum model on the following datasets:</p><p>PubMed <ref type="bibr" target="#b0">(Cohan et al., 2018)</ref>, arXiv <ref type="bibr" target="#b0">(Cohan et al., 2018)</ref>, and GovReport <ref type="bibr" target="#b7">(Huang et al., 2021)</ref>. The results are reported in <ref type="table" target="#tab_15">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Summarization Time</head><p>We analyzed the average time taken by MemSum to extractively summarize a source document from the test set. The average summarizaion time is positively correlated with the document length and the number of extracted sentences, <ref type="table" target="#tab_16">Table 9</ref>. On the one hand, on longer documents, it takes longer to compute the scores of remaining sentences, which delays the action of either stopping extraction or    The stopping threshold p thres is an important hyperparameter that sets the stopping probability in an episode, as described in the Implementation Details. We determined the optimal stopping threshold p * thres as follows: For each data set and each stopping threshold p thres ? {0.1, 0.2, . . . , 1.0}, we chose as optimal stopping threshold p * thres the one with maximal ROUGE score on the corresponding validating set. The ROUGE scores as a function of stopping threshold are shown in <ref type="figure" target="#fig_4">Figure 5</ref>, 6 and 8 on the validating set of the PubMed, the arXiv, and the GovReport data set, respectively. The functions exhibit a local maximum between 0.1 and 1.0, which implies that when p thres is too low, summaries tend to be too short, while when p thres is too high, summaries will be unduly lengthy. We chose p * thres = 0.6, 0.5, 0.8 and 0.6 for the PubMed, the arXiv, the PubMed trunc , and the GovReport dataset, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Creating High-ROUGE Episodes for Training</head><p>As introduced in Section 3.4 and Algorithm 1 in the main paper, at each training iteration, we sampled a high-ROUGE episode from the set E p . An episode can be viewed as a sequence of stateaction pairs as well as the final reward, such as (S 0 ,s a 0 ,. . . ,S T ?1 ,s a T ?1 , S T ,A stop ,r). Here, {s a 0 . . . s a T ?1 } is the extracted summary consisting of a set of T sentences, and r is the average of the associated ROUGE-1, ROUGE-2, and ROUGE-L F1 scores.</p><p>In <ref type="bibr" target="#b18">(Nallapati et al., 2017)</ref>, a greedy approach was proposed to select candidate summaries by sequentially selecting from the source document the optimal sentence that maximally improves the average ROUGE-1/2/L score once added to the current subset of selected sentences.</p><p>In this paper, we define a high-ROUGE episodes set E p as the set of multiple episodes where each episode has a high average ROUGE-1/2/L F1 score. To obtain not a single episode in E p but multiple episodes with high average ROUGE-1/2 scores, we modified the greedy approach by considering not only the optimal sentence at each sentence selection step but also B ? 1 sub-optimal sentences. This sentence-sampling step is repeated for each of these B new subsets to result in a potentially exponentially growing number of high ROUGE-score episodes. This process stops until no sentence can further improve the average ROUGE-1/2/L score or a maximum number N max of selected sentences per episode is reached. B can be considered the branching size, analogous to beam search strategies in neural machine translation <ref type="bibr" target="#b24">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Freitag and Al-Onaizan, 2017)</ref>. We set B = 2 by default.</p><p>In practice, we notice that ROUGE-L F1 score is computationally intensive. Because when creating E p we need to iteratively re-compute ROUGE scores once a new sentence is added to the current summary, including the ROUGE-L F1 score into computation would heavily slow down the process of creating the high-ROUGE episodes set for train-ing. As a compromise, we do not incorporate the ROUGE-L F1 score into the intermediate steps of our modified greedy approach. Instead, we calculate the ROUGE-L F1 score only once after a complete high-ROUGE episode is selected, and use this ROUGE-L F1 score together with ROUGE-1/2 F1 scores to compute the reward r for each episode. A similar strategy was adopted in <ref type="bibr">Zhou et al. (2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Padding and Truncation of Sentences and Documents</head><p>In the training process, we used mini-batch gradient descent. To enable efficient batch-wise parallel GPU computation, each document in a mini batch needs to have the same number of sentences, and each sentence needs to have the same number of tokens. Therefore, in order to unify the sentence length to a common value L sen , we appended "PAD" tokens at the end of sentences shorter than L sen , and we truncated sentences longer than L sen .</p><p>To unify the document length in terms of number of sentences to a common value L doc , we appended empty-string sentences at the end of documents shorter than L doc , and truncated documents longer than L doc . To ensure consistency between training and testing we also performed the same padding and truncation setting during testing. We set L doc to 500 for the PubMed, the arXiv, and the GovReport datasets and 50 for the PubMed trunc dataset based on the document length statistics shown in <ref type="table" target="#tab_2">Table 1</ref> in the main paper. We set L sen to 100 for the PubMed, the PubMed trunc , and the GovReport datasets and 150 for the arXiv dataset, because we noticed a larger variance in the length of sentences in the arXiv dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Interactive Web Interface for Human Evaluation</head><p>To provide for a convenient evaluation procedure for volunteers, we designed an interactive web interface based on Jupyter Widgets 3 . As shown in <ref type="figure" target="#fig_7">Figure 9</ref>, for each document, we display the reference summary, summary A, and summary B from left to right. The reference summary contains the ground-truth abstract. Summaries A and B are the summaries extracted by the two models assigned in a random order, so that the volunteers do not know which model either summary came from. Meanwhile, the volunteers were allowed to read the source document by clicking the button "Show Source Document &gt;&gt;&gt;". We also provided a sentence highlighting function to help the volunteers rapidly retrieve relevant content. We allowed evaluators to copy a sentence from the reference summary and paste it to the text box above. After clicking the button "Highlight relevant sentences given a query", relevant sentences in both summaries were highlighted, to help the volunteers rapidly find information of interest. The relevance score of a pair of sentences was given by the cosine similarity of the two sentences' embeddings computed with Sent2vec <ref type="bibr" target="#b21">(Pagliardini et al., 2018)</ref>.</p><p>In the evaluation panel the volunteers selected the better summary (A or B) by comparing the modelproduced summary with the reference summary on three criteria: overall quality, coverage (in terms of information content), and non-redundancy. After making a choice they clicked the button "Submit &amp; Eval Next" to submit the current evaluation result and evaluate the next summaries, or click "Skip" if they were not sure which summary was indeed better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Examples of Extracted Summaries</head><p>We provide summarization examples in <ref type="table" target="#tab_2">Table 10</ref> and 11. In <ref type="table" target="#tab_2">Table 10</ref>, we compared MemSum trained on the arXiv dataset with Dancer Pegasus (Gidiotis and Tsoumakas, 2020) on a typical paper on which MemSum achieved higher ROUGE-1 F score than Dancer Pegasus. In <ref type="table" target="#tab_2">Table 11</ref> we provide the extractive summary of this paper itself using our MemSum model. Sentences with similar meanings in different summaries are highlighted in the same color. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Reproducibility</head><p>The MemSum code and variants of MemSum that we used in our ablation study, as well as the Mem-Sum parameters trained on the PubMed dataset, can be found in the submitted code.zip file. Also, we provide a sample of the datasets used in this paper in the data.zip file, as well as the raw data for the human evaluation. This will ensure that the results in this work are well reproducible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DANCER PEGASUS</head><p>Language model pre-training has been shown to be effective for improving many natural language processing tasks such as sentence-level paraphrasing and entity recognition tasks. However, current approaches to pre-trained language models are restricted to unidirectional language models. In this paper, we propose a new approach to pretrained language models based on bidirectional encoder transformers (BERT). BERT is inspired by the pre-training objective of cloze task <ref type="bibr">(Taylor et al., 1953)</ref>, where the goal is to predict some masked language representations from the input. We introduce BERT and its detailed implementation in this paper. The BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Rich unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROUGE1-F1 36.52</head><p>MemSum Language model pre-training has been shown to be effective for improving many natural language processing tasks. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike <ref type="bibr">Radford et al. (2018)</ref>, which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.</p><p>ROUGE1-F1 44.29 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original abstract</head><p>We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture, MemSum obtains state-of-the-art test-set performance (ROUGE) in summarizing long documents taken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the importance of local, global, and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from MemSum's awareness of extraction history.</p><p>MemSum summary In this paper, we propose to model extractive summarization as a multi-step episodic Markov Decision Process (MDP). As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, at each time step in an episode, we define a sentence state composed of three sub-states: 1) the local content of the sentence, 2) the global context of the sentence within the document, and 3) information on the extraction history, including the previously selected set of unordered sentences and the remaining sentences. To efficiently encode local and global sentence states, we design an extraction agent based on LSTM networks. We show that extraction-history awareness allows our model to extract more compact summaries than models without history awareness and behave more robustly to redundancies in documents. 3) Our model outperforms both extractive and abstractive summarization models on PubMed, arXiv, and GovReport datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROUGE1-F1</head><p>48.57 <ref type="table" target="#tab_2">Table 11</ref>: MemSum summary of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>We modeled extractive summarization as a multi-step iterative process of scoring and selecting sentences. s i represents the i th sentence in the document D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>for each time step: t = 0,1,...,T: do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The position distribution of extracted sentences in the PubMed trunc dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The ROUGE scores for different stopping thresholds p thres on the PubMed validating set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>The ROUGE scores for different stopping thresholds p thres on the arXiv validating set. The ROUGE scores for different stopping thresholds p thres on the PubMed trunc validating set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>The ROUGE scores for different stopping thresholds p thres on the GovReport validating set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>We designed an interactive web interface for the human evaluation experiments introduced in Section 5.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The architecture of our MemSum extractive summarizer with a multi-step episodic MDP policy. With the updating of the extraction-history embeddings h at each time step t, the scores u of remaining sentences and the stopping probability p stop are updated as well.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Policy gradient</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>State</cell></row><row><cell>LSE MHP</cell><cell>GCE</cell><cell>EHE</cell><cell>MHA MHA</cell><cell>Extractor</cell><cell>MHP 1 Linear-</cell><cell>Linear-1 Sigmoid Sigmoid</cell><cell>sample actions</cell><cell cols="2">Update extracted sentences with stop? No Yes Compute ROUGE Reward</cell></row><row><cell>words of</cell><cell>All sentences'</cell><cell>Extracted</cell><cell>Remaining</cell><cell cols="4">Linear-1 Fully connected layers Sigmoid Sentence Concatenated</cell><cell>Sampled</cell><cell>gold summary</cell></row><row><cell>sentence</cell><cell>embeddings</cell><cell>sentences</cell><cell>sentences</cell><cell>Embedding</cell><cell></cell><cell></cell><cell>scores</cell><cell>actions</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Agent</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Action</cell><cell>Environment</cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>state S t . With ? as the learning rate, the parameter update rule is</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>An overview of datasets used in this paper. We count only strings composed of letters and numbers for # of words.</figDesc><table /><note>are the full bodies of scientific papers and the gold summaries are the corresponding abstracts. Zhong et al. (2020) proposed a truncated version of the PubMed dataset (PubMed trunc for simplicity) by defining a doument as the introduction section of a paper. The GovReport dataset (Huang et al., 2021) contains U.S. government reports with gold sum- maries written by experts. Except PubMed trunc , all the other datasets contain significantly longer docu- ments than the popular dataset CNN/DM (Table 1). Baselines. Extractive baselines include Lead (di- rectly using the first several sentences as the summary) (Gidiotis and Tsoumakas, 2020), Sum- maRuNNer (Nallapati et al., 2017), Atten-Cont (Xiao and Carenini, 2019), Sent-CLF and Sent- PTR (Pilault et al., 2020), MatchSum (Zhong et al., 2020), and the NeuSum model (Zhou et al., 2018) that we trained on our datasets. Abstractive summarization models include PE- GASUS (Zhang et al., 2020), BigBird (Zaheer et al., 2020), Dancer</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>MemSum (ours) 43.08* 16.71* 38.30* 59.43* 28.60* 56.69*</figDesc><table><row><cell></cell><cell>45.12</cell><cell>20.33</cell><cell>40.19</cell><cell>75.56</cell><cell>45.91</cell><cell>72.51</cell></row><row><cell cols="3">Extractive summarization baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lead</cell><cell>37.58</cell><cell>12.22</cell><cell>33.44</cell><cell>50.94</cell><cell>19.53</cell><cell>48.45</cell></row><row><cell>MatchSum</cell><cell>41.21</cell><cell>14.91</cell><cell>36.75</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NeuSum</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.94</cell><cell>25.38</cell><cell>55.80</cell></row><row><cell cols="3">Abstractive summarization baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Hepos-LSH</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>55.00</cell><cell>21.13</cell><cell>51.67</cell></row><row><cell>Hepos-Sinkhorn</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.86</cell><cell>22.62</cell><cell>53.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on PubMed trunc and GovReport.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Human-written Summary: (...) While CMS is generally required to disallow, or recoup, federal funds from states for eligibility-related improper payments if the state's eligibility error rate exceeds 3 percent, it has not done so for decades, (...) CMS issued revised procedures through which it can recoup funds for eligibility errors, beginning in fiscal year 2022.</figDesc><table /><note>(...) Hepos-Sinkhorn (abstractive): (...) The selected states also reported that they did not have adequate processes to address these issues. CMS has taken steps to improve its oversight of the Medicaid program, including issuing guidance to states on the use of MAGI- exempt bases for determining eligibility, but these efforts have not been fully implemented. (...) MemSum (ours, extractive): (...) implemented its statutory requirement to recoup funds associated with Medicaid eligibility-related improper pay- ments for states with an eligibility error rate above 3 per- cent through its MEQC program. (...) However, the agency has introduced new procedures through which it can, un- der certain circumstances, begin to recoup funds based on eligibility errors in fiscal year 2022. (...)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>) is consistent with the ROUGE comparison, showing that</figDesc><table><row><cell>Model</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell>MemSum</cell><cell cols="3">49.25 22.94 44.42</cell></row><row><cell>MemSum w/o LSE</cell><cell cols="3">48.12 22.04 43.36</cell></row><row><cell>MemSum w/o GCE</cell><cell cols="3">46.85 20.31 41.95</cell></row><row><cell>MemSum w/o EHE</cell><cell cols="3">48.08 22.77 43.55</cell></row><row><cell cols="4">MemSum with GRU-EHE 49.11 22.86 44.28</cell></row><row><cell>MemSum w/o auto-stop</cell><cell cols="3">48.25 22.63 43.70</cell></row><row><cell>MemSum with "STOP"</cell><cell cols="3">47.18 21.81 42.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the PubMed dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Performance on the redundant PubMed dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: The average ranking of NeuSum and Mem-</cell></row><row><cell>Sum is reported. The smaller the ranking, the better</cell></row><row><cell>the model. Four volunteers participated in these exper-</cell></row><row><cell>iments, and evaluated 67 and 63 pairs of summaries in</cell></row><row><cell>Experiment 1 and 2, respectively. "*" indicates statis-</cell></row><row><cell>tical significance (p&lt;0.005) in a Wilcoxon signed-rank</cell></row><row><cell>test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>PubMed 49.14 22.92 44.33 49.25 22.94 44.42 arXiv 48.23 20.17 42.31 48.42 20.30 42.54 PubMed trunc 43.46 16.77 38.65 43.08 16.71 38.30 GovReport 59.29 28.57 56.46 59.43 28.60 56.69</figDesc><table><row><cell>Datasets</cell><cell>Validating R-1 R-2 R-L</cell><cell>Test R-1 R-2 R-L</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Validating and testing scores of the MemSum model tested on the PubMed, the arXiv and the GovReport datasets.</figDesc><table><row><cell></cell><cell>avg. doc.</cell><cell>Avg. extractive</cell><cell>Avg. extractive</cell></row><row><cell>Datasets</cell><cell>length</cell><cell>summ. length</cell><cell>summ. time</cell></row><row><cell></cell><cell>(words)</cell><cell>(# sentences)</cell><cell>(ms)</cell></row><row><cell>PubMed</cell><cell>2,730</cell><cell>6.0 ? 1.2</cell><cell>91.7 ? 8.6</cell></row><row><cell>arXiv</cell><cell>5,206</cell><cell>4.8? 0.5</cell><cell>114.0 ? 5.0</cell></row><row><cell>PubMed trunc</cell><cell>408</cell><cell>5.3? 1.4</cell><cell>27.7 ? 4.6</cell></row><row><cell>GovReport</cell><cell>7,932</cell><cell>21.7 ? 1.8</cell><cell>197.0 ? 14.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: Average extractive summarization time of</cell></row><row><cell>MemSum on different datasets.</cell></row><row><cell>selecting a sentence. On the other hand, the more</cell></row><row><cell>sentences must be extracted, the more actions are</cell></row><row><cell>needed of selecting sentences within an episode.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>,s A ,S 1 ,s B ,S 2 ,s C ,S 3 ,A stop ,r) as "(s A , s B , s C )" for simplicity. Because permuted episodes (s A , s B , s C ), (s A , s C , s B ), and (s C , s B , s A , s B , s C ), the agent learns to extract s C from {s A , s B }, while under (s C , s B , s A ) it learns to extract s A from {s B , s C }. Thus, history plays a role in both cases.</figDesc><table><row><cell></cell><cell cols="4">to create the training dataset by maxi-</cell></row><row><cell cols="4">mizing ROUGE-2 F1 scores only.</cell><cell></cell></row><row><cell>We</cell><cell>refer</cell><cell>to</cell><cell>an</cell><cell>episode</cell></row><row><cell cols="5">(S 0 A ) have</cell></row><row><cell cols="5">nearly the same average ROUGE-1/2 scores</cell></row><row><cell cols="5">(although ROUGE-L score may differ), we decided</cell></row><row><cell cols="5">to equally sample them with the hope to avoid</cell></row><row><cell cols="5">overfitting. This decision does not interfere with</cell></row><row><cell cols="5">our usage of extraction history, because under</cell></row><row><cell>(s</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Title BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Original abstract We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>Example summaries for Dancer Pegasus(Gidiotis and Tsoumakas, 2020)  and MemSum.</figDesc><table><row><cell>Title</cell><cell>(This paper) MemSum: Extractive Summarization of Long Documents using</cell></row><row><cell></cell><cell>Multi-step Episodic Markov Decision Processes</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code and data are available at https://github. com/nianlonggu/MemSum</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://ipywidgets.readthedocs.io/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We acknowledge support from the Swiss National Science Foundation (grant 31003A_182638) and the NCCR Evolving Language, Swiss National Science Foundation Agreement No. 51NF40_180888. We also thank the anonymous reviewers for their useful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goharian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2097</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bandit-Sum: Extractive summarization as a contextual bandit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Herke Van Hoof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1409</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3739" to="3748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beam search strategies for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3207</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="56" to="60" />
		</imprint>
	</monogr>
	<note>Vancouver. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recent automatic text summarization techniques: A survey. Artif</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahak</forename><surname>Gambhir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-016-9475-9</idno>
	</analytic>
	<monogr>
		<title level="j">Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="66" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2020. A divide-and-conquer approach to the summarization of long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gidiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2020.3037401</idno>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3029" to="3040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient attentions for long document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Parulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1419" to="1436" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving abstractive text summarization with history aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1500</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5081" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1387</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reading like her: Human reading inspired extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3024" to="3034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A hierarchical self-attentive neural extractive summarizer via reinforcement learning (hsasrl)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farida</forename><surname>Mohsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Al-Sabahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3075" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ranking sentences for extractive summarization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1158</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1747" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Survey of Text Summarization Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-3223-4_3</idno>
	</analytic>
	<monogr>
		<title level="m">Mining Text Data</title>
		<editor>Charu C. Aggarwal and ChengXiang Zhai</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2018 -Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On extractive and abstractive neural document summarization with transformer language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.748</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9308" to="9319" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Bradford Book, Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey automatic text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oguzhan</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzad</forename><surname>Kiyani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PressAcademia Procedia</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="205" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00992696</idno>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Woolson</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780471462422.eoct979</idno>
		<title level="m">Wilcoxon Signed-Rank Test</title>
		<imprint>
			<publisher>John Wiley &amp; Sons, Ltd</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
