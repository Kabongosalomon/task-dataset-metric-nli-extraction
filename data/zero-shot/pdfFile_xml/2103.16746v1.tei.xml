<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Shu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
							<email>zhangzhipeng2017@ia.ac.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">NLPR</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
							<email>jiangbo@ahu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
							<email>tianyh@pcl.ac.cn</email>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
							<email>fengwu@ustc.edu.cn.</email>
							<affiliation key="aff5">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* The first two authors contribute equally to this work. Yaowei Wang is the corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tracking by natural language specification is a new rising research topic that aims at locating the target object in the video sequence based on its language description. Compared with traditional bounding box (BBox) based tracking, this setting guides object tracking with high-level semantic information, addresses the ambiguity of BBox, and links local and global search organically together. Those benefits may bring more flexible, robust and accurate tracking performance in practical scenarios. However, existing natural language initialized trackers are developed and compared on benchmark datasets proposed for trackingby-BBox, which can't reflect the true power of trackingby-language. In this work, we propose a new benchmark specifically dedicated to the tracking-by-language, including a large scale dataset, strong and diverse baseline methods. Specifically, we collect 2k video sequences (contains a total of 1,244,340 frames, 663 words) and split 1300/700 for the train/testing respectively. We densely annotate one sentence in English and corresponding bounding boxes of the target object for each video. We also introduce two new challenges into TNL2K for the object tracking task, i.e., adversarial samples and modality switch. A strong baseline method based on an adaptive local-global-search scheme is proposed for future works to compare. We believe this benchmark will greatly boost related researches on natural language guided tracking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single object tracking is one of the most important tasks in computer vision and it has been widely used in many applications such as video surveillance, robotics, and autonomous vehicles. Usually, they initialize the target object in the first frame with a bounding box (BBox), as shown in <ref type="figure">Fig. 1 (a)</ref>, and adjust the BBox along with the movement of the target object. Most of the existing single object trackers <ref type="bibr">[25-27, 64, 66, 80]</ref> are developed based on this setting 1 , and many benchmark datasets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71</ref>] are proposed for this task.</p><p>Although these trackers have been adopted in many applications, however, the setting of tracking-by-BBox still suffers from the following issues. <ref type="bibr" target="#b0">(1)</ref> The target object in the first frame with a BBox is inconvenient to initialize in practical scenarios. In another word, the initialization limits the wide applications of existing BBox initialized trackers. <ref type="bibr" target="#b1">(2)</ref> The initialized BBox may be not optimal for the representation of target object which may lead to ambiguity. As shown in <ref type="figure">Fig. 1 (a)</ref>, the tracker may be confused to track the bike or lower body of the pedestrian. Similar views can also be found in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b77">78]</ref>. (3) Current BBox-based trackers may perform poorly when facing abrupt appearance variation of the target object, like face/cloth changing or species variation in <ref type="figure">Fig. 1 (b)</ref>. Because the appearance feature initialized in the first frame and the object in the tracking procedure are vastly different. Only one sample initialized in the first frame is not enough to handle these challenging scenarios. These observations all inspire us to begin to think about how can we conduct tracking in a more applicable and accurate way? <ref type="figure">Figure 1</ref>. Comparison between the task of tracking-by-BBox and tracking-by-language. We can find that tracking-by-NL can specify target object more accurate and flexibly, and is also good at describing the appearance/species variation.</p><p>Recently, some researchers attempt to introduce the natural language description instead of the BBox for tracking <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b77">78]</ref>, termed tracking by natural language. This setting allows for a new type of human-machine interaction in object tracking. For example, it can enhance existing BBox based trackers by helping them against model drift, or simultaneous multiple-video tracking as noted in <ref type="bibr" target="#b42">[43]</ref>. More importantly, natural language is more convenient and intuitive to express for human beings compared with BBox. It can provide a more precise expression of the target object from spatial location to high-level semantic information like attributes, category, shape, properties, and structural relationship with other objects, etc. This information will be beneficial to address the ambiguity issue of BBox and the vast appearance variation of the target object. Meanwhile, the language can also specify target objects more flexibly, for example, "The player who controls the ball" in <ref type="figure">Fig. 1  (c)</ref>. The intelligent tracker should focus on target players even the ball passed to different persons, without having to re-initialize the target person like the standard setup of visual tracking. However, this research topic has received far less attention than standard target tracking. Only a few works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b77">78]</ref> are developed and compared on tracking benchmark datasets specially designed for BBox based tracking. These benchmarks may fail to nicely reflect the true power of tracking-by-language, and this inspires us to design a new and large-scale benchmark for this task.</p><p>In this work, we collect a large-scale dataset that contains 2, 000 video sequences, named TNL2K. These videos are collected from YouTube 2 , surveillance cameras, and mo-2 https://www.youtube.com/ bile. For each video, we densely annotate the location information of the target object for each frame and one sentence in English for the whole video. Specifically, we describe the category, shape, attributes, properties, and spatial location of the target object which will provide rich fine-grained appearance information and high-level semantic information for tracking. We select 1, 300 videos for training and the rest 700 videos for evaluation. Our videos also reflect two attributes for the tracking task, i.e., the adversarial samples and modality switch between RGB and thermal data. To provide a baseline method for other researchers to compare, we design a simple but strong algorithm based on an adaptive local-global-search scheme. Specifically, three kinds of baseline results are provided, i.e., Tracking-by-BBox, Tracking-by-Language, Tracking-by-BBox and Language.</p><p>The contributions of this paper can be summarized in the following three aspects:</p><p>? We propose the TNL2K dataset for the natural language-based tracking which consists of 2, 000 video sequences. It aims at offering a dedicated platform for the development and assessment of natural language-based tracking algorithms.</p><p>? We propose a simple but strong baseline approach (termed AdaSwitcher) for future works to compare, which can switch between the local tracking algorithm and global grounding module adaptively.</p><p>? To provide extensive baselines for the comparison on TNL2K dataset, we also evaluate more than 40 representative BBox-based trackers and analyze their performance using different evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Tracking by Bounding Box The standard trackers begin their tracking procedure based on an initialized BBox in the first frame, including classification based <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>, Siamese network based <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b71">72]</ref>, correlation filter based <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48]</ref>, and regression-based <ref type="bibr" target="#b25">[26]</ref>. Inspired by the success of neural networks on image classification, most of the recent trackers are developed based on deep learning. Specifically, the Siamese network based trackers achieve state-of-the-art performance on multiple tracking benchmarks. Previous Siamese trackers simply measure the similarity between the static target template with extracted proposals and treat the best-scored proposal as their tracking results. Recently, some researchers begin to collect the tracking results which can be used to dynamically update the target template and attain better results <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b80">81]</ref>. In addition to learn powerful feature representation and conduct a local search for tracking, some trackers attempt to achieve robust tracking by global search <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b73">74]</ref>. For more related works on standard visual tracking, please check the following survey papers <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b78">79]</ref>.</p><p>Tracking by Natural Language Due to it is a new rising topic, only a few algorithms are developed and the authors of <ref type="bibr" target="#b42">[43]</ref> first validated the effectiveness of natural language for the tracking task by designing three modules (i.e. Lingual Specification Only; Lingual First, then Visual Specification; Lingual and Visual Specification). Wang <ref type="bibr" target="#b64">[65]</ref> and Feng <ref type="bibr" target="#b20">[21]</ref> also propose to use the language information to generate global proposals for tracking. Yang et al. propose the GTI <ref type="bibr" target="#b77">[78]</ref> which decomposes the tracking problem into three sub-tasks, i.e., grounding, tracking and integration, and these modules operate simultaneously and predict the box sequence frame-by-frame. These methods are evaluated on datasets specifically designed for tracking-by-BBox which may fail to reflect the feature of tracking-bylanguage. To the best of our knowledge, there is still no public benchmark specifically dedicated to the tracking-bylanguage task. We believe our benchmark will greatly boost the researches on natural language related object tracking.</p><p>Benchmarks for Tracking Existing benchmarks for visual tracking can be concluded into two main categories according to whether contains training data. As shown in <ref type="table">Table 1</ref>, previous benchmarks <ref type="bibr">[32-34, 41, 41, 44, 70, 71]</ref> provide test videos only before deep trackers occurred. It is worthy to note that OTB-2013 <ref type="bibr" target="#b69">[70]</ref> and OTB-2015 <ref type="bibr" target="#b70">[71]</ref> are the first public benchmarks for visual tracking which contain 50 and 100 video sequences, respectively. In the deep learning era, several large scale tracking benchmarks are proposed for the training of deep trackers. For example, GOT-10k <ref type="bibr" target="#b27">[28]</ref> contains 10, 000 videos which can be categorized into 563 classes. TrackingNet <ref type="bibr" target="#b50">[51]</ref> is a subset (31K sequences selected) of video object detection benchmark YT-BB <ref type="bibr" target="#b53">[54]</ref> and the ground truth is manually labeled at 1 FPS. OxUvA <ref type="bibr" target="#b57">[58]</ref> and LaSOT <ref type="bibr" target="#b18">[19]</ref> are two long-term tracking benchmark which consists of 366 and 1400 video sequences respectively.</p><p>The aforementioned tracking benchmarks are all mainly designed for tracking by BBox, although the LaSOT indeed provides the language specification of the target object. However, they only describe the appearance of the target object but ignore the relative location which may limit the integration of natural language. In another word, their benchmark is suitable for natural language assisted tracking but is not for the task of language initialized tracking. Another issue of the existing benchmark is that these videos do not contain videos with significant appearance variations, such as clothing change for a pedestrian. This also limits the application of existing trackers in practical scenarios. Besides, these benchmarks also ignore the adversarial samples which limit the development of adversarial learning-based trackers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b72">73]</ref>. By contrast, our proposed TNL2K is specifically designed for tracking by natural language specification and contains multiple videos with significant appearance variation and adversarial samples. It also contains natural videos, animation videos, in-frared videos, virtual game videos, which are suitable for the evaluation of domain adaptation of current trackers. We also provide baseline results of three kinds of settings which will be beneficial for future trackers to compare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tracking by Natural Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">TNL2K Dataset</head><p>Data Collection and Annotation The proposed TNL2K dataset contains 2, 000 video sequences, and most of them are downloaded and clipped from YouTube, intelligent surveillance cameras, and mobile phones. We invite seven people for the annotation of these videos. Specifically, we annotate one sentence in English for each video and also one bounding box for each frame in this video. The left corner point (x 1 , y 1 ), width w and height h of the target's bounding box are used as the ground truth, i.e., [x 1 , y 1 , w, h]. The annotated natural language description indicates the spatial position, relative location with other objects, attribute, category and property of target object in the first frame. We also annotate the absent label for each frame to enrich the information that is available for more accurate tracking. To construct a rich and heterogeneous benchmark, we also borrow some thermal videos from existing datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46]</ref> and re-annotate the target object we want to track if necessary. Example sequences and annotations are illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Attribute Definition Following popular tracking benchmarks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b70">71]</ref>, we also define multiple attributes of each video sequence for the evaluation under each challenging factors. As shown in <ref type="table" target="#tab_1">Table 2</ref>, our proposed TNL2K dataset has the following 17 attributes: CM (Camera Motion), ROT (Rotate Of Target), DEF (DEFormation), FOC (Fully OCcluded), IV (Illumination Variation), OV (Out of View), POC (Partially OCcluded), VC (Viewpoint Change), SV (Scale Variation), BC (Background Clutter), MB (Motion Blur), ARC (Aspect Ratio Change), LR (Low Resolution), FM (Fast Motion), AS (Adversarial Sample), TC (Thermal Crossover), MS (Modality Switch). It is worthy to note that our dataset contains some thermal videos with challenging factors like TC (target object shares similar intensity with background), MS (the video contains both thermal and RGB images). To provide a good platform for the study of adversarial attack and defense of neural network for tracking, we also generate 100 videos contain adversarial samples as part of the testing subset using attack toolkit <ref type="bibr" target="#b29">[30]</ref>. Therefore, these videos contain additional challenging factor, i.e., AS (influence of Adversarial Samples). It is worthy to note that the AS and MS are two new attributes for tracking community first proposed in this work. A more detailed distribution of each challenge is shown in <ref type="figure" target="#fig_1">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c).</head><p>Statistical Analysis Our proposed TNL2K contains 663 English words and focuses on expressing the attributes, spa- <ref type="table">Table 1</ref>. Comparison of current datasets for object tracking. # denotes the number of corresponding item. Lang-A and Lang-I denote the dataset can be used for language assisted and initialized tracking task. SAV denotes the dataset contains many videos with significant appearance variation. Adv means the dataset contains adversarial samples (i.e., malicious attacks). DA is short for domain adaptation.   tial location of target objects, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>  We can find that our test set contains 144 long-term videos (larger than 1000 frames for each video) which will be suit-able for the evaluation of long-term trackers. From <ref type="figure" target="#fig_1">Fig. 3</ref> (c), we can find that our TNL2K contains many videos with challenging attributes like background clutter, scale variation, view change, partially occlusion, out-of-view and rotate. The videos with these challenging factors will provide a good platform for the evaluation of current trackers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Our Proposed Approach</head><p>In this paper, we propose the adaptive tracking and grounding switch framework for tracking by natural language specification, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We will first introduce the visual grounding and visual tracking module, then, we will focus on our AdaSwitcher module.</p><p>Visual Grounding Module In the tracking by natural language task, we need to first locate the target object only depends on the language description S = [w 1 , w 2 , ..., w T ]. It is a standard visual grounding task and we follow the algorithm <ref type="bibr" target="#b76">[77]</ref> proposed by Yang et al. due to its good performance and efficiency.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the visual grounding module takes video frame and natural language description as input. We use the backbone CNN to obtain the deep feature representation of the i-th video frame F i . For the natural language, we first embed the words into feature representations E = [e 1 , e 2 , ..., e T ] using a pre-trained BERT <ref type="bibr" target="#b15">[16]</ref> which is a widely used word embedding model in natural language related tasks. Then, this feature is fed into two fully connected layers for further fine tuning. Following <ref type="bibr" target="#b76">[77]</ref>, we also duplicate this feature vector into feature maps and concatenate them with visual features of video frame. Another important information for visual grounding is the spatial coordinates encoding due to the spatial configurations are usually adopted to refer to target object. Therefore, the spatial feature for each position is also explicitly encoded in this work by following <ref type="bibr" target="#b76">[77]</ref>.</p><p>The visual feature maps of global frame, duplicated language feature, and the spatial coordinates are concatenated together and fed into convolutional layers with kernel size 1 ? 1 for information fusion. The output feature map is then sent into the grounding module, which will output the predicted location of target object. We treat such visual grounding as a global search procedure for tracking by natural language, which plays an important role at the beginning of the video and when we need to re-detection the target object in tracking procedure. The integration of visual grounding and tracker SiamRPN++ <ref type="bibr" target="#b34">[35]</ref> is termed Ours-I in <ref type="table" target="#tab_4">Table 3</ref>. Besides, we also explore the target-aware attention (termed TANet) proposed in <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref>, i.e., Ours-II in <ref type="table" target="#tab_4">Table  3</ref>. The TANet takes the feature maps of target object and video image as input, and output corresponding global attention using de-convolutional network which can be used for search target object from global view. We refer the readers to check <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref> for further understanding of this module.</p><p>Visual Tracking Module Aforementioned visual grounding can help detect the target object at the beginning, however, only grounding is not enough for high performance tracking, since it is easily influenced by background clutter. In this work, we initialize a visual tracker for target object location in a local search manner based on the predicted bounding box from visual grounding in the first frame. The SiamRPN++ <ref type="bibr" target="#b34">[35]</ref> is adopted in our experiments due to its good performance.</p><p>AdaSwitcher Module Given the visual grounding and visual tracking module, we can capture the target object from global and local view, respectively. One thorny issue that still exists is when we use visual grounding for global search (or visual tracking for local search). One intuitive <ref type="figure">Figure 5</ref>. Illustration of current trackers with high response score but low IoU values (take the SiamRPN++ <ref type="bibr" target="#b34">[35]</ref> as an example). approach is to conduct such switch based on the confidence of tracker, however, the confidence score is not always reliable especially in the challenging scenarios. For example, as shown in <ref type="figure">Fig. 5</ref>, the confidence score is very high (larger than 0.9) in some frames, but the model actually locates wrong object. Inspired by anomaly detection (also called outlier detection) whose target is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. In this work, we take the failure of visual tracking as a kind of anomaly detection and propose a novel AdaSwitcher module to detect such failure. Once the anomaly is detected (the prediction from AdaSwitcher is larger than a pre-defined threshold), we can switch the candidate search regions from visual tracking to visual grounding for more robust and accurate tracking.</p><p>In this paper, confidence score (1-D), BBox (4-D), result image ((30 * 30 * 3)-D), response map ((23 * 23)-D) and language embedding (512-D) are exploited in this work as the input of our AdaSwitcher. This information can be collected from visual tracker easily for each frame. And the historical information of past video frames can also contribute to current anomaly detection. Assume we use the history of past N frames, then, the dimension of these input are N ? 1, N ? 4, N ? (23 * 23), N ? (30 * 30 * 3), and N ? 512, respectively. We use multiple parallel fully connected layers to encode this information and embed them into fixed feature vectors, specifically, we have F = [F s , F b , F img , F map , F emb ], whose dimension are N ? 10, N ?10, N ?512, N ?512, and N ?512, respectively. Then, these features are concatenated and fed into a bi-directional GRUs <ref type="bibr" target="#b6">[7]</ref> to learn the temporal information.</p><p>Inspired by the fact that various frames may contribute differently, we introduce attention mechanism to encode the inputs differently. The attention weights ? i (i = 1, ..., N ) can be obtained by the multilayer perceptron (MLP): <ref type="bibr" target="#b0">(1)</ref> where [, ] denotes concatenate operation. The attention weights ? i (i = 1, ..., N ) are stacked into feature vectors ? i (i = 1, ..., N ) which have same dimension with feature representation F i (i = 1, ..., N ) of each frame i. Therefore, the attended feature representations can be obtained by:</p><formula xml:id="formula_0">{?1, ?2, ..., ?N } = M LP ([Fs, F b , Fimg, Fmap, F emb ])</formula><formula xml:id="formula_1">[F 1 ,F 2 , ...,F N ] = [?1 * F 1 ,?2 * F 2 , ...,?N * F N ]<label>(2)</label></formula><p>After that, two fully connected layers are used to determine whether we should switch the candidate search regions from current tracking result to grounding result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>Training Phase In our experiments, we directly use the pre-trained weights of baseline tracker for visual tracking. For the visual grounding module, we train it on the training subset of our TNL2K dataset which contains 1, 300 video sequences for 40 epochs. The initial learning rate is 1e-4, batchsize is 5. The YOLO loss function is used for this network by following <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b76">77]</ref>. For the AdaSwitcher, we first collect the training data by running the baseline tracker on the training subset of our TNL2K dataset. In this process, we treat the video clips whose average IoU (Intersection over Union) score larger than 0.7 as the positive data, and less than 0.5 as the negative data. For the data with average IoU score range from 0.5 to 0.7, we directly discard them due to it may bring confusion to our model. Similar operations can also be found in <ref type="bibr" target="#b30">[31]</ref>. The learning rate is 1e-5, batchsize is 1, the Adagrad [18] is adopted as optimizer and trained for totally 30 epochs. We consider the switch between visual tracking and grounding as a binary classification problem, therefore, the BCE loss function is selected for the training of AdaSwitcher.</p><p>Inference Phase In this benchmark, three kinds of baseline methods are studied: 1). Tracking by Natural Language only: In this setting, only the natural language is provided for tracking, we need to first locate the target object using visual grounding module. Then, we can conduct adaptive tracking (SiamRPN++ <ref type="bibr" target="#b34">[35]</ref> used in this setting) and grounding for high performance object localization. 2). Tracking by Natural Language and BBox: We take the natural language as an external modality and conduct robust tracking based on both language and BBox. SiamRPN++ <ref type="bibr" target="#b34">[35]</ref> and TANet <ref type="bibr" target="#b65">[66]</ref> are used in this setting. 3). Tracking by BBox only: To construct a comprehensive benchmark, we also provide baseline results for tracking by BBox only, i.e., the standard setting of visual object tracking. All the evaluated trackers can be found in our supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Protocols</head><p>In our experiments, the OTB-Lang <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b70">71]</ref>, LaSOT <ref type="bibr" target="#b18">[19]</ref> and our proposed TNL2K dataset are used for the evaluation. The OTB-lang contains 99 videos released from <ref type="bibr" target="#b70">[71]</ref>, then, the natural language specification is provided by Li et al. <ref type="bibr" target="#b42">[43]</ref>. The LaSOT is a recently released long-term tracking dataset that provides both bounding box and natural language annotations. The test subset of LaSOT contains 280 video sequences.</p><p>Two popular metrics are adopted for the evaluation of tracking performance, including Precision Plot and Success Plot.</p><p>Specifically, Precision Plot illustrates the percentage of frames where the center location error between the object location and ground truth is smaller than a pre-defined threshold (20-pixels threshold is usually adopted). Success Plot demonstrates the percentage of frames the IoU of the predicted and the ground truth bounding boxes is higher than a given ratio. The evaluation toolkit of this paper can be found at: https://github.com/wangxiao5791509/ TNL2K_evaluation_toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmark Results</head><p>Results of Tracking by Natural Language Only As shown in <ref type="table" target="#tab_4">Table 3</ref>  <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b20">[21]</ref> respectively. When we take the result of visual grounding in the first frame as the initialized bbox of visual tracker SiamRPN++, we achieve 0.24|0.19 on the OTB-Lang dataset. On the LaSOT and TNL2K dataset, we attain 0.49|0.51 and 0.06|0.11|0.11 respectively. We can find that our method is comparable with Li et al. on the OTB-Lang dataset. On the larger dataset LaSOT, we attain better results than Feng et al. <ref type="bibr" target="#b19">[20]</ref>. These experimental results demonstrate that our baseline method can also achieve good performance on existing LaSOT and our proposed TNL2K dataset.</p><p>Results of Tracking by Bounding Box Only This setting is most widely used in existing tracking algorithms, and we provide the results of 43 representative trackers from 2015 to 2021, as shown in <ref type="figure" target="#fig_3">Fig. 6</ref>. These trackers contain Classification-based, SiameseNet-based, Correlation filter-based, Reinforcement learning-based, Long-termbased and Other trackers. More detailed introductions on these trackers can be found in our supplementary materials due to the limited space in this paper. From <ref type="figure" target="#fig_3">Fig. 6</ref>, we can find that SiamRCNN <ref type="bibr" target="#b58">[59]</ref> achieves the best performance on our benchmark dataset, i.e., 0.528|0.523 on the precision/success plot respectively. Other trackers also attain good performance such as LTMU <ref type="bibr" target="#b8">[9]</ref>, KYS <ref type="bibr" target="#b21">[22]</ref>, TACT <ref type="bibr" target="#b5">[6]</ref>, due to the use of joint local and global search scheme. These experiments fully demonstrate the importance of joint local and global search for visual tracking. We also find that Siamese network based trackers usually achieve better results than other trackers like multi-domain based trackers <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>, regression based trackers <ref type="bibr" target="#b25">[26]</ref>, and correlation filter based trackers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref>. We also notice that GlobalTrack <ref type="bibr" target="#b28">[29]</ref> which employs global search scheme only, achieves comparable performance with local search trackers <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b82">83]</ref>, but worse than state-of-the-art. This may demonstrate that only global search is not enough for robust tracking. Overall, the aforementioned observations demonstrate that the structure information mining of global scene, and offline learning indeed contribute to the highperformance visual tracking.</p><p>Results of Tracking by Joint Language and BBox As shown in <ref type="table" target="#tab_4">Table 3</ref>, there are five trackers designed for this setting <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b77">78]</ref>. Specifically, Li et al. <ref type="bibr" target="#b42">[43]</ref> achieve 0.72|0.55, while Feng et al. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> attain 0.73|0.67, 0.79|0.61 on the OTB-Lang dataset respectively. GTI <ref type="bibr" target="#b77">[78]</ref> combine SiamRPN++ and visual grounding module, and achieves 0.73|0.58, 0.47|0.47 on OTB-Lang and LaSOT dataset. In contrast, we can achieve 0.88|0.68 on the OTB-Lang, 0.55|0.51 on the LaSOT, 0.42|0.50|0.42 on the TNL2K (Our-II in <ref type="table" target="#tab_4">Table 3</ref>), which are significantly better than GTI <ref type="bibr" target="#b77">[78]</ref>, Wang et al. <ref type="bibr" target="#b64">[65]</ref> and Feng et al. <ref type="bibr" target="#b19">[20]</ref>. All the experiments on three benchmark datasets validate the effectiveness and advantages of our tracker. Visualization of related tracking results can be found in <ref type="figure" target="#fig_0">Fig. 12.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we first analyse the effectiveness of main components in our model. Then, we focus on validating the contributions of each input for AdaSwitcher. Finally, we give the parameter analysis, and attribute analysis.</p><p>Effectiveness of AdaSwitcher As shown in <ref type="table" target="#tab_6">Table 5</ref>, the baseline tracker SiamRPN++ <ref type="bibr" target="#b34">[35]</ref> (AlexNet version) achieves 0.344/0.353 on the precision and success plot, respectively. When integrated with the AdaSwicher module, the performance can be improved to 0.355/0.370. This result is also better than naive fused method (i.e. 0.347/0.362), which fully demonstrates the effectiveness of our adaptive switch mechanism for robust tracking.</p><p>Effectiveness of Frame Attention Due to different frames may contribute differently to our AdaSwitcher, we introduce the frame attention mechanism to achieve this goal. As shown in <ref type="table">Table 4</ref>, with the help of frame attention, the tracking results can be improved from 0.353/0.369 to 0.355/0.370. This fully demonstrates the important role of frame attention in our proposed framework. Effectiveness of Spatial Coordinates In our visual grounding module, the spatial coordinates are introduced to further improve the final results. As shown in <ref type="table">Table 4</ref>, our grounding module achieves 0.143/0.159 and 0.103/0.124, respectively, with and without the help of spatial coordinates. This result validates the important role of spatial coordinates for visual grounding. <ref type="table">Table 4</ref>. Component analysis of our proposed tracking algorithm. AS is short for AdaSwitcher, FA denotes frame attention in AdaSwitcher, SC is spatial coordinates used in visual grounding. Naive denotes switch method based on response score only. Analysis on History Information Our AdaSwitcher takes multiple inputs for the final decision, in this section, we analyze their contributions by comparing corresponding results in <ref type="table" target="#tab_6">Table 5</ref>. Specifically speaking, when the BBox is discarded, we find that the performance is dropped from 0.355/0.370 to 0.350/0.365, this demonstrates that the geometric information of predicted BBox is an important clue for our tracking. Similarly, we attain worse tracking results when the resulting image (i.e. ResImg) is ignored, the results drop from 0.355/0.370 to 0.345/0.362. When all these modules removed, it attains 0.344/0.353 only on the precision and success plot. This demonstrates that this informa- tion are very important for the anomaly (or failure) detection in tracking procedure. Parameter Analysis We report the tracking results with different switch thresholds in <ref type="table" target="#tab_7">Table 6</ref>. We can find that the performance is better when switch threshold is set as 0.7.  Attribute Analysis Evaluation under each challenging factors is one of the most important metrics in visual tracking community. In this benchmark, we also report results of evaluated trackers under all the defined 17 attributes.</p><p>However, due to limited space in this paper, we select 4 attributes, i.e., Adversarial Samples, Scale Variation, Background Clutter, and Full Occlusion, to demonstrate the ability of resistance of these trackers to these challenges. As shown in <ref type="figure" target="#fig_4">Fig. 7</ref>, we can find that SiamRCNN <ref type="bibr" target="#b58">[59]</ref> achieves the best performance which are much better than the second and third ones, i.e., DiMP <ref type="bibr" target="#b3">[4]</ref> and LTMU <ref type="bibr" target="#b8">[9]</ref> respectively. Interestingly, it is easy to find that the RTAA <ref type="bibr" target="#b29">[30]</ref> which is designed for adversarial attack achieves worse results on the challenging factor Adversarial Samples, even compared with their baseline DaSiamRPN <ref type="bibr" target="#b83">[84]</ref>. This demonstrates that the detection of adversarial samples is important for high performance tracking. More experimental results on the attribute analysis can be found in our supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Works</head><p>In this paper, we revisit the tracking by natural language, and propose a large-scale benchmark for this task. Specially, a large-scale dataset that contains 2,000 video sequences is proposed, named TNL2K. This dataset is densely annotated with bounding box and natural language description of target object. To construct a sound benchmark, we propose an adaptive switch based tracking algorithm as the baseline approach, i.e., the AdaSwicher, and also test current trackers according to following settings: tracking by natural language only, tracking by bbox, and tracking by joint bbox and language. We believe our benchmark will be greatly boost related researches on the natural language guided tracking. In our future works, we will consider to further extend this benchmark by introducing more videos and baseline trackers. Besides, we will focus on improving the visual grounding module to achieve high performance language initialized tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The TNL2K Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Motivation and Protocols</head><p>Motivation: Directly extending existing datasets like GOT-10k <ref type="bibr" target="#b27">[28]</ref> is an intuitive and good idea for this task, but GOT-10k contains few videos with special properties as mentioned in <ref type="figure">Fig.  1</ref> in our paper. Also, its videos are all short-term which can't reflect performance gain of re-detection with language. As for LaSOT <ref type="bibr" target="#b18">[19]</ref>, many of its language annotations can not point out target object clearly, as shown in <ref type="figure" target="#fig_6">Fig. 9</ref>. Thus, LaSOT is not suitable for tracking-by-language only. Similar views can also be found in GTI <ref type="bibr" target="#b77">[78]</ref>. Therefore, we build the TNL2K (from video collection, dense bbox and language annotation, to diverse baseline construction) to better reflect the characteristics (see below) of tracking by natural language. The target of this work is not to construct the largest tracking dataset, but to build the first benchmark specifically designed for tracking-by-language task. Compared with GOT-10k and LaSOT, the data collection of TNL2K is a compromise between length and quantity.</p><p>Protocol: When collecting the videos, we attempt to search the target object is severely occluded in the first frame, with significant appearance variation (e.g., cloth changing for human), can only be located with reasoning, which correspond to <ref type="figure">Fig. 1</ref> in our paper. Also, we collect videos from other thermal tracking datasets and annotate language descriptions only to check the robustness to certain challenging factors like domain adaptation, modality switch, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Why add Attribute Modality Switch (MS) ?</head><p>In the proposed TNL2K dataset, we design a new attribute termed Modality Switch (MS) for object tracking. This is mainly motivated by the fact that the RGB cameras work well in the daytime but nearly ineffective at night, meanwhile, the thermal cameras work well in the night time. If we track a target for an extremely long-term (e.g., several days or weeks), collaboration between RGB and thermal cameras are needed. Therefore, the connections between the two modalities need to be set up. Similar views can be found in cross-modality person re-identification <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>. There are still no works on object tracking try to build such connections and they usually study these two cameras separately (i.e., RGB tracking <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b69">70]</ref>, Thermal Tracking <ref type="bibr" target="#b45">[46]</ref>) or in an integrated approach (i.e., RGB-T tracking <ref type="bibr" target="#b36">[37]</ref>). In this work, we propose the modality switch and attempt to encourage researches on such cross-modality object tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Highlights of TNL2K Dataset</head><p>Generally speaking, our proposed benchmark TNL2K have the following features as shown in <ref type="table">Table 1:</ref> ? TNL2K is the first benchmark specifically designed for tracking-by-natural language. Different from regular tracking benchmarks like OTB, GOT10k, and TrackingNet, we provide both language annotation and dense bounding box annotation for each video sequence which will be a good platform for natural language-related tracking. Different from the recently released long-term tracking dataset La-SOT which also provides language annotation, their annotation only describes the attribute of target object, but ignores the spatial position. Therefore, this benchmark can be only used for the task of tracking by joint language and bbox. Our language annotations not only embody the attribute, category, shape, properties, and structural relationship with other objects, therefore, our dataset can also be used for the task of tracking by natural language only. Some video sequences and corresponding annotations are provided in <ref type="figure" target="#fig_6">Figure 9</ref> to give an intuitive understanding of the difference between our TNL2K and LaSOT.</p><p>? TNL2K is the first benchmark to provides videos with actively introduced adversarial samples which will be beneficial for the development of adversarial training for tracking.</p><p>? TNL2K is the first benchmark to provides videos with significant appearance variation, such as cloth/face changing. We believe our benchmark will greatly boost related research on abrupt appearance variation based tracking.</p><p>? TNL2K provides a heterogeneous dataset that contains RGB video, Thermal video 3 , Cartoon, and Synthetic data (i.e., videos from games). It can be used for the study of domain adaptation, e.g., train the tracker on RGB data and test it on Thermal videos.</p><p>? TNL2K provides three kinds of baseline methods for future works to compare, including Tracking-by-BBox, Tracking-by-Language, Tracking-by-Joint-BBox-Language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. YOLO Loss and BCE Loss Functions</head><p>In the training phase, we use the YOLO loss function for the optimization of the visual grounding module by following <ref type="bibr" target="#b76">[77]</ref>. This loss is first proposed in YOLOv3 <ref type="bibr" target="#b54">[55]</ref> which attempt to predict the five quantities of each anchor box by shifting its center, width, height, and the confidence on this shifted box. To better use it for visual grounding, the following two changes are modified by Yang et al.: 1). recalibrate its anchor boxes; 2). change its sigmoid layer to a softmax function. Due to the object detection is designed for output multiple locations, while visual grounding only needs to predict one bbox which best fit the language description. Therefore, the sigmoid function in YOLOv3 is replaced by softmax function. The cross-entropy is used for the measurement of confidence scores, and the regions with maximum IoU with ground truth are labeled as 1, other regions are set as 0. More details can be found in <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b76">77]</ref>. For the training of TANet, we adopt Binary Cross-Entropy (BCE) loss to measure the distance between the ground truth mask and the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Details of Evaluated Trackers</head><p>In this section, we provide the details of evaluated BBox-based trackers on our TNL2K dataset. As shown in <ref type="table">Table 7</ref>, the publication, feature representation, update or not, need pre-train or not, search scheme, tracking efficiency, and results (Precision Plot and Success Plot) on the TNL2K are all reported. These tracking algorithms are ranked according to the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Introduction to TANet</head><p>Inspired by <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref>, we introduce the TANet for the global search to replace the Grounding module <ref type="bibr" target="#b76">[77]</ref> in the setting of tracking-by-joint language and BBox, termed Ours-II. Generally speaking, the TANet is inspired by semantic segmentation, which takes the target object and video frames as input and output an attention map using a decoder network. The estimated attention maps can highlight the possible search regions from a global view. Therefore, it can be seen as a kind of global search scheme and can be integrated with the baseline tracker and our proposed AdaSwitcher module for robust and accurate tracking. Our experimental results also demonstrate that we can attain good performance on three used datasets, i.e., the OTB-Lang <ref type="bibr" target="#b42">[43]</ref>, La-SOT <ref type="bibr" target="#b18">[19]</ref>, and TNL2K. This will be a strong baseline method for future works to compare on the language guided visual tracking. The implementation of our all networks will be released for other researchers to follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Attribute Analysis</head><p>As shown in <ref type="figure">Figure 10</ref>, we provide experimental results of all the defined 17 attributes of our TNL2K dataset. Generally speaking, we can find that the SiamRCNN <ref type="bibr" target="#b58">[59]</ref> achieves the best performance on most of the attributes, like Scale Variation, Rotation, Background Clutter, Partial Occlusion, Adversarial Samples, Deformation, Fast Motion, Out-of-view, Motion Blur, Aspect Ration Change, Illumination Variation, Camera Motion, and Viewpoint Change. Meanwhile, the SuperDiMP <ref type="bibr" target="#b3">[4]</ref>, LTMU <ref type="bibr" target="#b8">[9]</ref>, PrDiMP <ref type="bibr" target="#b11">[12]</ref> and KYS <ref type="bibr" target="#b21">[22]</ref> also attains good performance on these attributes, and the KYS also achieves top-1 results on the Low Resolution. These results all demonstrate the strong performance of Siamese network based trackers with the help of pre-training and joint local and global search scheme. Interestingly, we can also find that on the attribute Thermal Crossover which are all thermal videos, the MDNet <ref type="bibr" target="#b51">[52]</ref> which is an online learned tracker attain the best results. Even the Staple and SRDCF are better than most of the other Siamese trackers, such as SiamKPN, SiamCAR, SiamRPN++, SiamRCNN, KYS, etc. The huge contrast demonstrates that online learning is very important for the tracker which is trained on one domain and tested on another domain (for example, the tracker trained on RGB videos and tested on Thermal videos).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Efficiency Analysis</head><p>In this work, two baseline methods are proposed for the natural language initialized tracking (Our-I) and natural language guided tracking (Our-II). For Our-I, the overall running efficiency is 24.39 FPS on the OTB-Lang, tested on a laptop with Intel Core I7, RTX2070. For Our-II, the overall efficiency on the OTB-Lang is 12.44 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. More Visualization</head><p>In this section, more visualization on the tracking results is given to better understand our proposed method. As shown in <ref type="figure">Figure 11</ref>, 20 video sequences from OTB-Lang are selected to demonstrate the results of the visual grounding module. From the first three rows, we can find that the grounding module can locate the target object accurately when the background is relatively clean. <ref type="table">Table 7</ref>. Summary of evaluated trackers on TNL2K dataset. <ref type="figure">Figure 11</ref>. Results of the first frame of visual grounding module. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Example sequences and annotations in TNL2K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>(a) Some words in our language description; (b, c) Distribution of sequences in each attribute and length in our TNL2K. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>An overview of our proposed adaptive tracking and grounding switch framework. AdaSwitcher is highlighted in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Benchmark results of tracking-by-BBox on TNL2K dataset. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Tracking results under partial attributes of TNL2K dataset. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of tracking results on TNL2K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Comparison between our proposed TNL2K dataset and existing LaSOT dataset. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .</head><label>12</label><figDesc>Tracking results of our method and other state-of-the-art tracking algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Description of 17 attributes in our TNL2K dataset.</figDesc><table><row><cell>Attributes</cell><cell>Definition</cell></row><row><cell>01. CM</cell><cell>Abrupt motion of the camera</cell></row><row><cell>02. ROT</cell><cell>Target object rotates in the video</cell></row><row><cell>03. DEF</cell><cell>The target is deformable</cell></row><row><cell>04. FOC</cell><cell>Target is fully occluded</cell></row><row><cell>05. IV</cell><cell>Illumination variation</cell></row><row><cell>06. OV</cell><cell>The target completely leaves the video sequence</cell></row><row><cell>07. POC</cell><cell>Partially occluded</cell></row><row><cell>08. VC</cell><cell>Viewpoint change</cell></row><row><cell>09. SV</cell><cell>Scale variation</cell></row><row><cell>10. BC</cell><cell>Background clutter</cell></row><row><cell>11. MB</cell><cell>Motion blur</cell></row><row><cell>12.</cell><cell></cell></row></table><note>ARC The ratio of bounding box aspect ratio is outside the range [0.5, 2] 13. LR Low resolution 14. FM The motion of the target is larger than the size of its bounding box 15. AS Influence of adversarial samples 16. TC Two targets with similar intensity cross each other 17. MS Video contain both color and thermal images</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, Li et al. [43] attain 0.29|0.25 on the OTB-Lang dataset, while Feng et al. achieve 0.56|0.54 and 0.78|0.54 in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Tracking results on the OTB-Lang, LaSOT, and TNL2K dataset. [Prec.|Norm. Prec. |Succ. Plot] are reported respectively.</figDesc><table><row><cell>Algorithm</cell><cell>Initialize</cell><cell>OTB-Lang</cell><cell>LaSOT</cell><cell>TNL2K</cell></row><row><cell>SiamFC [3]</cell><cell>BBox</cell><cell>-</cell><cell>0.40|0.34</cell><cell>0.29|0.35|0.30</cell></row><row><cell>MDNet [52]</cell><cell>BBox</cell><cell>-</cell><cell>0.46|0.40</cell><cell>0.37|0.46|0.38</cell></row><row><cell>VITAL [57]</cell><cell>BBox</cell><cell>-</cell><cell>0.45|0.39</cell><cell>0.35|0.44|0.37</cell></row><row><cell>GradNet [38]</cell><cell>BBox</cell><cell>-</cell><cell>0.35|0.37</cell><cell>0.32|0.40|0.32</cell></row><row><cell>ATOM [11]</cell><cell>BBox</cell><cell>-</cell><cell>0.51|0.51</cell><cell>0.39|0.47|0.40</cell></row><row><cell>SiamDW [82]</cell><cell>BBox</cell><cell>-</cell><cell>?|0.38</cell><cell>0.33|0.41|0.32</cell></row><row><cell>SiamRPN++ [35]</cell><cell>BBox</cell><cell>-</cell><cell>0.50|0.45</cell><cell>0.41|0.48|0.41</cell></row><row><cell>GlobalTrack [29]</cell><cell>BBox</cell><cell>-</cell><cell>0.53|0.52</cell><cell>0.39|0.46|0.41</cell></row><row><cell>SiamBAN [5]</cell><cell>BBox</cell><cell>-</cell><cell>0.60|0.51</cell><cell>0.42|0.49|0.41</cell></row><row><cell>Ocean [83]</cell><cell>BBox</cell><cell>-</cell><cell>0.57|0.56</cell><cell>0.38|0.45|0.38</cell></row><row><cell>Li et al. [43]</cell><cell>NL</cell><cell>0.29|0.25</cell><cell>-</cell><cell>-</cell></row><row><cell>Li et al. [43]</cell><cell>NL+BBox</cell><cell>0.72|0.55</cell><cell>-</cell><cell>-</cell></row><row><cell>Feng et al. [21]</cell><cell>NL</cell><cell>0.56|0.54</cell><cell>-</cell><cell>-</cell></row><row><cell>Feng et al. [21]</cell><cell>NL+BBox</cell><cell>0.73|0.67</cell><cell>0.56|0.50</cell><cell>0.27|0.34|0.25</cell></row><row><cell>Feng et al. [20]</cell><cell>NL</cell><cell>0.78|0.54</cell><cell>0.28|0.28</cell><cell>-</cell></row><row><cell>Feng et al. [20]</cell><cell>NL+BBox</cell><cell>0.79|0.61</cell><cell>0.35|0.35</cell><cell>0.27|0.33|0.25</cell></row><row><cell>Wang et al. [65]</cell><cell>NL+BBox</cell><cell>0.89|0.65</cell><cell>0.30|0.27</cell><cell>-</cell></row><row><cell>GTI [78]</cell><cell>NL+BBox</cell><cell>0.73|0.58</cell><cell>0.47|0.47</cell><cell>-</cell></row><row><cell>Ours-I</cell><cell>NL</cell><cell>0.24|0.19</cell><cell>0.49|0.51</cell><cell>0.06|0.11|0.11</cell></row><row><cell>Ours-II</cell><cell>NL+BBox</cell><cell>0.88|0.68</cell><cell>0.55|0.51</cell><cell>0.42|0.50|0.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Component analysis of history information.</figDesc><table><row><cell>BBox</cell><cell>Score</cell><cell>ResMap</cell><cell>ResImg</cell><cell>Lang</cell><cell>Results</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.355|0.370</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.350|0.365</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.352|0.368</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.352|0.368</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.345|0.362</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.352|0.368</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.344|0.353</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Results with different switch threshold.</figDesc><table><row><cell>Parameter</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1.0</cell><cell>1.2</cell></row><row><cell>Prec. Plot</cell><cell>0.350</cell><cell>0.351</cell><cell>0.355</cell><cell>0.353</cell><cell>0.349</cell><cell>0.352</cell><cell>0.272</cell></row><row><cell>Succ. Plot</cell><cell>0.367</cell><cell>0.367</cell><cell>0.370</cell><cell>0.369</cell><cell>0.368</cell><cell>0.368</cell><cell>0.301</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https : / / github . com / wangxiao5791509 / Single _ Object_Tracking_Paper_List</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">There are 518 videos totally borrowed from existing RGB-T dataset<ref type="bibr" target="#b36">[37]</ref> and infrared tracking dataset<ref type="bibr" target="#b45">[46]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Figure 10. Tracking results under each challenging factors on TNL2K dataset (Tracking-by-BBox). Best viewed by zooming in.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Also, it works well in some challenge videos, like car, and human head. For the fourth row, the grounding is not accurate enough for tracking, including the central location and scale. We can find that the performance of visual grounding is needed to be further improved for more accurate tracking. More experimental results of our proposed baseline and other trackers can be found in <ref type="figure">Figure  12</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on computer vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="983" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1401" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6182" to="6191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Siamese box adaptive network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zedu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6668" to="6677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual tracking by tridentalign and context embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junseok</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07109</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Fully convolutional online tracking. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-performance longterm tracking with meta-updater</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6298" to="6307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4660" to="4669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7183" to="7192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4310" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4310" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clnet: A compact latent network for fast adjusting siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5374" to="5383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time visual object tracking with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Ablavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="700" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Robust visual object tracking with natural language region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Ablavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02048</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Know your surroundings: Exploiting scene information for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhat</forename><surname>Goutam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danelljan</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timofte</forename><surname>Van Gool Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Siamcar: Siamese fully convolutional classification and regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6269" to="6277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Branchout: Regularization for online ensemble tracking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2217" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<title level="m">Struck: Structured output tracking with kernels. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2096" to="2109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Joa??o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Globaltrack: A simple and strong baseline for long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust tracking against adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time mdnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilchae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mooyeol</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashton</forename><surname>Hamed Kiani Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A novel performance evaluation methodology for single-target trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ale?</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luka?ehovin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2137" to="2155" />
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nus-Pro</surname></persName>
		</author>
		<title level="m">A New Visual Tracking Challenge. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="335" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4282" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Rgb-t object tracking: benchmark and baseline. Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106977</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gradnet: Gradient-guided network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep visual tracking: Review and experimental comparison. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="323" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Siamese keypoint prediction network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04078</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual object tracking for unmanned aerial vehicles: A benchmark and new motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Anthony Dick, and Anton Van Den Hengel. A survey of appearance models in visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2013" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tracking by natural language specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6495" to="6503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Encoding color information for visual tracking: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5630" to="5644" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Efficient adversarial attacks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00217</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ptb-tir: A thermal infrared pedestrian tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="666" to="675" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">D3s-a discriminative single shot segmentation tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7133" to="7142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4847" to="4856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep learning for visual tracking: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Seyed Mojtaba Marvasti-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Ghanei-Yakhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kasaei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00535</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="300" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Meta-tracker: Fast and robust online adaptation for visual object trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5296" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="68" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Vital: Visual tracking via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Long-term tracking in the wild: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Siam r-cnn: Visual tracking by re-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6578" to="6588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Tracking by instance detection: A metalearning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6288" to="6297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1308" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dynamic attention guided multi-trajectory analysis for single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sint++: Robust visual tracking via adversarial positive instance generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4864" to="4873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Describe and attend to track: Learning natural language guided structural representation and visual attention for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10014</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning target-aware attention for robust tracking with conditional adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30TH British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">131</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Physical adversarial textures that fool visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqi</forename><surname>Wiyatno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4822" to="4831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Rgb-ir person re-identification by cross-modality similarity preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rgb-infrared cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5380" to="5389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1834</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12549" to="12556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Cooling-shrinking attack: Blinding the tracker with imperceptible noises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="990" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">skimming-perusal&apos;tracking: A framework for real-time and robust long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning dynamic memory networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Roam: Recurrently optimizing tracking model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6718" to="6727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A fast and accurate onestage approach to visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4683" to="4693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Grounding-tracking-integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alper</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Action-decision networks for visual tracking with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Choi Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1349" to="1358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Learning the model update for siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad Shahbaz</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4010" to="4019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4591" to="4600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12366</biblScope>
			<biblScope unit="page" from="771" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
