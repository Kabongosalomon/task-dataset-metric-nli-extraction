<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Dual-Cycled Cross-View Transformer Network for Unified Road Layout Estimation and 3D Object Detection in the Bird&apos;s-Eye-View</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curie</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ue-Hwan</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">A Dual-Cycled Cross-View Transformer Network for Unified Road Layout Estimation and 3D Object Detection in the Bird&apos;s-Eye-View</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The bird's-eye-view (BEV) representation allows robust learning of multiple tasks for autonomous driving including road layout estimation and 3D object detection. However, contemporary methods for unified road layout estimation and 3D object detection rarely handle the class imbalance of the training dataset and multi-class learning to reduce the total number of networks required. To overcome these limitations, we propose a unified model for road layout estimation and 3D object detection inspired by the transformer architecture and the CycleGAN learning framework. The proposed model deals with the performance degradation due to the class imbalance of the dataset utilizing the focal loss and the proposed dual cycle loss. Moreover, we set up extensive learning scenarios to study the effect of multi-class learning for road layout estimation in various situations. To verify the effectiveness of the proposed model and the learning scheme, we conduct a thorough ablation study and a comparative study. The experiment results attest the effectiveness of our model; we achieve state-of-the-art performance in both the road layout estimation and 3D object detection tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recently, a number of prominent research outcomes are leading the advancement of autonomous driving: 3D object detection <ref type="bibr">[1]</ref>, semantic segmentation <ref type="bibr" target="#b1">[2]</ref>, and visual odometry <ref type="bibr" target="#b3">[3]</ref>. Among them, the task of concurrent semantic road layout estimation and 3D object detection in the bird'seye-view (BEV) <ref type="bibr" target="#b4">[4]</ref> is attracting growing interest; the low dimensional representation of the surrounding environments enhances crucial tasks for autonomous driving such as navigation and hazard avoidance in addition to the road layout estimation and 3D object detection tasks.</p><p>Nonetheless, contemporary methods for concurrent road layout estimation and 3D object detection in BEV display two major limitations. First, contemporary methods hardly consider the imbalance in the class distribution of the data for road layout estimation and 3D object detection. They simply learn to estimate the road layout and the 3D objects in a scene with naive objective functions. Next, contemporary approaches require multiple networks for each class for road layout estimation, resulting in computational inefficiency. Rather than learning one model for multiple classes, these approaches learn separate models for each class (e.g., road, vehicle, pedestrian and background).</p><p>To overcome the limitations of contemporary methods, we design a unified model for road layout estimation and 3D object detection inspired by the transformer architecture <ref type="bibr" target="#b5">[5]</ref> and the CycleGAN learning framework <ref type="bibr" target="#b6">[6]</ref>: the dualcycled cross-view transformer (DCT) architecture. First of all, the proposed model learns to estimate the road layout and detect 3D objects in a scene taking the class imbalance of the training dataset into account. Specifically, we propose to learn the model based on the focal loss <ref type="bibr" target="#b7">[7]</ref> so that it focuses on a sparse set of hard examples while restraining the large number of easy negatives from overpowering the training procedure. Moreover, we propose a dual cycle loss to further compensate for the performance degradation caused by the class imbalance in the dataset.</p><p>Furthermore, we thoroughly investigate the effect of multiclass learning on the road layout estimation task; since multiclass learning for road layout estimation could reduce the total number of neural networks, it plays a crucial role in autonomous driving where computational resources are restricted. To accomplish this goal, we construct comprehensive multi-class learning experiment scenarios for road layout estimation including the proposed learning scheme (the dual cycle loss), examine the multi-class learning experiment results meticulously, and reveal essential insights for a future research direction.</p><p>In summary, the main contributions of our work are as follows: 1) DCT Architecture: We propose the dual-cycled crossview transformer (DCT) network for unified road layout estimation and 3D object detection for autonomous driving along with the learning scheme to handle the class imbalance. 2) Multi-Class Learning: We investigate the effect of multi-class learning in the context of road layout estimation for the first time to the best of our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Ablation Study:</head><p>We conduct a thorough ablation study and reveal important intuitions for the effect of each design choice. 4) SoTA Performance: We achieve state-of-the-art performance on both road layout estimation and 3D object detection in the Argoverse and KITTI 3D Object datasets, respectively. 5) Open Source: We contribute to the research society by making the source code of the proposed DCT network and the pretrained network parameters public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In this section, we review previous methods relevant to the semantic road layout estimation and monocular 3D object detection tasks in BEV.</p><p>Semantic road layout estimation in BEV. With the advancement of large-scale datasets for autonomous driving <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, the task of semantic road layout estimation has emerged. Early approaches have assumed that scenes are in general planar in the autonomous driving settings and transformed images into BEV using simple homographies <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>. However, the planar scene assumption results in artifacts for dynamic objects such as moving vehicles and pedestrians. Thus, contemporary approaches directly learn the image-to-BEV transformation <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b14">[14]</ref>. For example, VED utilizes a variational autoencoder (VAE) model for producing the semantic road layout of the given image <ref type="bibr" target="#b15">[15]</ref>; VPN performs cross-view semantic segmentation by learning common feature representation across multiple views <ref type="bibr" target="#b14">[14]</ref>; and lift-splat-shoot (LSS) transforms 2D image features into 3D space and generates BEV grids <ref type="bibr" target="#b16">[16]</ref>.</p><p>Monocular 3D object detection. Monocular 3D object detection aims to estimate the dimension and the orientation of objects in the real-world coordinate from a single monocular image. One category of approaches formulates monocular 3D object detection as 2D object detection and evaluates the depth of the given monocular scene <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b17">[17]</ref>. Moreover, approaches based on psuedo-lidars first predict the depth map of the given 2D image, project the predicted depth map into 3D points and conduct 3D object detection directly on these projected 3D points <ref type="bibr" target="#b19">[18]</ref>, <ref type="bibr" target="#b20">[19]</ref> using 3D object detection algorithms <ref type="bibr" target="#b21">[20]</ref>, <ref type="bibr" target="#b22">[21]</ref>.</p><p>On the other hand, formulating 3D object detection as 2D segmentation in the top-view is drawing increasing attention due to the conciseness and diverse applicability of the lowdimensional representation in the context of autonomous driving <ref type="bibr" target="#b23">[22]</ref>, <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b17">[17]</ref>. For instance, Mono3D predicts 3D bounding boxes on the ground plane and projects the predicted 3D bounding boxes into the given monocular image <ref type="bibr" target="#b26">[24]</ref>; and OFTNet transforms the given monocular image into the top-view representation and performs 2D semantic segmentation for 3D object detection <ref type="bibr" target="#b27">[25]</ref>. Recently Monolayout <ref type="bibr" target="#b28">[26]</ref> and CVT <ref type="bibr" target="#b29">[27]</ref> have attempted to present a unified model for both the semantic road layout estimation and 3D object detection tasks in BEV from a single monocular image.</p><p>Transformers for computer vision. Currently, the transformer architecture <ref type="bibr" target="#b5">[5]</ref> is prevailing in a lot of computer vision fields <ref type="bibr" target="#b30">[28]</ref>, <ref type="bibr" target="#b31">[29]</ref>. Based on the attention mechanism first proposed for machine translation <ref type="bibr" target="#b32">[30]</ref>, transformers explicitly model pair-wise interactions of elements in a sequence. This ability of transformers could aid modeling the correlation between the features of views in different domains <ref type="bibr" target="#b33">[31]</ref> or in different time-steps <ref type="bibr" target="#b34">[32]</ref>. For semantic road layout estimation and 3D object detection in BEV, CVT has designed a crossview transformer for establishing the correlation between the front-view and the top-view <ref type="bibr" target="#b29">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>We describe the proposed dual-cycled cross-view transformer (DCT) architecture inspirited by the transformer architecture <ref type="bibr" target="#b5">[5]</ref> and the CycleGAN learning framework <ref type="bibr" target="#b6">[6]</ref> in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Overview</head><p>Fig. 1 depicts the overall architecture of the proposed dual-cycled cross-view transformer (DCT) network. During training, the DCT network receives both the top-view layout and the front-view image as input. The top-view layout and the front-view image get transformed to the front-view representation and the top-view representation, respectively. Then, the transformed front-view embedding and the transformed top-view embedding get cyclically retransformed into the top-view representation and the front-view representation, respectively. The proposed dual-cycled view projection module utilizes these transformed representations for ensuring the complete dual cycle consistency-leading to performance enhancement. Further, the top-view representation extracted from the input front-view image goes through the cross-view transformer <ref type="bibr" target="#b29">[27]</ref> and then becomes the top-view prediction, i.e., the semantic road layout from the single front-view image. In addition, we utilize an auxiliary supervision for better gradient flow during training. During testing or inference, the DCT network only receives a front-view image for road layout estimation and 3D object detection with the crossview transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Remedy for Class Imbalance</head><p>When the distribution of class labels in the training set is not even but skewed, the vast number of easy negative samples tend to overpower the training procedure and the model learns not meaningful representation with the naive cross-entropy loss <ref type="bibr" target="#b7">[7]</ref>. In the road layout estimation task, the layouts ordinarily contain way more background pixels than other classes, i.e., vehicles or roads. Thus, an appropriate management of the class imbalance is essential and it leads to significant performance enhancement.</p><p>To attain the goal, we propose to learn road layout estimation and 3D object detection in BEV from a single front-view image utilizing the focal loss <ref type="bibr" target="#b7">[7]</ref> as follows:</p><formula xml:id="formula_0">L f ocal = N c=1 (1 ? p c ) ? log(p c ),<label>(1)</label></formula><p>where p c = log exp(xc) C i=1 exp(xi) ? [0, 1], N and ? represent the estimated probability of the class c, the total number of class categories and a hyper-parameter, respectively. In the single class learning setting, c ? {foreground, background} and we set ? as 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Auxiliary Supervision</head><p>Neural architectures that incorporate multiple components could bring about a large number of parameters-making the optimization process challenging <ref type="bibr" target="#b35">[33]</ref>. To cope with a possible gradient vanishing problem and better flow the supervision signal throughout the entire architecture, we employ an auxiliary supervision branch. The auxiliary supervision branch takes X from the dual cycled view projection module and predicts the road layout from X using a separate decoder (the transform decoder).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dual Cycle Loss</head><p>Inspired by the CycleGAN learning framework <ref type="bibr" target="#b6">[6]</ref>, we propose the dual cycled view projection module which evaluates the dual cycle loss. First, the proposed dual cycled view projection module consists of four major components: a front-view feature extractor (i.e., encoder), a top-view feature extractor, a top-to-front multi-layer perceptron (MLP) and a front-to-top MLP. The two feature extractors learn representations for top-view layouts and front-view images. Moreover, the two MLPs conduct domain transfer from topto-front and front-to-top.</p><p>Next, the proposed dual cycle loss boosts the performance of the view transformation by letting the DCT network learn both the forward view transformation (front-view to top-view to front-view) and the backward view transformation (topview to front-view to top-view); the dual cycle loss comprise of a forward loss and a backward loss.</p><p>Furthermore, for mathematical description of the dual cycle loss, let X,X, F and G denote the front image embedding (i.e., the output of the front-view feature extractor), the top-view layout embedding (i.e., the output of the top-view feature extractor), the top-to-front MLP and the front-to-top MLP, respectively. Then, X = G(X), X = F (X ) = F (G(X)), andX = G(F (X)) lie in the top-view, the front-view and the top-view embedding spaces, respectively. Consequently, the forward loss becomes</p><formula xml:id="formula_1">L f w = F (G(X)) ? X 1 = X ? X 1 ,<label>(2)</label></formula><p>and similarly the backward loss is</p><formula xml:id="formula_2">L bw = G(F (X)) ?X 1 = X ?X 1 .<label>(3)</label></formula><p>Finally, we define the dual cycle loss as follows:</p><formula xml:id="formula_3">L dual?cycle = L f w + L bw = X ? X 1 + X ?X 1 .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Multi-Class Learning</head><p>Learning cross-view transformation of each entity (vehicles and roads) using separate neural networks results in a substantial computational inefficiency as practiced in conventional methods <ref type="bibr" target="#b28">[26]</ref>, <ref type="bibr" target="#b29">[27]</ref>. Thus, multi-class learning would hugely improve the computational efficiency for autonomous driving systems where the computational capacity is in general limited. To investigate the effect of multiclass learning in the unified road layout estimation and 3D object detection learning environments, we use c ? {vehicle, road, background} in (1) and accordingly N in (1) becomes 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Objective Function</head><p>Our objective function incorporates two losses as follows:</p><formula xml:id="formula_4">L = ? 1 ? L f ocal + ? 2 ? L dual?cycle ,<label>(5)</label></formula><p>where ? i ? {1, 2} indicates the weight factor for each loss and we set ? 1 and ? 2 as 10 and 1?e ?3 , respectively. Note that the focal loss evaluates the discrepancy between the outputs of the DCT network (the outputs of the two decoders, i.e., the decoded X ) and the ground-truth road layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>To verify the performance of the proposed DCT network, we conduct a set of extensive experiments and compare the proposed model against multiple baselines in various scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Settings</head><p>Datasets. We employ two datasets for evaluation: the KITTI <ref type="bibr" target="#b8">[8]</ref> and Argoverse <ref type="bibr" target="#b36">[34]</ref> datasets. First, the KITTI dataset provides a number of splits. We evaluate the performance of each model on three KITTI splits: the KITTI 3D object detection split (KITTI 3D Object) <ref type="bibr" target="#b26">[24]</ref> with 3,712 training and 3,769 validation images for 3D vehicle detection, the KITTI Odometry split for road layout estimation with 15,806 training and 6,636 validation images, and the KITTI Raw <ref type="bibr" target="#b37">[35]</ref> with 10,156 training and 5,074 validation images for road layout estimation. Next, the Argoverse dataset consists of 6,723 training and 2,418 validation images for road layout estimation.</p><p>Metrics. For quantitative analysis, we adopt the standard object detection and segmentation metrics <ref type="bibr" target="#b38">[36]</ref>: the mean of intersection-over-union (mIoU) and the mean average precision (mAP) metrics.</p><p>Baselines. We compare the performance of the proposed method against a set of state-of-the-art methods: MonoOccupancy <ref type="bibr" target="#b15">[15]</ref>, MonoLayout <ref type="bibr" target="#b28">[26]</ref>, VPN <ref type="bibr" target="#b14">[14]</ref>, Mono3D <ref type="bibr" target="#b26">[24]</ref>, OFTNet <ref type="bibr" target="#b27">[25]</ref>, and CVT <ref type="bibr" target="#b29">[27]</ref>. One thing to note is that we use Mono3D <ref type="bibr" target="#b26">[24]</ref> and OFTNet <ref type="bibr" target="#b27">[25]</ref> for detecting vehicles in the top-view and adapted versions of VPN <ref type="bibr" target="#b14">[14]</ref> and MonoOccupancy <ref type="bibr" target="#b15">[15]</ref> for fair comparison <ref type="bibr" target="#b29">[27]</ref>. Further, we utilize the latest performance of baselines reported online which is slightly distinctive than the reported results in their original papers. We exclude baselines that employ additional information such as multiple input images <ref type="bibr" target="#b39">[37]</ref>, <ref type="bibr" target="#b4">[4]</ref> rather than a monocular image input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>We implement the proposed model using the Pytorch framework and train the model using a single NVIDIA A30 GPU. We normalize the input images to 1024?1024. The size of the ground truth label of KITTI Odometry, KITTI 3D Object, KITTI Raw and Argoverse are originally 128?128, 256?256, 256?256, and 512?512, respectively; they all get interpolated as 1024?1024. Next, we render the ground truths of single-class instances as three-channel images by repeating the image content in the channel axis; we prioritize vehicles over roads to make each pixel contain one class label when ground truth pixels include multiple class labels. In addition, we render the ground truths of multiclass instances as one-hot encoded three-channel image. For feature extraction, we utilize the ResNet-18 architecture <ref type="bibr" target="#b40">[38]</ref> without bottleneck layers; for domain transfer between the front-view and the top-view, we employ MLPs with two fully-connected layers interspersed with the ReLU activation. For training, we employ the Adam optimizer <ref type="bibr" target="#b41">[39]</ref>. Moreover, the batch size, learning rate and the number of training epochs are 6, 1e-4 and 120, respectively. In the single class learning setting, we decay the learning rate once by 0.1 at the 50-th epoch; in the multi-class learning setting, we decay the learning rate once by 0.1 at the 100-th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>To verify the effectiveness of each component of the proposed learning scheme, we conduct ablation studies in two learning settings: single-class learning and multi-class learning.</p><p>Single-Class Learning. <ref type="table" target="#tab_0">Table I</ref> describes the ablation study result in the single-class learning scenario. The baseline model learned with the naive cross-entropy loss. As we applied the focal loss to the baseline model, the performance enhanced since the proposed learning scheme with the focal loss effectively dealt with the class imbalance. Moreover, the dual cycle loss further improved the performance as the loss incorporated the complete dual cycle consistency. In short, sequential application of the focal loss and the proposed dual cycle loss to the baseline model boosts the performance sequentially-which demonstrates the effectiveness of both the proposed application of the focal loss and the designed dual cycle loss.</p><p>Multi-Class Learning. <ref type="table" target="#tab_0">Table II</ref> describes the ablation study result in the multi-class learning scenario. Similar to the single-class learning scenario, the application of the focal loss and the dual-cycle loss in progression to the baseline model enhanced the performance with substantial performance margins consecutively-corroborating the validity of each component of the proposed learning scheme in the multi-class learning setting. However, note that the multiclass learning deteriorates performance and the performance drop is more severe for vehicles. We surmise that vehicles occupying less areas than roads in a road layout require more deliberate learning scheme design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparative Study</head><p>We perform comparative studies to establish the performance contribution against the baseline models. Note that, in Tables III and IV, * means the latest performance published online, which are slightly different from the reported results in the original papers.</p><p>Single-Class Learning. <ref type="table" target="#tab_0">Tables III and IV exhibit</ref>    with substantial performance gains. The performance gain is more prominent in mAP than mIoU (7.85% and 6.26% mAP enhancement in the KITTI 3D and Argoverse vehicle datasets, respectively) since we proposed to handle the class imbalance utilizing the focal loss. Moreover, <ref type="figure" target="#fig_3">Fig. 2</ref> presents the qualitative comparison of our approach against the baseline methods. The comparison result demonstrates that our method overcomes existing class imbalance problem effectively. The predicted road layout from the DCT network contains less noise than the baselines (in fact, almost zero noise). Plus, the DCT network generates road layouts with more accurate and clearer boundaries.</p><p>Next, our DCT method shows superior performance in the 3D object detection in BEV task. The DCT network successfully distinguishes densely arranged cars in a row (marked as red boxes in the figure) whereas the baseline models likely failed to differentiate them.</p><p>Multi-Class Learning. <ref type="table" target="#tab_4">Table V</ref> quantifies the comparative study result in the multi-class learning scenario; the proposed DCT network accomplished state-of-the-art performance with considerable margins. The performance mar-gin is more conspicuous for predicting vehicles where the class imbalance problem is more severe. In addition, all methods considered in the experiment displayed degraded performance in the multi-class learning scenario compared to the single-class learning scenario. We assume that vehicles taking up much fewer pixels than roads in a road layout cause the performance degradation-demanding an advanced learning scheme for multi-class learning.</p><p>Besides, <ref type="figure" target="#fig_4">Fig. 3</ref> illustrates the qualitative comparison result. The qualitative comparison result establishes the superiority of the proposed DCT architecture likewise. Our DCT architecture transforms the road layout from the front-view to the top-view with more accurate and clearer boundaries compared to the baseline model; furthermore, the road layouts from the DCT network contains less noise. In addition, the DCT network differentiates adjacent vehicles, which the baseline model can hardly distinguish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we proposed the dual-cycled cross-view transformer (DCT) network for unified road layout estimation and 3D object detection. The DCT network concurrently performs both the road layout estimation and 3D object detection tasks in BEV, which are essential for autonomous driving. We proposed to manage the class imbalance of the training data through the focal loss so that excessive negative samples would not overwhelm the learning process. Besides, we designed dual cycle consistency loss to further offset the performance loss caused by the class imbalance. Next, we investigated the effect of multi-class learning on road layout estimation; otherwise we need one neural network per each class. We revealed that multi-class learning requires an enhanced model design since the current model's learning    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported in part by the Institute for Information &amp; communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No.2022-0-00907, Development of AI Bots Collaboration Platform and Self-organizing AI, No.2019-0-01842, Artificial Intelligence Graduate School Program (GIST)) and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2022R1C1C1009989). The authors are with the AI Graduate School, GIST (Gwang-ju Institute of Science and Technology), Gwang-ju, 61005, Repulic of Korea (e-mail: {curie3170, uehwan}@gist.ac.kr). *Corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The proposed dual-cycled cross-view transformer (DCT) architecture. The DCT network requires both the top-view layout and the front-view images for training; these two inputs get transformed to another feature representation for the dual cycle loss. When deployed, the DCT network receives just a front-view image to predict the road layout and detect objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the comparative study result in the single-class learning scenario. The proposed DCT architecture shows state-of-theart performance on both the KITTI and Argoverse datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>The qualitative comparison results of road layout estimation and vehicle occupancy segmentation in BEV on the KITTI Odometry dataset and the KITTI 3D Object dataset, respectively. In short, the proposed DCT network displays superior performance compared to the latest baseline models in both tasks; the DCT network can transform the road layout from the front-view to the top-view more accurately and even distinguish adjacent vehicles in a row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>The qualitative comparison results of simultaneous road layout estimation and vehicle occupancy segmentation in BEV on the Argoverse dataset. The proposed DCT network displays less noise and produces more accurate segmentation results compared to the baseline method. capacity does not satisfy the complexity of the task. To establish the effectiveness of the DCT architecture, we conducted a set of comprehensive experiments. The experiment results corroborate the effectiveness of the proposed model and we obtained state-of-the-art performance on both semantic raod layout estimation and 3D object detection benchmark datasets. The source code and the pretrained mode weights are open-source.REFERENCES[1] T. Jiang, N. Song, H. Liu, R. Yin, Y. Gong, and J. Yao, "Vic-net:Voxelization information compensation network for point cloud 3d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ABLATION</head><label>I</label><figDesc>STUDY RESULTS USING DIFFERENT LOSSES IN THE SINGLE-CLASS LEARNING SCENARIO ON THE ARGOVERSE DATASET.</figDesc><table><row><cell></cell><cell cols="2">Argoverse Road</cell><cell cols="2">Argoverse Vehicle</cell></row><row><cell>Loss items</cell><cell cols="4">mIoU (%) mAP (%) mIoU (%) mAP (%)</cell></row><row><cell>Baseline</cell><cell>76.56</cell><cell>87.30</cell><cell>47.86</cell><cell>62.69</cell></row><row><cell>Focal</cell><cell>76.74</cell><cell>88.47</cell><cell>47.41</cell><cell>67.48</cell></row><row><cell>Focal +Dual Cycle</cell><cell>76.71</cell><cell>88.86</cell><cell>47.94</cell><cell>68.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II ABLATION</head><label>II</label><figDesc>STUDY RESULTS USING DIFFERENT LOSSES IN THE MULTI-CLASS LEARNING SCENEARIO ON THE ARGOVERSE DATASET.</figDesc><table><row><cell></cell><cell cols="2">Argoverse Road</cell><cell cols="2">Argoverse Vehicle</cell></row><row><cell>Loss items</cell><cell cols="4">mIoU (%) mAP (%) mIoU (%) mAP (%)</cell></row><row><cell>Baseline</cell><cell>74.45</cell><cell>86.51</cell><cell>29.53</cell><cell>43.84</cell></row><row><cell>Focal</cell><cell>73.93</cell><cell>86.00</cell><cell>31.89</cell><cell>46.19</cell></row><row><cell>Focal +Dual Cycle</cell><cell>74.73</cell><cell>86.76</cell><cell>31.75</cell><cell>46.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARATIVE</head><label>III</label><figDesc>STUDY RESULTS OF TOP-VIEW SEMANTIC SEGMENTATION ON THE KITTI AND ARGOVERSE DATASETS.</figDesc><table><row><cell>Method</cell><cell cols="8">KITTI Raw mIoU (%) mAP (%) mIoU (%) mAP (%) mIoU (%) mAP (%) mIoU (%) mAP (%) KITTI Odometry Argoverse Road Argoverse Vehicle</cell></row><row><cell>MonoOccupancy [15]</cell><cell>58.41</cell><cell>66.01</cell><cell>65.74</cell><cell>67.84</cell><cell>72.84</cell><cell>78.11</cell><cell>24.16</cell><cell>36.83</cell></row><row><cell>VPN [14]</cell><cell>59.58</cell><cell>79.07</cell><cell>66.81</cell><cell>81.79</cell><cell>71.07</cell><cell>86.83</cell><cell>16.58</cell><cell>39.73</cell></row><row><cell>MonoLayout [26]</cell><cell>66.02</cell><cell>75.73</cell><cell>76.15</cell><cell>85.25</cell><cell>73.25</cell><cell>84.56</cell><cell>32.58</cell><cell>51.06</cell></row><row><cell>CVT  *  [27]</cell><cell>64.13</cell><cell>83.37</cell><cell>77.47</cell><cell>86.39</cell><cell>76.56</cell><cell>87.30</cell><cell>47.86</cell><cell>62.69</cell></row><row><cell>Ours</cell><cell>65.86</cell><cell>86.56</cell><cell>77.15</cell><cell>88.28</cell><cell>76.71</cell><cell>88.86</cell><cell>47.94</cell><cell>68.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARATIVE</head><label>IV</label><figDesc>STUDY RESULTS ON THE KITTI 3D OBJECT DATASET.</figDesc><table><row><cell>Method</cell><cell cols="2">KITTI 3D Object mIoU (%) mAP (%)</cell></row><row><cell>MonoOccupancy [15]</cell><cell>20.45</cell><cell>22.59</cell></row><row><cell>Mono3D [24]</cell><cell>17.11</cell><cell>26.62</cell></row><row><cell>OFTNet [25]</cell><cell>25.24</cell><cell>34.69</cell></row><row><cell>VPN [14]</cell><cell>16.80</cell><cell>35.54</cell></row><row><cell>MonoLayout [26]</cell><cell>30.18</cell><cell>45.91</cell></row><row><cell>CVT  *  [27]</cell><cell>38.85</cell><cell>51.04</cell></row><row><cell>Ours</cell><cell>39.44</cell><cell>58.89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARATIVE</head><label>V</label><figDesc>STUDY RESULTS OF MULTI-CLASS LEARNING ON THE ARGOVERSE DATASET.</figDesc><table><row><cell></cell><cell cols="2">Argoverse Road</cell><cell cols="2">Argoverse Vehicle</cell></row><row><cell>Method</cell><cell cols="4">mIoU (%) mAP (%) mIoU (%) mAP (%)</cell></row><row><cell>MonoLayout [26]</cell><cell>67.29</cell><cell>79.57</cell><cell>16.39</cell><cell>26.42</cell></row><row><cell>CVT [27]</cell><cell>74.40</cell><cell>87.07</cell><cell>30.02</cell><cell>44.07</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Particularly, we employ the focal loss in two stages of the proposed learning scheme for the top-view prediction (road layout estimation and 3D object detection): 1) the auxiliary supervision and 2) the top-view prediction using the crossview transformer.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">object detection</title>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cabinet: efficient context aggregation network for low-latency semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="13" to="517" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simvodis++: Neural semantic visual odometry in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4244" to="4251" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating images into maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mendez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="9200" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One thousand and one hours: Self-driving motion prediction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zuidhof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Omari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning. PMLR, 2021</title>
		<imprint>
			<biblScope unit="page" from="409" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic dense visual semantic mapping from street-level imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladick?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="857" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d-lanenet: end-to-end 3d multiple lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pe&amp;apos;er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning based vehicle position and orientation estimation via inverse perspective mapping image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="317" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Crossview semantic segmentation for sensing surroundings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="4867" to="4873" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Monocular semantic occupancy grid mapping with convolutional variational encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J G</forename><surname>Van De Molengraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="445" to="452" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="194" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monocular plan view networks for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2876" to="2883" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking pseudo-lidar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="311" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d object detection with pointformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7463" to="7472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning 2d to 3d lifting for object detection in 3d for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="4504" to="4511" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monolayout: Amodal scene layout from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Jatavallabhula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1689" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Projecting your view attentively: Monocular road scene layout estimation via cross-view transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic biopsy tool presence and episode recognition in robotic bronchoscopy using a multi-task vision transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rafii-Tari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022" />
			<biblScope unit="page" from="7349" to="7355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Patchformer: An efficient point transformer with patch attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="11" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving 360 monocular depth estimation via non-local dense prediction transformer and joint supervised and self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3224" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformer tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8126" to="8135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8748" to="8757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to look around objects for top-view representations of outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="787" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Jperceiver: Joint perception network for depth, pose and layout estimation in driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
